[docstring] [EOL] from setuptools import setup , find_packages [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] VERSION = [string] [EOL] [EOL] [EOL] setup ( name = [string] , version = VERSION , description = [string] , classifiers = [ [string] , [string] , [string] , [string] , [string] , ] , keywords = [string] , url = [string] , author = [string] , author_email = [string] , license = [string] , packages = find_packages ( ) , install_requires = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , setup_requires = [ [string] ] , tests_require = [ [string] ] , include_package_data = True , python_requires = [string] , zip_safe = False ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] [comment] [EOL] import sys [EOL] if sys . version_info < ( [number] , [number] ) : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] import spacy , torch , numpy [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import logging [EOL] import os [EOL] import sys [EOL] [EOL] sys . path . insert ( [number] , os . path . dirname ( os . path . abspath ( os . path . join ( __file__ , os . pardir ) ) ) ) [EOL] logging . basicConfig ( format = [string] , level = logging . INFO ) [EOL] [EOL] from allennlp . commands import main [comment] [EOL] [EOL] if __name__ == [string] : [EOL] main ( prog = [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] from allennlp . common import Registrable [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . models import Model [EOL] from allennlp . models . archival import Archive , load_archive [EOL] [EOL] [EOL] class Predictor ( Registrable ) : [EOL] [docstring] [EOL] def __init__ ( self , model , dataset_reader ) : [EOL] self . _model = model [EOL] self . _dataset_reader = dataset_reader [EOL] [EOL] def predict_json ( self , inputs , cuda_device = - [number] ) : [EOL] instance = self . _json_to_instance ( inputs ) [EOL] outputs = self . _model . forward_on_instance ( instance , cuda_device ) [EOL] return sanitize ( outputs ) [EOL] [EOL] def _json_to_instance ( self , json ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def predict_batch_json ( self , inputs , cuda_device = - [number] ) : [EOL] instances = self . _batch_json_to_instances ( inputs ) [EOL] outputs = self . _model . forward_on_instances ( instances , cuda_device ) [EOL] return sanitize ( outputs ) [EOL] [EOL] def _batch_json_to_instances ( self , json ) : [EOL] [docstring] [EOL] instances = [ ] [EOL] for blob in json : [EOL] instances . append ( self . _json_to_instance ( blob ) ) [EOL] return instances [EOL] [EOL] @ classmethod def from_archive ( cls , archive , predictor_name ) : [EOL] [docstring] [EOL] config = archive . config [EOL] [EOL] dataset_reader_params = config [ [string] ] [EOL] dataset_reader = DatasetReader . from_params ( dataset_reader_params ) [EOL] [EOL] model = archive . model [EOL] model . eval ( ) [EOL] [EOL] return Predictor . by_name ( predictor_name ) ( model , dataset_reader ) [EOL] [EOL] [EOL] class DemoModel : [EOL] [docstring] [EOL] def __init__ ( self , archive_file , predictor_name ) : [EOL] self . archive_file = archive_file [EOL] self . predictor_name = predictor_name [EOL] [EOL] def predictor ( self ) : [EOL] archive = load_archive ( self . archive_file ) [EOL] return Predictor . from_archive ( archive , self . predictor_name ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $allennlp.data.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.DatasetReader$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $'Predictor'$ 0 0 0 $allennlp.models.archival.Archive$ 0 $builtins.str$ 0 0 0 0 0 0 0 $allennlp.models.archival.Archive$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.models.archival.Archive$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . models import Model [EOL] from allennlp . service . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class SemanticRoleLabelerPredictor ( Predictor ) : [EOL] [docstring] [EOL] def __init__ ( self , model , dataset_reader ) : [EOL] super ( ) . __init__ ( model , dataset_reader ) [EOL] self . _tokenizer = SpacyWordSplitter ( language = [string] , pos_tags = True ) [EOL] [EOL] @ staticmethod def make_srl_string ( words , tags ) : [EOL] frame = [ ] [EOL] chunk = [ ] [EOL] [EOL] for ( token , tag ) in zip ( words , tags ) : [EOL] if tag . startswith ( [string] ) : [EOL] chunk . append ( token ) [EOL] else : [EOL] if chunk : [EOL] frame . append ( [string] + [string] . join ( chunk ) + [string] ) [EOL] chunk = [ ] [EOL] [EOL] if tag . startswith ( [string] ) : [EOL] chunk . append ( tag [ [number] : ] + [string] + token ) [EOL] elif tag == [string] : [EOL] frame . append ( token ) [EOL] [EOL] if chunk : [EOL] frame . append ( [string] + [string] . join ( chunk ) + [string] ) [EOL] [EOL] return [string] . join ( frame ) [EOL] [EOL] def _json_to_instance ( self , json ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] @ overrides def _batch_json_to_instances ( self , json ) : [EOL] raise NotImplementedError ( [string] ) [EOL] [EOL] @ overrides def predict_json ( self , inputs , cuda_device = - [number] ) : [EOL] [docstring] [EOL] sentence = inputs [ [string] ] [EOL] [EOL] tokens = self . _tokenizer . split_words ( sentence ) [EOL] words = [ token . text for token in tokens ] [EOL] results = { [string] : words , [string] : [ ] } [EOL] for i , word in enumerate ( tokens ) : [EOL] if word . pos_ == [string] : [EOL] verb = word . text [EOL] verb_labels = [ [number] for _ in words ] [EOL] verb_labels [ i ] = [number] [EOL] instance = self . _dataset_reader . text_to_instance ( tokens , verb_labels ) [EOL] output = self . _model . forward_on_instance ( instance , cuda_device ) [EOL] tags = output [ [string] ] [EOL] [EOL] description = SemanticRoleLabelerPredictor . make_srl_string ( words , tags ) [EOL] [EOL] results [ [string] ] . append ( { [string] : verb , [string] : description , [string] : tags , } ) [EOL] [EOL] results [ [string] ] = words [EOL] [EOL] return sanitize ( results ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . models import Model [EOL] from allennlp . service . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class SentenceTaggerPredictor ( Predictor ) : [EOL] [docstring] [EOL] def __init__ ( self , model , dataset_reader ) : [EOL] super ( ) . __init__ ( model , dataset_reader ) [EOL] self . _tokenizer = SpacyWordSplitter ( language = [string] , pos_tags = True ) [EOL] [EOL] @ overrides def _json_to_instance ( self , json ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] @ overrides def predict_json ( self , inputs , cuda_device = - [number] ) : [EOL] [docstring] [EOL] sentence = inputs [ [string] ] [EOL] tokens = self . _tokenizer . split_words ( sentence ) [EOL] instance = self . _dataset_reader . text_to_instance ( tokens ) [EOL] [EOL] output = self . _model . forward_on_instance ( instance , cuda_device ) [EOL] [EOL] output [ [string] ] = [ token . text for token in tokens ] [EOL] [EOL] return sanitize ( output ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $allennlp.data.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0
[docstring] [EOL] from . predictor import Predictor , DemoModel [EOL] from . bidaf import BidafPredictor [EOL] from . decomposable_attention import DecomposableAttentionPredictor [EOL] from . semantic_role_labeler import SemanticRoleLabelerPredictor [EOL] from . coref import CorefPredictor [EOL] from . sentence_tagger import SentenceTaggerPredictor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] import math [EOL] import random [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import group_by_count [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . dataset import Dataset [EOL] from allennlp . data . instance import Instance [EOL] [EOL] [EOL] @ DataIterator . register ( [string] ) class BasicIterator ( DataIterator ) : [EOL] [docstring] [EOL] def __init__ ( self , batch_size = [number] ) : [EOL] self . _batch_size = batch_size [EOL] [EOL] @ overrides def get_num_batches ( self , dataset ) : [EOL] return math . ceil ( len ( dataset . instances ) / self . _batch_size ) [EOL] [EOL] @ overrides def _create_batches ( self , dataset , shuffle ) : [EOL] instances = dataset . instances [EOL] if shuffle : [EOL] random . shuffle ( instances ) [EOL] grouped_instances = group_by_count ( instances , self . _batch_size , None ) [EOL] [comment] [EOL] [comment] [EOL] grouped_instances [ - [number] ] = [ instance for instance in grouped_instances [ - [number] ] if instance is not None ] [EOL] return grouped_instances [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] batch_size = params . pop ( [string] , [number] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( batch_size = batch_size ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataset.Dataset$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset.Dataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[allennlp.data.instance.Instance]]$ 0 0 0 $allennlp.data.dataset.Dataset$ 0 $builtins.bool$ 0 0 0 0 0 $allennlp.data.dataset.Dataset$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 $'BasicIterator'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] from allennlp . data . tokenizers . token import Token [EOL] [EOL] [EOL] class WordFilter ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def filter_words ( self , words ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] choice = params . pop_choice ( [string] , cls . list_available ( ) , default_to_first_choice = True ) [EOL] params . assert_empty ( [string] ) [EOL] return cls . by_name ( choice ) ( ) [EOL] [EOL] [EOL] @ WordFilter . register ( [string] ) class PassThroughWordFilter ( WordFilter ) : [EOL] [docstring] [EOL] @ overrides def filter_words ( self , words ) : [EOL] return words [EOL] [EOL] [EOL] @ WordFilter . register ( [string] ) class StopwordFilter ( WordFilter ) : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] [comment] [EOL] [comment] [EOL] self . stopwords = set ( [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] @ overrides def filter_words ( self , words ) : [EOL] return [ word for word in words if word . text . lower ( ) not in self . stopwords ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 $'WordFilter'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] [EOL] [EOL] @ Tokenizer . register ( [string] ) class CharacterTokenizer ( Tokenizer ) : [EOL] [docstring] [EOL] def __init__ ( self , byte_encoding = None , lowercase_characters = False , start_tokens = None , end_tokens = None ) : [EOL] self . _byte_encoding = byte_encoding [EOL] self . _lowercase_characters = lowercase_characters [EOL] self . _start_tokens = start_tokens or [ ] [EOL] [comment] [EOL] [comment] [EOL] self . _start_tokens . reverse ( ) [EOL] self . _end_tokens = end_tokens or [ ] [EOL] [EOL] @ overrides def tokenize ( self , text ) : [EOL] if self . _lowercase_characters : [EOL] text = text . lower ( ) [EOL] if self . _byte_encoding is not None : [EOL] [comment] [EOL] [comment] [EOL] tokens = [ Token ( text_id = c + [number] ) for c in text . encode ( self . _byte_encoding ) ] [EOL] else : [EOL] tokens = [ Token ( t ) for t in list ( text ) ] [EOL] for start_token in self . _start_tokens : [EOL] if isinstance ( start_token , int ) : [EOL] token = Token ( text_id = start_token , idx = [number] ) [EOL] else : [EOL] token = Token ( text = start_token , idx = [number] ) [EOL] tokens . insert ( [number] , token ) [EOL] for end_token in self . _end_tokens : [EOL] if isinstance ( end_token , int ) : [EOL] token = Token ( text_id = end_token , idx = [number] ) [EOL] else : [EOL] token = Token ( text = end_token , idx = [number] ) [EOL] tokens . append ( token ) [EOL] return tokens [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] byte_encoding = params . pop ( [string] , None ) [EOL] lowercase_characters = params . pop ( [string] , False ) [EOL] start_tokens = params . pop ( [string] , None ) [EOL] end_tokens = params . pop ( [string] , None ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( byte_encoding = byte_encoding , lowercase_characters = lowercase_characters , start_tokens = start_tokens , end_tokens = end_tokens ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $'CharacterTokenizer'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
[docstring] [EOL] [EOL] from allennlp . data . tokenizers . tokenizer import Token , Tokenizer [EOL] from allennlp . data . tokenizers . word_tokenizer import WordTokenizer [EOL] from allennlp . data . tokenizers . character_tokenizer import CharacterTokenizer [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . framenet . full_text_reader import FrameNetFullTextReader [EOL] from allennlp . data . dataset_readers . framenet . ontology_reader import FrameOntology [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . reading_comprehension . squad import SquadReader [EOL] from allennlp . data . dataset_readers . reading_comprehension . triviaqa import TriviaQaReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . data . dataset_readers . ontonotes . crf_srl_reader import CrfSrlReader [EOL] from allennlp . data . dataset_readers . ontonotes . syntactic_constituent_reader import SyntacticConstitReader [EOL] from allennlp . data . dataset_readers . ontonotes . span_annotation_reader import SpanAnnotationReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Tuple , List , Dict , DefaultDict [EOL] import allennlp [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import codecs [EOL] from collections import defaultdict [EOL] import os [EOL] import logging [EOL] from typing import Dict , List , Optional , Tuple [EOL] [EOL] from overrides import overrides [EOL] import tqdm [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset import Dataset [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , TextField , ListField , IndexField , SequenceLabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class SyntacticConstitReader ( DatasetReader ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , max_span_width , token_indexers = None , label_namespace = [string] , parent_label_namespace = [string] ) : [EOL] self . max_span_width = max_span_width [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . label_namespace = label_namespace [EOL] self . parent_label_namespace = parent_label_namespace [EOL] self . _tag_widths = { } [EOL] [EOL] def _process_sentence ( self , sentence_tokens , predicate_index , constits , parents ) : [EOL] [docstring] [EOL] def construct_matrix ( labels ) : [EOL] default = [string] [EOL] [EOL] def get_new_label ( original , newer ) : [EOL] return newer if original == default else [string] . format ( newer , original ) [EOL] [EOL] constit_matrix = [ [ default for _ in range ( self . max_span_width ) ] for _ in sentence_tokens ] [EOL] for span in labels : [EOL] start , end = span [EOL] diff = end - start [EOL] [EOL] [comment] [EOL] if diff >= self . max_span_width : [EOL] continue [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] constit_matrix [ end ] [ diff ] = get_new_label ( constit_matrix [ end ] [ diff ] , labels [ span ] ) [EOL] return constit_matrix [EOL] [EOL] predicates = [ [number] for _ in sentence_tokens ] [EOL] predicates [ predicate_index ] = [number] [EOL] return self . text_to_instance ( sentence_tokens , predicates , predicate_index , construct_matrix ( constits ) , construct_matrix ( parents ) ) [EOL] [EOL] @ overrides def read ( self , file_path ) : [EOL] [comment] [EOL] file_path = cached_path ( file_path ) [EOL] [EOL] instances = [ ] [EOL] [EOL] sentence = [ ] [EOL] open_constits = [ ] [EOL] constits = { } [EOL] parent_constits = { } [EOL] predicate_index = None [comment] [EOL] [EOL] logger . info ( [string] , file_path ) [EOL] for root , _ , files in tqdm . tqdm ( list ( os . walk ( file_path ) ) ) : [EOL] for data_file in files : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not data_file . endswith ( [string] ) : [EOL] continue [EOL] with codecs . open ( os . path . join ( root , data_file ) , [string] , encoding = [string] ) as open_file : [EOL] for line in open_file : [EOL] line = line . strip ( ) [EOL] if line == [string] or line . startswith ( [string] ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not sentence : [EOL] continue [EOL] if not predicate_index : [EOL] predicate_index = int ( len ( sentence ) / [number] ) [EOL] instances . append ( self . _process_sentence ( sentence , predicate_index , constits , parent_constits ) ) [EOL] [comment] [EOL] sentence = [ ] [EOL] open_constits = [ ] [EOL] constits = { } [EOL] parent_constits = { } [EOL] predicate_index = None [EOL] continue [EOL] [EOL] conll_components = line . split ( ) [EOL] word = conll_components [ [number] ] [EOL] [EOL] sentence . append ( word ) [EOL] word_index = len ( sentence ) - [number] [EOL] [EOL] [comment] [EOL] if [string] in line : [EOL] predicate_index = word_index [EOL] [EOL] syn_label = conll_components [ [number] ] [EOL] [EOL] if syn_label == [string] : [EOL] continue [EOL] if [string] in syn_label : [EOL] starts = syn_label . split ( [string] ) [EOL] for con in starts [ [number] : ] : [EOL] label = con . strip ( [string] ) . strip ( [string] ) [EOL] open_constits . append ( ( label , word_index ) ) [EOL] if [string] in syn_label : [EOL] ends = syn_label . count ( [string] ) [EOL] for _ in range ( ends ) : [EOL] assert open_constits [EOL] label , start = open_constits . pop ( ) [EOL] parent_label = [string] [EOL] if open_constits : [EOL] parent_label , _ = open_constits [ - [number] ] [EOL] if ( start , word_index ) in constits : [EOL] label = [string] . format ( constits [ ( start , word_index ) ] , label ) [EOL] constits [ ( start , word_index ) ] = label [EOL] parent_constits [ ( start , word_index ) ] = [string] . format ( parent_label , label ) [EOL] if label not in self . _tag_widths : [EOL] self . _tag_widths [ label ] = [ ] [EOL] self . _tag_widths [ label ] . append ( word_index - start + [number] ) [EOL] [EOL] if not instances : [EOL] raise ConfigurationError ( [string] [string] . format ( file_path ) ) [EOL] logger . info ( [string] , len ( instances ) ) [EOL] [comment] [EOL] return Dataset ( instances ) [EOL] [EOL] def analyze_span_width ( self ) : [EOL] total_tag_width = [number] [EOL] total_spans = [number] [EOL] widths = defaultdict ( int ) [EOL] [EOL] for tag in self . _tag_widths : [EOL] if tag in [ [string] , [string] , [string] ] : [EOL] continue [EOL] total_tag_width += sum ( self . _tag_widths [ tag ] ) [EOL] total_spans += len ( self . _tag_widths [ tag ] ) [EOL] [EOL] for l in self . _tag_widths [ tag ] : [EOL] widths [ l ] += [number] [EOL] [EOL] x = [ ] [EOL] for l in sorted ( widths ) : [EOL] if len ( x ) == [number] : [EOL] x . append ( ( l , widths [ l ] * [number] / total_spans ) ) [EOL] else : [EOL] x . append ( ( l , x [ - [number] ] [ [number] ] + widths [ l ] * [number] / total_spans ) ) [EOL] logger . info ( [string] , x [ - [number] ] [ [number] ] , x [ - [number] ] [ [number] ] ) [EOL] [comment] [EOL] [EOL] logger . info ( [string] , ( total_tag_width / total_spans ) ) [EOL] import ipdb [EOL] ipdb . set_trace ( ) [EOL] [EOL] def text_to_instance ( self , sentence_tokens , predicates , predicate_index , constits = None , parents = None ) : [EOL] [docstring] [EOL] [comment] [EOL] text_field = TextField ( [ Token ( t ) for t in sentence_tokens ] , token_indexers = self . _token_indexers ) [EOL] verb_field = SequenceLabelField ( predicates , text_field ) [EOL] predicate_field = IndexField ( predicate_index , text_field ) [EOL] [EOL] [comment] [EOL] span_starts = [ ] [EOL] span_ends = [ ] [EOL] span_mask = [ [number] for _ in range ( len ( sentence_tokens ) * self . max_span_width ) ] [EOL] span_labels = [ ] if constits is not None else None [EOL] parent_labels = [ ] if parents is not None else None [EOL] [EOL] for j in range ( len ( sentence_tokens ) ) : [EOL] for diff in range ( self . max_span_width ) : [EOL] width = diff [EOL] if j - diff < [number] : [EOL] [comment] [EOL] span_mask [ j * self . max_span_width + diff ] = [number] [EOL] width = j [EOL] [EOL] span_starts . append ( IndexField ( j - width , text_field ) ) [EOL] span_ends . append ( IndexField ( j , text_field ) ) [EOL] [EOL] if constits is not None : [EOL] label = constits [ j ] [ diff ] [EOL] span_labels . append ( label ) [EOL] [EOL] if parents is not None : [EOL] parent_labels . append ( parents [ j ] [ diff ] ) [EOL] [EOL] start_fields = ListField ( span_starts ) [EOL] end_fields = ListField ( span_ends ) [EOL] span_mask_fields = SequenceLabelField ( span_mask , start_fields ) [EOL] [EOL] fields = { [string] : text_field , [string] : verb_field , [string] : start_fields , [string] : end_fields , [string] : span_mask_fields , [string] : predicate_field } [EOL] [EOL] if constits : [EOL] fields [ [string] ] = SequenceLabelField ( span_labels , start_fields , label_namespace = self . label_namespace ) [EOL] fields [ [string] ] = SequenceLabelField ( parent_labels , start_fields , label_namespace = self . parent_label_namespace ) [EOL] return Instance ( fields ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] max_span_width = params . pop ( [string] ) [EOL] token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] label_namespace = params . pop ( [string] , [string] ) [EOL] parent_label_namespace = params . pop ( [string] , [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return SyntacticConstitReader ( max_span_width = max_span_width , token_indexers = token_indexers , label_namespace = label_namespace , parent_label_namespace = parent_label_namespace ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Tuple , List , OrderedDict , Dict , DefaultDict [EOL] import collections [EOL] import typing [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] import codecs [EOL] import os [EOL] import logging [EOL] from collections import defaultdict [EOL] from typing import Dict , List , Optional , Tuple [EOL] [EOL] import tqdm [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset import Dataset [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , TextField , ListField , SequenceLabelField , IndexField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class SpanAnnotationReader ( DatasetReader ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , max_span_width , token_indexers = None ) : [EOL] self . max_span_width = max_span_width [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] [EOL] [comment] [EOL] self . _tag_widths = { } [EOL] self . _total_args = [number] [EOL] self . _non_constit_args = [number] [EOL] self . _constits_args = { } [EOL] [EOL] def calculate_span_size ( self , tag_sequence ) : [EOL] def remove_bio ( tag ) : [EOL] return tag [ [number] : ] if tag . startswith ( [string] ) or tag . startswith ( [string] ) else tag [EOL] sizes = [ ( remove_bio ( tag_sequence [ [number] ] ) , [number] ) ] [EOL] [EOL] for idx , tag in enumerate ( tag_sequence [ [number] : ] , [number] ) : [EOL] if tag != tag_sequence [ idx - [number] ] or tag . startswith ( [string] ) : [EOL] sizes . append ( ( remove_bio ( tag ) , [number] ) ) [EOL] else : [EOL] last_tag , length = sizes . pop ( ) [EOL] sizes . append ( ( last_tag , length + [number] ) ) [EOL] [EOL] for pair in sizes : [EOL] tag , length = pair [EOL] if tag not in self . _tag_widths : [EOL] self . _tag_widths [ tag ] = [ ] [EOL] self . _tag_widths [ tag ] . append ( length ) [EOL] [EOL] def _convert_bio_into_matrix ( self , tag_sequence ) : [EOL] def remove_bio ( tag ) : [EOL] return tag [ [number] : ] if tag . startswith ( [string] ) or tag . startswith ( [string] ) else tag [EOL] [comment] [EOL] [EOL] spans = [ [ [string] for _ in range ( self . max_span_width ) ] for _ in range ( len ( tag_sequence ) ) ] [EOL] [EOL] start_span = [number] [EOL] current_tag = tag_sequence [ [number] ] [EOL] for pos , tag in enumerate ( tag_sequence [ [number] : ] , [number] ) : [EOL] width = pos - start_span [EOL] if tag . startswith ( [string] ) or ( tag == [string] and tag_sequence [ pos - [number] ] != [string] ) : [EOL] width = pos - [number] - start_span [EOL] spans [ pos - [number] ] [ width ] = remove_bio ( current_tag ) [EOL] start_span = pos [EOL] current_tag = tag [EOL] width = pos - start_span [EOL] elif width == self . max_span_width - [number] : [comment] [EOL] spans [ pos ] [ width ] = remove_bio ( current_tag ) [EOL] start_span = pos + [number] [EOL] if pos + [number] < len ( tag_sequence ) : [EOL] current_tag = tag_sequence [ pos + [number] ] [EOL] spans [ len ( tag_sequence ) - [number] ] [ len ( tag_sequence ) - [number] - start_span ] = remove_bio ( tag_sequence [ - [number] ] ) [EOL] return spans [EOL] [EOL] def _process_sentence ( self , sentence_tokens , constits , verbal_predicates , predicate_argument_labels ) : [EOL] [docstring] [EOL] default = [string] [EOL] [EOL] def get_new_label ( original , newer ) : [EOL] return newer if original == default else [string] . format ( newer , original ) [EOL] [EOL] constit_matrix = [ [ default for _ in range ( self . max_span_width ) ] for _ in sentence_tokens ] [EOL] for span in constits : [EOL] start , end = span [EOL] diff = end - start [EOL] if diff >= self . max_span_width : [EOL] continue [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] constit_matrix [ end ] [ diff ] = get_new_label ( constit_matrix [ end ] [ diff ] , constits [ span ] ) [EOL] [EOL] tokens = [ Token ( t ) for t in sentence_tokens ] [EOL] if not verbal_predicates : [EOL] [comment] [EOL] tags = [ [string] for _ in sentence_tokens ] [EOL] verb_label = [ [number] for _ in sentence_tokens ] [EOL] srl_args = self . _convert_bio_into_matrix ( tags ) [EOL] dummy_verb_index = [number] [EOL] return [ self . text_to_instance ( tokens , verb_label , dummy_verb_index , constit_matrix , srl_args ) ] [EOL] else : [EOL] instances = [ ] [EOL] [EOL] for verb_index , tags in zip ( verbal_predicates , predicate_argument_labels ) : [EOL] verb_label = [ [number] for _ in sentence_tokens ] [EOL] verb_label [ verb_index ] = [number] [EOL] srl_args = self . _convert_bio_into_matrix ( tags ) [EOL] instances . append ( self . text_to_instance ( tokens , verb_label , verb_index , constit_matrix , srl_args ) ) [EOL] self . find_overlap ( srl_args , constit_matrix ) [EOL] return instances [EOL] [EOL] def find_overlap ( self , srl_args , constit_matrix ) : [EOL] for j in range ( len ( srl_args ) ) : [EOL] for diff in range ( len ( srl_args [ [number] ] ) ) : [EOL] arg = srl_args [ j ] [ diff ] [EOL] constit = constit_matrix [ j ] [ diff ] [EOL] if arg == [string] or arg == [string] or arg == [string] : [EOL] continue [EOL] self . _total_args += [number] [EOL] if constit == [string] : [EOL] if j - diff != [number] : [EOL] self . _non_constit_args += [number] [EOL] continue [EOL] if constit not in self . _constits_args : [EOL] self . _constits_args [ constit ] = [number] [EOL] self . _constits_args [ constit ] += [number] [EOL] [EOL] @ overrides def read ( self , file_path ) : [EOL] [comment] [EOL] file_path = cached_path ( file_path ) [EOL] [EOL] instances = [ ] [EOL] [EOL] sentence = [ ] [EOL] open_constits = [ ] [EOL] constits = { } [EOL] verbal_predicates = [ ] [EOL] predicate_argument_labels = [ ] [EOL] current_span_label = [ ] [EOL] [EOL] logger . info ( [string] , file_path ) [EOL] for root , _ , files in tqdm . tqdm ( list ( os . walk ( file_path ) ) ) : [EOL] for data_file in files : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not data_file . endswith ( [string] ) : [EOL] continue [EOL] with codecs . open ( os . path . join ( root , data_file ) , [string] , encoding = [string] ) as open_file : [EOL] for line in open_file : [EOL] line = line . strip ( ) [EOL] if line == [string] or line . startswith ( [string] ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not sentence : [EOL] continue [EOL] instances . extend ( self . _process_sentence ( sentence , constits , verbal_predicates , predicate_argument_labels ) ) [EOL] [comment] [EOL] sentence = [ ] [EOL] open_constits = [ ] [EOL] constits = { } [EOL] verbal_predicates = [ ] [EOL] predicate_argument_labels = [ ] [EOL] current_span_label = [ ] [EOL] continue [EOL] [EOL] conll_components = line . split ( ) [EOL] word = conll_components [ [number] ] [EOL] [EOL] sentence . append ( word ) [EOL] word_index = len ( sentence ) - [number] [EOL] [EOL] [comment] [EOL] syn_label = conll_components [ [number] ] [EOL] if syn_label != [string] : [EOL] if [string] in syn_label : [EOL] starts = syn_label . split ( [string] ) [EOL] for con in starts [ [number] : ] : [EOL] clabel = con . strip ( [string] ) . strip ( [string] ) [EOL] open_constits . append ( ( clabel , word_index ) ) [EOL] if [string] in syn_label : [EOL] ends = syn_label . count ( [string] ) [EOL] for _ in range ( ends ) : [EOL] assert open_constits [EOL] clabel , start = open_constits . pop ( ) [EOL] if ( start , word_index ) in constits : [EOL] clabel = [string] . format ( constits [ ( start , word_index ) ] , clabel ) [EOL] constits [ ( start , word_index ) ] = clabel [EOL] if clabel not in self . _tag_widths : [EOL] self . _tag_widths [ clabel ] = [ ] [EOL] self . _tag_widths [ clabel ] . append ( word_index - start + [number] ) [EOL] [EOL] [comment] [EOL] if word_index == [number] : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] predicate_argument_labels = [ [ ] for _ in conll_components [ [number] : - [number] ] ] [EOL] current_span_label = [ None for _ in conll_components [ [number] : - [number] ] ] [EOL] prev_span_label = [ None for _ in conll_components [ [number] : - [number] ] ] [EOL] [EOL] num_annotations = len ( predicate_argument_labels ) [EOL] is_verbal_predicate = False [EOL] [EOL] [comment] [EOL] for annotation_index in range ( num_annotations ) : [EOL] annotation = conll_components [ [number] + annotation_index ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if [string] in annotation : [EOL] is_verbal_predicate = True [EOL] [EOL] label = annotation . strip ( [string] ) [EOL] [EOL] if [string] in annotation : [EOL] [comment] [EOL] [comment] [EOL] bio_label = [string] + label [EOL] predicate_argument_labels [ annotation_index ] . append ( bio_label ) [EOL] current_span_label [ annotation_index ] = label [EOL] elif current_span_label [ annotation_index ] is not None : [EOL] [comment] [EOL] [comment] [EOL] bio_label = [string] + current_span_label [ annotation_index ] [EOL] predicate_argument_labels [ annotation_index ] . append ( bio_label ) [EOL] else : [EOL] [comment] [EOL] predicate_argument_labels [ annotation_index ] . append ( [string] ) [EOL] [EOL] prev_span_label [ annotation_index ] = current_span_label [ annotation_index ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] if [string] in annotation : [EOL] current_span_label [ annotation_index ] = None [EOL] [EOL] if is_verbal_predicate : [EOL] verbal_predicates . append ( word_index ) [EOL] [EOL] if not instances : [EOL] raise ConfigurationError ( [string] [string] . format ( file_path ) ) [EOL] logger . info ( [string] , len ( instances ) ) [EOL] [comment] [EOL] [comment] [EOL] return Dataset ( instances ) [EOL] [EOL] def analyze_overlap ( self ) : [EOL] logger . info ( [string] , self . _non_constit_args * [number] / self . _total_args ) [EOL] from collections import OrderedDict [EOL] from operator import itemgetter [EOL] x = { k : self . _constits_args [ k ] / self . _total_args for k in self . _constits_args } [EOL] d = OrderedDict ( sorted ( x . items ( ) , key = itemgetter ( [number] ) , reverse = True ) ) [EOL] print ( d ) [EOL] [EOL] def analyze_span_width ( self ) : [EOL] total_tag_width = [number] [EOL] total_spans = [number] [EOL] widths = defaultdict ( int ) [EOL] [EOL] for tag in self . _tag_widths : [EOL] if tag in [ [string] , [string] , [string] ] : [EOL] continue [EOL] total_tag_width += sum ( self . _tag_widths [ tag ] ) [EOL] total_spans += len ( self . _tag_widths [ tag ] ) [EOL] [EOL] for l in self . _tag_widths [ tag ] : [EOL] widths [ l ] += [number] [EOL] [EOL] x = [ ] [EOL] for l in sorted ( widths ) : [EOL] if len ( x ) == [number] : [EOL] x . append ( ( l , widths [ l ] * [number] / total_spans ) ) [EOL] else : [EOL] x . append ( ( l , x [ - [number] ] [ [number] ] + widths [ l ] * [number] / total_spans ) ) [EOL] print ( x [ - [number] ] ) [EOL] [EOL] print ( [string] . format ( total_tag_width / total_spans ) ) [EOL] import ipdb [EOL] ipdb . set_trace ( ) [EOL] [EOL] def text_to_instance ( self , tokens , verb_label , verb_index , constituents = None , srl_args = None ) : [EOL] [docstring] [EOL] [comment] [EOL] [EOL] [comment] [EOL] text_field = TextField ( tokens , token_indexers = self . _token_indexers ) [EOL] verb_field = SequenceLabelField ( verb_label , text_field ) [EOL] target_field = IndexField ( verb_index , text_field ) [EOL] [EOL] [comment] [EOL] span_starts = [ ] [EOL] span_ends = [ ] [EOL] span_mask = [ [number] for _ in range ( len ( tokens ) * self . max_span_width ) ] [EOL] span_labels = [ ] if srl_args is not None else None [EOL] constit_labels = [ ] if constituents is not None else None [EOL] [EOL] for j in range ( len ( tokens ) ) : [EOL] for diff in range ( self . max_span_width ) : [EOL] width = diff [EOL] if j - diff < [number] : [EOL] [comment] [EOL] span_mask [ j * self . max_span_width + diff ] = [number] [EOL] width = j [EOL] [EOL] span_starts . append ( IndexField ( j - width , text_field ) ) [EOL] span_ends . append ( IndexField ( j , text_field ) ) [EOL] [EOL] if srl_args : [EOL] current_label = srl_args [ j ] [ diff ] [EOL] span_labels . append ( current_label ) [EOL] [EOL] if constituents : [EOL] label = constituents [ j ] [ diff ] [EOL] constit_labels . append ( label ) [EOL] [EOL] start_fields = ListField ( span_starts ) [EOL] end_fields = ListField ( span_ends ) [EOL] span_mask_fields = SequenceLabelField ( span_mask , start_fields ) [EOL] [EOL] fields = { [string] : text_field , [string] : verb_field , [string] : target_field , [string] : start_fields , [string] : end_fields , [string] : span_mask_fields } [EOL] [EOL] if srl_args : [EOL] fields [ [string] ] = SequenceLabelField ( span_labels , start_fields ) [EOL] if constituents : [EOL] fields [ [string] ] = SequenceLabelField ( constit_labels , start_fields , label_namespace = [string] ) [EOL] return Instance ( fields ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] max_span_width = params . pop ( [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return SpanAnnotationReader ( token_indexers = token_indexers , max_span_width = max_span_width ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'SpanAnnotationReader'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Optional , Any , Tuple , List , Set , Dict , DefaultDict [EOL] import allennlp [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import logging [EOL] import re [EOL] import collections [EOL] from typing import Any , Dict , List , Optional , Tuple , DefaultDict , Set [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset import Dataset [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , ListField , TextField , IndexField , MetadataField , SequenceLabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class _DocumentState : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] self . sentence_buffer = [ ] [EOL] self . sentences = [ ] [EOL] self . num_total_words = [number] [EOL] [comment] [EOL] self . clusters = collections . defaultdict ( list ) [EOL] [comment] [EOL] self . coref_stacks = collections . defaultdict ( list ) [EOL] [EOL] def assert_document_is_finished ( self ) : [EOL] if self . sentence_buffer : [EOL] raise ConfigurationError ( [string] [string] [string] . format ( self . sentence_buffer ) ) [EOL] [EOL] if not self . sentences or self . num_total_words == [number] : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] if any ( x for x in self . coref_stacks . values ( ) ) : [EOL] raise ConfigurationError ( [string] [string] [string] [string] . format ( self . coref_stacks ) ) [EOL] [EOL] def complete_sentence ( self ) : [EOL] self . sentences . append ( self . sentence_buffer ) [EOL] self . sentence_buffer = [ ] [EOL] [EOL] def add_word ( self , word ) : [EOL] self . sentence_buffer . append ( word ) [EOL] self . num_total_words += [number] [EOL] [EOL] def canonicalize_clusters ( self ) : [EOL] [docstring] [EOL] merged_clusters = [ ] [EOL] for cluster in self . clusters . values ( ) : [EOL] cluster_with_overlapping_mention = None [EOL] for mention in cluster : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for cluster2 in merged_clusters : [EOL] if mention in cluster2 : [EOL] [comment] [EOL] [comment] [EOL] cluster_with_overlapping_mention = cluster2 [EOL] break [EOL] [comment] [EOL] if cluster_with_overlapping_mention is not None : [EOL] break [EOL] if cluster_with_overlapping_mention is not None : [EOL] [comment] [EOL] [comment] [EOL] cluster_with_overlapping_mention . update ( cluster ) [EOL] else : [EOL] merged_clusters . append ( set ( cluster ) ) [EOL] return [ list ( c ) for c in merged_clusters ] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class ConllCorefReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , max_span_width , token_indexers = None ) : [EOL] self . _max_span_width = max_span_width [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . _begin_document_regex = re . compile ( [string] ) [EOL] [EOL] @ overrides def read ( self , file_path ) : [EOL] [comment] [EOL] file_path = cached_path ( file_path ) [EOL] [EOL] logger . info ( [string] , file_path ) [EOL] instances = [ ] [EOL] with open ( file_path ) as dataset_file : [EOL] document_state = _DocumentState ( ) [EOL] [EOL] for line in dataset_file : [EOL] [EOL] if self . _begin_document_regex . match ( line ) : [EOL] [comment] [EOL] document_state = _DocumentState ( ) [EOL] [EOL] elif line . startswith ( [string] ) : [EOL] [comment] [EOL] document_state . assert_document_is_finished ( ) [EOL] clusters = document_state . canonicalize_clusters ( ) [EOL] instance = self . text_to_instance ( document_state . sentences , clusters ) [EOL] instances . append ( instance ) [EOL] else : [EOL] [comment] [EOL] self . _handle_line ( line , document_state ) [EOL] [EOL] if not instances : [EOL] raise ConfigurationError ( [string] [string] . format ( file_path ) ) [EOL] return Dataset ( instances ) [EOL] [EOL] @ overrides def text_to_instance ( self , sentences , gold_clusters = None ) : [EOL] [comment] [EOL] [docstring] [EOL] flattened_sentences = [ token for sentence in sentences for token in sentence ] [EOL] [EOL] metadata = { [string] : flattened_sentences } [EOL] if gold_clusters is not None : [EOL] metadata [ [string] ] = gold_clusters [EOL] [EOL] text_field = TextField ( [ Token ( word ) for word in flattened_sentences ] , self . _token_indexers ) [EOL] [EOL] cluster_dict = { } [EOL] if gold_clusters is not None : [EOL] for cluster_id , cluster in enumerate ( gold_clusters ) : [EOL] for mention in cluster : [EOL] cluster_dict [ tuple ( mention ) ] = cluster_id [EOL] [EOL] span_starts = [ ] [EOL] span_ends = [ ] [EOL] span_labels = [ ] if gold_clusters is not None else None [EOL] [EOL] sentence_offset = [number] [EOL] for sentence in sentences : [EOL] for start_index in range ( len ( sentence ) ) : [EOL] for end_index in range ( start_index , min ( start_index + self . _max_span_width , len ( sentence ) ) ) : [EOL] start = sentence_offset + start_index [EOL] end = sentence_offset + end_index [EOL] [EOL] if span_labels is not None : [EOL] if ( start , end ) in cluster_dict : [EOL] span_labels . append ( cluster_dict [ ( start , end ) ] ) [EOL] else : [EOL] span_labels . append ( - [number] ) [EOL] [EOL] span_starts . append ( IndexField ( start , text_field ) ) [EOL] span_ends . append ( IndexField ( end , text_field ) ) [EOL] sentence_offset += len ( sentence ) [EOL] [EOL] span_starts_field = ListField ( span_starts ) [EOL] span_ends_field = ListField ( span_ends ) [EOL] metadata_field = MetadataField ( metadata ) [EOL] [EOL] fields = { [string] : text_field , [string] : span_starts_field , [string] : span_ends_field , [string] : metadata_field } [EOL] if span_labels is not None : [EOL] fields [ [string] ] = SequenceLabelField ( span_labels , span_starts_field ) [EOL] [EOL] return Instance ( fields ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] max_span_width = params . pop ( [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( token_indexers = token_indexers , max_span_width = max_span_width ) [EOL] [EOL] @ staticmethod def _normalize_word ( word ) : [EOL] if word == [string] or word == [string] : [EOL] return word [ [number] : ] [EOL] else : [EOL] return word [EOL] [EOL] def _handle_line ( self , line , document_state ) : [EOL] row = line . split ( ) [EOL] if not row : [EOL] [comment] [EOL] [comment] [EOL] document_state . complete_sentence ( ) [EOL] else : [EOL] if len ( row ) < [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( row ) ) [EOL] word = self . _normalize_word ( row [ [number] ] ) [EOL] coref = row [ - [number] ] [EOL] word_index = document_state . num_total_words [EOL] document_state . add_word ( word ) [EOL] [EOL] if coref != [string] : [EOL] for segment in coref . split ( [string] ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if segment [ [number] ] == [string] : [EOL] [comment] [EOL] if segment [ - [number] ] == [string] : [EOL] [comment] [EOL] cluster_id = int ( segment [ [number] : - [number] ] ) [EOL] document_state . clusters [ cluster_id ] . append ( ( word_index , word_index ) ) [EOL] else : [EOL] [comment] [EOL] cluster_id = int ( segment [ [number] : ] ) [EOL] document_state . coref_stacks [ cluster_id ] . append ( word_index ) [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] cluster_id = int ( segment [ : - [number] ] ) [EOL] start = document_state . coref_stacks [ cluster_id ] . pop ( ) [EOL] document_state . clusters [ cluster_id ] . append ( ( start , word_index ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.DefaultDict[builtins.int,typing.List[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[builtins.int,typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 $typing.List[typing.Set[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Set[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Set[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Set[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 0 0 $typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]$ 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 $typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.coreference_resolution.conll._DocumentState$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 $typing.Optional[typing.List[typing.List[typing.Tuple[builtins.int,builtins.int]]]]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $"ConllCorefReader"$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $_DocumentState$ 0 0 0 $typing.List[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $_DocumentState$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.int$ 0 $_DocumentState$ 0 0 0 $_DocumentState$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $_DocumentState$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $_DocumentState$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $_DocumentState$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $_DocumentState$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . coreference_resolution . conll import ConllCorefReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . data . dataset_readers . dataset_utils . ontonotes import Ontonotes [EOL] from allennlp . data . dataset_readers . dataset_utils . ontonotes import OntonotesSentence [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import TypeVar , Any , Dict , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Dict , List , TypeVar , Generic [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] TokenType = TypeVar ( [string] , int , List [ int ] ) [comment] [EOL] [EOL] class TokenIndexer ( Generic [ TokenType ] , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def count_vocab_items ( self , token , counter ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def token_to_indices ( self , token , vocabulary ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_token ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_lengths ( self , token ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def pad_token_sequence ( self , tokens , desired_num_tokens , padding_lengths ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [comment] [EOL] choice = params . pop_choice ( [string] , cls . list_available ( ) , default_to_first_choice = True ) [EOL] return cls . by_name ( choice ) . from_params ( params ) [EOL] [EOL] @ classmethod def dict_from_params ( cls , params ) : [comment] [EOL] [docstring] [EOL] token_indexers = { } [EOL] for name , indexer_params in params . items ( ) : [EOL] token_indexers [ name ] = cls . from_params ( indexer_params ) [EOL] if token_indexers == { } : [EOL] token_indexers = None [EOL] return token_indexers [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $TokenType$ 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 $typing.List[TokenType]$ 0 0 0 $typing.List[TokenType]$ 0 $builtins.int$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $'TokenIndexer'$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $'Dict[str,TokenIndexer]'$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $None$ 0 0 0 0 $None$ 0
import builtins [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] [EOL] [EOL] class SequenceField ( Field [ DataArray ] ) : [EOL] [docstring] [EOL] def sequence_length ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
from typing import TypeVar , Dict , List [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from collections import defaultdict [EOL] from typing import Dict , Generic , List , TypeVar [EOL] [EOL] import torch [EOL] [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] DataArray = TypeVar ( [string] , torch . Tensor , Dict [ str , torch . Tensor ] ) [comment] [EOL] [EOL] [EOL] class Field ( Generic [ DataArray ] ) : [EOL] [docstring] [EOL] def count_vocab_items ( self , counter ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def index ( self , vocab ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def get_padding_lengths ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def as_tensor ( self , padding_lengths , cuda_device = - [number] , for_training = True ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def empty_field ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def batch_tensors ( cls , tensor_list ) : [comment] [EOL] [docstring] [EOL] if isinstance ( tensor_list [ [number] ] , dict ) : [EOL] [comment] [EOL] [comment] [EOL] token_indexer_key_to_batch_dict = defaultdict ( list ) [EOL] for encoding_name_dict in tensor_list : [EOL] for indexer_name , tensor in encoding_name_dict . items ( ) : [EOL] token_indexer_key_to_batch_dict [ indexer_name ] . append ( tensor ) [EOL] return { indexer_name : torch . stack ( tensor_list ) for indexer_name , tensor_list in token_indexer_key_to_batch_dict . items ( ) } [EOL] else : [EOL] return torch . stack ( tensor_list ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Tuple , Any , List , Dict [EOL] import torch [EOL] import typing [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] [docstring] [EOL] [EOL] import logging [EOL] import os [EOL] import shutil [EOL] import time [EOL] from typing import Dict , Optional , List , Tuple , Generator [EOL] [EOL] import torch [EOL] import torch . optim . lr_scheduler [EOL] from torch . nn . utils . clip_grad import clip_grad_norm [EOL] from torch . optim . lr_scheduler import _LRScheduler as PytorchLRScheduler [comment] [EOL] import tqdm [EOL] from tensorboard import SummaryWriter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Dataset [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . models . archival import archive_model [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn import util [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] from allennlp . training . optimizers import Optimizer [EOL] from allennlp . training . trainer import TensorboardWriter [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class MultiTaskTrainer : [EOL] def __init__ ( self , model , optimizer , iterator , iterator_aux , train_dataset , train_dataset_aux , mixing_ratio = [number] , cutoff_epoch = - [number] , validation_dataset = None , validation_dataset_aux = None , patience = [number] , validation_metric = [string] , num_epochs = [number] , serialization_dir = None , files_to_archive = None , cuda_device = - [number] , grad_norm = None , grad_clipping = None , learning_rate_scheduler = None , no_tqdm = False ) : [EOL] [docstring] [EOL] self . _model = model [EOL] self . _iterator = iterator [EOL] self . _iterator_aux = iterator_aux [EOL] self . _optimizer = optimizer [EOL] self . _train_dataset = train_dataset [EOL] self . _train_dataset_aux = train_dataset_aux [EOL] self . _validation_dataset = validation_dataset [EOL] self . _validation_dataset_aux = validation_dataset_aux [EOL] self . _mixing_ratio = mixing_ratio [EOL] self . _cutoff_epoch = cutoff_epoch [EOL] [EOL] self . _patience = patience [EOL] self . _num_epochs = num_epochs [EOL] self . _serialization_dir = serialization_dir [EOL] self . _files_to_archive = files_to_archive [EOL] self . _cuda_device = cuda_device [EOL] self . _grad_norm = grad_norm [EOL] self . _grad_clipping = grad_clipping [EOL] self . _learning_rate_scheduler = learning_rate_scheduler [EOL] [EOL] increase_or_decrease = validation_metric [ [number] ] [EOL] if increase_or_decrease not in [ [string] , [string] ] : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] self . _validation_metric = validation_metric [ [number] : ] [EOL] self . _validation_metric_decreases = increase_or_decrease == [string] [EOL] self . _no_tqdm = no_tqdm [EOL] [EOL] if self . _cuda_device >= [number] : [EOL] self . _model = self . _model . cuda ( self . _cuda_device ) [EOL] torch . cuda . set_device ( self . _cuda_device ) [EOL] [EOL] self . _log_interval = [number] [comment] [EOL] self . _summary_interval = [number] [comment] [EOL] [EOL] self . _last_log = [number] [comment] [EOL] [EOL] if serialization_dir is not None : [EOL] train_log = SummaryWriter ( os . path . join ( serialization_dir , [string] , [string] ) ) [EOL] validation_log = SummaryWriter ( os . path . join ( serialization_dir , [string] , [string] ) ) [EOL] self . _tensorboard = TensorboardWriter ( train_log , validation_log ) [EOL] else : [EOL] self . _tensorboard = TensorboardWriter ( ) [EOL] [EOL] def _enable_gradient_clipping ( self ) : [EOL] if self . _grad_clipping is not None : [EOL] [comment] [EOL] [comment] [EOL] def clip_function ( grad ) : return grad . clamp ( - self . _grad_clipping , self . _grad_clipping ) [EOL] for parameter in self . _model . parameters ( ) : [EOL] if parameter . requires_grad : [EOL] parameter . register_hook ( clip_function ) [EOL] [EOL] def _rescale_gradients ( self ) : [EOL] [docstring] [EOL] if self . _grad_norm : [EOL] clip_grad_norm ( self . _model . parameters ( ) , self . _grad_norm ) [EOL] [EOL] def _batch_loss ( self , batch , for_training , batch_aux = None ) : [EOL] [docstring] [EOL] output_dict = self . _model ( ** batch ) [EOL] try : [EOL] loss = output_dict [ [string] ] [EOL] if for_training : [EOL] loss += self . _model . get_regularization_penalty ( ) [EOL] except KeyError : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] if batch_aux is not None : [EOL] output_dict_aux = self . _model ( ** batch_aux ) [EOL] try : [EOL] loss_aux = output_dict_aux [ [string] ] [EOL] if for_training : [EOL] loss_aux += self . _model . get_regularization_penalty ( ) [EOL] except KeyError : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] loss = loss + self . _mixing_ratio * loss_aux [EOL] return loss [EOL] [EOL] def _get_metrics ( self , total_loss , batch_num , reset = False ) : [EOL] [docstring] [EOL] metrics = self . _model . get_metrics ( reset = reset ) [EOL] metrics [ [string] ] = float ( total_loss / batch_num ) [EOL] return metrics [EOL] [EOL] def _train_epoch ( self , epoch ) : [EOL] [docstring] [EOL] logger . info ( [string] , epoch , self . _num_epochs - [number] ) [EOL] train_loss = [number] [EOL] [comment] [EOL] self . _model . train ( ) [EOL] [EOL] [comment] [EOL] train_generator = self . _iterator ( self . _train_dataset , num_epochs = [number] , cuda_device = self . _cuda_device ) [EOL] num_training_batches = self . _iterator . get_num_batches ( self . _train_dataset ) [EOL] train_generator_tqdm = tqdm . tqdm ( train_generator , disable = self . _no_tqdm , total = num_training_batches ) [EOL] [EOL] train_generator_aux = self . _iterator_aux ( self . _train_dataset_aux , num_epochs = [number] , cuda_device = self . _cuda_device ) [EOL] [EOL] self . _last_log = time . time ( ) [EOL] batch_num = [number] [EOL] [EOL] scaffolded_training = False [EOL] if epoch > self . _cutoff_epoch : [EOL] scaffolded_training = True [EOL] logger . info ( [string] ) [EOL] else : [EOL] logger . info ( [string] ) [EOL] [EOL] for batch , batch_aux in zip ( train_generator_tqdm , train_generator_aux ) : [EOL] batch_num += [number] [EOL] self . _optimizer . zero_grad ( ) [EOL] [EOL] if scaffolded_training : [EOL] loss = self . _batch_loss ( batch , for_training = True , batch_aux = batch_aux ) [EOL] else : [EOL] loss = self . _batch_loss ( batch , for_training = True ) [EOL] loss . backward ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] train_loss += loss . data . cpu ( ) . numpy ( ) [EOL] [EOL] self . _rescale_gradients ( ) [EOL] [EOL] self . _optimizer . step ( ) [EOL] [EOL] [comment] [EOL] metrics = self . _get_metrics ( train_loss , batch_num ) [EOL] description = self . _description_from_metrics ( metrics ) [EOL] train_generator_tqdm . set_description ( description ) [EOL] [EOL] [comment] [EOL] batch_num_total = num_training_batches * epoch + batch_num [EOL] if batch_num_total % self . _summary_interval == [number] : [EOL] for name , param in self . _model . named_parameters ( ) : [EOL] self . _tensorboard . add_train_scalar ( [string] + name , param . data . mean ( ) , batch_num_total ) [EOL] self . _tensorboard . add_train_scalar ( [string] + name , param . data . std ( ) , batch_num_total ) [EOL] if param . grad is not None : [EOL] self . _tensorboard . add_train_scalar ( [string] + name , param . grad . data . mean ( ) , batch_num_total ) [EOL] self . _tensorboard . add_train_scalar ( [string] + name , param . grad . data . std ( ) , batch_num_total ) [EOL] self . _tensorboard . add_train_scalar ( [string] , metrics [ [string] ] , batch_num_total ) [EOL] self . _metrics_to_tensorboard ( batch_num_total , { [string] + k : v for k , v in metrics . items ( ) } ) [EOL] [EOL] [comment] [EOL] if self . _no_tqdm and time . time ( ) - self . _last_log > self . _log_interval : [EOL] logger . info ( [string] , batch_num , num_training_batches , description ) [EOL] self . _last_log = time . time ( ) [EOL] [EOL] return self . _get_metrics ( train_loss , batch_num , reset = True ) [EOL] [EOL] def _should_stop_early ( self , metric_history ) : [EOL] [docstring] [EOL] if len ( metric_history ) > self . _patience : [EOL] [comment] [EOL] if self . _validation_metric_decreases : [EOL] return min ( metric_history [ - self . _patience : ] ) > min ( metric_history ) [EOL] else : [EOL] return max ( metric_history [ - self . _patience : ] ) < max ( metric_history ) [EOL] [EOL] return False [EOL] [EOL] def _metrics_to_tensorboard ( self , epoch , train_metrics , val_metrics = None ) : [EOL] [docstring] [EOL] for name , value in train_metrics . items ( ) : [EOL] if [string] not in name and [string] not in name : [EOL] continue [EOL] self . _tensorboard . add_train_scalar ( name , value , epoch ) [EOL] if val_metrics : [EOL] self . _tensorboard . add_validation_scalar ( name , val_metrics [ name ] , epoch ) [EOL] [EOL] def _metrics_to_console ( self , train_metrics , val_metrics = None ) : [EOL] [docstring] [EOL] if val_metrics : [EOL] message_template = [string] [EOL] else : [EOL] message_template = [string] [EOL] [EOL] for name , value in train_metrics . items ( ) : [EOL] if [string] not in name and [string] not in name : [EOL] continue [EOL] if val_metrics : [EOL] logger . info ( message_template , name , value , name , val_metrics [ name ] ) [EOL] else : [EOL] logger . info ( message_template , name , value ) [EOL] [EOL] def _update_learning_rate ( self , epoch , val_metric = None ) : [EOL] if not self . _learning_rate_scheduler : [EOL] return [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] reduce_on_plateau = isinstance ( self . _learning_rate_scheduler , torch . optim . lr_scheduler . ReduceLROnPlateau ) [EOL] [EOL] if reduce_on_plateau and val_metric is None : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] elif reduce_on_plateau : [EOL] self . _learning_rate_scheduler . step ( val_metric , epoch ) [EOL] else : [EOL] self . _learning_rate_scheduler . step ( epoch ) [EOL] [EOL] def _validation_loss ( self ) : [EOL] [docstring] [EOL] logger . info ( [string] ) [EOL] [EOL] self . _model . eval ( ) [EOL] [EOL] val_generator = self . _iterator ( self . _validation_dataset , num_epochs = [number] , cuda_device = self . _cuda_device , for_training = False ) [EOL] [EOL] num_validation_batches = self . _iterator . get_num_batches ( self . _validation_dataset ) [EOL] val_generator_tqdm = tqdm . tqdm ( val_generator , disable = self . _no_tqdm , total = num_validation_batches ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] batch_num = [number] [EOL] val_loss = [number] [EOL] [comment] [EOL] for batch in val_generator_tqdm : [EOL] batch_num += [number] [EOL] [EOL] loss = self . _batch_loss ( batch , for_training = False ) [EOL] val_loss += loss . data . cpu ( ) . numpy ( ) [EOL] [EOL] [comment] [EOL] val_metrics = self . _get_metrics ( val_loss , batch_num ) [EOL] description = self . _description_from_metrics ( val_metrics ) [EOL] val_generator_tqdm . set_description ( description ) [EOL] [EOL] [comment] [EOL] if self . _no_tqdm and time . time ( ) - self . _last_log > self . _log_interval : [EOL] logger . info ( [string] , batch_num , num_validation_batches , description ) [EOL] self . _last_log = time . time ( ) [EOL] [EOL] return val_loss , batch_num [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] epoch_counter , validation_metric_per_epoch = self . _restore_checkpoint ( ) [EOL] self . _enable_gradient_clipping ( ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] training_start_time = time . time ( ) [EOL] for epoch in range ( epoch_counter , self . _num_epochs ) : [EOL] epoch_start_time = time . time ( ) [EOL] train_metrics = self . _train_epoch ( epoch ) [EOL] [EOL] if self . _validation_dataset is not None : [EOL] [comment] [EOL] val_loss , num_batches = self . _validation_loss ( ) [EOL] val_metrics = self . _get_metrics ( val_loss , num_batches , reset = True ) [EOL] [EOL] [comment] [EOL] this_epoch_val_metric = val_metrics [ self . _validation_metric ] [EOL] validation_metric_per_epoch . append ( this_epoch_val_metric ) [EOL] if self . _should_stop_early ( validation_metric_per_epoch ) : [EOL] logger . info ( [string] ) [EOL] break [EOL] [EOL] [comment] [EOL] if self . _validation_metric_decreases : [EOL] is_best_so_far = this_epoch_val_metric == min ( validation_metric_per_epoch ) [EOL] else : [EOL] is_best_so_far = this_epoch_val_metric == max ( validation_metric_per_epoch ) [EOL] else : [EOL] [comment] [EOL] is_best_so_far = True [EOL] val_metrics = this_epoch_val_metric = None [EOL] [EOL] if epoch % [number] == [number] or is_best_so_far : [EOL] self . _save_checkpoint ( epoch , validation_metric_per_epoch , is_best = is_best_so_far ) [EOL] self . _metrics_to_tensorboard ( epoch , train_metrics , val_metrics = val_metrics ) [EOL] self . _metrics_to_console ( train_metrics , val_metrics ) [EOL] self . _update_learning_rate ( epoch , val_metric = this_epoch_val_metric ) [EOL] [EOL] epoch_elapsed_time = time . time ( ) - epoch_start_time [EOL] logger . info ( [string] , time . strftime ( [string] , time . gmtime ( epoch_elapsed_time ) ) ) [EOL] [EOL] if epoch < self . _num_epochs - [number] : [EOL] training_elapsed_time = time . time ( ) - training_start_time [EOL] estimated_time_remaining = training_elapsed_time * ( ( self . _num_epochs - epoch_counter ) / float ( epoch - epoch_counter + [number] ) - [number] ) [EOL] formatted_time = time . strftime ( [string] , time . gmtime ( estimated_time_remaining ) ) [EOL] logger . info ( [string] , formatted_time ) [EOL] [EOL] def _description_from_metrics ( self , metrics ) : [EOL] [comment] [EOL] return [string] . join ( [ [string] % ( name , value ) for name , value in metrics . items ( ) if [string] in name or [string] in name ] ) + [string] [EOL] [EOL] def _save_checkpoint ( self , epoch , val_metric_per_epoch , is_best = None ) : [EOL] [docstring] [EOL] if self . _serialization_dir is not None : [EOL] model_path = os . path . join ( self . _serialization_dir , [string] . format ( epoch ) ) [EOL] model_state = self . _model . state_dict ( ) [EOL] torch . save ( model_state , model_path ) [EOL] [EOL] training_state = { [string] : epoch , [string] : val_metric_per_epoch , [string] : self . _optimizer . state_dict ( ) } [EOL] torch . save ( training_state , os . path . join ( self . _serialization_dir , [string] . format ( epoch ) ) ) [EOL] if is_best : [EOL] logger . info ( [string] [string] , self . _serialization_dir ) [EOL] shutil . copyfile ( model_path , os . path . join ( self . _serialization_dir , [string] ) ) [EOL] archive_model ( self . _serialization_dir , files_to_archive = self . _files_to_archive ) [EOL] [EOL] def _restore_checkpoint ( self ) : [EOL] [docstring] [EOL] have_checkpoint = ( self . _serialization_dir is not None and any ( [string] in x for x in os . listdir ( self . _serialization_dir ) ) ) [EOL] [EOL] if not have_checkpoint : [EOL] [comment] [EOL] return [number] , [ ] [EOL] [EOL] serialization_files = os . listdir ( self . _serialization_dir ) [EOL] model_checkpoints = [ x for x in serialization_files if [string] in x ] [EOL] epoch_to_load = max ( [ int ( x . split ( [string] ) [ - [number] ] . strip ( [string] ) ) for x in model_checkpoints ] ) [EOL] [EOL] model_path = os . path . join ( self . _serialization_dir , [string] . format ( epoch_to_load ) ) [EOL] training_state_path = os . path . join ( self . _serialization_dir , [string] . format ( epoch_to_load ) ) [EOL] [EOL] model_state = torch . load ( model_path , map_location = util . device_mapping ( self . _cuda_device ) ) [EOL] training_state = torch . load ( training_state_path , map_location = util . device_mapping ( self . _cuda_device ) ) [EOL] self . _model . load_state_dict ( model_state ) [EOL] self . _optimizer . load_state_dict ( training_state [ [string] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if [string] not in training_state : [EOL] logger . warning ( [string] ) [EOL] val_metric_per_epoch = [ ] [EOL] else : [EOL] val_metric_per_epoch = training_state [ [string] ] [EOL] [EOL] return training_state [ [string] ] + [number] , val_metric_per_epoch [EOL] [EOL] @ classmethod def from_params ( cls , model , serialization_dir , iterator , iterator_aux , train_dataset , train_dataset_aux , mixing_ratio , cutoff_epoch , validation_dataset , validation_dataset_aux , params , files_to_archive ) : [EOL] [EOL] patience = params . pop ( [string] , [number] ) [EOL] validation_metric = params . pop ( [string] , [string] ) [EOL] num_epochs = params . pop ( [string] , [number] ) [EOL] cuda_device = params . pop ( [string] , - [number] ) [EOL] grad_norm = params . pop ( [string] , None ) [EOL] grad_clipping = params . pop ( [string] , None ) [EOL] lr_scheduler_params = params . pop ( [string] , None ) [EOL] [EOL] if cuda_device >= [number] : [EOL] model = model . cuda ( cuda_device ) [EOL] parameters = [ p for p in model . parameters ( ) if p . requires_grad ] [EOL] optimizer = Optimizer . from_params ( parameters , params . pop ( [string] ) ) [EOL] [EOL] if lr_scheduler_params : [EOL] scheduler = LearningRateScheduler . from_params ( optimizer , lr_scheduler_params ) [EOL] else : [EOL] scheduler = None [EOL] no_tqdm = params . pop ( [string] , False ) [EOL] [EOL] params . assert_empty ( cls . __name__ ) [EOL] return MultiTaskTrainer ( model = model , optimizer = optimizer , iterator = iterator , iterator_aux = iterator_aux , train_dataset = train_dataset , train_dataset_aux = train_dataset_aux , mixing_ratio = mixing_ratio , cutoff_epoch = cutoff_epoch , validation_dataset = validation_dataset , validation_dataset_aux = validation_dataset_aux , patience = patience , validation_metric = validation_metric , num_epochs = num_epochs , serialization_dir = serialization_dir , files_to_archive = files_to_archive , cuda_device = cuda_device , grad_norm = grad_norm , grad_clipping = grad_clipping , learning_rate_scheduler = scheduler , no_tqdm = no_tqdm ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'MultiTaskTrainer'$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.Dataset$ 0 $allennlp.data.Dataset$ 0 $builtins.float$ 0 $builtins.int$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $allennlp.common.Params$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 $allennlp.data.Dataset$ 0 $allennlp.data.Dataset$ 0 $allennlp.data.Dataset$ 0 $allennlp.data.Dataset$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $typing.Optional[allennlp.data.Dataset]$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $None$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from allennlp . training . trainer import Trainer [EOL] from allennlp . training . multitask_trainer import MultiTaskTrainer [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] [docstring] [EOL] [EOL] from typing import List [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] [EOL] [EOL] class Optimizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] @ classmethod def from_params ( cls , model_parameters , params ) : [EOL] if isinstance ( params , str ) : [EOL] optimizer = params [EOL] params = Params ( { } ) [EOL] else : [EOL] optimizer = params . pop_choice ( [string] , Optimizer . list_available ( ) ) [EOL] return Optimizer . by_name ( optimizer ) ( model_parameters , ** params . as_dict ( ) ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] Registrable . _registry [ Optimizer ] = { [string] : torch . optim . Adam , [string] : torch . optim . Adagrad , [string] : torch . optim . Adadelta , [string] : torch . optim . SGD , [string] : torch . optim . RMSprop , } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.nn.Parameter]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[torch.nn.Parameter]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Tuple , Any , List , Set , Dict [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , Optional , Set , Tuple [EOL] from collections import defaultdict [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . nn . util import get_lengths_from_binary_sequence_mask , ones_like [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class SpanBasedF1Measure ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self , vocabulary , tag_namespace = [string] , ignore_classes = None ) : [EOL] [docstring] [EOL] self . _label_vocabulary = vocabulary . get_index_to_token_vocabulary ( tag_namespace ) [EOL] self . _ignore_classes = ignore_classes or [ ] [EOL] [EOL] [comment] [EOL] self . _true_positives = defaultdict ( int ) [EOL] self . _false_positives = defaultdict ( int ) [EOL] self . _false_negatives = defaultdict ( int ) [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , prediction_map = None ) : [EOL] [docstring] [EOL] if mask is None : [EOL] mask = ones_like ( gold_labels ) [EOL] [comment] [EOL] predictions , gold_labels , mask , prediction_map = self . unwrap_to_tensors ( predictions , gold_labels , mask , prediction_map ) [EOL] [EOL] num_classes = predictions . size ( - [number] ) [EOL] if ( gold_labels >= num_classes ) . any ( ) : [EOL] raise ConfigurationError ( [string] [string] . format ( num_classes ) ) [EOL] [EOL] sequence_lengths = get_lengths_from_binary_sequence_mask ( mask ) [EOL] argmax_predictions = predictions . max ( - [number] ) [ [number] ] [EOL] [EOL] if prediction_map is not None : [EOL] argmax_predictions = torch . gather ( prediction_map , [number] , argmax_predictions ) [EOL] gold_labels = torch . gather ( prediction_map , [number] , gold_labels . long ( ) ) [EOL] [EOL] argmax_predictions = argmax_predictions . float ( ) [EOL] [EOL] [comment] [EOL] batch_size = gold_labels . size ( [number] ) [EOL] for i in range ( batch_size ) : [EOL] sequence_prediction = argmax_predictions [ i , : ] [EOL] sequence_gold_label = gold_labels [ i , : ] [EOL] length = sequence_lengths [ i ] [EOL] [EOL] if length == [number] : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] prediction_spans = self . _extract_spans ( sequence_prediction [ : length ] . tolist ( ) ) [EOL] gold_spans = self . _extract_spans ( sequence_gold_label [ : length ] . tolist ( ) ) [EOL] [EOL] for span in prediction_spans : [EOL] if span in gold_spans : [EOL] self . _true_positives [ span [ [number] ] ] += [number] [EOL] gold_spans . remove ( span ) [EOL] else : [EOL] self . _false_positives [ span [ [number] ] ] += [number] [EOL] [comment] [EOL] for span in gold_spans : [EOL] self . _false_negatives [ span [ [number] ] ] += [number] [EOL] [EOL] def _extract_spans ( self , tag_sequence ) : [EOL] [docstring] [EOL] spans = set ( ) [EOL] span_start = [number] [EOL] span_end = [number] [EOL] active_conll_tag = None [EOL] for index , integer_tag in enumerate ( tag_sequence ) : [EOL] [comment] [EOL] string_tag = self . _label_vocabulary [ integer_tag ] [EOL] bio_tag = string_tag [ [number] ] [EOL] conll_tag = string_tag [ [number] : ] [EOL] if bio_tag == [string] or conll_tag in self . _ignore_classes : [EOL] [comment] [EOL] if active_conll_tag : [EOL] spans . add ( ( ( span_start , span_end ) , active_conll_tag ) ) [EOL] active_conll_tag = None [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] elif bio_tag == [string] : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if active_conll_tag : [EOL] spans . add ( ( ( span_start , span_end ) , active_conll_tag ) ) [EOL] spans . add ( ( ( index , index ) , conll_tag ) ) [EOL] active_conll_tag = None [EOL] elif bio_tag == [string] : [EOL] [comment] [EOL] [comment] [EOL] if active_conll_tag : [EOL] spans . add ( ( ( span_start , span_end ) , active_conll_tag ) ) [EOL] active_conll_tag = conll_tag [EOL] span_start = index [EOL] span_end = index [EOL] elif bio_tag == [string] and conll_tag == active_conll_tag : [EOL] [comment] [EOL] span_end += [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if active_conll_tag : [EOL] spans . add ( ( ( span_start , span_end ) , active_conll_tag ) ) [EOL] active_conll_tag = conll_tag [EOL] span_start = index [EOL] span_end = index [EOL] [comment] [EOL] if active_conll_tag : [EOL] spans . add ( ( ( span_start , span_end ) , active_conll_tag ) ) [EOL] return spans [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] all_tags = set ( ) [EOL] all_tags . update ( self . _true_positives . keys ( ) ) [EOL] all_tags . update ( self . _false_positives . keys ( ) ) [EOL] all_tags . update ( self . _false_negatives . keys ( ) ) [EOL] all_metrics = { } [EOL] for tag in all_tags : [EOL] precision , recall , f1_measure = self . _compute_metrics ( self . _true_positives [ tag ] , self . _false_positives [ tag ] , self . _false_negatives [ tag ] ) [EOL] precision_key = [string] + [string] + tag [EOL] recall_key = [string] + [string] + tag [EOL] f1_key = [string] + [string] + tag [EOL] all_metrics [ precision_key ] = precision [EOL] all_metrics [ recall_key ] = recall [EOL] all_metrics [ f1_key ] = f1_measure [EOL] [EOL] [comment] [EOL] precision , recall , f1_measure = self . _compute_metrics ( sum ( self . _true_positives . values ( ) ) , sum ( self . _false_positives . values ( ) ) , sum ( self . _false_negatives . values ( ) ) ) [EOL] all_metrics [ [string] ] = precision [EOL] all_metrics [ [string] ] = recall [EOL] all_metrics [ [string] ] = f1_measure [EOL] if reset : [EOL] self . reset ( ) [EOL] return all_metrics [EOL] [EOL] @ staticmethod def _compute_metrics ( true_positives , false_positives , false_negatives ) : [EOL] precision = float ( true_positives ) / float ( true_positives + false_positives + [number] ) [EOL] recall = float ( true_positives ) / float ( true_positives + false_negatives + [number] ) [EOL] f1_measure = [number] * ( ( precision * recall ) / ( precision + recall + [number] ) ) [EOL] return precision , recall , f1_measure [EOL] [EOL] def reset ( self ) : [EOL] self . _true_positives = defaultdict ( int ) [EOL] self . _false_positives = defaultdict ( int ) [EOL] self . _false_negatives = defaultdict ( int ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[typing.Tuple[builtins.int,builtins.int],builtins.str]]$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0
from typing import Optional , Any [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] from typing import Optional [EOL] [EOL] import torch [EOL] [EOL] from allennlp . training . metrics . metric import Metric [EOL] from allennlp . nn . util import ones_like [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class F1Measure ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self , positive_label ) : [EOL] self . _positive_label = positive_label [EOL] self . _true_positives = [number] [EOL] self . _true_negatives = [number] [EOL] self . _false_positives = [number] [EOL] self . _false_negatives = [number] [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None ) : [EOL] [docstring] [EOL] [comment] [EOL] predictions , gold_labels , mask = self . unwrap_to_tensors ( predictions , gold_labels , mask ) [EOL] [EOL] num_classes = predictions . size ( - [number] ) [EOL] if ( gold_labels >= num_classes ) . any ( ) : [EOL] raise ConfigurationError ( [string] [string] . format ( num_classes ) ) [EOL] if mask is None : [EOL] mask = ones_like ( gold_labels ) [EOL] mask = mask . float ( ) [EOL] gold_labels = gold_labels . float ( ) [EOL] positive_label_mask = gold_labels . eq ( self . _positive_label ) . float ( ) [EOL] negative_label_mask = [number] - positive_label_mask [EOL] [EOL] argmax_predictions = predictions . topk ( [number] , - [number] ) [ [number] ] . float ( ) . squeeze ( - [number] ) [EOL] [EOL] [comment] [EOL] correct_null_predictions = ( argmax_predictions != self . _positive_label ) . float ( ) * negative_label_mask [EOL] self . _true_negatives += ( correct_null_predictions . float ( ) * mask ) . sum ( ) [EOL] [EOL] [comment] [EOL] correct_non_null_predictions = ( argmax_predictions == self . _positive_label ) . float ( ) * positive_label_mask [EOL] self . _true_positives += ( correct_non_null_predictions * mask ) . sum ( ) [EOL] [EOL] [comment] [EOL] incorrect_null_predictions = ( argmax_predictions != self . _positive_label ) . float ( ) * positive_label_mask [EOL] self . _false_negatives += ( incorrect_null_predictions * mask ) . sum ( ) [EOL] [EOL] [comment] [EOL] incorrect_non_null_predictions = ( argmax_predictions == self . _positive_label ) . float ( ) * negative_label_mask [EOL] self . _false_positives += ( incorrect_non_null_predictions * mask ) . sum ( ) [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] precision = float ( self . _true_positives ) / float ( self . _true_positives + self . _false_positives + [number] ) [EOL] recall = float ( self . _true_positives ) / float ( self . _true_positives + self . _false_negatives + [number] ) [EOL] f1_measure = [number] * ( ( precision * recall ) / ( precision + recall + [number] ) ) [EOL] if reset : [EOL] self . reset ( ) [EOL] return precision , recall , f1_measure [EOL] [EOL] def reset ( self ) : [EOL] self . _true_positives = [number] [EOL] self . _true_negatives = [number] [EOL] self . _false_positives = [number] [EOL] self . _false_negatives = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0
from typing import Optional , Any , Dict , List [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , Optional [EOL] [EOL] import numpy [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . modules . linear import Linear [EOL] import torch . nn . functional as F [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . modules import Seq2SeqEncoder , TimeDistributed , TextFieldEmbedder [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] from allennlp . nn . util import get_text_field_mask , sequence_cross_entropy_with_logits [EOL] from allennlp . training . metrics import CategoricalAccuracy [EOL] [EOL] [EOL] @ Model . register ( [string] ) class SimpleTagger ( Model ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vocab , text_field_embedder , stacked_encoder , initializer = InitializerApplicator ( ) , regularizer = None ) : [EOL] super ( SimpleTagger , self ) . __init__ ( vocab , regularizer ) [EOL] [EOL] self . text_field_embedder = text_field_embedder [EOL] self . num_classes = self . vocab . get_vocab_size ( [string] ) [EOL] self . stacked_encoder = stacked_encoder [EOL] self . tag_projection_layer = TimeDistributed ( Linear ( self . stacked_encoder . get_output_dim ( ) , self . num_classes ) ) [EOL] [EOL] if text_field_embedder . get_output_dim ( ) != stacked_encoder . get_input_dim ( ) : [EOL] raise ConfigurationError ( [string] [string] [string] . format ( text_field_embedder . get_output_dim ( ) , stacked_encoder . get_input_dim ( ) ) ) [EOL] self . metrics = { [string] : CategoricalAccuracy ( ) , [string] : CategoricalAccuracy ( top_k = [number] ) } [EOL] [EOL] initializer ( self ) [EOL] [EOL] @ overrides def forward ( self , tokens , tags = None ) : [EOL] [comment] [EOL] [docstring] [EOL] embedded_text_input = self . text_field_embedder ( tokens ) [EOL] batch_size , sequence_length , _ = embedded_text_input . size ( ) [EOL] mask = get_text_field_mask ( tokens ) [EOL] encoded_text = self . stacked_encoder ( embedded_text_input , mask ) [EOL] [EOL] logits = self . tag_projection_layer ( encoded_text ) [EOL] reshaped_log_probs = logits . view ( - [number] , self . num_classes ) [EOL] class_probabilities = F . softmax ( reshaped_log_probs , dim = - [number] ) . view ( [ batch_size , sequence_length , self . num_classes ] ) [EOL] [EOL] output_dict = { [string] : logits , [string] : class_probabilities } [EOL] [EOL] if tags is not None : [EOL] loss = sequence_cross_entropy_with_logits ( logits , tags , mask ) [EOL] for metric in self . metrics . values ( ) : [EOL] metric ( logits , tags , mask . float ( ) ) [EOL] output_dict [ [string] ] = loss [EOL] [EOL] return output_dict [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] all_predictions = output_dict [ [string] ] [EOL] all_predictions = all_predictions . cpu ( ) . data . numpy ( ) [EOL] if all_predictions . ndim == [number] : [EOL] predictions_list = [ all_predictions [ i ] for i in range ( all_predictions . shape [ [number] ] ) ] [EOL] else : [EOL] predictions_list = [ all_predictions ] [EOL] all_tags = [ ] [EOL] for predictions in predictions_list : [EOL] argmax_indices = numpy . argmax ( predictions , axis = - [number] ) [EOL] tags = [ self . vocab . get_token_from_index ( x , namespace = [string] ) for x in argmax_indices ] [EOL] all_tags . append ( tags ) [EOL] output_dict [ [string] ] = all_tags [EOL] return output_dict [EOL] [EOL] @ overrides def get_metrics ( self , reset = False ) : [EOL] return { metric_name : metric . get_metric ( reset ) for metric_name , metric in self . metrics . items ( ) } [EOL] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [EOL] embedder_params = params . pop ( [string] ) [EOL] text_field_embedder = TextFieldEmbedder . from_params ( vocab , embedder_params ) [EOL] stacked_encoder = Seq2SeqEncoder . from_params ( params . pop ( [string] ) ) [EOL] [EOL] initializer = InitializerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] regularizer = RegularizerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] [EOL] return cls ( vocab = vocab , text_field_embedder = text_field_embedder , stacked_encoder = stacked_encoder , initializer = initializer , regularizer = regularizer ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'SimpleTagger'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
[docstring] [EOL] [EOL] from allennlp . models . archival import archive_model , load_archive [EOL] from allennlp . models . crf_tagger import CrfTagger [EOL] from allennlp . models . decomposable_attention import DecomposableAttention [EOL] from allennlp . models . encoder_decoders . simple_seq2seq import SimpleSeq2Seq [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . reading_comprehension . bidaf import BidirectionalAttentionFlow [EOL] from allennlp . models . simple_tagger import SimpleTagger [EOL] from allennlp . models . coreference_resolution . coref import CoreferenceResolver [EOL] from allennlp . models . semantic_role_labeler import SemanticRoleLabeler [EOL] from allennlp . models . span_srl . semi_crf_srl import SemiCrfSemanticRoleLabeler [EOL] from allennlp . models . span_srl . frame_semi_crf_srl import FrameSemanticRoleLabeler [EOL] from allennlp . models . span_srl . scaffolding . constit_labeler import ConstitLabeler [EOL] from allennlp . models . span_srl . propbank_scaffolded_span_srl import PropBankScaffoldSpanSrl [EOL] from allennlp . models . span_srl . scaffolding . scaffolded_pb_srl import ScaffoldedPropBankSrl [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . models . encoder_decoders . simple_seq2seq import SimpleSeq2Seq [EOL]	0 0 0 0 0 0 0 0 0 0 0
from allennlp . models . coreference_resolution . coref import CoreferenceResolver [EOL]	0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , TextIO , Any , List [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , TextIO , Optional [EOL] [EOL] import torch [EOL] from torch . autograd import Variable [EOL] import torch . nn . functional as F [EOL] [EOL] from allennlp . modules . token_embedders import Embedding [EOL] from allennlp . modules import TimeDistributed [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] def compute_span_representations ( max_span_width , encoded_text , target_index , span_starts , span_ends , span_width_embedding , span_direction_embedding , span_distance_embedding , span_distance_bin , head_scorer ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] _ , sequence_length , _ = encoded_text . size ( ) [EOL] contextualized_embeddings = encoded_text [EOL] [EOL] [comment] [EOL] batch_size , num_spans = span_starts . size ( ) [EOL] assert num_spans == sequence_length * max_span_width [EOL] [EOL] start_embeddings = util . batched_index_select ( contextualized_embeddings , span_starts . squeeze ( - [number] ) ) [EOL] end_embeddings = util . batched_index_select ( contextualized_embeddings , span_ends . squeeze ( - [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] span_widths = span_ends - span_starts [EOL] [comment] [EOL] span_width_embeddings = span_width_embedding ( span_widths . squeeze ( - [number] ) ) [EOL] [EOL] target_index = target_index . view ( batch_size , [number] ) [EOL] span_dist = torch . abs ( span_ends - target_index ) [EOL] span_dist = span_dist * ( span_dist < span_distance_bin ) . long ( ) [EOL] span_dist_embeddings = span_distance_embedding ( span_dist . squeeze ( - [number] ) ) [EOL] [EOL] span_dir = ( ( span_ends - target_index ) > [number] ) . long ( ) [EOL] span_dir_embeddings = span_direction_embedding ( span_dir . squeeze ( - [number] ) ) [EOL] [EOL] [comment] [EOL] head_scores = head_scorer ( contextualized_embeddings ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] attended_text_embeddings = create_attended_span_representations ( max_span_width , head_scores , encoded_text , span_ends , span_widths ) [EOL] [comment] [EOL] span_embeddings = torch . cat ( [ start_embeddings , end_embeddings , span_width_embeddings , span_dist_embeddings , span_dir_embeddings , attended_text_embeddings ] , - [number] ) [EOL] span_embeddings = span_embeddings . view ( batch_size , sequence_length , max_span_width , - [number] ) [EOL] return span_embeddings [EOL] [EOL] [EOL] def compute_simple_span_representations ( max_span_width , encoded_text , span_starts , span_ends , span_width_embedding , head_scorer ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] _ , sequence_length , _ = encoded_text . size ( ) [EOL] contextualized_embeddings = encoded_text [EOL] [EOL] [comment] [EOL] batch_size , num_spans = span_starts . size ( ) [EOL] assert num_spans == sequence_length * max_span_width [EOL] [EOL] start_embeddings = util . batched_index_select ( contextualized_embeddings , span_starts . squeeze ( - [number] ) ) [EOL] end_embeddings = util . batched_index_select ( contextualized_embeddings , span_ends . squeeze ( - [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] span_widths = span_ends - span_starts [EOL] [comment] [EOL] span_width_embeddings = span_width_embedding ( span_widths . squeeze ( - [number] ) ) [EOL] [EOL] [comment] [EOL] head_scores = head_scorer ( contextualized_embeddings ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] attended_text_embeddings = create_attended_span_representations ( max_span_width , head_scores , encoded_text , span_ends , span_widths ) [EOL] [comment] [EOL] span_embeddings = torch . cat ( [ start_embeddings , end_embeddings , span_width_embeddings , attended_text_embeddings ] , - [number] ) [EOL] span_embeddings = span_embeddings . view ( batch_size , sequence_length , max_span_width , - [number] ) [EOL] return span_embeddings [EOL] [EOL] [EOL] def create_attended_span_representations ( max_span_width , head_scores , encoded_text , span_ends , span_widths ) : [EOL] [docstring] [EOL] [comment] [EOL] max_span_range_indices = util . get_range_vector ( max_span_width , encoded_text . is_cuda ) . view ( [number] , [number] , - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] span_ends = span_ends . unsqueeze ( - [number] ) [EOL] span_widths = span_widths . unsqueeze ( - [number] ) [EOL] span_mask = ( max_span_range_indices <= span_widths ) . float ( ) [EOL] raw_span_indices = span_ends - max_span_range_indices [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] span_mask = span_mask * ( raw_span_indices >= [number] ) . float ( ) [EOL] [comment] [EOL] span_indices = F . relu ( raw_span_indices . float ( ) ) . long ( ) [EOL] [EOL] [comment] [EOL] flat_span_indices = util . flatten_and_batch_shift_indices ( span_indices , encoded_text . size ( [number] ) ) [EOL] [EOL] [comment] [EOL] span_text_embeddings = util . batched_index_select ( encoded_text , span_indices , flat_span_indices ) [EOL] [EOL] [comment] [EOL] span_head_scores = util . batched_index_select ( head_scores , span_indices , flat_span_indices ) . squeeze ( - [number] ) [EOL] [EOL] [comment] [EOL] span_head_weights = util . last_dim_softmax ( span_head_scores , span_mask ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] attended_text_embeddings = util . weighted_sum ( span_text_embeddings , span_head_weights ) [EOL] [EOL] return attended_text_embeddings [EOL] [EOL] [EOL] def get_tag_mask ( num_classes , valid_frame_elements , batch_size ) : [EOL] [docstring] [EOL] [comment] [EOL] zeros = torch . zeros ( batch_size , num_classes ) . cuda ( ) [EOL] valid_frame_elements = valid_frame_elements . view ( batch_size , - [number] ) [EOL] indices = F . relu ( valid_frame_elements . float ( ) ) . long ( ) . view ( batch_size , - [number] ) . data [EOL] values = ( valid_frame_elements >= [number] ) . data . float ( ) [EOL] tag_mask = zeros . scatter_ ( [number] , indices , values ) [EOL] return Variable ( tag_mask ) [EOL] [EOL] [EOL] def write_to_conll_eval_file ( prediction_file , gold_file , verb_index , sentence , prediction , gold_labels ) : [EOL] [docstring] [EOL] verb_only_sentence = [ [string] ] * len ( sentence ) [EOL] if verb_index : [EOL] verb_only_sentence [ verb_index ] = sentence [ verb_index ] [EOL] [EOL] conll_format_predictions = convert_bio_tags_to_conll_format ( prediction ) [EOL] conll_format_gold_labels = convert_bio_tags_to_conll_format ( gold_labels ) [EOL] [EOL] for word , predicted , gold in zip ( verb_only_sentence , conll_format_predictions , conll_format_gold_labels ) : [EOL] prediction_file . write ( word . ljust ( [number] ) ) [EOL] prediction_file . write ( predicted . rjust ( [number] ) + [string] ) [EOL] gold_file . write ( word . ljust ( [number] ) ) [EOL] gold_file . write ( gold . rjust ( [number] ) + [string] ) [EOL] prediction_file . write ( [string] ) [EOL] gold_file . write ( [string] ) [EOL] [EOL] [EOL] def convert_bio_tags_to_conll_format ( labels ) : [EOL] [docstring] [EOL] sentence_length = len ( labels ) [EOL] conll_labels = [ ] [EOL] for i , label in enumerate ( labels ) : [EOL] if label == [string] : [EOL] conll_labels . append ( [string] ) [EOL] continue [EOL] new_label = [string] [EOL] [comment] [EOL] [comment] [EOL] if label [ [number] ] == [string] or i == [number] or label [ [number] : ] != labels [ i - [number] ] [ [number] : ] : [EOL] new_label = [string] + label [ [number] : ] + new_label [EOL] [comment] [EOL] [comment] [EOL] if i == sentence_length - [number] or labels [ i + [number] ] [ [number] ] == [string] or label [ [number] : ] != labels [ i + [number] ] [ [number] : ] : [EOL] new_label = new_label + [string] [EOL] conll_labels . append ( new_label ) [EOL] return conll_labels [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.autograd.Variable$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Dict , List [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] from typing import Dict , List , TextIO , Optional [EOL] [EOL] import torch [EOL] from torch . autograd import Variable [EOL] from torch . nn . modules import Linear , Dropout [EOL] import torch . nn . functional as F [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . span_srl import span_srl_util [EOL] from allennlp . modules import Seq2SeqEncoder , FeedForward , TimeDistributed , TextFieldEmbedder , SemiMarkovConditionalRandomField [EOL] from allennlp . modules . token_embedders import Embedding [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] from allennlp . nn import util [EOL] from allennlp . training . metrics import NonBioSpanBasedF1Measure [EOL] [EOL] [EOL] @ Model . register ( [string] ) class ScaffoldedFrameSrl ( Model ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vocab , text_field_embedder , stacked_encoder , span_feedforward , binary_feature_dim , max_span_width , binary_feature_size , distance_feature_size , ontology_path , embedding_dropout = [number] , srl_label_namespace = [string] , constit_label_namespace = [string] , fast_mode = True , loss_type = [string] , unlabeled_constits = False , np_pp_constits = False , initializer = InitializerApplicator ( ) , regularizer = None ) : [EOL] super ( ScaffoldedFrameSrl , self ) . __init__ ( vocab , regularizer ) [EOL] [EOL] [comment] [EOL] self . text_field_embedder = text_field_embedder [EOL] self . embedding_dropout = Dropout ( p = embedding_dropout ) [EOL] [comment] [EOL] self . binary_feature_embedding = Embedding ( [number] , binary_feature_dim ) [EOL] self . stacked_encoder = stacked_encoder [EOL] if text_field_embedder . get_output_dim ( ) + binary_feature_dim != stacked_encoder . get_input_dim ( ) : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] [comment] [EOL] self . max_span_width = max_span_width [EOL] self . span_width_embedding = Embedding ( max_span_width , binary_feature_size ) [EOL] [comment] [EOL] self . span_distance_bin = [number] [EOL] self . span_distance_embedding = Embedding ( self . span_distance_bin , distance_feature_size ) [EOL] self . span_direction_embedding = Embedding ( [number] , binary_feature_size ) [EOL] self . span_feedforward = TimeDistributed ( span_feedforward ) [EOL] self . head_scorer = TimeDistributed ( torch . nn . Linear ( stacked_encoder . get_output_dim ( ) , [number] ) ) [EOL] [EOL] self . num_srl_args = self . vocab . get_vocab_size ( srl_label_namespace ) [EOL] self . not_a_span_tag = self . vocab . get_token_index ( [string] , srl_label_namespace ) [EOL] self . outside_span_tag = self . vocab . get_token_index ( [string] , srl_label_namespace ) [EOL] self . semi_crf = SemiMarkovConditionalRandomField ( num_tags = self . num_srl_args , max_span_width = max_span_width , default_tag = self . not_a_span_tag , outside_span_tag = self . outside_span_tag , loss_type = loss_type ) [EOL] [comment] [EOL] self . unlabeled_constits = unlabeled_constits [EOL] self . np_pp_constits = np_pp_constits [EOL] self . constit_label_namespace = constit_label_namespace [EOL] [EOL] assert not ( unlabeled_constits and np_pp_constits ) [EOL] if unlabeled_constits : [EOL] self . num_constit_tags = [number] [EOL] elif np_pp_constits : [EOL] self . num_constit_tags = [number] [EOL] else : [EOL] self . num_constit_tags = self . vocab . get_vocab_size ( constit_label_namespace ) [EOL] [EOL] [comment] [EOL] self . srl_arg_projection_layer = TimeDistributed ( Linear ( span_feedforward . get_output_dim ( ) , self . num_srl_args ) ) [EOL] self . constit_arg_projection_layer = TimeDistributed ( Linear ( span_feedforward . get_output_dim ( ) , self . num_constit_tags ) ) [EOL] [EOL] [comment] [EOL] self . metrics = { [string] : NonBioSpanBasedF1Measure ( vocab , tag_namespace = constit_label_namespace , ignore_classes = [ [string] ] ) , [string] : NonBioSpanBasedF1Measure ( vocab , tag_namespace = srl_label_namespace , ignore_classes = [ [string] , [string] ] , ontology_path = ontology_path ) } [EOL] [EOL] [comment] [EOL] self . fast_mode = fast_mode [EOL] initializer ( self ) [EOL] [EOL] def forward ( self , tokens , targets , target_index , span_starts , span_ends , tags = None , ** kwargs ) : [EOL] [comment] [EOL] [docstring] [EOL] embedded_text_input = self . embedding_dropout ( self . text_field_embedder ( tokens ) ) [EOL] text_mask = util . get_text_field_mask ( tokens ) [EOL] [EOL] embedded_verb_indicator = self . binary_feature_embedding ( targets . long ( ) ) [EOL] [comment] [EOL] [comment] [EOL] embedded_text_with_verb_indicator = torch . cat ( [ embedded_text_input , embedded_verb_indicator ] , - [number] ) [EOL] embedding_dim_with_binary_feature = embedded_text_with_verb_indicator . size ( ) [ [number] ] [EOL] [EOL] if self . stacked_encoder . get_input_dim ( ) != embedding_dim_with_binary_feature : [EOL] raise ConfigurationError ( [string] [string] [string] [string] ) [EOL] [EOL] encoded_text = self . stacked_encoder ( embedded_text_with_verb_indicator , text_mask ) [EOL] [EOL] batch_size , num_spans = tags . size ( ) [EOL] assert num_spans % self . max_span_width == [number] [EOL] tags = tags . view ( batch_size , - [number] , self . max_span_width ) [EOL] [EOL] span_starts = F . relu ( span_starts . float ( ) ) . long ( ) . view ( batch_size , - [number] ) [EOL] span_ends = F . relu ( span_ends . float ( ) ) . long ( ) . view ( batch_size , - [number] ) [EOL] target_index = F . relu ( target_index . float ( ) ) . long ( ) . view ( batch_size ) [EOL] [EOL] [comment] [EOL] span_embeddings = span_srl_util . compute_span_representations ( self . max_span_width , encoded_text , target_index , span_starts , span_ends , self . span_width_embedding , self . span_direction_embedding , self . span_distance_embedding , self . span_distance_bin , self . head_scorer ) [EOL] span_scores = self . span_feedforward ( span_embeddings ) [EOL] [EOL] [comment] [EOL] fn_args = [ ] [EOL] for extra_arg in [ [string] , [string] ] : [EOL] if extra_arg in kwargs and kwargs [ extra_arg ] is not None : [EOL] fn_args . append ( kwargs [ extra_arg ] ) [EOL] [EOL] if fn_args : [comment] [EOL] frame , valid_frame_elements = fn_args [EOL] output_dict = self . compute_srl_graph ( span_scores = span_scores , frame = frame , valid_frame_elements = valid_frame_elements , tags = tags , text_mask = text_mask , target_index = target_index ) [EOL] else : [comment] [EOL] if [string] in kwargs and kwargs [ [string] ] is not None : [EOL] span_mask = kwargs [ [string] ] [EOL] if [string] in kwargs and kwargs [ [string] ] is not None : [EOL] parent_tags = kwargs [ [string] ] [EOL] if self . unlabeled_constits : [EOL] not_a_constit = self . vocab . get_token_index ( [string] , self . constit_label_namespace ) [EOL] tags = ( tags != not_a_constit ) . float ( ) . view ( batch_size , - [number] , self . max_span_width ) [EOL] elif self . constit_label_namespace == [string] : [EOL] tags = parent_tags . view ( batch_size , - [number] , self . max_span_width ) [EOL] elif self . np_pp_constits : [EOL] tags = self . get_new_tags_np_pp ( tags , batch_size ) [EOL] output_dict = self . compute_constit_graph ( span_mask = span_mask , span_scores = span_scores , constit_tags = tags , text_mask = text_mask ) [EOL] [EOL] if self . fast_mode and not self . training : [EOL] output_dict [ [string] ] = Variable ( torch . FloatTensor ( [ [number] ] ) ) [EOL] [EOL] return output_dict [EOL] [EOL] def compute_srl_graph ( self , span_scores , frame , valid_frame_elements , tags , text_mask , target_index ) : [EOL] srl_logits = self . srl_arg_projection_layer ( span_scores ) [EOL] output_dict = { [string] : text_mask , [string] : srl_logits } [EOL] [EOL] batch_size = tags . size ( [number] ) [EOL] [comment] [EOL] tag_mask = span_srl_util . get_tag_mask ( self . num_srl_args , valid_frame_elements , batch_size ) [EOL] [EOL] [comment] [EOL] if not self . training or ( self . training and not self . fast_mode ) : [EOL] srl_prediction , srl_probabilites = self . semi_crf . viterbi_tags ( srl_logits , text_mask , tag_masks = tag_mask ) [EOL] output_dict [ [string] ] = srl_prediction [EOL] output_dict [ [string] ] = srl_probabilites [EOL] [EOL] frames = [ self . vocab . get_token_from_index ( f [ [number] ] , [string] ) for f in frame [ [string] ] . data . tolist ( ) ] [EOL] srl_prediction = srl_prediction . view ( batch_size , - [number] , self . max_span_width ) [EOL] self . metrics [ [string] ] ( predictions = srl_prediction , gold_labels = tags , mask = text_mask , frames = frames , target_indices = target_index ) [EOL] [EOL] [comment] [EOL] if self . training or ( not self . training and not self . fast_mode ) : [EOL] if tags is not None : [EOL] srl_log_likelihood , _ = self . semi_crf ( srl_logits , tags , text_mask , tag_mask = tag_mask ) [EOL] output_dict [ [string] ] = - srl_log_likelihood [EOL] [EOL] return output_dict [EOL] [EOL] def compute_constit_graph ( self , span_scores , span_mask , constit_tags , text_mask ) : [EOL] batch_size = text_mask . size ( [number] ) [EOL] [comment] [EOL] constit_logits = self . constit_arg_projection_layer ( span_scores ) [EOL] output_dict = { [string] : text_mask , [string] : constit_logits } [EOL] [EOL] [comment] [EOL] if not self . training or ( self . training and not self . fast_mode ) : [EOL] reshaped_log_probs = constit_logits . view ( - [number] , self . num_constit_tags ) [EOL] constit_probabilities = F . softmax ( reshaped_log_probs , dim = - [number] ) . view ( batch_size , - [number] , self . num_constit_tags ) [EOL] constit_predictions = constit_probabilities . max ( - [number] ) [ [number] ] [EOL] output_dict [ [string] ] = constit_probabilities [EOL] self . metrics [ [string] ] ( predictions = constit_predictions . view ( batch_size , - [number] , self . max_span_width ) , gold_labels = constit_tags , mask = text_mask ) [EOL] [EOL] [comment] [EOL] if self . training or ( not self . training and not self . fast_mode ) : [EOL] if constit_tags is not None : [EOL] [comment] [EOL] flat_tags = constit_tags . view ( batch_size , - [number] ) [EOL] cross_entropy_loss = util . sequence_cross_entropy_with_logits ( constit_logits , flat_tags , span_mask ) [EOL] output_dict [ [string] ] = cross_entropy_loss [EOL] [EOL] return output_dict [EOL] [EOL] def get_new_tags_np_pp ( self , tags , batch_size ) : [EOL] not_a_constit = self . vocab . get_token_index ( [string] , self . constit_label_namespace ) [EOL] np_constit = self . vocab . get_token_index ( [string] , self . constit_label_namespace ) [EOL] pp_constit = self . vocab . get_token_index ( [string] , self . constit_label_namespace ) [EOL] [EOL] other_tags = ( tags != not_a_constit ) & ( tags != np_constit ) & ( tags != pp_constit ) [EOL] np_pp_tags = ( tags == np_constit ) | ( tags == pp_constit ) [EOL] non_constit_tags = ( tags == not_a_constit ) [EOL] all_tags = [number] * non_constit_tags + [number] * np_pp_tags + [number] * other_tags [EOL] return all_tags . float ( ) . view ( batch_size , - [number] , self . max_span_width ) [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] tag_matrices = output_dict [ [string] ] [EOL] sequence_lengths = util . get_lengths_from_binary_sequence_mask ( output_dict [ [string] ] ) . data . tolist ( ) [EOL] tag_matrix_list = tag_matrices . cpu ( ) . tolist ( ) [EOL] [comment] [EOL] [EOL] all_tag_sequences = [ ] [EOL] for tag_matrix , sent_len in zip ( tag_matrix_list , sequence_lengths ) : [EOL] tag_sequence = self . convert_spans_into_sequence_of_tags ( tag_matrix , sent_len ) [EOL] all_tag_sequences . append ( tag_sequence ) [EOL] [EOL] output_dict [ [string] ] = all_tag_sequences [EOL] return output_dict [EOL] [EOL] def convert_spans_into_sequence_of_tags ( self , tag_matrix , sentence_length ) : [EOL] tag_sequence = [ self . outside_span_tag for _ in range ( sentence_length ) ] [EOL] end_idx = [number] [EOL] for span_list in tag_matrix : [EOL] if end_idx > sentence_length : [EOL] break [EOL] diff = [number] [EOL] assert len ( span_list ) == self . max_span_width [EOL] for span_tag in span_list : [EOL] if diff > end_idx : [EOL] break [EOL] if span_tag == self . not_a_span_tag : [EOL] continue [EOL] start_idx = end_idx - diff [EOL] for position in range ( start_idx , end_idx + [number] ) : [EOL] [comment] [EOL] assert tag_sequence [ position ] == self . outside_span_tag [EOL] tag_sequence [ position ] = span_tag [EOL] diff += [number] [EOL] end_idx += [number] [EOL] return tag_sequence [EOL] [EOL] def get_metrics ( self , reset = False ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] metric_dict = self . metrics [ [string] ] . get_metric ( reset = reset ) [EOL] return metric_dict [EOL] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [EOL] embedder_params = params . pop ( [string] ) [EOL] text_field_embedder = TextFieldEmbedder . from_params ( vocab , embedder_params ) [EOL] stacked_encoder = Seq2SeqEncoder . from_params ( params . pop ( [string] ) ) [EOL] span_feedforward = FeedForward . from_params ( params . pop ( [string] ) ) [EOL] binary_feature_dim = params . pop ( [string] ) [EOL] max_span_width = params . pop ( [string] ) [EOL] binary_feature_size = params . pop ( [string] ) [EOL] distance_feature_size = params . pop ( [string] , [number] ) [EOL] ontology_path = params . pop ( [string] ) [EOL] fast_mode = params . pop ( [string] , True ) [EOL] loss_type = params . pop ( [string] , [string] ) [EOL] srl_label_namespace = params . pop ( [string] , [string] ) [EOL] constit_label_namespace = params . pop ( [string] , [string] ) [EOL] unlabeled_constits = params . pop ( [string] , False ) [EOL] np_pp_constits = params . pop ( [string] , False ) [EOL] initializer = InitializerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] regularizer = RegularizerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] [EOL] return cls ( vocab = vocab , text_field_embedder = text_field_embedder , stacked_encoder = stacked_encoder , binary_feature_dim = binary_feature_dim , span_feedforward = span_feedforward , max_span_width = max_span_width , ontology_path = ontology_path , binary_feature_size = binary_feature_size , distance_feature_size = distance_feature_size , srl_label_namespace = srl_label_namespace , constit_label_namespace = constit_label_namespace , unlabeled_constits = unlabeled_constits , np_pp_constits = np_pp_constits , fast_mode = fast_mode , loss_type = loss_type , initializer = initializer , regularizer = regularizer ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 $'ScaffoldedFrameSrl'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from allennlp . models . span_srl . semi_crf_srl import SemiCrfSemanticRoleLabeler [EOL] from allennlp . models . span_srl . frame_semi_crf_srl import FrameSemanticRoleLabeler [EOL] from allennlp . models . span_srl . scaffolding . constit_labeler import ConstitLabeler [EOL] from allennlp . models . span_srl . scaffolded_frame_srl import ScaffoldedFrameSrl	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Dict , List [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , TextIO , Optional [EOL] [EOL] import torch [EOL] from torch . autograd import Variable [EOL] from torch . nn . modules import Linear , Dropout [EOL] import torch . nn . functional as F [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . span_srl import span_srl_util [EOL] from allennlp . modules import Seq2SeqEncoder , FeedForward , TimeDistributed , TextFieldEmbedder , SemiMarkovConditionalRandomField [EOL] from allennlp . modules . token_embedders import Embedding [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] from allennlp . nn import util [EOL] from allennlp . training . metrics import NonBioSpanBasedF1Measure [EOL] [EOL] [EOL] @ Model . register ( [string] ) class SemiCrfSemanticRoleLabeler ( Model ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vocab , text_field_embedder , stacked_encoder , span_feedforward , binary_feature_dim , max_span_width , binary_feature_size , distance_feature_size , embedding_dropout = [number] , label_namespace = [string] , fast_mode = False , loss_type = [string] , initializer = InitializerApplicator ( ) , regularizer = None ) : [EOL] super ( SemiCrfSemanticRoleLabeler , self ) . __init__ ( vocab , regularizer ) [EOL] [EOL] [comment] [EOL] self . text_field_embedder = text_field_embedder [EOL] self . embedding_dropout = Dropout ( p = embedding_dropout ) [EOL] [comment] [EOL] self . binary_feature_embedding = Embedding ( [number] , binary_feature_dim ) [EOL] self . stacked_encoder = stacked_encoder [EOL] if text_field_embedder . get_output_dim ( ) + binary_feature_dim != stacked_encoder . get_input_dim ( ) : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] [EOL] [comment] [EOL] self . max_span_width = max_span_width [EOL] self . span_width_embedding = Embedding ( max_span_width , binary_feature_size ) [EOL] [comment] [EOL] self . span_distance_bin = [number] [EOL] self . span_distance_embedding = Embedding ( self . span_distance_bin , distance_feature_size ) [EOL] self . span_direction_embedding = Embedding ( [number] , binary_feature_size ) [EOL] self . span_feedforward = TimeDistributed ( span_feedforward ) [EOL] self . head_scorer = TimeDistributed ( torch . nn . Linear ( stacked_encoder . get_output_dim ( ) , [number] ) ) [EOL] [EOL] self . num_classes = self . vocab . get_vocab_size ( label_namespace ) [EOL] self . not_a_span_tag = self . vocab . get_token_index ( [string] , label_namespace ) [EOL] self . outside_span_tag = self . vocab . get_token_index ( [string] , label_namespace ) [EOL] self . semi_crf = SemiMarkovConditionalRandomField ( num_tags = self . num_classes , max_span_width = max_span_width , loss_type = loss_type , default_tag = self . not_a_span_tag , outside_span_tag = self . outside_span_tag ) [EOL] [EOL] [comment] [EOL] self . tag_projection_layer = TimeDistributed ( Linear ( span_feedforward . get_output_dim ( ) , self . num_classes ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . non_bio_span_metric = NonBioSpanBasedF1Measure ( vocab , tag_namespace = label_namespace , ignore_classes = [ [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] self . fast_mode = fast_mode [EOL] initializer ( self ) [EOL] [EOL] def forward ( self , tokens , verb_indicator , target_index , span_starts , span_ends , tags = None ) : [EOL] [comment] [EOL] [docstring] [EOL] embedded_text_input = self . embedding_dropout ( self . text_field_embedder ( tokens ) ) [EOL] text_mask = util . get_text_field_mask ( tokens ) [EOL] embedded_verb_indicator = self . binary_feature_embedding ( verb_indicator . long ( ) ) [EOL] [comment] [EOL] [comment] [EOL] embedded_text_with_verb_indicator = torch . cat ( [ embedded_text_input , embedded_verb_indicator ] , - [number] ) [EOL] embedding_dim_with_binary_feature = embedded_text_with_verb_indicator . size ( ) [ [number] ] [EOL] [EOL] if self . stacked_encoder . get_input_dim ( ) != embedding_dim_with_binary_feature : [EOL] raise ConfigurationError ( [string] [string] [string] [string] ) [EOL] [EOL] encoded_text = self . stacked_encoder ( embedded_text_with_verb_indicator , text_mask ) [EOL] [EOL] batch_size , num_spans = tags . size ( ) [EOL] assert num_spans % self . max_span_width == [number] [EOL] tags = tags . view ( batch_size , - [number] , self . max_span_width ) [EOL] [EOL] span_starts = F . relu ( span_starts . float ( ) ) . long ( ) . view ( batch_size , - [number] ) [EOL] span_ends = F . relu ( span_ends . float ( ) ) . long ( ) . view ( batch_size , - [number] ) [EOL] target_index = F . relu ( target_index . float ( ) ) . long ( ) . view ( batch_size ) [EOL] [comment] [EOL] span_embeddings = span_srl_util . compute_span_representations ( self . max_span_width , encoded_text , target_index , span_starts , span_ends , self . span_width_embedding , self . span_direction_embedding , self . span_distance_embedding , self . span_distance_bin , self . head_scorer ) [EOL] span_scores = self . span_feedforward ( span_embeddings ) [EOL] [EOL] logits = self . tag_projection_layer ( span_scores ) [EOL] output_dict = { [string] : logits , [string] : text_mask } [EOL] [EOL] [comment] [EOL] if not self . training or ( self . training and not self . fast_mode ) : [EOL] predicted_tags , class_probabilities = self . semi_crf . viterbi_tags ( logits , text_mask ) [EOL] output_dict [ [string] ] = predicted_tags [EOL] output_dict [ [string] ] = class_probabilities [EOL] self . non_bio_span_metric ( predictions = predicted_tags . view ( batch_size , - [number] , self . max_span_width ) , gold_labels = tags , mask = text_mask ) [EOL] [EOL] [comment] [EOL] if self . training or ( not self . training and not self . fast_mode ) : [EOL] if tags is not None : [EOL] log_likelihood , _ = self . semi_crf ( logits , tags , mask = text_mask ) [EOL] output_dict [ [string] ] = - log_likelihood [EOL] if self . fast_mode and not self . training : [EOL] output_dict [ [string] ] = Variable ( torch . FloatTensor ( [ [number] ] ) ) [EOL] [EOL] return output_dict [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] tag_matrices = output_dict [ [string] ] [EOL] sequence_lengths = util . get_lengths_from_binary_sequence_mask ( output_dict [ [string] ] ) . data . tolist ( ) [EOL] tag_matrix_list = tag_matrices . cpu ( ) . tolist ( ) [EOL] [comment] [EOL] [EOL] all_tag_sequences = [ ] [EOL] for tag_matrix , sent_len in zip ( tag_matrix_list , sequence_lengths ) : [EOL] tag_sequence = self . convert_spans_into_sequence_of_tags ( tag_matrix , sent_len ) [EOL] all_tag_sequences . append ( tag_sequence ) [EOL] [EOL] output_dict [ [string] ] = all_tag_sequences [EOL] return output_dict [EOL] [EOL] def convert_spans_into_sequence_of_tags ( self , tag_matrix , sentence_length ) : [EOL] tag_sequence = [ self . outside_span_tag for _ in range ( sentence_length ) ] [EOL] end_idx = [number] [EOL] for span_list in tag_matrix : [EOL] if end_idx > sentence_length : [EOL] break [EOL] diff = [number] [EOL] assert len ( span_list ) == self . max_span_width [EOL] for span_tag in span_list : [EOL] if diff > end_idx : [EOL] break [EOL] if span_tag == self . not_a_span_tag : [EOL] continue [EOL] start_idx = end_idx - diff [EOL] for position in range ( start_idx , end_idx + [number] ) : [EOL] [comment] [EOL] assert tag_sequence [ position ] == self . outside_span_tag [EOL] tag_sequence [ position ] = span_tag [EOL] diff += [number] [EOL] end_idx += [number] [EOL] return tag_sequence [EOL] [EOL] def get_metrics ( self , reset = False ) : [EOL] metric_dict = self . non_bio_span_metric . get_metric ( reset = reset ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return metric_dict [EOL] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [EOL] embedder_params = params . pop ( [string] ) [EOL] text_field_embedder = TextFieldEmbedder . from_params ( vocab , embedder_params ) [EOL] stacked_encoder = Seq2SeqEncoder . from_params ( params . pop ( [string] ) ) [EOL] span_feedforward = FeedForward . from_params ( params . pop ( [string] ) ) [EOL] binary_feature_dim = params . pop ( [string] ) [EOL] max_span_width = params . pop ( [string] ) [EOL] binary_feature_size = params . pop ( [string] ) [EOL] distance_feature_size = params . pop ( [string] , [number] ) [EOL] fast_mode = params . pop ( [string] , True ) [EOL] loss_type = params . pop ( [string] , [string] ) [EOL] label_namespace = params . pop ( [string] , [string] ) [EOL] initializer = InitializerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] regularizer = RegularizerApplicator . from_params ( params . pop ( [string] , [ ] ) ) [EOL] [EOL] return cls ( vocab = vocab , text_field_embedder = text_field_embedder , stacked_encoder = stacked_encoder , binary_feature_dim = binary_feature_dim , span_feedforward = span_feedforward , max_span_width = max_span_width , binary_feature_size = binary_feature_size , distance_feature_size = distance_feature_size , label_namespace = label_namespace , loss_type = loss_type , fast_mode = fast_mode , initializer = initializer , regularizer = regularizer ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $'SemiCrfSemanticRoleLabeler'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0
from allennlp . models . span_srl . scaffolding . constit_labeler import ConstitLabeler [EOL] from allennlp . models . span_srl . scaffolding . scaffolded_pb_srl import ScaffoldedPropBankSrl	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . models . reading_comprehension . bidaf import BidirectionalAttentionFlow [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . nn . activations import Activation [EOL] from allennlp . nn . initializers import Initializer , InitializerApplicator [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class Regularizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __call__ ( self , parameter ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0
from typing import Sequence , Union , Any , List [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] from typing import Sequence , Union [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . nn import Activation [EOL] [EOL] [EOL] class FeedForward ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , input_dim , num_layers , hidden_dims , activations , dropout = [number] ) : [EOL] super ( FeedForward , self ) . __init__ ( ) [EOL] if not isinstance ( hidden_dims , list ) : [EOL] hidden_dims = [ hidden_dims ] * num_layers [comment] [EOL] if not isinstance ( activations , list ) : [EOL] activations = [ activations ] * num_layers [comment] [EOL] if not isinstance ( dropout , list ) : [EOL] dropout = [ dropout ] * num_layers [comment] [EOL] if len ( hidden_dims ) != num_layers : [EOL] raise ConfigurationError ( [string] % ( len ( hidden_dims ) , num_layers ) ) [EOL] if len ( activations ) != num_layers : [EOL] raise ConfigurationError ( [string] % ( len ( activations ) , num_layers ) ) [EOL] if len ( dropout ) != num_layers : [EOL] raise ConfigurationError ( [string] % ( len ( dropout ) , num_layers ) ) [EOL] self . _activations = activations [EOL] input_dims = [ input_dim ] + hidden_dims [ : - [number] ] [EOL] linear_layers = [ ] [EOL] for layer_input_dim , layer_output_dim in zip ( input_dims , hidden_dims ) : [EOL] linear_layers . append ( torch . nn . Linear ( layer_input_dim , layer_output_dim ) ) [EOL] self . _linear_layers = torch . nn . ModuleList ( linear_layers ) [EOL] dropout_layers = [ torch . nn . Dropout ( p = value ) for value in dropout ] [EOL] self . _dropout = torch . nn . ModuleList ( dropout_layers ) [EOL] self . _output_dim = hidden_dims [ - [number] ] [EOL] self . input_dim = input_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _output_dim [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . input_dim [EOL] [EOL] def forward ( self , inputs ) : [EOL] [comment] [EOL] output = inputs [EOL] for layer , activation , dropout in zip ( self . _linear_layers , self . _activations , self . _dropout ) : [EOL] output = dropout ( activation ( layer ( output ) ) ) [EOL] return output [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] input_dim = params . pop ( [string] ) [EOL] num_layers = params . pop ( [string] ) [EOL] hidden_dims = params . pop ( [string] ) [EOL] activations = params . pop ( [string] ) [EOL] dropout = params . pop ( [string] , [number] ) [EOL] if isinstance ( activations , list ) : [EOL] activations = [ Activation . by_name ( name ) ( ) for name in activations ] [EOL] else : [EOL] activations = Activation . by_name ( activations ) ( ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( input_dim = input_dim , num_layers = num_layers , hidden_dims = hidden_dims , activations = activations , dropout = dropout ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $builtins.int$ 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Union[typing.Sequence[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] [EOL] class SimilarityFunction ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def forward ( self , tensor_1 , tensor_2 ) : [EOL] [comment] [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] choice = params . pop_choice ( [string] , cls . list_available ( ) , default_to_first_choice = True ) [EOL] return cls . by_name ( choice ) . from_params ( params ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'SimilarityFunction'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.common.Params$ 0 0
from typing import Any , Dict , List [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Dict [EOL] [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] [EOL] @ TextFieldEmbedder . register ( [string] ) class BasicTextFieldEmbedder ( TextFieldEmbedder ) : [EOL] [docstring] [EOL] def __init__ ( self , token_embedders ) : [EOL] super ( BasicTextFieldEmbedder , self ) . __init__ ( ) [EOL] self . _token_embedders = token_embedders [EOL] for key , embedder in token_embedders . items ( ) : [EOL] name = [string] % key [EOL] self . add_module ( name , embedder ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] output_dim = [number] [EOL] for embedder in self . _token_embedders . values ( ) : [EOL] output_dim += embedder . get_output_dim ( ) [EOL] return output_dim [EOL] [EOL] def forward ( self , text_field_input ) : [EOL] if self . _token_embedders . keys ( ) != text_field_input . keys ( ) : [EOL] message = [string] % ( str ( self . _token_embedders . keys ( ) ) , str ( text_field_input . keys ( ) ) ) [EOL] raise ConfigurationError ( message ) [EOL] embedded_representations = [ ] [EOL] keys = sorted ( text_field_input . keys ( ) ) [EOL] for key in keys : [EOL] tensor = text_field_input [ key ] [EOL] embedder = self . _token_embedders [ key ] [EOL] token_vectors = embedder ( tensor ) [EOL] embedded_representations . append ( token_vectors ) [EOL] return torch . cat ( embedded_representations , dim = - [number] ) [EOL] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [EOL] token_embedders = { } [EOL] keys = list ( params . keys ( ) ) [EOL] for key in keys : [EOL] embedder_params = params . pop ( key ) [EOL] token_embedders [ key ] = TokenEmbedder . from_params ( vocab , embedder_params ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( token_embedders ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $torch.Tensor$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $'BasicTextFieldEmbedder'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $allennlp.common.Params$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $allennlp.data.Vocabulary$ 0 $typing.Any$ 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0
[docstring] [EOL] [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . text_field_embedders . basic_text_field_embedder import BasicTextFieldEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import TypeVar , Any , List , Tuple , Callable , Union , Dict , Type [EOL] import allennlp [EOL] import typing [EOL] import spacy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from itertools import zip_longest [EOL] from typing import Any , Callable , Dict , List , Tuple , TypeVar , Union [EOL] import random [EOL] [EOL] import torch [EOL] import numpy [EOL] import spacy [EOL] from spacy . language import Language as SpacyModelType [EOL] [EOL] from allennlp . common . checks import log_pytorch_version_info [EOL] from allennlp . common . params import Params [EOL] [EOL] JsonDict = Dict [ str , Any ] [comment] [EOL] [EOL] def sanitize ( x ) : [comment] [EOL] [docstring] [EOL] if isinstance ( x , ( str , float , int , bool ) ) : [EOL] [comment] [EOL] return x [EOL] elif isinstance ( x , torch . autograd . Variable ) : [EOL] return sanitize ( x . data ) [EOL] elif isinstance ( x , torch . _TensorBase ) : [comment] [EOL] [comment] [EOL] return x . cpu ( ) . tolist ( ) [EOL] elif isinstance ( x , numpy . ndarray ) : [EOL] [comment] [EOL] return x . tolist ( ) [EOL] elif isinstance ( x , numpy . number ) : [EOL] [comment] [EOL] return x . item ( ) [EOL] elif isinstance ( x , dict ) : [EOL] [comment] [EOL] return { key : sanitize ( value ) for key , value in x . items ( ) } [EOL] elif isinstance ( x , ( list , tuple ) ) : [EOL] [comment] [EOL] return [ sanitize ( x_i ) for x_i in x ] [EOL] else : [EOL] raise ValueError ( [string] . format ( x , type ( x ) ) ) [EOL] [EOL] def group_by_count ( iterable , count , default_value ) : [EOL] [docstring] [EOL] return [ list ( l ) for l in zip_longest ( * [ iter ( iterable ) ] * count , fillvalue = default_value ) ] [EOL] [EOL] [EOL] def pad_sequence_to_length ( sequence , desired_length , default_value = lambda : [number] , padding_on_right = True ) : [EOL] [docstring] [EOL] [comment] [EOL] if padding_on_right : [EOL] padded_sequence = sequence [ : desired_length ] [EOL] else : [EOL] padded_sequence = sequence [ - desired_length : ] [EOL] [comment] [EOL] for _ in range ( desired_length - len ( padded_sequence ) ) : [EOL] if padding_on_right : [EOL] padded_sequence . append ( default_value ( ) ) [EOL] else : [EOL] padded_sequence . insert ( [number] , default_value ( ) ) [EOL] return padded_sequence [EOL] [EOL] [EOL] A = TypeVar ( [string] ) [EOL] def add_noise_to_dict_values ( dictionary , noise_param ) : [EOL] [docstring] [EOL] new_dict = { } [EOL] for key , value in dictionary . items ( ) : [EOL] noise_value = value * noise_param [EOL] noise = random . uniform ( - noise_value , noise_value ) [EOL] new_dict [ key ] = value + noise [EOL] return new_dict [EOL] [EOL] [EOL] def namespace_match ( pattern , namespace ) : [EOL] [docstring] [EOL] if pattern [ [number] ] == [string] and namespace . endswith ( pattern [ [number] : ] ) : [EOL] return True [EOL] elif pattern == namespace : [EOL] return True [EOL] return False [EOL] [EOL] [EOL] def prepare_environment ( params ) : [EOL] [docstring] [EOL] seed = params . pop ( [string] , [number] ) [EOL] numpy_seed = params . pop ( [string] , [number] ) [EOL] torch_seed = params . pop ( [string] , [number] ) [EOL] [EOL] if seed is not None : [EOL] random . seed ( seed ) [EOL] if numpy_seed is not None : [EOL] numpy . random . seed ( numpy_seed ) [EOL] if torch_seed is not None : [EOL] torch . manual_seed ( torch_seed ) [EOL] [comment] [EOL] if torch . cuda . is_available ( ) : [EOL] torch . cuda . manual_seed_all ( torch_seed ) [EOL] [EOL] log_pytorch_version_info ( ) [EOL] [EOL] [EOL] LOADED_SPACY_MODELS = { } [EOL] [EOL] [EOL] def get_spacy_model ( spacy_model_name , pos_tags , parse , ner ) : [EOL] [docstring] [EOL] options = ( spacy_model_name , pos_tags , parse , ner ) [EOL] if options not in LOADED_SPACY_MODELS : [EOL] disable = [ [string] , [string] ] [EOL] if not pos_tags : [EOL] disable . append ( [string] ) [EOL] if not parse : [EOL] disable . append ( [string] ) [EOL] if not ner : [EOL] disable . append ( [string] ) [EOL] spacy_model = spacy . load ( spacy_model_name , disable = disable ) [EOL] LOADED_SPACY_MODELS [ options ] = spacy_model [EOL] return LOADED_SPACY_MODELS [ options ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[A,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $spacy.language.Language$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Tuple , Any [EOL] import urllib [EOL] import typing [EOL] import logging [EOL] import requests [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple [EOL] import os [EOL] import base64 [EOL] import logging [EOL] import shutil [EOL] import tempfile [EOL] from urllib . parse import urlparse [EOL] [EOL] import requests [EOL] import tqdm [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] CACHE_ROOT = os . path . expanduser ( os . path . join ( [string] , [string] ) ) [EOL] DATASET_CACHE = os . path . join ( CACHE_ROOT , [string] ) [EOL] [EOL] def url_to_filename ( url , etag = None ) : [EOL] [docstring] [EOL] url_bytes = url . encode ( [string] ) [EOL] b64_bytes = base64 . b64encode ( url_bytes ) [EOL] decoded = b64_bytes . decode ( [string] ) [EOL] [EOL] if etag : [EOL] [comment] [EOL] etag = etag . replace ( [string] , [string] ) [EOL] return f"{ decoded } [string] { etag }" [EOL] else : [EOL] return decoded [EOL] [EOL] def filename_to_url ( filename ) : [EOL] [docstring] [EOL] try : [EOL] [comment] [EOL] decoded , etag = filename . split ( [string] , [number] ) [EOL] except ValueError : [EOL] [comment] [EOL] decoded , etag = filename , None [EOL] [EOL] filename_bytes = decoded . encode ( [string] ) [EOL] url_bytes = base64 . b64decode ( filename_bytes ) [EOL] return url_bytes . decode ( [string] ) , etag [EOL] [EOL] def cached_path ( url_or_filename , cache_dir = None ) : [EOL] [docstring] [EOL] if cache_dir is None : [EOL] cache_dir = DATASET_CACHE [EOL] [EOL] parsed = urlparse ( url_or_filename ) [EOL] [EOL] if parsed . scheme in ( [string] , [string] ) : [EOL] [comment] [EOL] return get_from_cache ( url_or_filename , cache_dir ) [EOL] elif parsed . scheme == [string] and os . path . exists ( url_or_filename ) : [EOL] [comment] [EOL] return url_or_filename [EOL] elif parsed . scheme == [string] : [EOL] [comment] [EOL] raise FileNotFoundError ( [string] . format ( url_or_filename ) ) [EOL] else : [EOL] [comment] [EOL] raise ValueError ( [string] . format ( url_or_filename ) ) [EOL] [EOL] [EOL] [comment] [EOL] def get_from_cache ( url , cache_dir = None ) : [EOL] [docstring] [EOL] if cache_dir is None : [EOL] cache_dir = DATASET_CACHE [EOL] [EOL] os . makedirs ( cache_dir , exist_ok = True ) [EOL] [EOL] [comment] [EOL] response = requests . head ( url ) [EOL] if response . status_code != [number] : [EOL] raise IOError ( [string] . format ( url ) ) [EOL] [EOL] [comment] [EOL] etag = response . headers . get ( [string] ) [EOL] filename = url_to_filename ( url , etag ) [EOL] [EOL] [comment] [EOL] cache_path = os . path . join ( cache_dir , filename ) [EOL] [EOL] if not os . path . exists ( cache_path ) : [EOL] [comment] [EOL] [comment] [EOL] _ , temp_filename = tempfile . mkstemp ( ) [EOL] logger . info ( [string] , url , temp_filename ) [EOL] [EOL] [comment] [EOL] req = requests . get ( url , stream = True ) [EOL] content_length = req . headers . get ( [string] ) [EOL] total = int ( content_length ) if content_length is not None else None [EOL] progress = tqdm . tqdm ( unit = [string] , total = total ) [EOL] with open ( temp_filename , [string] ) as temp_file : [EOL] for chunk in req . iter_content ( chunk_size = [number] ) : [EOL] if chunk : [comment] [EOL] progress . update ( len ( chunk ) ) [EOL] temp_file . write ( chunk ) [EOL] [EOL] progress . close ( ) [EOL] [EOL] logger . info ( [string] , temp_filename , cache_path ) [EOL] shutil . copyfile ( temp_filename , cache_path ) [EOL] logger . info ( [string] , temp_filename ) [EOL] os . remove ( temp_filename ) [EOL] [EOL] return cache_path [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import logging [EOL] [docstring] [EOL] [EOL] import logging [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class ConfigurationError ( Exception ) : [EOL] [docstring] [EOL] def __init__ ( self , message ) : [EOL] super ( ConfigurationError , self ) . __init__ ( ) [EOL] self . message = message [EOL] [EOL] def __str__ ( self ) : [EOL] return repr ( self . message ) [EOL] [EOL] [EOL] def log_pytorch_version_info ( ) : [EOL] import torch [EOL] logger . info ( [string] , torch . __version__ ) [EOL]	0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0
from typing import TextIO , Any [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import TextIO [EOL] import os [EOL] [EOL] [EOL] class TeeLogger : [EOL] [docstring] [EOL] def __init__ ( self , filename , terminal ) : [EOL] self . terminal = terminal [EOL] parent_directory = os . path . dirname ( filename ) [EOL] os . makedirs ( parent_directory , exist_ok = True ) [EOL] self . log = open ( filename , [string] ) [EOL] [EOL] def write ( self , message ) : [EOL] self . terminal . write ( message ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if [string] in message : [EOL] message = message . replace ( [string] , [string] ) [EOL] if not message or message [ - [number] ] != [string] : [EOL] message += [string] [EOL] self . log . write ( message ) [EOL] [EOL] def flush ( self ) : [EOL] self . terminal . flush ( ) [EOL] self . log . flush ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $typing.TextIO$ 0 0 0 0 0 $typing.TextIO$ 0 $typing.TextIO$ 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . common . params import Params [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . common . util import JsonDict [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import argparse [EOL] import builtins [EOL] [docstring] [EOL] import argparse [EOL] [EOL] class Subcommand : [EOL] [docstring] [EOL] def add_subparser ( self , name , parser ) : [EOL] [comment] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $builtins.str$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0
from typing import Any , List , Set , TextIO , Dict [EOL] import typing [EOL] import logging [EOL] import allennlp [EOL] import argparse [EOL] import builtins [EOL] [docstring] [EOL] from typing import Dict [EOL] import argparse [EOL] import json [EOL] import logging [EOL] import os [EOL] import random [EOL] import sys [EOL] from copy import deepcopy [EOL] [EOL] from allennlp . commands . evaluate import evaluate [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . tee_logger import TeeLogger [EOL] from allennlp . common . util import prepare_environment [EOL] from allennlp . data import Dataset , Vocabulary [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . models . archival import archive_model [EOL] from allennlp . models . model import Model [EOL] from allennlp . training . multitask_trainer import MultiTaskTrainer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] class TrainMultitask ( Subcommand ) : [EOL] def add_subparser ( self , name , parser ) : [EOL] [comment] [EOL] description = [string] [EOL] subparser = parser . add_parser ( name , description = description , help = [string] ) [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] [comment] [EOL] serialization = subparser . add_mutually_exclusive_group ( required = True ) [EOL] serialization . add_argument ( [string] , [string] , type = str , help = [string] ) [EOL] serialization . add_argument ( [string] , type = str , help = argparse . SUPPRESS ) [EOL] [EOL] subparser . set_defaults ( func = train_model_from_args ) [EOL] [EOL] return subparser [EOL] [EOL] [EOL] def train_model_from_args ( args ) : [EOL] [docstring] [EOL] train_model_from_file ( args . param_path , args . serialization_dir ) [EOL] [EOL] [EOL] def train_model_from_file ( parameter_filename , serialization_dir ) : [EOL] [docstring] [EOL] [comment] [EOL] params = Params . from_file ( parameter_filename ) [EOL] return train_model ( params , serialization_dir ) [EOL] [EOL] [EOL] def train_model ( params , serialization_dir ) : [EOL] [docstring] [EOL] prepare_environment ( params ) [EOL] [EOL] os . makedirs ( serialization_dir , exist_ok = True ) [EOL] sys . stdout = TeeLogger ( os . path . join ( serialization_dir , [string] ) , sys . stdout ) [comment] [EOL] sys . stderr = TeeLogger ( os . path . join ( serialization_dir , [string] ) , sys . stderr ) [comment] [EOL] handler = logging . FileHandler ( os . path . join ( serialization_dir , [string] ) ) [EOL] handler . setLevel ( logging . INFO ) [EOL] handler . setFormatter ( logging . Formatter ( [string] ) ) [EOL] logging . getLogger ( ) . addHandler ( handler ) [EOL] serialization_params = deepcopy ( params ) . as_dict ( quiet = True ) [EOL] with open ( os . path . join ( serialization_dir , [string] ) , [string] ) as param_file : [EOL] json . dump ( serialization_params , param_file , indent = [number] ) [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] dataset_reader = DatasetReader . from_params ( params . pop ( [string] ) ) [EOL] train_data_path = params . pop ( [string] ) [EOL] logger . info ( [string] , train_data_path ) [EOL] train_data = dataset_reader . read ( train_data_path ) [EOL] [EOL] [comment] [EOL] dataset_reader_aux = DatasetReader . from_params ( params . pop ( [string] ) ) [EOL] train_data_path_aux = params . pop ( [string] ) [EOL] logger . info ( [string] , train_data_path_aux ) [EOL] train_data_aux = dataset_reader_aux . read ( train_data_path_aux ) [EOL] [EOL] [comment] [EOL] aux_sample_fraction = params . pop ( [string] , [number] ) [EOL] if aux_sample_fraction < [number] : [EOL] sample_size = int ( aux_sample_fraction * len ( train_data_aux . instances ) ) [EOL] train_data_aux = Dataset ( random . sample ( train_data_aux . instances , sample_size ) ) [EOL] [EOL] [comment] [EOL] train_size = len ( train_data . instances ) [EOL] aux_train_size = len ( train_data_aux . instances ) [EOL] mixing_ratio = params . pop ( [string] ) [EOL] [comment] [EOL] [EOL] if train_size > aux_train_size : [comment] [EOL] difference = train_size - aux_train_size [EOL] aux_sample = [ random . choice ( train_data_aux . instances ) for _ in range ( difference ) ] [EOL] train_data_aux = Dataset ( train_data_aux . instances + aux_sample ) [EOL] logger . info ( [string] . format ( aux_train_size , len ( train_data_aux . instances ) ) ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] all_datasets = { [string] : train_data } [EOL] all_datasets_aux = { [string] : train_data_aux } [EOL] [EOL] [comment] [EOL] validation_data_path = params . pop ( [string] , None ) [EOL] if validation_data_path is not None : [EOL] logger . info ( [string] , validation_data_path ) [EOL] validation_data = dataset_reader . read ( validation_data_path ) [EOL] all_datasets [ [string] ] = validation_data [EOL] else : [EOL] validation_data = None [EOL] [EOL] [comment] [EOL] validation_data_path_aux = params . pop ( [string] , None ) [EOL] if validation_data_path_aux is not None : [EOL] logger . info ( [string] , validation_data_path_aux ) [EOL] validation_data_aux = dataset_reader_aux . read ( validation_data_path_aux ) [EOL] all_datasets_aux [ [string] ] = validation_data_aux [EOL] else : [EOL] validation_data_aux = None [EOL] [EOL] [comment] [EOL] test_data_path = params . pop ( [string] , None ) [EOL] if test_data_path is not None : [EOL] logger . info ( [string] , test_data_path ) [EOL] test_data = dataset_reader . read ( test_data_path ) [EOL] all_datasets [ [string] ] = test_data [EOL] else : [EOL] test_data = None [EOL] [EOL] [comment] [EOL] test_data_path_aux = params . pop ( [string] , None ) [EOL] if test_data_path_aux is not None : [EOL] logger . info ( [string] , test_data_path_aux ) [EOL] test_data_aux = dataset_reader_aux . read ( test_data_path_aux ) [EOL] all_datasets_aux [ [string] ] = test_data_aux [EOL] else : [EOL] test_data_aux = None [EOL] [EOL] datasets_for_vocab_creation = set ( params . pop ( [string] , all_datasets ) ) [EOL] datasets_for_vocab_creation_aux = set ( params . pop ( [string] , all_datasets_aux ) ) [EOL] [EOL] for dataset in datasets_for_vocab_creation : [EOL] if dataset not in all_datasets : [EOL] raise ConfigurationError ( f" [string] { dataset }" ) [EOL] [EOL] logger . info ( [string] , [string] . join ( datasets_for_vocab_creation ) ) [EOL] dataset_primary = Dataset ( [ instance for key , dataset in all_datasets . items ( ) for instance in dataset . instances if key in datasets_for_vocab_creation ] ) [EOL] dataset_aux = Dataset ( [ instance for key , dataset in all_datasets_aux . items ( ) for instance in dataset . instances if key in datasets_for_vocab_creation_aux ] ) [EOL] vocab = Vocabulary . from_params ( params . pop ( [string] , { } ) , dataset_primary , dataset_aux = dataset_aux ) [EOL] vocab . save_to_files ( os . path . join ( serialization_dir , [string] ) ) [EOL] [EOL] model = Model . from_params ( vocab , params . pop ( [string] ) ) [EOL] iterator = DataIterator . from_params ( params . pop ( [string] ) ) [EOL] iterator_aux = DataIterator . from_params ( params . pop ( [string] ) ) [EOL] [EOL] train_data . index_instances ( vocab ) [EOL] train_data_aux . index_instances ( vocab ) [EOL] if validation_data : [EOL] validation_data . index_instances ( vocab ) [EOL] if validation_data_aux : [EOL] validation_data_aux . index_instances ( vocab ) [EOL] [EOL] cutoff_epoch = params . pop ( [string] , - [number] ) [EOL] [EOL] trainer_params = params . pop ( [string] ) [EOL] trainer = MultiTaskTrainer . from_params ( model = model , serialization_dir = serialization_dir , iterator = iterator , iterator_aux = iterator_aux , train_dataset = train_data , train_dataset_aux = train_data_aux , mixing_ratio = mixing_ratio , cutoff_epoch = cutoff_epoch , validation_dataset = validation_data , validation_dataset_aux = validation_data_aux , params = trainer_params , files_to_archive = params . files_to_archive ) [EOL] [EOL] evaluate_on_test = params . pop ( [string] , False ) [EOL] params . assert_empty ( [string] ) [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] archive_model ( serialization_dir , files_to_archive = params . files_to_archive ) [EOL] [EOL] if test_data and evaluate_on_test : [EOL] test_data . index_instances ( vocab ) [EOL] evaluate ( model , test_data , iterator , cuda_device = trainer . _cuda_device ) [comment] [EOL] [EOL] elif test_data : [EOL] logger . info ( [string] [string] ) [EOL] [EOL] if test_data_aux and evaluate_on_test : [EOL] test_data_aux . index_instances ( vocab ) [EOL] evaluate ( model , test_data_aux , iterator_aux , cuda_device = trainer . _cuda_device ) [comment] [EOL] [EOL] elif test_data_aux : [EOL] logger . info ( [string] [string] ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $builtins.str$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._MutuallyExclusiveGroup$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse._MutuallyExclusiveGroup$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._MutuallyExclusiveGroup$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Optional , Tuple , Any , List , Dict [EOL] import _importlib_modulespec [EOL] import typing [EOL] import sphinx [EOL] from distutils . version import LooseVersion [EOL] import sphinx_rtd_theme [EOL] import os [EOL] import sys [EOL] import inspect [EOL] import spacy [EOL] sys . path . insert ( [number] , os . path . abspath ( [string] ) ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] extensions = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] templates_path = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] source_suffix = [string] [EOL] [EOL] [comment] [EOL] master_doc = [string] [EOL] [EOL] [comment] [EOL] project = [string] [EOL] copyright = [string] [EOL] author = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] version = [string] [EOL] [comment] [EOL] release = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] language = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclude_patterns = [ [string] ] [EOL] [EOL] [comment] [EOL] pygments_style = [string] [EOL] [EOL] [comment] [EOL] todo_include_todos = False [EOL] [EOL] numpydoc_show_class_members = False [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_theme = [string] [EOL] [EOL] html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_theme_options = { [string] : False , [string] : True , [string] : True , } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_static_path = [ [string] ] [EOL] html_logo = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_sidebars = { [string] : [ [string] , [string] , [string] , [string] , [string] , ] } [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] htmlhelp_basename = [string] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] latex_elements = { } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] latex_documents = [ ( master_doc , [string] , [string] , [string] , [string] ) , ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] man_pages = [ ( master_doc , [string] , [string] , [ author ] , [number] ) ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] texinfo_documents = [ ( master_doc , [string] , [string] , author , [string] , [string] , [string] ) , ] [EOL] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] def linkcode_resolve ( domain , info ) : [EOL] [docstring] [EOL] if domain != [string] : [EOL] return None [EOL] [EOL] modname = info [ [string] ] [EOL] fullname = info [ [string] ] [EOL] [EOL] submod = sys . modules . get ( modname ) [EOL] if submod is None : [EOL] return None [EOL] [EOL] obj = submod [EOL] for part in fullname . split ( [string] ) : [EOL] try : [EOL] obj = getattr ( obj , part ) [EOL] except : [EOL] return None [EOL] [EOL] try : [EOL] fn = inspect . getsourcefile ( obj ) [EOL] except : [EOL] fn = None [EOL] if not fn : [EOL] return None [EOL] [EOL] try : [EOL] source , lineno = inspect . getsourcelines ( obj ) [EOL] except : [EOL] lineno = None [EOL] [EOL] if lineno : [EOL] linespec = [string] % ( lineno , lineno + len ( source ) - [number] ) [EOL] else : [EOL] linespec = [string] [EOL] [EOL] filename = info [ [string] ] . replace ( [string] , [string] ) [EOL] return [string] % ( filename , linespec ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,typing.List[builtins.str],builtins.int]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Tuple , Any , List [EOL] import typing [EOL] import itertools [EOL] import math [EOL] [EOL] from pytest import approx [EOL] import torch [EOL] from torch . autograd import Variable [EOL] [EOL] from allennlp . modules import SemiMarkovConditionalRandomField [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestSemiMarkovConditionalRandomField ( AllenNlpTestCase ) : [EOL] [docstring] [EOL] [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . batch_size = [number] [EOL] self . sentence_len = [number] [EOL] self . max_segment_length = [number] [EOL] self . num_tags = [number] [comment] [EOL] ninf = - [number] [EOL] self . logits = Variable ( torch . Tensor ( [ [ [ [ [number] , [number] , [number] ] , [ ninf , ninf , ninf ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , ninf ] , [ [number] , [number] , [number] ] ] ] , [ [ [ [number] , [number] , [number] ] , [ ninf , ninf , ninf ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ ninf , ninf , ninf ] , [ ninf , ninf , ninf ] ] ] ] ) . cuda ( ) ) [EOL] self . tags_sequence = [ [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ] [EOL] self . span_mask = Variable ( torch . LongTensor ( [ [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ] ) . cuda ( ) ) [EOL] self . default_tag = [number] [EOL] self . outside_span_tag = [number] [EOL] self . tags_tensor = Variable ( torch . LongTensor ( self . tags_sequence ) . cuda ( ) ) [EOL] self . mask = Variable ( torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) . cuda ( ) ) [EOL] [EOL] [comment] [EOL] self . semi_crf = SemiMarkovConditionalRandomField ( num_tags = self . num_tags , max_span_width = self . max_segment_length , default_tag = self . default_tag , outside_span_tag = self . outside_span_tag , false_negative_penalty = [number] , false_positive_penalty = [number] ) [EOL] [EOL] def score ( self , logits , tags ) : [EOL] [docstring] [EOL] total = [number] [EOL] [comment] [EOL] [comment] [EOL] logit_reshaped = logits . view ( - [number] , self . num_tags ) [EOL] tag_reshaped = tags . view ( - [number] ) [EOL] mask = tag_reshaped != self . default_tag [EOL] for i , t in enumerate ( tag_reshaped ) : [EOL] if mask [ i ] . data [ [number] ] : [EOL] total += logit_reshaped [ i ] [ t ] [EOL] return total [EOL] [EOL] def create_tags_tensor_from_sequence ( self , tag_sequence ) : [EOL] assert len ( tag_sequence ) <= self . sentence_len [EOL] tags = [ [ self . default_tag for _ in range ( self . max_segment_length ) ] for _ in range ( self . sentence_len ) ] [EOL] [EOL] start_span = [number] [EOL] for current in range ( [number] , len ( tag_sequence ) ) : [EOL] current_position_tagged = False [EOL] if tag_sequence [ current ] != tag_sequence [ current - [number] ] : [EOL] tags [ current - [number] ] [ current - [number] - start_span ] = tag_sequence [ current - [number] ] [EOL] start_span = current [EOL] elif current - start_span == self . max_segment_length - [number] : [EOL] tags [ current ] [ current - start_span ] = tag_sequence [ current ] [EOL] current_position_tagged = True [EOL] start_span = current + [number] [EOL] if current == len ( tag_sequence ) - [number] and not current_position_tagged : [EOL] tags [ current ] [ current - start_span ] = tag_sequence [ current ] [EOL] [EOL] return Variable ( torch . LongTensor ( tags ) . cuda ( ) ) [EOL] [EOL] def get_manual_log_likelihood ( self ) : [EOL] manual_log_likelihood = [number] [EOL] sentence_lengths = list ( torch . sum ( self . mask , - [number] ) . data ) [EOL] [EOL] spurious_scores = [ [ ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) , ( [ [number] , [number] , [number] ] , [number] ) ] , [ ( [ [number] , [number] ] , [number] ) , ( [ [number] , [number] ] , [number] ) ] ] [EOL] numerators = [ ] [EOL] [comment] [EOL] for logit , tag , sentence_length , spurious_score in zip ( self . logits , self . tags_tensor , sentence_lengths , spurious_scores ) : [EOL] numerator = self . score ( logit , tag ) [EOL] numerators . append ( numerator . data [ [number] ] ) [EOL] [EOL] all_tags = [ ( seq , self . create_tags_tensor_from_sequence ( seq ) ) for seq in itertools . product ( range ( self . num_tags - [number] ) , repeat = sentence_length ) ] [EOL] all_scores = [ ( tag_tensor [ [number] ] , self . score ( logit , tag_tensor [ [number] ] ) ) for tag_tensor in all_tags ] [EOL] [EOL] denominator = math . log ( sum ( [ math . exp ( score [ [number] ] ) for score in spurious_score + all_scores ] ) ) [EOL] manual_log_likelihood += numerator - denominator [EOL] [EOL] return numerators , manual_log_likelihood . data [ [number] ] [EOL] [EOL] def test_forward ( self ) : [EOL] [docstring] [EOL] assert torch . Size ( [ self . batch_size , self . sentence_len , self . max_segment_length , self . num_tags ] ) == self . logits . size ( ) [EOL] [EOL] ll , numerator = self . semi_crf ( self . logits , self . tags_tensor , self . mask ) [EOL] expected_numerator , expected_ll = self . get_manual_log_likelihood ( ) [EOL] [EOL] print ( [string] , ll . data [ [number] ] , [string] , expected_ll ) [EOL] print ( [string] , numerator . data . tolist ( ) , [string] , expected_numerator ) [EOL] assert expected_ll == approx ( ll . data [ [number] ] ) [EOL] assert expected_numerator == numerator . data . tolist ( ) [EOL] [EOL] def test_viterbi_tags ( self ) : [EOL] [docstring] [EOL] viterbi_tags , _ = self . semi_crf . viterbi_tags ( self . logits , self . mask ) [EOL] assert viterbi_tags . tolist ( ) == self . tags_sequence [EOL] [EOL] def test_viterbi_tags_merging ( self ) : [EOL] [comment] [EOL] pass [EOL] [EOL] def test_forward_with_tag_mask ( self ) : [EOL] [comment] [EOL] pass [EOL] [EOL] def test_hamming_cost ( self ) : [EOL] hamming_cost = [ [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ] , [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ] ] [EOL] assert self . semi_crf . _get_hamming_cost ( self . tags_tensor ) . data . tolist ( ) == hamming_cost [EOL] [EOL] def test_simple_recall_cost ( self ) : [EOL] recall_cost = [ [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ] , [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ] ] [EOL] assert self . semi_crf . _get_simple_recall_cost ( self . tags_tensor ) . data . tolist ( ) == recall_cost [EOL] [EOL] def test_recall_oriented_cost ( self ) : [EOL] cost = self . semi_crf . _get_recall_oriented_cost ( self . tags_tensor ) [EOL] gold_cost = self . semi_crf . _joint_likelihood ( cost , self . tags_tensor , self . mask ) [EOL] print ( [string] , cost . data . tolist ( ) ) [EOL] print ( [string] , gold_cost . data . tolist ( ) ) [EOL] [comment] [EOL] [EOL] num_labeled = self . semi_crf . _get_labeled_spans_count ( self . tags_tensor ) [EOL] print ( [string] , num_labeled . data . tolist ( ) ) [EOL] [comment] [EOL] [EOL] def test_roc_loss ( self ) : [EOL] [comment] [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Tuple[typing.List[builtins.int],builtins.float]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.List[typing.Tuple[typing.List[builtins.int],builtins.float]]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Tuple[builtins.int,...],typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Tuple[builtins.int,...],typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Tuple[builtins.int,...],typing.Any]]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Tuple[builtins.int,...],typing.Any]]$ 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from numpy . testing import assert_almost_equal [EOL] import torch [EOL] from torch . autograd import Variable [EOL] [EOL] from allennlp . modules import Highway [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestHighway ( AllenNlpTestCase ) : [EOL] def test_forward_works_on_simple_input ( self ) : [EOL] highway = Highway ( [number] , [number] ) [EOL] [comment] [EOL] highway . _layers [ [number] ] . weight . data . fill_ ( [number] ) [EOL] highway . _layers [ [number] ] . bias . data . fill_ ( [number] ) [EOL] highway . _layers [ [number] ] . weight . data . fill_ ( [number] ) [EOL] highway . _layers [ [number] ] . bias . data . fill_ ( - [number] ) [EOL] input_tensor = Variable ( torch . FloatTensor ( [ [ - [number] , [number] ] , [ [number] , - [number] ] ] ) ) [EOL] result = highway ( input_tensor ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] ) [EOL] [comment] [EOL] assert_almost_equal ( result , [ [ - [number] , [number] ] , [ [number] , - [number] ] ] , decimal = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from numpy . testing import assert_allclose [EOL] import torch [EOL] from torch . autograd import Variable [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . modules import MatrixAttention [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestMatrixAttention ( AllenNlpTestCase ) : [EOL] def test_forward_works_on_simple_input ( self ) : [EOL] attention = MatrixAttention ( ) [EOL] sentence_1_tensor = Variable ( torch . FloatTensor ( [ [ [ [number] , [number] , [number] ] , [ - [number] , [number] , [number] ] ] ] ) ) [EOL] sentence_2_tensor = Variable ( torch . FloatTensor ( [ [ [ [number] , [number] , [number] ] , [ - [number] , [number] , [number] ] , [ - [number] , - [number] , - [number] ] ] ] ) ) [EOL] result = attention ( sentence_1_tensor , sentence_2_tensor ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] , [number] ) [EOL] assert_allclose ( result , [ [ [ [number] , [number] , - [number] ] , [ [number] , [number] , [number] ] ] ] ) [EOL] [EOL] def test_can_build_from_params ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] } } ) [EOL] attention = MatrixAttention . from_params ( params ) [EOL] [comment] [EOL] assert attention . _similarity_function . __class__ . __name__ == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] from torch . autograd import Variable [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2seq_encoders import MultiHeadSelfAttention [EOL] [EOL] [EOL] class MultiHeadSelfAttentionTest ( AllenNlpTestCase ) : [EOL] def test_multi_head_self_attention_runs_forward ( self ) : [EOL] attention = MultiHeadSelfAttention ( num_heads = [number] , input_dim = [number] , attention_dim = [number] , values_dim = [number] ) [EOL] inputs = Variable ( torch . randn ( [number] , [number] , [number] ) ) [EOL] assert list ( attention ( inputs ) . size ( ) ) == [ [number] , [number] , [number] ] [EOL] [EOL] def test_multi_head_self_attention_respects_masking ( self ) : [EOL] attention = MultiHeadSelfAttention ( num_heads = [number] , input_dim = [number] , attention_dim = [number] , values_dim = [number] , attention_dropout_prob = [number] ) [EOL] tensor = Variable ( torch . randn ( [number] , [number] , [number] ) ) [EOL] mask = Variable ( torch . ones ( [ [number] , [number] ] ) ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] result = attention ( tensor , mask ) [EOL] [comment] [EOL] [comment] [EOL] result_without_mask = attention ( tensor [ : , : [number] , : ] ) [EOL] numpy . testing . assert_almost_equal ( result [ [number] , : [number] , : ] . data . cpu ( ) . numpy ( ) , result_without_mask [ [number] , : , : ] . data . cpu ( ) . numpy ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
[comment] [EOL] from typing import Tuple , Any , List [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Dataset , Instance , Token , Vocabulary [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . iterators import BasicIterator [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] [EOL] class IteratorTest ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( IteratorTest , self ) . setUp ( ) [EOL] self . token_indexers = { [string] : SingleIdTokenIndexer ( ) } [EOL] self . vocab = Vocabulary ( ) [EOL] self . this_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . is_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . a_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . sentence_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . another_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . yet_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . very_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . long_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . instances = [ self . create_instance ( [ [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] ] ) , ] [EOL] self . dataset = Dataset ( self . instances ) [EOL] [EOL] def create_instance ( self , str_tokens ) : [EOL] tokens = [ Token ( t ) for t in str_tokens ] [EOL] instance = Instance ( { [string] : TextField ( tokens , self . token_indexers ) } ) [EOL] instance . index_fields ( self . vocab ) [EOL] return instance [EOL] [EOL] def assert_instances_are_correct ( self , candidate_instances ) : [EOL] [comment] [EOL] [comment] [EOL] candidate_instances = [ tuple ( w for w in instance if w != [number] ) for instance in candidate_instances ] [EOL] expected_instances = [ tuple ( instance . fields [ [string] ] . _indexed_tokens [ [string] ] ) for instance in self . instances ] [EOL] assert set ( candidate_instances ) == set ( expected_instances ) [EOL] [EOL] [EOL] class TestBasicIterator ( IteratorTest ) : [EOL] [comment] [EOL] def test_yield_one_epoch_iterates_over_the_data_once ( self ) : [EOL] iterator = BasicIterator ( batch_size = [number] ) [EOL] batches = list ( iterator ( self . dataset , num_epochs = [number] ) ) [EOL] [comment] [EOL] instances = [ tuple ( instance . data . cpu ( ) . numpy ( ) ) for batch in batches for instance in batch [ [string] ] [ [string] ] ] [EOL] assert len ( instances ) == [number] [EOL] self . assert_instances_are_correct ( instances ) [EOL] [EOL] def test_call_iterates_over_data_forever ( self ) : [EOL] generator = BasicIterator ( batch_size = [number] ) ( self . dataset ) [EOL] batches = [ next ( generator ) for _ in range ( [number] ) ] [comment] [EOL] [comment] [EOL] instances = [ tuple ( instance . data . cpu ( ) . numpy ( ) ) for batch in batches for instance in batch [ [string] ] [ [string] ] ] [EOL] assert len ( instances ) == [number] * [number] [EOL] self . assert_instances_are_correct ( instances ) [EOL] [EOL] def test_create_batches_groups_correctly ( self ) : [EOL] [comment] [EOL] iterator = BasicIterator ( batch_size = [number] ) [EOL] grouped_instances = iterator . _create_batches ( self . dataset , shuffle = False ) [EOL] assert grouped_instances == [ [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] ] ] [EOL] [EOL] def test_from_params ( self ) : [EOL] [comment] [EOL] params = Params ( { } ) [EOL] iterator = BasicIterator . from_params ( params ) [EOL] assert iterator . _batch_size == [number] [comment] [EOL] [EOL] params = Params ( { [string] : [number] } ) [EOL] iterator = BasicIterator . from_params ( params ) [EOL] assert iterator . _batch_size == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from allennlp . data . dataset_readers import Seq2SeqDatasetReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestSeq2SeqDatasetReader ( AllenNlpTestCase ) : [EOL] def test_default_format ( self ) : [EOL] reader = Seq2SeqDatasetReader ( ) [EOL] dataset = reader . read ( [string] ) [EOL] [EOL] assert len ( dataset . instances ) == [number] [EOL] fields = dataset . instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] fields = dataset . instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] ] [EOL] fields = dataset . instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] def test_source_add_start_token ( self ) : [EOL] reader = Seq2SeqDatasetReader ( source_add_start_token = False ) [EOL] dataset = reader . read ( [string] ) [EOL] [EOL] assert len ( dataset . instances ) == [number] [EOL] fields = dataset . instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] [EOL] from typing import Tuple , Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . data . dataset_readers . framenet . full_text_reader import FrameNetFullTextReader [EOL] from allennlp . data . dataset_readers . framenet . ontology_reader import FrameOntology [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestFullTextReader ( AllenNlpTestCase ) : [EOL] [EOL] def test_read_from_file ( self ) : [EOL] max_span_width = [number] [EOL] reader = FrameNetFullTextReader ( max_span_width = max_span_width , data_path = [string] ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = dataset . instances [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] span_starts = [ j - d if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert span_starts == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] span_ends = [ j for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert span_ends == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert fields [ [string] ] . tokens [ [number] ] . text == [string] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] expected_valid_frame_elements = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert expected_valid_frame_elements == [ x . label for x in fields [ [string] ] . field_list ] [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert fields [ [string] ] . tokens [ [number] ] . text == [string] [EOL] [comment] [EOL] expected_valid_frame_elements = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert expected_valid_frame_elements == [ x . label for x in fields [ [string] ] . field_list ] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] assert span_starts == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] assert span_ends == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] [comment] [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert fields [ [string] ] . tokens [ [number] ] . text == [string] [EOL] [comment] [EOL] expected_valid_frame_elements = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert expected_valid_frame_elements == [ x . label for x in fields [ [string] ] . field_list ] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] assert span_starts == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] assert span_ends == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] [comment] [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert fields [ [string] ] . tokens [ [number] ] . text == [string] [EOL] [comment] [EOL] expected_valid_frame_elements = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert expected_valid_frame_elements == [ x . label for x in fields [ [string] ] . field_list ] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] assert span_starts == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] assert span_ends == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert fields [ [string] ] . tokens [ [number] ] . text == [string] [EOL] [comment] [EOL] expected_valid_frame_elements = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert expected_valid_frame_elements == [ x . label for x in fields [ [string] ] . field_list ] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] span_starts = [ j - d if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert span_starts == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] span_ends = [ j for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert span_ends == [ x . sequence_index for x in fields [ [string] ] . field_list ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def test_fill_missing_spans ( self ) : [EOL] sentence_length = [number] [EOL] fes = [ ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) ] [EOL] reader = FrameNetFullTextReader ( max_span_width = [number] , data_path = [string] ) [EOL] refilled_fes = reader . fill_missing_spans ( fes , sentence_length ) [EOL] expected_fes = [ ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) ] [EOL] assert refilled_fes == expected_fes [EOL] [EOL] fes = [ ] [EOL] sentence_length = [number] [EOL] refilled_fes = reader . fill_missing_spans ( fes , sentence_length ) [EOL] expected_fes = [ ( [number] , [number] , [string] ) ] [EOL] assert refilled_fes == expected_fes [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataset_readers.framenet.full_text_reader.FrameNetFullTextReader$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.dataset_readers.framenet.full_text_reader.FrameNetFullTextReader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.framenet.full_text_reader.FrameNetFullTextReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.dataset_readers.framenet.full_text_reader.FrameNetFullTextReader$ 0 0 0 $typing.List[typing.Any]$ 0 $builtins.int$ 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.str]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $allennlp.data.dataset_readers.framenet.full_text_reader.FrameNetFullTextReader$ 0 0 0 $typing.List[typing.Any]$ 0 $builtins.int$ 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.str]]$ 0
[comment] [EOL] [EOL] from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . data . dataset_readers . ontonotes . span_annotation_reader import SpanAnnotationReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestSpanAnnotationReader ( AllenNlpTestCase ) : [EOL] [EOL] def check_spans ( self , span_starts , span_ends , sent_len , max_span_width ) : [EOL] for end_pos in range ( sent_len ) : [EOL] for diff in range ( max_span_width ) : [EOL] idx = end_pos * max_span_width + diff [EOL] [EOL] span_start = span_starts . field_list [ idx ] . sequence_index [EOL] if end_pos - diff >= [number] : [EOL] expected_span_start = end_pos - diff [EOL] assert span_start == expected_span_start [EOL] else : [EOL] expected_span_start = [number] [EOL] assert span_start == expected_span_start [EOL] [EOL] span_end = span_ends . field_list [ idx ] . sequence_index [EOL] assert span_end == end_pos [EOL] [EOL] def test_read_from_file ( self ) : [EOL] max_span_width = [number] [EOL] reader = SpanAnnotationReader ( max_span_width ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = dataset . instances [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] [comment] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL] assert fields [ [string] ] . sequence_index == [number] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] assert fields [ [string] ] . labels == span_mask [EOL] assert fields [ [string] ] . sequence_index == [number] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] [comment] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL] assert fields [ [string] ] . sequence_index == [number] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] assert fields [ [string] ] . labels == span_mask [EOL] assert fields [ [string] ] . sequence_index == [number] [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] ] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL] assert fields [ [string] ] . sequence_index == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataset_readers.ontonotes.span_annotation_reader.SpanAnnotationReader$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $allennlp.data.dataset_readers.ontonotes.span_annotation_reader.SpanAnnotationReader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any , List [EOL] import typing [EOL] from allennlp . data . dataset_readers . ontonotes . crf_srl_reader import CrfSrlReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestCrfSrlReader ( AllenNlpTestCase ) : [EOL] [EOL] def check_spans ( self , span_starts , span_ends , sent_len , max_span_width ) : [EOL] for end_pos in range ( sent_len ) : [EOL] for diff in range ( max_span_width ) : [EOL] idx = end_pos * max_span_width + diff [EOL] [EOL] span_start = span_starts . field_list [ idx ] . sequence_index [EOL] if end_pos - diff >= [number] : [EOL] expected_span_start = end_pos - diff [EOL] assert span_start == expected_span_start [EOL] else : [EOL] expected_span_start = [number] [EOL] assert span_start == expected_span_start [EOL] [EOL] span_end = span_ends . field_list [ idx ] . sequence_index [EOL] assert span_end == end_pos [EOL] [EOL] def test_read_from_file ( self ) : [EOL] max_span_width = [number] [EOL] reader = CrfSrlReader ( max_span_width ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = dataset . instances [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels [ [number] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL] [EOL] [comment] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == [ [number] , [number] , [number] , [number] , [number] ] [EOL] [comment] [EOL] gold_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_spans [ [number] ] [ [number] ] = [string] [EOL] gold_spans_flat = [ label for rows in gold_spans for label in rows ] [EOL] assert fields [ [string] ] . labels == gold_spans_flat [EOL] self . check_spans ( fields [ [string] ] , fields [ [string] ] , len ( tokens ) , max_span_width ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $builtins.int$ 0 0
[comment] [EOL] [EOL] from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . data . dataset_readers . ontonotes . syntactic_constituent_reader import SyntacticConstitReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestSyntacticConstitReader ( AllenNlpTestCase ) : [EOL] [EOL] def test_read_from_file ( self ) : [EOL] max_span_width = [number] [EOL] reader = SyntacticConstitReader ( max_span_width ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = dataset . instances [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] [comment] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] [comment] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] ] [EOL] gold_constit_spans = [ [ [string] for d in range ( max_span_width ) ] for j in range ( len ( tokens ) ) ] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans [ [number] ] [ [number] ] = [string] [EOL] gold_constit_spans_flat = [ gs for rows in gold_constit_spans for gs in rows ] [EOL] assert fields [ [string] ] . labels == gold_constit_spans_flat [EOL] span_mask = [ [number] if j - d >= [number] else [number] for j in range ( len ( tokens ) ) for d in range ( max_span_width ) ] [EOL] assert fields [ [string] ] . labels == span_mask [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataset_readers.ontonotes.syntactic_constituent_reader.SyntacticConstitReader$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $allennlp.data.dataset_readers.ontonotes.syntactic_constituent_reader.SyntacticConstitReader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0
	0
	0
[comment] [EOL] [EOL] from typing import List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . tokenizers import CharacterTokenizer [EOL] [EOL] class TestCharacterTokenizer ( AllenNlpTestCase ) : [EOL] def test_splits_into_characters ( self ) : [EOL] tokenizer = CharacterTokenizer ( start_tokens = [ [string] , [string] ] , end_tokens = [ [string] , [string] ] ) [EOL] sentence = [string] [EOL] tokens = [ t . text for t in tokenizer . tokenize ( sentence ) ] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_handles_byte_encoding ( self ) : [EOL] tokenizer = CharacterTokenizer ( byte_encoding = [string] , start_tokens = [ [number] ] , end_tokens = [ [number] ] ) [EOL] word = [string] [EOL] tokens = [ t . text_id for t in tokenizer . tokenize ( word ) ] [EOL] [comment] [EOL] expected_tokens = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert tokens == expected_tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 $typing.List[builtins.int]$ 0
	0
[comment] [EOL] from typing import Any , List [EOL] import typing [EOL] import argparse [EOL] import argparse [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . commands . train import Train , train_model , train_model_from_args [EOL] [EOL] [EOL] class TestTrain ( AllenNlpTestCase ) : [EOL] def test_train_model ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : [string] , [string] : [number] } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } } , [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : { [string] : [string] , [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } } ) [EOL] [EOL] train_model ( params , serialization_dir = self . TEST_DIR ) [EOL] [EOL] def test_train_with_test_set ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : [string] , [string] : [number] } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } } , [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [string] , [string] : True , [string] : { [string] : [string] , [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } } ) [EOL] [EOL] train_model ( params , serialization_dir = self . TEST_DIR ) [EOL] [EOL] def test_train_args ( self ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Train ( ) . add_subparser ( [string] , subparsers ) [EOL] [EOL] for serialization_arg in [ [string] , [string] , [string] ] : [EOL] raw_args = [ [string] , [string] , serialization_arg , [string] ] [EOL] [EOL] args = parser . parse_args ( raw_args ) [EOL] [EOL] assert args . func == train_model_from_args [EOL] assert args . param_path == [string] [EOL] assert args . serialization_dir == [string] [EOL] [EOL] [comment] [EOL] with self . assertRaises ( SystemExit ) as cm : [comment] [EOL] args = parser . parse_args ( [ [string] , [string] , [string] ] ) [EOL] assert cm . exception . code == [number] [comment] [EOL] [EOL] [comment] [EOL] with self . assertRaises ( SystemExit ) as cm : [comment] [EOL] args = parser . parse_args ( [ [string] , [string] ] ) [EOL] assert cm . exception . code == [number] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import tests [EOL] from unittest import TestCase [EOL] import logging [EOL] import sys [EOL] [EOL] from allennlp . commands import main [EOL] from allennlp . commands . subcommand import Subcommand [EOL] [EOL] class TestMain ( TestCase ) : [EOL] [EOL] def test_fails_on_unknown_command ( self ) : [EOL] sys . argv = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] with self . assertRaises ( SystemExit ) as cm : [comment] [EOL] main ( ) [EOL] [EOL] assert cm . exception . code == [number] [comment] [EOL] [EOL] def test_warn_on_deprecated_flags ( self ) : [EOL] sys . argv = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] with self . assertLogs ( level = logging . WARNING ) as context : [EOL] main ( ) [EOL] assert set ( context . output ) == { [string] [string] , [string] [string] , [string] [string] , } [EOL] [EOL] [EOL] def test_subcommand_overrides ( self ) : [EOL] def do_nothing ( _ ) : [EOL] pass [EOL] [EOL] class FakeEvaluate ( Subcommand ) : [EOL] add_subparser_called = False [EOL] [EOL] def add_subparser ( self , name , parser ) : [EOL] subparser = parser . add_parser ( name , description = [string] , help = [string] ) [EOL] [EOL] subparser . set_defaults ( func = do_nothing ) [EOL] self . add_subparser_called = True [EOL] [EOL] return subparser [EOL] [EOL] fake_evaluate = FakeEvaluate ( ) [EOL] [EOL] sys . argv = [ [string] ] [EOL] main ( subcommand_overrides = { [string] : fake_evaluate } ) [EOL] [EOL] assert fake_evaluate . add_subparser_called [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.commands.main_test.TestMain.test_subcommand_overrides.FakeEvaluate$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.commands.main_test.TestMain.test_subcommand_overrides.FakeEvaluate$ 0 0 0 0 0 $tests.commands.main_test.TestMain.test_subcommand_overrides.FakeEvaluate$ 0 0 0
[comment] [EOL] from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import builtins [EOL] import argparse [EOL] import json [EOL] import os [EOL] import sys [EOL] import tempfile [EOL] from unittest import TestCase [EOL] [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . commands import main [EOL] from allennlp . commands . predict import Predict , DEFAULT_PREDICTORS [EOL] from allennlp . service . predictors import Predictor , BidafPredictor [EOL] [EOL] [EOL] class TestPredict ( TestCase ) : [EOL] [EOL] def test_add_predict_subparser ( self ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Predict ( DEFAULT_PREDICTORS ) . add_subparser ( [string] , subparsers ) [EOL] [EOL] snake_args = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] kebab_args = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] for raw_args in [ snake_args , kebab_args ] : [EOL] args = parser . parse_args ( raw_args ) [EOL] [EOL] assert args . func . __name__ == [string] [EOL] assert args . archive_file == [string] [EOL] assert args . output_file . name == [string] [EOL] assert args . batch_size == [number] [EOL] assert args . cuda_device == [number] [EOL] assert args . silent [EOL] [EOL] [EOL] def test_works_with_known_model ( self ) : [EOL] tempdir = tempfile . mkdtemp ( ) [EOL] infile = os . path . join ( tempdir , [string] ) [EOL] outfile = os . path . join ( tempdir , [string] ) [EOL] [EOL] with open ( infile , [string] ) as f : [EOL] f . write ( [string] [string] ) [EOL] f . write ( [string] [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , [string] , infile , [string] , outfile , [string] ] [EOL] [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( outfile ) [EOL] [EOL] with open ( outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] def test_batch_prediction_works_with_known_model ( self ) : [EOL] tempdir = tempfile . mkdtemp ( ) [EOL] infile = os . path . join ( tempdir , [string] ) [EOL] outfile = os . path . join ( tempdir , [string] ) [EOL] [EOL] with open ( infile , [string] ) as f : [EOL] f . write ( [string] [string] ) [EOL] f . write ( [string] [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , [string] , infile , [string] , outfile , [string] , [string] , [string] ] [EOL] [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( outfile ) [EOL] with open ( outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] def test_fails_without_required_args ( self ) : [EOL] sys . argv = [ [string] , [string] , [string] , ] [EOL] [EOL] with self . assertRaises ( SystemExit ) as cm : [comment] [EOL] main ( ) [EOL] [EOL] assert cm . exception . code == [number] [comment] [EOL] [EOL] def test_can_override_predictors ( self ) : [EOL] [EOL] @ Predictor . register ( [string] ) class Bidaf2Predictor ( BidafPredictor ) : [EOL] [docstring] [EOL] def predict_json ( self , inputs , cuda_device = - [number] ) : [EOL] result = super ( ) . predict_json ( inputs ) [EOL] result [ [string] ] = True [EOL] return result [EOL] [EOL] tempdir = tempfile . mkdtemp ( ) [EOL] infile = os . path . join ( tempdir , [string] ) [EOL] outfile = os . path . join ( tempdir , [string] ) [EOL] [EOL] with open ( infile , [string] ) as f : [EOL] f . write ( [string] [string] ) [EOL] f . write ( [string] [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , [string] , infile , [string] , outfile , [string] ] [EOL] [EOL] main ( predictor_overrides = { [string] : [string] } ) [EOL] assert os . path . exists ( outfile ) [EOL] [EOL] with open ( outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] [comment] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Union , Type , Any , Dict [EOL] import typing [EOL] import tests [EOL] import flask [EOL] import allennlp [EOL] import builtins [EOL] import copy [EOL] import json [EOL] import os [EOL] import pathlib [EOL] from collections import defaultdict [EOL] [EOL] from flask import Response [EOL] [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . service . predictors import Predictor [EOL] from allennlp . service . server_flask import make_app [EOL] from allennlp . service . db import InMemoryDemoDatabase [EOL] [EOL] TEST_ARCHIVE_FILES = { [string] : [string] , [string] : [string] , [string] : [string] } [EOL] [EOL] PREDICTORS = { name : Predictor . from_archive ( load_archive ( archive_file ) , predictor_name = name ) for name , archive_file in TEST_ARCHIVE_FILES . items ( ) } [EOL] [EOL] [EOL] class CountingPredictor ( Predictor ) : [EOL] [docstring] [EOL] [comment] [EOL] def __init__ ( self ) : [comment] [EOL] self . calls = defaultdict ( int ) [EOL] [EOL] def predict_json ( self , inputs , cuda_device = - [number] ) : [EOL] key = json . dumps ( inputs ) [EOL] self . calls [ key ] += [number] [EOL] return copy . deepcopy ( inputs ) [EOL] [EOL] class TestFlask ( AllenNlpTestCase ) : [EOL] [EOL] client = None [EOL] [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [comment] [EOL] pathlib . Path ( os . path . join ( self . TEST_DIR , [string] ) ) . touch ( ) [EOL] [EOL] if self . client is None : [EOL] [EOL] self . app = make_app ( build_dir = self . TEST_DIR ) [EOL] self . app . predictors = PREDICTORS [EOL] self . app . testing = True [EOL] self . client = self . app . test_client ( ) [EOL] [EOL] def post_json ( self , endpoint , data ) : [EOL] return self . client . post ( endpoint , content_type = [string] , data = json . dumps ( data ) ) [EOL] [EOL] [EOL] def tearDown ( self ) : [EOL] super ( ) . tearDown ( ) [EOL] try : [EOL] os . remove ( [string] ) [EOL] os . remove ( [string] ) [EOL] except FileNotFoundError : [EOL] pass [EOL] [EOL] def test_list_models ( self ) : [EOL] response = self . client . get ( [string] ) [EOL] data = json . loads ( response . get_data ( ) ) [EOL] assert [string] in set ( data [ [string] ] ) [EOL] [EOL] def test_unknown_model ( self ) : [EOL] response = self . post_json ( [string] , data = { [string] : [string] } ) [EOL] assert response . status_code == [number] [EOL] data = response . get_data ( ) [EOL] assert [string] in data and [string] in data [EOL] [EOL] def test_machine_comprehension ( self ) : [EOL] response = self . post_json ( [string] , data = { [string] : [string] , [string] : [string] } ) [EOL] [EOL] assert response . status_code == [number] [EOL] results = json . loads ( response . data ) [EOL] assert [string] in results [EOL] [EOL] def test_textual_entailment ( self ) : [EOL] response = self . post_json ( [string] , data = { [string] : [string] , [string] : [string] } ) [EOL] assert response . status_code == [number] [EOL] results = json . loads ( response . data ) [EOL] assert [string] in results [EOL] [EOL] def test_semantic_role_labeling ( self ) : [EOL] response = self . post_json ( [string] , data = { [string] : [string] } ) [EOL] assert response . status_code == [number] [EOL] results = json . loads ( response . get_data ( ) ) [EOL] assert [string] in results [EOL] [EOL] def test_caching ( self ) : [EOL] predictor = CountingPredictor ( ) [EOL] data = { [string] : [string] , [string] : [number] } [EOL] key = json . dumps ( data ) [EOL] [EOL] self . app . predictors [ [string] ] = predictor [EOL] [EOL] [comment] [EOL] assert not predictor . calls [EOL] [EOL] response = self . post_json ( [string] , data = data ) [EOL] assert response . status_code == [number] [EOL] assert json . loads ( response . get_data ( ) ) == data [EOL] [EOL] [comment] [EOL] assert predictor . calls . get ( key ) == [number] [EOL] assert len ( predictor . calls ) == [number] [EOL] [EOL] [comment] [EOL] noyes = { [string] : [string] } [EOL] response = self . post_json ( [string] , data = noyes ) [EOL] assert response . status_code == [number] [EOL] assert json . loads ( response . get_data ( ) ) == noyes [EOL] [EOL] [comment] [EOL] assert predictor . calls [ key ] == [number] [EOL] assert predictor . calls [ json . dumps ( noyes ) ] == [number] [EOL] assert len ( predictor . calls ) == [number] [EOL] [EOL] [comment] [EOL] for _ in range ( [number] ) : [EOL] response = self . post_json ( [string] , data = data ) [EOL] assert response . status_code == [number] [EOL] assert json . loads ( response . get_data ( ) ) == data [EOL] [EOL] [comment] [EOL] assert predictor . calls [ key ] == [number] [EOL] assert predictor . calls [ json . dumps ( noyes ) ] == [number] [EOL] assert len ( predictor . calls ) == [number] [EOL] [EOL] def test_disable_caching ( self ) : [EOL] import allennlp . service . server_flask as server [EOL] server . CACHE_SIZE = [number] [EOL] [EOL] predictor = CountingPredictor ( ) [EOL] app = server . make_app ( build_dir = self . TEST_DIR ) [EOL] app . predictors = { [string] : predictor } [EOL] app . testing = True [EOL] client = app . test_client ( ) [EOL] [EOL] data = { [string] : [string] , [string] : [number] } [EOL] key = json . dumps ( data ) [EOL] [EOL] assert not predictor . calls [EOL] [EOL] for i in range ( [number] ) : [EOL] response = client . post ( [string] , content_type = [string] , data = json . dumps ( data ) ) [EOL] assert response . status_code == [number] [EOL] assert json . loads ( response . get_data ( ) ) == data [EOL] [EOL] [comment] [EOL] assert predictor . calls [ key ] == i + [number] [EOL] assert len ( predictor . calls ) == [number] [EOL] [EOL] def test_missing_static_dir ( self ) : [EOL] fake_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] [EOL] with self . assertRaises ( SystemExit ) as cm : [EOL] make_app ( fake_dir ) [EOL] assert cm . code == - [number] [comment] [EOL] [EOL] def test_permalinks_fail_gracefully_with_no_database ( self ) : [EOL] app = make_app ( build_dir = self . TEST_DIR ) [EOL] predictor = CountingPredictor ( ) [EOL] app . predictors = { [string] : predictor } [EOL] app . testing = True [EOL] client = app . test_client ( ) [EOL] [EOL] [comment] [EOL] data = { [string] : [string] } [EOL] response = client . post ( [string] , content_type = [string] , data = json . dumps ( data ) ) [EOL] [EOL] assert response . status_code == [number] [EOL] [EOL] [comment] [EOL] result = json . loads ( response . get_data ( ) ) [EOL] assert [string] not in result [EOL] [EOL] [comment] [EOL] response = self . client . post ( [string] , data = [string] ) [EOL] assert response . status_code == [number] [EOL] [EOL] def test_permalinks_work ( self ) : [EOL] db = InMemoryDemoDatabase ( ) [EOL] app = make_app ( build_dir = self . TEST_DIR , demo_db = db ) [EOL] predictor = CountingPredictor ( ) [EOL] app . predictors = { [string] : predictor } [EOL] app . testing = True [EOL] client = app . test_client ( ) [EOL] [EOL] def post ( endpoint , data ) : [EOL] return client . post ( endpoint , content_type = [string] , data = json . dumps ( data ) ) [EOL] [EOL] data = { [string] : [string] } [EOL] response = post ( [string] , data = data ) [EOL] [EOL] assert response . status_code == [number] [EOL] result = json . loads ( response . get_data ( ) ) [EOL] slug = result . get ( [string] ) [EOL] assert slug is not None [EOL] [EOL] response = post ( [string] , data = { [string] : [string] } ) [EOL] assert response . status_code == [number] [EOL] [EOL] response = post ( [string] , data = { [string] : slug } ) [EOL] assert response . status_code == [number] [EOL] result2 = json . loads ( response . get_data ( ) ) [EOL] assert set ( result2 . keys ( ) ) == { [string] , [string] , [string] } [EOL] assert result2 [ [string] ] == [string] [EOL] assert result2 [ [string] ] == data [EOL] assert result2 [ [string] ] == result [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $builtins.bytes$ 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 $builtins.str$ 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 $builtins.str$ 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $tests.service.server_flask_test.CountingPredictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $flask.Response$ 0 $builtins.str$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $flask.wrappers.Response$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
[comment] [EOL] from typing import Any , Dict [EOL] import allennlp [EOL] import typing [EOL] from unittest import TestCase [EOL] [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . service . predictors import Predictor [EOL] [EOL] [EOL] class TestSrlPredictor ( TestCase ) : [EOL] def test_uses_named_inputs ( self ) : [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] result = predictor . predict_json ( inputs ) [EOL] [EOL] words = result . get ( [string] ) [EOL] assert words == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] num_words = len ( words ) [EOL] [EOL] verbs = result . get ( [string] ) [EOL] assert verbs is not None [EOL] assert isinstance ( verbs , list ) [EOL] [EOL] assert any ( v [ [string] ] == [string] for v in verbs ) [EOL] assert any ( v [ [string] ] == [string] for v in verbs ) [EOL] assert any ( v [ [string] ] == [string] for v in verbs ) [EOL] [EOL] for verb in verbs : [EOL] tags = verb . get ( [string] ) [EOL] assert tags is not None [EOL] assert isinstance ( tags , list ) [EOL] assert all ( isinstance ( tag , str ) for tag in tags ) [EOL] assert len ( tags ) == num_words [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0
	0
[comment] [EOL] from typing import Union , Any , Dict , List [EOL] import allennlp [EOL] import typing [EOL] from unittest import TestCase [EOL] import math [EOL] [EOL] from pytest import approx [EOL] [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . service . predictors import Predictor [EOL] [EOL] [EOL] class TestDecomposableAttentionPredictor ( TestCase ) : [EOL] def test_uses_named_inputs ( self ) : [EOL] inputs = { [string] : [string] , [string] : [string] } [EOL] [EOL] archive = load_archive ( [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] result = predictor . predict_json ( inputs ) [EOL] [EOL] [comment] [EOL] label_probs = result . get ( [string] ) [EOL] assert label_probs is not None [EOL] assert isinstance ( label_probs , list ) [EOL] assert len ( label_probs ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in label_probs ) [EOL] assert all ( x >= [number] for x in label_probs ) [EOL] assert sum ( label_probs ) == approx ( [number] ) [EOL] [EOL] [comment] [EOL] label_logits = result . get ( [string] ) [EOL] assert label_logits is not None [EOL] assert isinstance ( label_logits , list ) [EOL] assert len ( label_logits ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in label_logits ) [EOL] [EOL] exps = [ math . exp ( x ) for x in label_logits ] [EOL] sumexps = sum ( exps ) [EOL] for e , p in zip ( exps , label_probs ) : [EOL] assert e / sumexps == approx ( p ) [EOL] [EOL] def test_batch_prediction ( self ) : [EOL] batch_inputs = [ { [string] : [string] , [string] : [string] } , { [string] : [string] , [string] : [string] } , ] [EOL] [EOL] archive = load_archive ( [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] results = predictor . predict_batch_json ( batch_inputs ) [EOL] print ( results ) [EOL] assert len ( results ) == [number] [EOL] [EOL] for result in results : [EOL] [comment] [EOL] label_logits = result . get ( [string] ) [EOL] [comment] [EOL] label_probs = result . get ( [string] ) [EOL] assert label_probs is not None [EOL] assert isinstance ( label_probs , list ) [EOL] assert len ( label_probs ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in label_probs ) [EOL] assert all ( x >= [number] for x in label_probs ) [EOL] assert sum ( label_probs ) == approx ( [number] ) [EOL] [EOL] assert label_logits is not None [EOL] assert isinstance ( label_logits , list ) [EOL] assert len ( label_logits ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in label_logits ) [EOL] [EOL] exps = [ math . exp ( x ) for x in label_logits ] [EOL] sumexps = sum ( exps ) [EOL] for e , p in zip ( exps , label_probs ) : [EOL] assert e / sumexps == approx ( p ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $allennlp.service.predictors.predictor.Predictor$ 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Any , List [EOL] import typing [EOL] import torch [EOL] import numpy as np [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . training . metrics import NonBioSpanBasedF1Measure , Metric [EOL] from allennlp . common . params import Params [EOL] [EOL] [EOL] class NonBioSpanBasedF1Test ( AllenNlpTestCase ) : [EOL] [EOL] def setUp ( self ) : [EOL] super ( NonBioSpanBasedF1Test , self ) . setUp ( ) [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] self . vocab = vocab [EOL] [EOL] def test_extract_spans ( self ) : [EOL] [comment] [EOL] max_span_width = [number] [EOL] sent_len = [number] [EOL] metric = NonBioSpanBasedF1Measure ( self . vocab , tag_namespace = [string] , ignore_classes = [ [string] , [string] ] ) [EOL] [EOL] tag_matrix = [ [ [string] for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] indices = [ [ self . vocab . get_token_index ( x , [string] ) for x in tag_list ] for tag_list in tag_matrix ] [EOL] spans = metric . _extract_spans ( indices ) [EOL] assert spans == { ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) } [EOL] [EOL] [comment] [EOL] spans = metric . _extract_spans ( indices , merge = True ) [EOL] assert spans == { ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) } [EOL] [EOL] [comment] [EOL] indices = [ [ self . vocab . get_token_index ( [string] , [string] ) for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] spans = metric . _extract_spans ( indices ) [EOL] assert not spans [EOL] [EOL] [comment] [EOL] max_span_width = [number] [EOL] sent_len = [number] [EOL] tag_matrix = [ [ [string] for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] indices = [ [ self . vocab . get_token_index ( x , [string] ) for x in tag_list ] for tag_list in tag_matrix ] [EOL] spans = metric . _extract_spans ( indices ) [EOL] assert spans == { ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) , ( [number] , [number] , [string] ) } [EOL] [EOL] [comment] [EOL] spans = metric . _extract_spans ( indices , merge = True ) [EOL] assert spans == { ( [number] , [number] , [string] ) } [EOL] [EOL] tag_matrix = [ [ [string] for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] tag_matrix [ [number] ] [ [number] ] = [string] [EOL] indices = [ [ self . vocab . get_token_index ( x , [string] ) for x in tag_list ] for tag_list in tag_matrix ] [EOL] spans = metric . _extract_spans ( indices , merge = True ) [EOL] assert spans == { ( [number] , [number] , [string] ) } [EOL] [EOL] def test_span_metrics_are_computed_correctly_for_correct_segmentation ( self ) : [EOL] max_span_width = [number] [EOL] sent_len = [number] [EOL] batch_size = [number] [EOL] tags_to_ignore = [ [string] , [string] , [string] ] [EOL] metric = NonBioSpanBasedF1Measure ( self . vocab , [string] , tags_to_ignore ) [EOL] [EOL] gold_spans = [ [ [ [string] for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] for _ in range ( batch_size ) ] [EOL] [EOL] [comment] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] [EOL] [comment] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] [EOL] [comment] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] gold_spans [ [number] ] [ [number] ] [ [number] ] = [string] [EOL] [EOL] arg1 = self . vocab . get_token_index ( [string] , [string] ) [EOL] arg2 = self . vocab . get_token_index ( [string] , [string] ) [EOL] o = self . vocab . get_token_index ( [string] , [string] ) [EOL] [comment] [EOL] [EOL] mask = torch . ones ( batch_size , sent_len ) [EOL] mask [ [number] ] [ [number] ] = [number] [EOL] mask [ [number] ] [ [number] ] = [number] [EOL] mask [ [number] ] [ [number] ] = [number] [EOL] [EOL] gold_indices = [ [ [ self . vocab . get_token_index ( x , [string] ) for x in rows ] for rows in ex ] for ex in gold_spans ] [EOL] gold_tensor = torch . Tensor ( gold_indices ) [EOL] [EOL] [comment] [EOL] prediction_tensor = gold_tensor [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] [EOL] assert metric . _true_positives [ [string] ] == [number] [comment] [EOL] assert metric . _true_positives [ [string] ] == [number] [comment] [EOL] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] [EOL] for tag in tags_to_ignore : [EOL] assert tag not in metric . _true_positives . keys ( ) [EOL] assert tag not in metric . _false_negatives . keys ( ) [EOL] assert tag not in metric . _false_positives . keys ( ) [EOL] [EOL] metric_dict = metric . get_metric ( ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] np . testing . assert_almost_equal ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def test_span_metrics_are_computed_correctly_for_incorrect_segmentation ( self ) : [EOL] max_span_width = [number] [EOL] sent_len = [number] [EOL] batch_size = [number] [EOL] tags_to_ignore = [ [string] , [string] ] [EOL] metric = NonBioSpanBasedF1Measure ( self . vocab , [string] , tags_to_ignore ) [EOL] [EOL] star = self . vocab . get_token_index ( [string] , [string] ) [EOL] arg2 = self . vocab . get_token_index ( [string] , [string] ) [EOL] o = self . vocab . get_token_index ( [string] , [string] ) [EOL] [EOL] gold_spans = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] gold_spans [ [number] ] [ [number] ] = o [EOL] gold_spans [ [number] ] [ [number] ] = arg2 [EOL] gold_tensor = torch . Tensor ( [ gold_spans ] ) [EOL] [EOL] mask = torch . ones ( batch_size , sent_len ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction [ [number] ] [ [number] ] = arg2 [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] [comment] [EOL] prediction = [ [ star for _ in range ( max_span_width ) ] for _ in range ( sent_len ) ] [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction [ [number] ] [ [number] ] = o [EOL] prediction_tensor = torch . Tensor ( [ prediction ] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] metric . reset ( ) [EOL] [EOL] def test_span_f1_can_build_from_params ( self ) : [EOL] params = Params ( { [string] : [string] , [string] : [string] , [string] : [ [string] ] } ) [EOL] metric = Metric . from_params ( params , self . vocab ) [EOL] assert metric . _ignore_classes == [ [string] ] [EOL] assert metric . _label_vocabulary == self . vocab . get_index_to_token_vocabulary ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[typing.List[typing.Any]]]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] from allennlp . commands . serve import DEFAULT_MODELS [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class SniffTest ( AllenNlpTestCase ) : [EOL] [EOL] def test_config ( self ) : [EOL] assert set ( DEFAULT_MODELS . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] [EOL] def test_machine_comprehension ( self ) : [EOL] predictor = DEFAULT_MODELS [ [string] ] . predictor ( ) [EOL] [EOL] passage = [string] [comment] [EOL] question = [string] [EOL] [EOL] result = predictor . predict_json ( { [string] : passage , [string] : question } ) [EOL] [EOL] correct = [string] [EOL] [EOL] assert correct == result [ [string] ] [EOL] [EOL] [EOL] def test_semantic_role_labeling ( self ) : [EOL] predictor = DEFAULT_MODELS [ [string] ] . predictor ( ) [EOL] [EOL] sentence = [string] [EOL] [EOL] result = predictor . predict_json ( { [string] : sentence } ) [EOL] [EOL] assert result [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] assert result [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] assert result [ [string] ] == [ { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } , { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] } ] [EOL] [EOL] def test_textual_entailment ( self ) : [EOL] predictor = DEFAULT_MODELS [ [string] ] . predictor ( ) [EOL] [EOL] result = predictor . predict_json ( { [string] : [string] , [string] : [string] } ) [EOL] [EOL] assert result [ [string] ] [ [number] ] > [number] [comment] [EOL] [EOL] result = predictor . predict_json ( { [string] : [string] , [string] : [string] } ) [EOL] [EOL] assert result [ [string] ] [ [number] ] > [number] [comment] [EOL] [EOL] result = predictor . predict_json ( { [string] : [string] , [string] : [string] } ) [EOL] [EOL] assert result [ [string] ] [ [number] ] > [number] [comment] [EOL] [EOL] def test_coreference_resolution ( self ) : [EOL] predictor = DEFAULT_MODELS [ [string] ] . predictor ( ) [EOL] [EOL] document = [string] [EOL] [EOL] result = predictor . predict_json ( { [string] : document } ) [EOL] assert result [ [string] ] == [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] ] [EOL] assert result [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] def test_ner ( self ) : [EOL] predictor = DEFAULT_MODELS [ [string] ] . predictor ( ) [EOL] [EOL] sentence = [string] [EOL] [EOL] result = predictor . predict_json ( { [string] : sentence } ) [EOL] [EOL] assert result [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert result [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import subprocess [EOL] import os [EOL] [EOL] import torch [EOL] from torch . autograd import Variable [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . models . span_srl . frame_semi_crf_srl import FrameSemanticRoleLabeler [EOL] from allennlp . nn . util import get_lengths_from_binary_sequence_mask [EOL] [EOL] [EOL] class FrameSemiCrfSemanticRoleLabelerTest ( ModelTestCase ) : [EOL] def testMakeSpanMask ( self ) : [EOL] [comment] [EOL] valid_frame_elements = Variable ( torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , - [number] , - [number] , - [number] ] ] ) . cuda ( ) ) [EOL] [EOL] num_classes = [number] [EOL] batch_size = [number] [EOL] tag_mask = FrameSemanticRoleLabeler . get_tag_mask ( num_classes , valid_frame_elements , batch_size ) [EOL] expected_tag_mask = Variable ( torch . FloatTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ] ) . cuda ( ) ) [EOL] [EOL] assert torch . eq ( tag_mask , expected_tag_mask ) . data . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import subprocess [EOL] import os [EOL] [EOL] from flaky import flaky [EOL] import pytest [EOL] import numpy [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . models import Model [EOL] from allennlp . nn . util import get_lengths_from_binary_sequence_mask [EOL] [EOL] [EOL] class SemiCrfSemanticRoleLabelerTest ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( SemiCrfSemanticRoleLabelerTest , self ) . setUp ( ) [EOL] self . set_up_model ( [string] , [string] ) [EOL] [EOL] def test_crf_srl_model_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_forward_pass_runs_correctly ( self ) : [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] class_probs = output_dict [ [string] ] [ [number] ] . data . numpy ( ) [EOL] numpy . testing . assert_almost_equal ( numpy . sum ( class_probs , - [number] ) , numpy . ones ( class_probs . shape [ [number] ] ) ) [EOL] [EOL] def test_decode_runs_correctly ( self ) : [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] decode_output_dict = self . model . decode ( output_dict ) [EOL] lengths = get_lengths_from_binary_sequence_mask ( decode_output_dict [ [string] ] ) . data . tolist ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for prediction , length in zip ( decode_output_dict [ [string] ] , lengths ) : [EOL] assert len ( prediction ) == length [EOL] [EOL] def test_mismatching_dimensions_throws_configuration_error ( self ) : [EOL] params = Params . from_file ( self . param_file ) [EOL] [comment] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] [ [string] ] = [number] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Model . from_params ( self . vocab , params . pop ( [string] ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
from typing import Tuple , Any , Dict , List [EOL] import typing [EOL] import json [EOL] import os [EOL] import sys [EOL] from copy import deepcopy [EOL] [EOL] import torch [EOL] import tqdm [EOL] [EOL] sys . path . insert ( [number] , os . path . dirname ( os . path . abspath ( os . path . join ( __file__ , os . pardir ) ) ) ) [EOL] import squad_eval [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . params import replace_none [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import DataIterator , DatasetReader , Vocabulary [EOL] from allennlp . models import Model [EOL] from allennlp . nn . util import arrays_to_variables , device_mapping [EOL] [EOL] [EOL] def main ( config_file ) : [EOL] config = Params . from_file ( config_file ) [EOL] dataset_reader = DatasetReader . from_params ( config [ [string] ] ) [EOL] iterator_params = config [ [string] ] [EOL] iterator_keys = list ( iterator_params . keys ( ) ) [EOL] for key in iterator_keys : [EOL] if key != [string] : [EOL] del iterator_params [ key ] [EOL] iterator_params [ [string] ] = [string] [EOL] iterator = DataIterator . from_params ( iterator_params ) [EOL] evaluation_data_path = config [ [string] ] [EOL] [EOL] expected_version = [string] [EOL] with open ( evaluation_data_path ) as dataset_file : [EOL] dataset_json = json . load ( dataset_file ) [EOL] if ( dataset_json [ [string] ] != expected_version ) : [EOL] print ( [string] + expected_version + [string] + dataset_json [ [string] ] , file = sys . stderr ) [EOL] official_script_dataset = dataset_json [ [string] ] [EOL] [EOL] cuda_device = [number] [EOL] squad_eval . verbosity = [number] [EOL] model = Model . load ( config , cuda_device = cuda_device ) [EOL] [EOL] [comment] [EOL] print ( [string] % evaluation_data_path ) [EOL] dataset = dataset_reader . read ( evaluation_data_path ) [EOL] dataset . index_instances ( model . _vocab ) [EOL] [EOL] model . eval ( ) [EOL] generator = iterator ( dataset , num_epochs = [number] , shuffle = False ) [EOL] print ( [string] ) [EOL] best_spans = [ ] [EOL] result_dict = { } [EOL] for batch in tqdm . tqdm ( generator ) : [EOL] tensor_batch = arrays_to_variables ( batch , cuda_device , for_training = False ) [EOL] result = model ( ** tensor_batch ) [EOL] best_span_tensor = result [ [string] ] [EOL] for i in range ( best_span_tensor . size ( [number] ) ) : [EOL] best_spans . append ( best_span_tensor [ i ] . data . cpu ( ) . tolist ( ) ) [EOL] for best_span , instance in zip ( best_spans , dataset . instances ) : [EOL] span_tokens = instance . fields [ [string] ] . tokens [ best_span [ [number] ] : best_span [ [number] ] ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] span_text = fix_span_text ( span_tokens , instance . metadata [ [string] ] ) [EOL] question_id = instance . metadata [ [string] ] [EOL] result_dict [ question_id ] = span_text [EOL] metrics = model . get_metrics ( ) [EOL] official_result = squad_eval . evaluate ( official_script_dataset , result_dict ) [EOL] print ( [string] , metrics ) [EOL] print ( [string] , official_result ) [EOL] [EOL] [EOL] def fix_span_text ( span_tokens , passage ) : [EOL] [comment] [EOL] span_tokens = [ token . lower ( ) for token in span_tokens ] [EOL] passage = passage . lower ( ) [EOL] [EOL] [comment] [EOL] interword_punctuation = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] for punct in interword_punctuation : [EOL] keep_trying = True [EOL] while keep_trying : [EOL] try : [EOL] index = span_tokens . index ( punct ) [EOL] prev_num_tokens = len ( span_tokens ) [EOL] span_tokens = try_combine_tokens ( span_tokens , index , passage ) [EOL] keep_trying = len ( span_tokens ) < prev_num_tokens [EOL] except ValueError : [EOL] keep_trying = False [EOL] word_initial_punctuation = [ [string] , [string] , [string] ] [EOL] for punct in word_initial_punctuation : [EOL] keep_trying = True [EOL] while keep_trying : [EOL] keep_trying = False [EOL] for index , token in enumerate ( span_tokens ) : [EOL] if token . startswith ( punct ) : [EOL] prev_num_tokens = len ( span_tokens ) [EOL] [comment] [EOL] span_tokens = try_combine_tokens ( span_tokens , index , passage , end_offset = [number] ) [EOL] if len ( span_tokens ) < prev_num_tokens : [EOL] keep_trying = True [EOL] break [EOL] [comment] [EOL] span_tokens = try_combine_tokens ( span_tokens , index , passage , start_offset = [number] , end_offset = [number] ) [EOL] if len ( span_tokens ) < prev_num_tokens : [EOL] keep_trying = True [EOL] break [EOL] word_final_punctuation = [ [string] ] [EOL] for punct in word_final_punctuation : [EOL] keep_trying = True [EOL] while keep_trying : [EOL] keep_trying = False [EOL] for index , token in enumerate ( span_tokens ) : [EOL] if token . endswith ( punct ) : [EOL] prev_num_tokens = len ( span_tokens ) [EOL] [comment] [EOL] span_tokens = try_combine_tokens ( span_tokens , index , passage , start_offset = [number] , end_offset = [number] ) [EOL] if len ( span_tokens ) < prev_num_tokens : [EOL] keep_trying = True [EOL] break [EOL] [EOL] span_text = [string] . join ( span_tokens ) [EOL] [EOL] span_text = span_text . replace ( [string] , [string] ) [EOL] span_text = span_text . replace ( [string] , [string] ) [EOL] span_text = span_text . replace ( [string] , [string] ) [EOL] span_text = span_text . replace ( [string] , [string] ) [EOL] span_text = [string] . join ( span_text . strip ( ) . split ( ) ) [EOL] if span_text not in passage and span_text . replace ( [string] , [string] ) in passage : [EOL] span_text = span_text . replace ( [string] , [string] ) [EOL] return span_text [EOL] [EOL] [EOL] def try_combine_tokens ( span_tokens , index , passage , start_offset = [number] , end_offset = [number] ) : [EOL] [docstring] [EOL] span_tokens = deepcopy ( span_tokens ) [EOL] start_index = max ( index - start_offset , [number] ) [EOL] end_index = min ( index + end_offset , len ( span_tokens ) ) [EOL] tokens_to_combine = span_tokens [ start_index : end_index ] [EOL] current_text = [string] . join ( tokens_to_combine ) [EOL] replaced = [string] . join ( tokens_to_combine ) [EOL] if current_text not in passage and replaced in passage : [EOL] span_tokens [ start_index : end_index ] = [ replaced ] [EOL] return span_tokens [EOL] [EOL] [EOL] def test_fix_span_text ( ) : [EOL] test_cases = [ ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] , [string] , [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] ] , [string] ) , ( [string] , [ [string] , [string] ] , [string] ) , ] [EOL] for passage , tokens , expected_text in test_cases : [EOL] assert fix_span_text ( tokens , passage ) == expected_text , expected_text [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] main ( sys . argv [ [number] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0