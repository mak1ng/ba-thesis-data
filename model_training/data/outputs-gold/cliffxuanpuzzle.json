from typing import Any [EOL] import argparse [EOL] import typing [EOL] import argparse [EOL] [EOL] from scipy import signal [EOL] import pandas as pd [EOL] import numpy as np [EOL] [EOL] [EOL] def argument_parser ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] parser . add_argument ( [string] , type = argparse . FileType ( [string] ) , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = argparse . FileType ( [string] ) , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , help = [string] ) [EOL] return parser [EOL] [EOL] [EOL] def main ( argv = None ) : [EOL] args = argument_parser ( ) . parse_args ( argv ) [EOL] df = pd . read_csv ( args . filename , parse_dates = [ [string] ] ) [EOL] interval = ( df . loc [ [number] ] . timestamp - df . loc [ [number] ] . timestamp ) . seconds [EOL] new_size = len ( df ) * interval // args . rate [EOL] new_timestamp = pd . date_range ( df . loc [ [number] ] . timestamp , periods = new_size , freq = f'{ args . rate } [string] ' ) [EOL] if args . format : [EOL] new_timestamp = new_timestamp . map ( lambda x : x . strftime ( args . format ) ) [EOL] new_value = signal . resample ( df [ [string] ] , new_size ) [EOL] new_df = pd . DataFrame ( { [string] : new_timestamp , [string] : new_value } ) [EOL] if args . output : [EOL] new_df . to_csv ( args . output ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any [EOL] import argparse [EOL] import builtins [EOL] import typing [EOL] import pandas [EOL] [docstring] [EOL] import argparse [EOL] import datetime as dt [EOL] [EOL] import pandas as pd [EOL] import matplotlib . pyplot as plt [EOL] [EOL] [EOL] def valid_date ( date ) : [EOL] [docstring] [EOL] try : [EOL] return pd . Timestamp ( dt . datetime . strptime ( date , [string] ) ) [EOL] except ValueError : [EOL] msg = f" [string] { date } [string] " [EOL] raise argparse . ArgumentTypeError ( msg ) [EOL] [EOL] [EOL] def argument_parser ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] parser . add_argument ( [string] , type = argparse . FileType ( [string] ) , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , help = [string] , default = [string] ) [EOL] parser . add_argument ( [string] , [string] , help = [string] , default = [string] ) [EOL] parser . add_argument ( [string] , [string] , action = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = argparse . FileType ( [string] ) , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = valid_date , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = valid_date , help = [string] ) [EOL] return parser [EOL] [EOL] [EOL] def main ( argv = None ) : [EOL] args = argument_parser ( ) . parse_args ( argv ) [EOL] print ( args ) [EOL] df = pd . read_csv ( args . filename , parse_dates = [ args . timestamp ] ) [EOL] df = df . rename ( columns = { args . timestamp : [string] , args . value : [string] } ) [ [ [string] , [string] ] ] [EOL] df . sort_values ( by = [ [string] ] ) [EOL] if args . start : [EOL] df = df [ df [ [string] ] >= args . start ] [EOL] if args . end : [EOL] df = df [ df [ [string] ] < args . end ] [EOL] if args . output : [EOL] df . to_csv ( args . output ) [EOL] if args . plot : [EOL] plt . plot ( df [ [string] ] , df [ [string] ] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.Timestamp$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] import os [EOL] [EOL] import matplotlib . pyplot as plt [EOL] import pandas as pd [EOL] [EOL] PWD = os . path . abspath ( os . path . dirname ( __file__ ) ) [EOL] DATA_DIR = os . path . join ( PWD , [string] ) [EOL] [EOL] [EOL] def main ( ) : [EOL] filename = [string] [EOL] df = pd . read_csv ( os . path . join ( DATA_DIR , f'{ filename } [string] ' ) , parse_dates = [ [string] ] ) [EOL] df = df . sort_values ( by = [ [string] ] ) [EOL] count = len ( df ) [EOL] training_size = int ( count * [number] ) [EOL] training = df . iloc [ : training_size ] [EOL] training . to_csv ( f'{ filename } [string] ' ) [EOL] test = df . iloc [ training_size : ] [EOL] test . to_csv ( f'{ filename } [string] ' ) [EOL] plt . plot ( training [ [string] ] , training [ [string] ] ) [EOL] plt . plot ( test [ [string] ] , test [ [string] ] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List [EOL] import typing [EOL] import os [EOL] [EOL] import numpy as np [EOL] [EOL] from utils import softmax , rnn_forward , rnn_backward , initialize_parameters , get_initial_loss , smooth , print_sample [EOL] [EOL] PWD = os . path . abspath ( os . path . dirname ( __file__ ) ) [EOL] [EOL] [EOL] def sample ( parameters , char_to_ix , seed ) : [EOL] Waa = parameters [ [string] ] [EOL] Wax = parameters [ [string] ] [EOL] Wya = parameters [ [string] ] [EOL] by = parameters [ [string] ] [EOL] b = parameters [ [string] ] [EOL] [EOL] vocab_size = by . shape [ [number] ] [EOL] n_a = Waa . shape [ [number] ] [EOL] [EOL] x = np . zeros ( ( vocab_size , [number] ) ) [EOL] a_prev = np . zeros ( ( n_a , [number] ) ) [EOL] [EOL] indices = [ ] [EOL] [EOL] idx = - [number] [EOL] [EOL] counter = [number] [EOL] newline_character = char_to_ix [ [string] ] [EOL] [EOL] while idx != newline_character and counter != [number] : [EOL] a = np . tanh ( Wax . dot ( x ) + Waa . dot ( a_prev ) + b ) [EOL] z = Wya . dot ( a ) + by [EOL] y = softmax ( z ) [EOL] [EOL] np . random . seed ( counter + seed ) [EOL] idx = np . random . choice ( list ( char_to_ix . values ( ) ) , p = y . ravel ( ) ) [EOL] [EOL] indices . append ( idx ) [EOL] [EOL] x = np . zeros ( ( vocab_size , [number] ) ) [EOL] x [ idx ] = [ [number] ] [EOL] [EOL] a_prev = a [EOL] [EOL] seed += [number] [EOL] counter += [number] [EOL] [EOL] if counter == [number] : [EOL] indices . append ( char_to_ix [ [string] ] ) [EOL] [EOL] return indices [EOL] [EOL] [EOL] def clip ( gradients , maxValue ) : [EOL] [docstring] [EOL] [EOL] dWaa , dWax , dWya , db , dby = gradients [ [string] ] , gradients [ [string] ] , gradients [ [string] ] , gradients [ [string] ] , gradients [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] for gradient in [ dWax , dWaa , dWya , db , dby ] : [EOL] np . clip ( gradient , - maxValue , maxValue , out = gradient ) [EOL] [comment] [EOL] [EOL] gradients = { [string] : dWaa , [string] : dWax , [string] : dWya , [string] : db , [string] : dby } [EOL] [EOL] return gradients [EOL] [EOL] [EOL] def optimize ( X , Y , a_prev , parameters , learning_rate = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] loss , cache = rnn_forward ( X , Y , a_prev , parameters ) [EOL] [EOL] [comment] [EOL] gradients , a = rnn_backward ( X , Y , parameters , cache ) [EOL] [EOL] [comment] [EOL] gradients = clip ( gradients , [number] ) [EOL] [EOL] [comment] [EOL] parameters = gradients [EOL] [EOL] [comment] [EOL] [EOL] return loss , gradients , a [ len ( X ) - [number] ] [EOL] [EOL] [EOL] def model ( data , ix_to_char , char_to_ix , num_iterations = [number] , n_a = [number] , dino_names = [number] , vocab_size = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] n_x , n_y = vocab_size , vocab_size [EOL] [EOL] [comment] [EOL] parameters = initialize_parameters ( n_a , n_x , n_y ) [EOL] [EOL] [comment] [EOL] loss = get_initial_loss ( vocab_size , dino_names ) [EOL] [EOL] [comment] [EOL] with open ( [string] ) as f : [EOL] examples = f . readlines ( ) [EOL] examples = [ x . lower ( ) . strip ( ) for x in examples ] [EOL] [EOL] [comment] [EOL] np . random . seed ( [number] ) [EOL] np . random . shuffle ( examples ) [EOL] [EOL] [comment] [EOL] a_prev = np . zeros ( ( n_a , [number] ) ) [EOL] [EOL] [comment] [EOL] for j in range ( num_iterations ) : [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] index = j % len ( examples ) [EOL] X = [ None ] + [ char_to_ix [ ch ] for ch in examples [ index ] ] [EOL] Y = X [ [number] : ] + [ char_to_ix [ [string] ] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] curr_loss , gradients , a_prev = optimize ( X , Y , a_prev , parameters ) [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] loss = smooth ( loss , curr_loss ) [EOL] [EOL] [comment] [EOL] if j % [number] == [number] : [EOL] [EOL] print ( [string] % ( j , loss ) + [string] ) [EOL] [EOL] [comment] [EOL] seed = [number] [EOL] for name in range ( dino_names ) : [EOL] [EOL] [comment] [EOL] sampled_indices = sample ( parameters , char_to_ix , seed ) [EOL] print_sample ( sampled_indices , ix_to_char ) [EOL] [EOL] seed += [number] [comment] [EOL] [EOL] print ( [string] ) [EOL] [EOL] return parameters [EOL] [EOL] [EOL] data = open ( os . path . join ( PWD , [string] ) , [string] ) . read ( ) [EOL] data = data . lower ( ) [EOL] chars = list ( set ( data ) ) [EOL] data_size , vocab_size = len ( data ) , len ( chars ) [EOL] char_to_ix = { ch : i for i , ch in enumerate ( sorted ( chars ) ) } [EOL] ix_to_char = { i : ch for i , ch in enumerate ( sorted ( chars ) ) } [EOL] print ( [string] % ( data_size , vocab_size ) ) [EOL] [EOL] [EOL] def test_sample ( ) : [EOL] np . random . seed ( [number] ) [EOL] _ , n_a = [number] , [number] [EOL] Wax = np . random . randn ( n_a , vocab_size ) [EOL] Waa = np . random . randn ( n_a , n_a ) [EOL] Wya = np . random . randn ( vocab_size , n_a ) [EOL] [EOL] b , by = np . random . randn ( n_a , [number] ) , np . random . randn ( vocab_size , [number] ) [EOL] parameters = { [string] : Wax , [string] : Waa , [string] : Wya , [string] : b , [string] : by } [EOL] [EOL] indices = sample ( parameters , char_to_ix , [number] ) [EOL] [EOL] print ( [string] ) [EOL] print ( [string] , indices ) [EOL] print ( [string] , [ ix_to_char [ i ] for i in indices ] ) [EOL] [EOL] [EOL] def test_optimise ( ) : [EOL] np . random . seed ( [number] ) [EOL] vocab_size , n_a = [number] , [number] [EOL] a_prev = np . random . randn ( n_a , [number] ) [EOL] Wax = np . random . randn ( n_a , vocab_size ) [EOL] Waa = np . random . randn ( n_a , n_a ) [EOL] Wya = np . random . randn ( vocab_size , n_a ) [EOL] b , by = np . random . randn ( n_a , [number] ) , np . random . randn ( vocab_size , [number] ) [EOL] parameters = { [string] : Wax , [string] : Waa , [string] : Wya , [string] : b , [string] : by } [EOL] X = [ [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] Y = [ [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] loss , gradients , a_last = optimize ( X , Y , a_prev , parameters , learning_rate = [number] ) [EOL] print ( [string] , loss ) [EOL] print ( [string] , gradients [ [string] ] [ [number] ] [ [number] ] ) [EOL] print ( [string] , np . argmax ( gradients [ [string] ] ) ) [EOL] print ( [string] , gradients [ [string] ] [ [number] ] [ [number] ] ) [EOL] print ( [string] , gradients [ [string] ] [ [number] ] ) [EOL] print ( [string] , gradients [ [string] ] [ [number] ] ) [EOL] print ( [string] , a_last [ [number] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Dict[builtins.int,builtins.str]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Dict[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , Tuple [EOL] import typing [EOL] import numpy as np [EOL] [EOL] [EOL] def softmax ( x ) : [EOL] e_x = np . exp ( x - np . max ( x ) ) [EOL] return e_x / e_x . sum ( axis = [number] ) [EOL] [EOL] [EOL] def smooth ( loss , cur_loss ) : [EOL] return loss * [number] + cur_loss * [number] [EOL] [EOL] [EOL] def print_sample ( sample_ix , ix_to_char ) : [EOL] txt = [string] . join ( ix_to_char [ ix ] for ix in sample_ix ) [EOL] txt = txt [ [number] ] . upper ( ) + txt [ [number] : ] [comment] [EOL] print ( [string] % ( txt , ) , end = [string] ) [EOL] [EOL] [EOL] def get_initial_loss ( vocab_size , seq_length ) : [EOL] return - np . log ( [number] / vocab_size ) * seq_length [EOL] [EOL] [EOL] def initialize_parameters ( n_a , n_x , n_y ) : [EOL] [docstring] [EOL] np . random . seed ( [number] ) [EOL] Wax = np . random . randn ( n_a , n_x ) * [number] [comment] [EOL] Waa = np . random . randn ( n_a , n_a ) * [number] [comment] [EOL] Wya = np . random . randn ( n_y , n_a ) * [number] [comment] [EOL] b = np . zeros ( ( n_a , [number] ) ) [comment] [EOL] by = np . zeros ( ( n_y , [number] ) ) [comment] [EOL] [EOL] parameters = { [string] : Wax , [string] : Waa , [string] : Wya , [string] : b , [string] : by } [EOL] [EOL] return parameters [EOL] [EOL] [EOL] def rnn_step_forward ( parameters , a_prev , x ) : [EOL] [EOL] Waa , Wax , Wya , by , b = ( parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , ) [EOL] a_next = np . tanh ( np . dot ( Wax , x ) + np . dot ( Waa , a_prev ) + b ) [comment] [EOL] p_t = softmax ( np . dot ( Wya , a_next ) + by ) [comment] [EOL] [EOL] return a_next , p_t [EOL] [EOL] [EOL] def rnn_step_backward ( dy , gradients , parameters , x , a , a_prev ) : [EOL] [EOL] gradients [ [string] ] += np . dot ( dy , a . T ) [EOL] gradients [ [string] ] += dy [EOL] da = np . dot ( parameters [ [string] ] . T , dy ) + gradients [ [string] ] [comment] [EOL] daraw = ( [number] - a * a ) * da [comment] [EOL] gradients [ [string] ] += daraw [EOL] gradients [ [string] ] += np . dot ( daraw , x . T ) [EOL] gradients [ [string] ] += np . dot ( daraw , a_prev . T ) [EOL] gradients [ [string] ] = np . dot ( parameters [ [string] ] . T , daraw ) [EOL] return gradients [EOL] [EOL] [EOL] def update_parameters ( parameters , gradients , lr ) : [EOL] [EOL] parameters [ [string] ] += - lr * gradients [ [string] ] [EOL] parameters [ [string] ] += - lr * gradients [ [string] ] [EOL] parameters [ [string] ] += - lr * gradients [ [string] ] [EOL] parameters [ [string] ] += - lr * gradients [ [string] ] [EOL] parameters [ [string] ] += - lr * gradients [ [string] ] [EOL] return parameters [EOL] [EOL] [EOL] def rnn_forward ( X , Y , a0 , parameters , vocab_size = [number] ) : [EOL] [EOL] [comment] [EOL] x , a , y_hat = { } , { } , { } [EOL] [EOL] a [ - [number] ] = np . copy ( a0 ) [EOL] [EOL] [comment] [EOL] loss = [number] [EOL] [EOL] for t in range ( len ( X ) ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] x [ t ] = np . zeros ( ( vocab_size , [number] ) ) [EOL] if X [ t ] is not None : [EOL] x [ t ] [ X [ t ] ] = [number] [EOL] [EOL] [comment] [EOL] a [ t ] , y_hat [ t ] = rnn_step_forward ( parameters , a [ t - [number] ] , x [ t ] ) [EOL] [EOL] [comment] [EOL] loss -= np . log ( y_hat [ t ] [ Y [ t ] , [number] ] ) [EOL] [EOL] cache = ( y_hat , a , x ) [EOL] [EOL] return loss , cache [EOL] [EOL] [EOL] def rnn_backward ( X , Y , parameters , cache ) : [EOL] [comment] [EOL] gradients = { } [EOL] [EOL] [comment] [EOL] ( y_hat , a , x ) = cache [EOL] Waa , Wax , Wya , by , b = ( parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , parameters [ [string] ] , ) [EOL] [EOL] [comment] [EOL] gradients [ [string] ] , gradients [ [string] ] , gradients [ [string] ] = ( np . zeros_like ( Wax ) , np . zeros_like ( Waa ) , np . zeros_like ( Wya ) , ) [EOL] gradients [ [string] ] , gradients [ [string] ] = np . zeros_like ( b ) , np . zeros_like ( by ) [EOL] gradients [ [string] ] = np . zeros_like ( a [ [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] for t in reversed ( range ( len ( X ) ) ) : [EOL] dy = np . copy ( y_hat [ t ] ) [EOL] dy [ Y [ t ] ] -= [number] [EOL] gradients = rnn_step_backward ( dy , gradients , parameters , x [ t ] , a [ t ] , a [ t - [number] ] ) [EOL] [comment] [EOL] [EOL] return gradients , a [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] import tensorflow as tf [EOL] import numpy as np [EOL] [EOL] [EOL] def compute_content_cost ( a_C , a_G ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] m , n_H , n_W , n_C = a_C . get_shape ( ) . as_list ( ) [EOL] [EOL] [comment] [EOL] a_C_unrolled = tf . reshape ( a_C , [ n_H * n_W , n_C ] ) [EOL] a_G_unrolled = tf . reshape ( a_G , [ n_H * n_W , n_C ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] J_content = tf . reduce_sum ( ( a_C_unrolled - a_G_unrolled ) ** [number] ) / ( [number] * n_H * n_W * n_C ) [EOL] return J_content [EOL] [EOL] [EOL] def gram_matrix ( A ) : [EOL] [docstring] [EOL] GA = tf . matmul ( A , tf . transpose ( A ) ) [EOL] return GA [EOL] [EOL] [EOL] def compute_layer_style_cost ( a_S , a_G ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] m , n_H , n_W , n_C = a_S . get_shape ( ) . as_list ( ) [EOL] [EOL] [comment] [EOL] a_S = tf . transpose ( tf . reshape ( a_S , [ n_H * n_W , n_C ] ) ) [EOL] a_G = tf . transpose ( tf . reshape ( a_G , [ n_H * n_W , n_C ] ) ) [EOL] [EOL] [comment] [EOL] GS = gram_matrix ( a_S ) [EOL] GG = gram_matrix ( a_G ) [EOL] [EOL] [comment] [EOL] J_style_layer = tf . reduce_sum ( tf . square ( GS - GG ) ) / ( ( [number] * n_H * n_C * n_W ) ** [number] ) [EOL] [EOL] return J_style_layer [EOL] [EOL] [EOL] def test_compute_content_cost ( ) : [EOL] with tf . Session ( ) : [EOL] tf . set_random_seed ( [number] ) [EOL] a_C = tf . random . normal ( [ [number] , [number] , [number] , [number] ] , mean = [number] , stddev = [number] ) [EOL] a_G = tf . random . normal ( [ [number] , [number] , [number] , [number] ] , mean = [number] , stddev = [number] ) [EOL] J_content = compute_content_cost ( a_C , a_G ) [EOL] result = J_content . eval ( ) [EOL] print ( f" [string] { result }" ) [EOL] assert result == [number] [EOL] [EOL] [EOL] def test_gram_matrix ( ) : [EOL] with tf . Session ( ) : [EOL] tf . set_random_seed ( [number] ) [EOL] A = tf . random_normal ( [ [number] , [number] ] , mean = [number] , stddev = [number] ) [EOL] GA = gram_matrix ( A ) [EOL] result = GA . eval ( ) [EOL] assert result == np . array ( [ [ [number] , - [number] , - [number] ] , [ - [number] , [number] , [number] ] , [ - [number] , [number] , [number] ] ] ) [EOL] [EOL] [EOL] def main ( ) : [EOL] [comment] [EOL] test_gram_matrix ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Dict , List , Any , Tuple , Optional [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] import typing [EOL] [EOL] import pytest [EOL] [EOL] [EOL] [comment] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) ] ) def test_possible_path ( m , n , expected ) : [EOL] [comment] [EOL] assert possible_path_memorized ( m , n ) == expected [EOL] [EOL] [EOL] def possible_path ( m , n ) : [EOL] if m == [number] or n == [number] : [EOL] return [number] [EOL] return possible_path ( m - [number] , n ) + possible_path ( m , n - [number] ) [EOL] [EOL] [EOL] def possible_path_memorized ( m , n , solutions = None ) : [EOL] if solutions is None : [EOL] solutions = { } [EOL] if ( m , n ) in solutions : [EOL] return solutions [ ( m , n ) ] [EOL] if m == [number] or n == [number] : [EOL] return [number] [EOL] result = ( possible_path_memorized ( m - [number] , n , solutions ) + possible_path_memorized ( m , n - [number] , solutions ) ) [EOL] solutions [ ( m , n ) ] = result [EOL] return result [EOL] [EOL] [EOL] [comment] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [ [ [number] ] ] , [number] ) , ( [ [ [number] ] ] , [number] ) , ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , [number] ) , ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [number] ) , ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [number] ) , ( [ [ [number] ] * [number] ] * [number] , [number] ) , ( [ [ [number] ] * [number] ] * [number] , [number] ) , ] ) def test_possible_path_without_full_access ( m , expected ) : [EOL] [comment] [EOL] assert possible_path_without_full_access_memorized ( m ) == expected [EOL] [EOL] [EOL] def possible_path_without_full_access ( m ) : [EOL] [comment] [EOL] if len ( m ) == [number] or len ( m [ [number] ] ) == [number] : [EOL] return [number] [EOL] [comment] [EOL] if m [ [number] ] [ [number] ] == [number] : [EOL] return [number] [EOL] [comment] [EOL] if m == [ [ [number] ] ] : [EOL] return [number] [EOL] right = possible_path_without_full_access ( [ row [ [number] : ] for row in m ] ) [EOL] down = possible_path_without_full_access ( m [ [number] : ] ) [EOL] return right + down [EOL] [EOL] [EOL] def possible_path_without_full_access_memorized ( m , solutions = None ) : [EOL] if solutions is None : [EOL] solutions = { } [EOL] key = tuple ( tuple ( row ) for row in m ) [EOL] try : [EOL] return solutions [ key ] [EOL] except KeyError : [EOL] pass [EOL] if len ( m ) == [number] or len ( m [ [number] ] ) == [number] : [EOL] return [number] [EOL] if m [ [number] ] [ [number] ] == [number] : [EOL] return [number] [EOL] if len ( m ) == [number] and len ( m [ [number] ] ) == [number] and m [ [number] ] [ [number] ] == [number] : [EOL] return [number] [EOL] right = possible_path_without_full_access_memorized ( [ row [ [number] : ] for row in m ] , solutions ) [EOL] down = possible_path_without_full_access_memorized ( m [ [number] : ] , solutions ) [EOL] result = right + down [EOL] solutions [ key ] = result [EOL] return result [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any , Dict [EOL] import typing [EOL] [docstring] [EOL] import numpy as np [EOL] import h5py [EOL] [EOL] [EOL] def load_dataset ( ) : [EOL] train_dataset = h5py . File ( [string] , [string] ) [EOL] train_set_x_orig = np . array ( train_dataset [ [string] ] [ : ] ) [EOL] train_set_y_orig = np . array ( train_dataset [ [string] ] [ : ] ) [EOL] [EOL] test_dataset = h5py . File ( [string] , [string] ) [EOL] test_set_x_orig = np . array ( test_dataset [ [string] ] [ : ] ) [EOL] test_set_y_orig = np . array ( test_dataset [ [string] ] [ : ] ) [EOL] [EOL] classes = np . array ( test_dataset [ [string] ] [ : ] ) [EOL] [EOL] train_set_y_orig = train_set_y_orig . reshape ( ( [number] , train_set_y_orig . shape [ [number] ] ) ) [EOL] test_set_y_orig = test_set_y_orig . reshape ( ( [number] , test_set_y_orig . shape [ [number] ] ) ) [EOL] [EOL] return train_set_x_orig , train_set_y_orig , test_set_x_orig , test_set_y_orig , classes [EOL] [EOL] [EOL] def test_propagate ( ) : [EOL] w = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] b = [number] [EOL] X = np . array ( [ [ [number] , [number] , - [number] ] , [ [number] , [number] , - [number] ] ] ) [EOL] Y = np . array ( [ [ [number] , [number] , [number] ] ] ) [EOL] grads , cost = propagate ( w , b , X , Y ) [EOL] dw = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] [EOL] assert np . array_equal ( grads [ [string] ] , dw ) [EOL] assert grads [ [string] ] == [number] [EOL] assert cost == [number] [EOL] [EOL] [EOL] def sigmoid ( z ) : [EOL] [docstring] [EOL] s = [number] / ( [number] + np . exp ( - z ) ) [EOL] return s [EOL] [EOL] [EOL] def initialize_with_zeros ( dimention ) : [EOL] [docstring] [EOL] w = np . zeros ( [ dimention , [number] ] ) [EOL] b = [number] [EOL] return w , b [EOL] [EOL] [EOL] def propagate ( w , b , X , Y ) : [EOL] [docstring] [EOL] [EOL] m = X . shape [ [number] ] [EOL] [EOL] [comment] [EOL] A = sigmoid ( np . dot ( w . T , X ) + b ) [EOL] cost = - ( Y * np . log ( A ) + ( [number] - Y ) * np . log ( [number] - A ) ) . sum ( ) / m [EOL] [EOL] [comment] [EOL] dw = np . dot ( X , ( A - Y ) . T ) / m [EOL] db = ( A - Y ) . sum ( ) / m [EOL] [EOL] assert dw . shape == w . shape [EOL] assert db . dtype == float [EOL] cost = np . squeeze ( cost ) [EOL] assert cost . shape == ( ) [EOL] [EOL] grads = { [string] : dw , [string] : db } [EOL] [EOL] return grads , cost [EOL] [EOL] [EOL] def optimize ( w , b , X , Y , num_iterations , learning_rate , print_cost = False ) : [EOL] [docstring] [EOL] [EOL] costs = [ ] [EOL] [EOL] for i in range ( num_iterations ) : [EOL] [comment] [EOL] grads , cost = propagate ( w , b , X , Y ) [EOL] [EOL] [comment] [EOL] dw = grads [ [string] ] [EOL] db = grads [ [string] ] [EOL] [EOL] [comment] [EOL] w = w - learning_rate * dw [EOL] b = b - learning_rate * db [EOL] [EOL] [comment] [EOL] if i % [number] == [number] : [EOL] costs . append ( cost ) [EOL] [EOL] [comment] [EOL] if print_cost : [EOL] print ( f" [string] { i } [string] { cost }" ) [EOL] [EOL] params = { [string] : w , [string] : b } [EOL] [EOL] grads = { [string] : dw , [string] : db } [EOL] [EOL] return params , grads , costs [EOL] [EOL] [EOL] def predict ( w , b , X ) : [EOL] [docstring] [EOL] [EOL] m = X . shape [ [number] ] [EOL] Y_prediction = np . zeros ( ( [number] , m ) ) [EOL] w = w . reshape ( X . shape [ [number] ] , [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] A = sigmoid ( np . dot ( w . T , X ) + b ) [EOL] [EOL] for i in range ( A . shape [ [number] ] ) : [EOL] [comment] [EOL] Y_prediction [ [number] , i ] = A [ [number] , i ] > [number] [EOL] assert Y_prediction . shape == ( [number] , m ) [EOL] return Y_prediction [EOL] [EOL] [EOL] def model ( X_train , Y_train , X_test , Y_test , num_iterations = [number] , learning_rate = [number] , print_cost = False , ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] w , b = initialize_with_zeros ( X_train . shape [ [number] ] ) [EOL] [EOL] [comment] [EOL] parameters , grads , costs = optimize ( w , b , X_train , Y_train , num_iterations , learning_rate , print_cost ) [EOL] [EOL] [comment] [EOL] w = parameters [ [string] ] [EOL] b = parameters [ [string] ] [EOL] [EOL] [comment] [EOL] Y_prediction_test = predict ( w , b , X_test ) [EOL] Y_prediction_train = predict ( w , b , X_train ) [EOL] [EOL] [comment] [EOL] print ( [string] . format ( [number] - np . mean ( np . abs ( Y_prediction_train - Y_train ) ) * [number] ) ) [EOL] print ( [string] . format ( [number] - np . mean ( np . abs ( Y_prediction_test - Y_test ) ) * [number] ) ) [EOL] [EOL] d = { [string] : costs , [string] : Y_prediction_test , [string] : Y_prediction_train , [string] : w , [string] : b , [string] : learning_rate , [string] : num_iterations , } [EOL] [EOL] return d [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] import numpy as np [EOL] import numpy . linalg as la [EOL] import pytest [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) def test_page_rank_fast ( damping ) : [EOL] matrix = np . array ( [ [ [number] , [number] / [number] , [number] / [number] , [number] , [number] , [number] ] , [ [number] / [number] , [number] , [number] , [number] , [number] / [number] , [number] ] , [ [number] / [number] , [number] / [number] , [number] , [number] , [number] , [number] / [number] ] , [ [number] / [number] , [number] , [number] / [number] , [number] , [number] / [number] , [number] / [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] / [number] , [number] , [number] , [number] ] ] ) [EOL] fast_rank = page_rank_fast ( matrix , damping ) [EOL] [EOL] rank = page_rank ( matrix , damping ) [EOL] [EOL] assert np . linalg . norm ( fast_rank - rank ) < [number] ** - [number] [EOL] [EOL] [EOL] def page_rank_fast ( link_matrix , damping ) : [EOL] [docstring] [EOL] dimension = link_matrix . shape [ [number] ] [EOL] adjusted = ( damping * link_matrix + ( [number] - damping ) / dimension * np . ones ( [ dimension , dimension ] ) ) [EOL] eigen_values , eigen_vectors = la . eig ( adjusted ) [EOL] order = np . absolute ( eigen_values ) . argsort ( ) [ : : - [number] ] [EOL] eigen_values = eigen_values [ order ] [EOL] eigen_vectors = eigen_vectors [ : , order ] [EOL] principle_eigen_vector = eigen_vectors [ : , [number] ] [EOL] probability = np . real ( principle_eigen_vector / np . sum ( principle_eigen_vector ) ) [EOL] return probability [EOL] [EOL] [EOL] def page_rank ( link_matrix , damping ) : [EOL] dimension = link_matrix . shape [ [number] ] [EOL] adjusted = ( damping * link_matrix + ( [number] - damping ) / dimension * np . ones ( [ dimension , dimension ] ) ) [EOL] r = np . ones ( dimension ) / dimension [EOL] i = [number] [EOL] while True : [EOL] last_r = r [EOL] i += [number] [EOL] r = adjusted @ r [EOL] if la . norm ( r - last_r ) < [number] ** - [number] : [EOL] print ( str ( i ) + [string] ) [EOL] break [EOL] return r [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0