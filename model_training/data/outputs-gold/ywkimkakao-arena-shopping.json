[comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import argparse [EOL] import typing [EOL] [docstring] [EOL] import argparse [EOL] import os [EOL] [EOL] import tensorflow as tf [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] [EOL] [comment] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] input_path = os . path . expanduser ( args . input_path ) [EOL] output_path = os . path . expanduser ( args . output_path ) [EOL] [EOL] output_dir = os . path . dirname ( output_path ) [EOL] tf . gfile . MakeDirs ( output_dir ) [EOL] [EOL] sess = tf . Session ( config = tf . ConfigProto ( allow_soft_placement = True ) ) [EOL] saver = tf . train . import_meta_graph ( input_path + [string] ) [EOL] saver . restore ( sess , input_path ) [EOL] model_variables = [ var for var in tf . global_variables ( ) if not var . name . startswith ( [string] ) ] [EOL] saver = tf . train . Saver ( model_variables ) [EOL] saver . save ( sess , output_path ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Dict , Union , Any [EOL] import argparse [EOL] import shopping [EOL] import typing [EOL] import multiprocessing [EOL] import predict [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import os [EOL] from multiprocessing import Pool [EOL] [EOL] import numpy as np [EOL] from tensor2tensor . utils import trainer_lib [EOL] from tensor2tensor . utils import decoding [EOL] from tensor2tensor . utils import data_reader [EOL] from tensor2tensor . data_generators import problem as t2t_problem [EOL] import tensorflow as tf [EOL] from tqdm . auto import tqdm [EOL] [EOL] from shopping . data . datasets import shopping [EOL] [EOL] [EOL] def label_compatibility ( high , low ) : [EOL] [docstring] [EOL] [comment] [EOL] if high [ [number] ] != low [ [number] ] : [EOL] return False [EOL] if high [ [number] ] != low [ [number] ] : [EOL] return False [EOL] if high [ [number] ] == - [number] : [comment] [EOL] return low [ [number] ] > - [number] [EOL] if high [ [number] ] != low [ [number] ] : [EOL] return False [EOL] if high [ [number] ] == - [number] : [comment] [EOL] return low [ [number] ] > - [number] [EOL] return False [comment] [EOL] [EOL] [EOL] def compute_descendant_matrix ( data_dir ) : [EOL] id_to_label , _ = shopping . load_labels ( data_dir , narrow = False ) [EOL] num_classes = len ( id_to_label ) [EOL] score_matrix = np . array ( [ [ label_compatibility ( id_to_label [ x ] , id_to_label [ y ] ) for y in range ( num_classes ) ] for x in tqdm ( range ( num_classes ) , desc = [string] ) ] ) [EOL] return score_matrix [EOL] [EOL] [EOL] class SequentialGreedy : [EOL] def __init__ ( self , data_dir ) : [EOL] self . id_to_label , _ = shopping . load_labels ( data_dir , narrow = False ) [EOL] self . descendent_matrix = compute_descendant_matrix ( data_dir ) [EOL] [EOL] def class_id_to_label ( self , class_id ) : [EOL] return self . id_to_label [ class_id ] [EOL] [EOL] def predict_label_by_logit ( self , logits ) : [EOL] [docstring] [EOL] prob = shopping . softmax ( logits ) + [number] [EOL] descendent_mask = np . ones_like ( logits ) [EOL] while descendent_mask . any ( ) : [EOL] class_id = np . argmax ( prob * descendent_mask ) [EOL] descendent_mask = self . descendent_matrix [ class_id ] [EOL] return self . class_id_to_label ( class_id ) [EOL] [EOL] def __call__ ( self , prediction ) : [EOL] logits = prediction [ [string] ] . ravel ( ) [EOL] label = self . predict_label_by_logit ( logits ) [EOL] return label [EOL] [EOL] [EOL] class PostProcessor : [EOL] def __init__ ( self , label_predictor ) : [EOL] self . label_predictor = label_predictor [EOL] [EOL] def __call__ ( self , prediction ) : [EOL] pid = prediction [ [string] ] . decode ( [string] ) [EOL] label = self . label_predictor ( prediction ) [EOL] return pid , label [EOL] [EOL] [EOL] def is_not_padding_example ( prediction ) : [EOL] pid = prediction [ [string] ] . decode ( [string] ) [EOL] return bool ( pid ) [EOL] [EOL] [EOL] def predict_input_fn ( problem , hparams , data_dir = None , params = None , config = None , dataset_kwargs = None ) : [EOL] [docstring] [EOL] mode = tf . estimator . ModeKeys . PREDICT [EOL] partition_id , num_partitions = problem . _dataset_partition ( mode , config ) [EOL] [EOL] num_threads = [number] [EOL] [EOL] if config and hasattr ( config , [string] ) and config . data_parallelism : [EOL] num_shards = config . data_parallelism . n [EOL] else : [EOL] num_shards = [number] [EOL] [EOL] assert num_shards == [number] , [string] [EOL] [EOL] def define_shapes ( example ) : [EOL] batch_size = config and config . use_tpu and params [ [string] ] [EOL] return t2t_problem . standardize_shapes ( example , batch_size = batch_size ) [EOL] [EOL] [comment] [EOL] data_dir = data_dir or ( hasattr ( hparams , [string] ) and hparams . data_dir ) [EOL] [EOL] dataset_kwargs = dataset_kwargs or { } [EOL] dataset_kwargs . update ( { [string] : mode , [string] : data_dir , [string] : num_threads , [string] : hparams , [string] : partition_id , [string] : num_partitions , } ) [EOL] [EOL] dataset = problem . dataset ( ** dataset_kwargs ) [EOL] [EOL] dataset = dataset . map ( data_reader . cast_ints_to_int32 , num_parallel_calls = num_threads ) [EOL] [EOL] [comment] [EOL] assert hparams . use_fixed_batch_size , [string] [EOL] [comment] [EOL] dataset = dataset . apply ( tf . contrib . data . bucket_by_sequence_length ( data_reader . example_length , [ ] , [ hparams . batch_size ] ) ) [EOL] [EOL] dataset = dataset . map ( define_shapes , num_parallel_calls = num_threads ) [EOL] [EOL] def prepare_for_output ( example ) : [EOL] if mode == tf . estimator . ModeKeys . PREDICT : [EOL] example [ [string] ] = example . pop ( [string] ) [EOL] return example [EOL] else : [EOL] return example , example [ [string] ] [EOL] [EOL] dataset = dataset . map ( prepare_for_output , num_parallel_calls = num_threads ) [EOL] dataset = dataset . prefetch ( [number] ) [EOL] [EOL] if mode == tf . estimator . ModeKeys . PREDICT : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] tf . add_to_collection ( tf . GraphKeys . QUEUE_RUNNERS , data_reader . DummyQueueRunner ( ) ) [EOL] [EOL] return dataset [EOL] [EOL] [EOL] def make_estimator_predict_input_fn ( problem , hparams , data_dir = None , dataset_kwargs = None ) : [EOL] [docstring] [EOL] [EOL] def estimator_input_fn ( params , config ) : [EOL] return predict_input_fn ( problem , hparams , data_dir = data_dir , params = params , config = config , dataset_kwargs = dataset_kwargs , ) [EOL] [EOL] return estimator_input_fn [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] [EOL] [comment] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , default = [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , default = False , dest = [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , default = False , dest = [string] , action = [string] , help = [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] [comment] [EOL] hparams_set = args . hparams_set [EOL] data_dir = os . path . expanduser ( args . data_dir ) [EOL] problem_name = args . problem [EOL] hparams = trainer_lib . create_hparams ( hparams_set , data_dir = data_dir , problem_name = problem_name ) [EOL] [EOL] tokens_per_example = [number] [EOL] hparams . use_fixed_batch_size = True [EOL] hparams . batch_size = hparams . batch_size // tokens_per_example [EOL] [EOL] model_name = args . model [EOL] decode_hp = decoding . decode_hparams ( ) [EOL] run_config = trainer_lib . create_run_config ( model_name ) [EOL] estimator = trainer_lib . create_estimator ( model_name , hparams , run_config , decode_hparams = decode_hp , use_tpu = False ) [EOL] [EOL] if problem_name == [string] : [EOL] problem = shopping . ShoppingPublicLB ( ) [EOL] hierarchical = False [EOL] if problem_name == [string] : [EOL] problem = shopping . ShoppingPrivateLB ( ) [EOL] hierarchical = False [EOL] if problem_name == [string] : [EOL] problem = shopping . HierarchicalShoppingPublicLB ( ) [EOL] hierarchical = True [EOL] if problem_name == [string] : [EOL] problem = shopping . HierarchicalShoppingPrivateLB ( ) [EOL] hierarchical = True [EOL] [EOL] dataset_kwargs = { [string] : None , [string] : [string] , [string] : - [number] } [EOL] infer_input_fn = make_estimator_predict_input_fn ( problem , hparams , dataset_kwargs = dataset_kwargs ) [EOL] [EOL] checkpoint_path = os . path . expanduser ( args . checkpoint_path ) [EOL] predictions = estimator . predict ( infer_input_fn , checkpoint_path = checkpoint_path ) [EOL] [EOL] if hierarchical : [EOL] post_processor = PostProcessor ( shopping . HierarchicalArgMax ( data_dir ) ) [EOL] else : [EOL] if args . greedy : [EOL] post_processor = PostProcessor ( SequentialGreedy ( data_dir ) ) [EOL] else : [EOL] post_processor = PostProcessor ( shopping . ScoreMax ( data_dir ) ) [EOL] [EOL] predictions = filter ( is_not_padding_example , predictions ) [EOL] with Pool ( ) as p : [EOL] if args . debug : [EOL] predictions = map ( post_processor , predictions ) [EOL] else : [EOL] predictions = p . imap ( post_processor , predictions , chunksize = [number] ) [EOL] output_filepath = os . path . expanduser ( args . output_file ) [EOL] with open ( output_filepath , [string] ) as output_file : [EOL] for pid , label in tqdm ( predictions ) : [EOL] output_file . write ( [string] . join ( [ pid ] + list ( map ( str , label ) ) ) + [string] ) [EOL] output_file . flush ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] from typing import List , Dict [EOL] import typing [EOL] import os [EOL] [EOL] from setuptools import find_packages , setup [EOL] [EOL] [comment] [EOL] NAME = [string] [EOL] DESCRIPTION = [string] [EOL] URL = [string] [EOL] EMAIL = [string] [EOL] AUTHOR = [string] [EOL] REQUIRES_PYTHON = [string] [EOL] VERSION = [string] [EOL] [EOL] [comment] [EOL] REQUIRED = [ [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] EXTRAS = { [string] : [ [string] ] , [string] : [ [string] ] } [EOL] [EOL] here = os . path . abspath ( os . path . dirname ( __file__ ) ) [EOL] [EOL] [comment] [EOL] setup ( name = NAME , version = VERSION , description = DESCRIPTION , long_description_content_type = [string] , author = AUTHOR , author_email = EMAIL , python_requires = REQUIRES_PYTHON , url = URL , packages = find_packages ( exclude = ( [string] , ) ) , install_requires = REQUIRED , extras_require = EXTRAS , include_package_data = True , license = [string] , classifiers = [ [string] , [string] , [string] , [string] , [string] , ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any [EOL] import argparse [EOL] import typing [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import os [EOL] [EOL] from tqdm . auto import tqdm [EOL] [EOL] from shopping . data . datasets . shopping import normalize [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] input_filepath = os . path . expanduser ( args . input_file ) [EOL] output_filepath = os . path . expanduser ( args . output_file ) [EOL] [EOL] with open ( output_filepath , [string] , encoding = [string] ) as output_file : [EOL] with open ( input_filepath , encoding = [string] ) as input_file : [EOL] for line in tqdm ( input_file ) : [EOL] normalized = normalize ( line . rstrip ( ) ) [EOL] output_file . write ( normalized + [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] from __future__ import absolute_import [EOL] from __future__ import division [EOL] from __future__ import print_function [EOL] [EOL] import os [EOL] import numpy as np [EOL] import six [EOL] from six . moves import zip [comment] [EOL] import tensorflow as tf [EOL] [EOL] flags = tf . flags [EOL] FLAGS = flags . FLAGS [EOL] [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] [string] ) [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] [EOL] [EOL] def checkpoint_exists ( path ) : [EOL] return ( tf . gfile . Exists ( path ) or tf . gfile . Exists ( path + [string] ) or tf . gfile . Exists ( path + [string] ) ) [EOL] [EOL] [EOL] def main ( _ ) : [EOL] if FLAGS . checkpoints : [EOL] [comment] [EOL] checkpoints = [ c . strip ( ) for c in FLAGS . checkpoints . split ( [string] ) ] [EOL] checkpoints = [ c for c in checkpoints if c ] [EOL] if not checkpoints : [EOL] raise ValueError ( [string] ) [EOL] if FLAGS . prefix : [EOL] checkpoints = [ FLAGS . prefix + c for c in checkpoints ] [EOL] else : [EOL] assert FLAGS . num_last_checkpoints >= [number] , [string] [EOL] assert FLAGS . prefix , ( [string] [string] ) [EOL] checkpoint_state = tf . train . get_checkpoint_state ( os . path . dirname ( FLAGS . prefix ) ) [EOL] [comment] [EOL] checkpoints = checkpoint_state . all_model_checkpoint_paths [ - FLAGS . num_last_checkpoints : ] [EOL] [EOL] checkpoints = [ c for c in checkpoints if checkpoint_exists ( c ) ] [EOL] if not checkpoints : [EOL] if FLAGS . checkpoints : [EOL] raise ValueError ( [string] % FLAGS . checkpoints ) [EOL] else : [EOL] raise ValueError ( [string] % os . path . dirname ( FLAGS . prefix ) ) [EOL] [EOL] [comment] [EOL] tf . logging . info ( [string] ) [EOL] for c in checkpoints : [EOL] tf . logging . info ( [string] , c ) [EOL] var_list = tf . contrib . framework . list_variables ( checkpoints [ [number] ] ) [EOL] var_values , var_dtypes = { } , { } [EOL] for ( name , shape ) in var_list : [EOL] if not name . startswith ( [string] ) : [EOL] var_values [ name ] = np . zeros ( shape ) [EOL] for checkpoint in checkpoints : [EOL] reader = tf . contrib . framework . load_checkpoint ( checkpoint ) [EOL] for name in var_values : [EOL] tensor = reader . get_tensor ( name ) [EOL] var_dtypes [ name ] = tensor . dtype [EOL] var_values [ name ] += tensor [EOL] tf . logging . info ( [string] , checkpoint ) [EOL] for name in var_values : [comment] [EOL] var_values [ name ] /= len ( checkpoints ) [EOL] [EOL] with tf . variable_scope ( tf . get_variable_scope ( ) , reuse = tf . AUTO_REUSE ) : [EOL] tf_vars = [ tf . get_variable ( v , shape = var_values [ v ] . shape , dtype = var_dtypes [ v ] ) for v in var_values ] [EOL] placeholders = [ tf . placeholder ( v . dtype , shape = v . shape ) for v in tf_vars ] [EOL] assign_ops = [ tf . assign ( v , p ) for ( v , p ) in zip ( tf_vars , placeholders ) ] [EOL] global_step = tf . Variable ( [number] , name = [string] , trainable = False , dtype = tf . int64 ) [EOL] saver = tf . train . Saver ( tf . all_variables ( ) ) [EOL] [EOL] [comment] [EOL] with tf . Session ( ) as sess : [EOL] sess . run ( tf . initialize_all_variables ( ) ) [EOL] for p , assign_op , ( name , value ) in zip ( placeholders , assign_ops , six . iteritems ( var_values ) ) : [EOL] sess . run ( assign_op , { p : value } ) [EOL] [comment] [EOL] saver . save ( sess , FLAGS . output_path , global_step = global_step ) [EOL] [EOL] tf . logging . info ( [string] , FLAGS . output_path ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] tf . app . run ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import List , Any , Iterator [EOL] import shopping [EOL] import argparse [EOL] import typing [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import os [EOL] [EOL] from tqdm . auto import tqdm [EOL] [EOL] from shopping . data . datasets . shopping import DatasetReader , FeatureKeys [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] [EOL] [comment] [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] parser . add_argument ( [string] , default = None , type = str , help = [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] input_file = os . path . expanduser ( args . input_file ) [EOL] if args . output_file : [EOL] output_file = os . path . expanduser ( args . output_file ) [EOL] else : [EOL] output_file = input_file + [string] [EOL] [EOL] ds = DatasetReader ( [ input_file ] ) [EOL] [EOL] iterators = [ ds . text_column_generator ( FeatureKeys . ID ) , ds . text_column_generator ( FeatureKeys . TITLE ) , ds . text_column_generator ( FeatureKeys . MODEL ) , ds . text_column_generator ( [string] ) , ds . text_column_generator ( [string] ) , ds . column_generator ( [string] ) , ds . text_column_generator ( [string] ) , ds . column_generator ( [string] ) , ds . column_generator ( [string] ) , ds . column_generator ( [string] ) , ds . column_generator ( [string] ) , ] [EOL] [EOL] def remove_tabs ( s ) : [EOL] return [string] . join ( s . split ( ) ) [EOL] [EOL] with open ( output_file , [string] , encoding = [string] ) as fp : [EOL] for features in tqdm ( zip ( * iterators ) , total = ds . size ( ) ) : [EOL] features = map ( str , features ) [EOL] features = map ( remove_tabs , features ) [EOL] fp . write ( [string] . join ( features ) + [string] ) [EOL] fp . flush ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] [comment] [EOL] import sys [EOL] [EOL] if sys . version_info < ( [number] , [number] , [number] ) : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] [EOL] import shopping . data . datasets [EOL] import shopping . models [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import List , Sequence , Iterable [EOL] import shopping [EOL] import builtins [EOL] import typing [EOL] import abc [EOL] from typing import List , Iterable , Sequence [EOL] [EOL] import sentencepiece as spm [EOL] [EOL] [EOL] class Token : [EOL] def __init__ ( self , text ) : [EOL] self . text = text [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( self , other . __class__ ) : [EOL] return self . __dict__ == other . __dict__ [EOL] return NotImplemented [EOL] [EOL] [EOL] class Tokenizer ( abc . ABC ) : [EOL] [docstring] [EOL] [EOL] @ abc . abstractmethod def tokenize ( self , sentence ) : [EOL] [docstring] [EOL] [EOL] def detokenize ( self , tokens ) : [EOL] return NotImplemented [EOL] [EOL] [EOL] class SentencePieceTokenizer ( Tokenizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_file , remove_space_symbol = False ) : [EOL] self . should_remove_space_symbol = remove_space_symbol [EOL] self . _tokenizer = spm . SentencePieceProcessor ( ) [EOL] self . _tokenizer . Load ( model_file ) [EOL] [EOL] def __deepcopy__ ( self , memo ) : [EOL] [comment] [EOL] return None [EOL] [EOL] def tokenize ( self , sentence ) : [EOL] tokens = self . _tokenizer . EncodeAsPieces ( sentence ) [EOL] if self . should_remove_space_symbol : [EOL] tokens = map ( self . _remove_space_symbol , tokens ) [EOL] [comment] [EOL] tokens = filter ( None , tokens ) [EOL] tokens = [ Token ( token ) for token in tokens ] [EOL] return tokens [EOL] [EOL] def _remove_space_symbol ( self , s ) : [EOL] return s . replace ( [string] , [string] ) [EOL] [EOL] def detokenize ( self , tokens ) : [EOL] if self . should_remove_space_symbol : [EOL] return [string] . join ( tokens ) [EOL] return self . _tokenizer . DecodePieces ( tokens ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Sequence[Token]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Iterable[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[Token]$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 0 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 0 0 0 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 $typing.List[shopping.data.tokenizer.Token]$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Iterable[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Iterable[builtins.str]$ 0 0
[docstring] [EOL] [EOL] from shopping . data . datasets import shopping [EOL]	0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from shopping . models import image [EOL]	0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import tensorflow as tf [EOL] from tensorflow . contrib . layers . python . layers import utils [EOL] from tensor2tensor . utils import registry [EOL] from tensor2tensor . utils import t2t_model [EOL] from tensor2tensor . layers import common_layers [EOL] from tensor2tensor . layers import common_attention [EOL] from tensor2tensor . models import basic [EOL] from tensor2tensor . models import lstm [EOL] from tensor2tensor . models . research import vqa_attention [EOL] [EOL] [EOL] def basic_fc_relu ( hparams , x , name = [string] ) : [EOL] with tf . variable_scope ( name ) : [EOL] shape = common_layers . shape_list ( x ) [EOL] x = tf . reshape ( x , [ - [number] , shape [ [number] ] * shape [ [number] ] * shape [ [number] ] ] ) [EOL] for i in range ( hparams . num_hidden_layers ) : [EOL] x = tf . layers . dense ( x , hparams . hidden_size , name = [string] % i ) [EOL] x = tf . nn . dropout ( x , keep_prob = [number] - hparams . dropout ) [EOL] x = tf . nn . relu ( x ) [EOL] return tf . expand_dims ( tf . expand_dims ( x , axis = [number] ) , axis = [number] ) [comment] [EOL] [EOL] [EOL] @ registry . register_model class ImageOnly ( t2t_model . T2TModel ) : [EOL] [docstring] [EOL] [EOL] def body ( self , features ) : [EOL] hparams = self . hparams [EOL] image_feat = features [ [string] ] [EOL] outputs = basic_fc_relu ( hparams , image_feat ) [EOL] return outputs [EOL] [EOL] [EOL] @ registry . register_model class ImageAndPrice ( t2t_model . T2TModel ) : [EOL] [docstring] [EOL] [EOL] def body ( self , features ) : [EOL] hparams = self . hparams [EOL] image_feat = features [ [string] ] [EOL] price = tf . math . log ( tf . to_float ( features [ [string] ] ) + [number] ) [EOL] [EOL] print ( [string] ) [EOL] print ( [string] . format ( image_feat . shape , image_feat . dtype ) ) [EOL] print ( [string] . format ( price . shape , price . dtype ) ) [EOL] [EOL] price = common_layers . expand_squeeze_to_nd ( price , len ( image_feat . shape ) , expand_dim = [number] ) [EOL] inputs = tf . concat ( [ image_feat , price ] , axis = - [number] ) [EOL] [EOL] print ( [string] . format ( inputs . shape , inputs . dtype ) ) [EOL] [EOL] outputs = basic_fc_relu ( hparams , inputs ) [EOL] return outputs [EOL] [EOL] [EOL] @ registry . register_hparams def image_only_base ( ) : [EOL] [docstring] [EOL] hparams = basic . basic_fc_small ( ) [EOL] hparams . hidden_size = [number] [EOL] hparams . batch_size = [number] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def image_only_small ( ) : [EOL] [docstring] [EOL] hparams = image_only_base ( ) [EOL] hparams . hidden_size = [number] [EOL] hparams . batch_size = [number] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def image_only_tall ( ) : [EOL] [docstring] [EOL] hparams = image_only_base ( ) [EOL] hparams . batch_size = [number] [EOL] hparams . num_hidden_layers = [number] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_model class ImageLSTMConcat ( lstm . LSTMSeq2seqAttentionBidirectionalEncoder ) : [EOL] [docstring] [EOL] [EOL] def body ( self , features ) : [EOL] base_outputs = super ( ) . body ( features ) [EOL] [EOL] hparams = self . hparams [EOL] x = features [ [string] ] [EOL] image_outputs = basic_fc_relu ( hparams , x ) [EOL] [EOL] outputs = tf . concat ( [ base_outputs , image_outputs ] , axis = - [number] ) [EOL] return outputs [EOL] [EOL] [EOL] def attn ( image_feat , query , hparams , name = [string] ) : [EOL] [docstring] [EOL] print ( [string] ) [EOL] print ( [string] . format ( image_feat . shape , image_feat . dtype ) ) [EOL] print ( [string] . format ( query . shape , query . dtype ) ) [EOL] with tf . variable_scope ( name , [string] , values = [ image_feat , query ] ) : [EOL] attn_dim = hparams . attn_dim [EOL] num_glimps = hparams . num_glimps [EOL] num_channels = common_layers . shape_list ( image_feat ) [ - [number] ] [EOL] if len ( common_layers . shape_list ( image_feat ) ) == [number] : [EOL] image_feat = common_layers . flatten4d3d ( image_feat ) [EOL] if len ( common_layers . shape_list ( query ) ) == [number] : [EOL] query = common_layers . flatten4d3d ( query ) [EOL] if len ( common_layers . shape_list ( query ) ) == [number] : [EOL] query = tf . expand_dims ( query , [number] ) [EOL] image_proj = common_attention . compute_attention_component ( image_feat , attn_dim , name = [string] ) [EOL] query_proj = common_attention . compute_attention_component ( query , attn_dim , name = [string] ) [EOL] h = tf . nn . relu ( image_proj + query_proj ) [EOL] h_proj = common_attention . compute_attention_component ( h , num_glimps , name = [string] ) [EOL] p = tf . nn . softmax ( h_proj , axis = [number] ) [EOL] image_ave = tf . matmul ( image_feat , p , transpose_a = True ) [EOL] image_ave = tf . reshape ( image_ave , [ - [number] , num_channels * num_glimps ] ) [EOL] [EOL] return image_ave [EOL] [EOL] [EOL] def mlp ( feature , hparams , name = [string] ) : [EOL] [docstring] [EOL] print ( [string] ) [EOL] print ( [string] . format ( feature . shape , feature . dtype ) ) [EOL] with tf . variable_scope ( name , [string] , values = [ feature ] ) : [EOL] num_mlp_layers = hparams . num_mlp_layers [EOL] mlp_dim = hparams . mlp_dim [EOL] for _ in range ( num_mlp_layers ) : [EOL] feature = common_layers . dense ( feature , mlp_dim , activation = tf . nn . relu ) [EOL] feature = tf . nn . dropout ( feature , keep_prob = [number] - hparams . dropout ) [EOL] return feature [EOL] [EOL] [EOL] @ registry . register_model class ImageFcAttention ( lstm . LSTMSeq2seqAttentionBidirectionalEncoder ) : [EOL] [docstring] [EOL] [EOL] def body ( self , features ) : [EOL] query = super ( ) . body ( features ) [EOL] [EOL] hparams = self . hparams [EOL] image_feat = features [ [string] ] [EOL] [EOL] [comment] [EOL] image_feat = common_layers . l2_norm ( image_feat ) [EOL] [EOL] [comment] [EOL] image_feat = basic_fc_relu ( hparams , image_feat ) [EOL] [EOL] [comment] [EOL] image_ave = attn ( image_feat , query , hparams ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_ave , axis = - [number] ) ) [EOL] [EOL] query = tf . squeeze ( query , axis = [ [number] , [number] ] ) [EOL] [EOL] image_text = tf . concat ( [ image_ave , query ] , axis = [number] ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_text , axis = - [number] ) ) [EOL] image_text = tf . nn . dropout ( image_text , [number] - hparams . dropout ) [EOL] [EOL] [comment] [EOL] output = mlp ( image_text , hparams ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( output , axis = - [number] ) ) [EOL] [EOL] [comment] [EOL] return tf . expand_dims ( tf . expand_dims ( output , axis = [number] ) , axis = [number] ) [EOL] [EOL] [EOL] @ registry . register_model class ImageSelfAttention ( lstm . LSTMSeq2seqAttentionBidirectionalEncoder ) : [EOL] [docstring] [EOL] [EOL] def body ( self , features ) : [EOL] query = super ( ) . body ( features ) [EOL] [EOL] hparams = self . hparams [EOL] image_feat = features [ [string] ] [EOL] [EOL] image_feat = common_layers . flatten4d3d ( image_feat ) [EOL] image_feat = tf . nn . dropout ( image_feat , keep_prob = [number] - hparams . dropout ) [EOL] [EOL] image_feat = vqa_attention . image_encoder ( image_feat , hparams ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_feat , axis = - [number] ) ) [EOL] [EOL] [comment] [EOL] image_feat = common_layers . l2_norm ( image_feat ) [EOL] [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_feat , axis = - [number] ) ) [EOL] [EOL] [comment] [EOL] image_ave = attn ( image_feat , query , hparams ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_ave , axis = - [number] ) ) [EOL] [EOL] query = tf . squeeze ( query , axis = [ [number] , [number] ] ) [EOL] [EOL] image_text = tf . concat ( [ image_ave , query ] , axis = [number] ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( image_text , axis = - [number] ) ) [EOL] image_text = tf . nn . dropout ( image_text , [number] - hparams . dropout ) [EOL] [EOL] [comment] [EOL] output = mlp ( image_text , hparams ) [EOL] utils . collect_named_outputs ( [string] , [string] , tf . norm ( output , axis = - [number] ) ) [EOL] [EOL] [comment] [EOL] return tf . expand_dims ( tf . expand_dims ( output , axis = [number] ) , axis = [number] ) [EOL] [EOL] [EOL] @ registry . register_hparams def image_attention_base ( ) : [EOL] [docstring] [EOL] hparams = lstm_base_batch_4k_hidden_1k ( ) [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_base_batch_8k_hidden_1k ( ) : [EOL] [docstring] [EOL] hparams = lstm . lstm_attention ( ) [EOL] [EOL] hparams . batch_size = [number] [EOL] hparams . hidden_size = [number] [EOL] hparams . attention_layer_size = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_base_batch_4k_hidden_1k ( ) : [EOL] [docstring] [EOL] hparams = lstm_base_batch_8k_hidden_1k ( ) [EOL] [EOL] hparams . batch_size = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_base_batch_16k_hidden_1k ( ) : [EOL] [docstring] [EOL] hparams = lstm_base_batch_8k_hidden_1k ( ) [EOL] [EOL] hparams . batch_size = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_big ( ) : [EOL] [docstring] [EOL] hparams = lstm_base_batch_16k_hidden_1k ( ) [EOL] [EOL] hparams . optimizer = [string] [EOL] hparams . optimizer_adam_beta1 = [number] [EOL] hparams . optimizer_adam_beta2 = [number] [EOL] hparams . optimizer_adam_epsilon = [number] [EOL] [EOL] hparams . learning_rate_schedule = ( [string] ) [EOL] hparams . learning_rate_constant = [number] [EOL] hparams . learning_rate_warmup_steps = [number] [EOL] hparams . learning_rate_decay_steps = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_big_multistep8 ( ) : [EOL] [docstring] [EOL] hparams = lstm_big ( ) [EOL] [EOL] hparams . optimizer = [string] [EOL] hparams . optimizer_multistep_accumulate_steps = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_base_batch_8k_hidden_2k ( ) : [EOL] [docstring] [EOL] hparams = lstm_base_batch_8k_hidden_1k ( ) [EOL] [EOL] hparams . hidden_size = [number] [EOL] hparams . attention_layer_size = [number] [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_base_batch_32k_hidden_512 ( ) : [EOL] [docstring] [EOL] hparams = lstm . lstm_attention ( ) [EOL] [EOL] hparams . batch_size = [number] [EOL] hparams . hidden_size = [number] [EOL] hparams . attention_layer_size = [number] [EOL] [EOL] hparams . weight_decay = [number] [EOL] hparams . dropout = [number] [EOL] [EOL] hparams . optimizer = [string] [EOL] hparams . optimizer_adam_beta1 = [number] [EOL] hparams . optimizer_adam_beta2 = [number] [EOL] hparams . optimizer_adam_epsilon = [number] [EOL] [EOL] hparams . learning_rate_schedule = ( [string] ) [EOL] hparams . learning_rate_constant = [number] [EOL] hparams . learning_rate_warmup_steps = [number] [EOL] hparams . learning_rate_decay_steps = [number] [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] [comment] [EOL] hparams . norm_type = [string] [EOL] hparams . layer_preprocess_sequence = [string] [EOL] hparams . layer_postprocess_sequence = [string] [EOL] hparams . layer_prepostprocess_dropout = [number] [EOL] hparams . attention_dropout = [number] [EOL] hparams . relu_dropout = [number] [EOL] hparams . image_hidden_size = [number] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [comment] [EOL] hparams . num_heads = [number] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [string] ) [EOL] hparams . add_hparam ( [string] , True ) [EOL] [EOL] return hparams [EOL] [EOL] [EOL] @ registry . register_hparams def lstm_tall_ls0 ( ) : [EOL] [docstring] [EOL] hparams = lstm . lstm_attention ( ) [EOL] [EOL] hparams . batch_size = [number] [EOL] hparams . hidden_size = [number] [EOL] hparams . attention_layer_size = [number] [EOL] [EOL] hparams . num_hidden_layers = [number] [EOL] [EOL] hparams . label_smoothing = [number] [EOL] [EOL] hparams . optimizer = [string] [EOL] hparams . optimizer_adam_beta1 = [number] [EOL] hparams . optimizer_adam_beta2 = [number] [EOL] hparams . optimizer_adam_epsilon = [number] [EOL] [EOL] hparams . learning_rate_schedule = ( [string] ) [EOL] hparams . learning_rate_constant = [number] [EOL] hparams . learning_rate_warmup_steps = [number] [EOL] hparams . learning_rate_decay_steps = [number] [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [EOL] [comment] [EOL] hparams . norm_type = [string] [EOL] hparams . layer_preprocess_sequence = [string] [EOL] hparams . layer_postprocess_sequence = [string] [EOL] hparams . layer_prepostprocess_dropout = [number] [EOL] hparams . attention_dropout = [number] [EOL] hparams . relu_dropout = [number] [EOL] hparams . image_hidden_size = [number] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] [comment] [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [number] ) [EOL] hparams . add_hparam ( [string] , [string] ) [EOL] hparams . add_hparam ( [string] , True ) [EOL] [EOL] return hparams [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0