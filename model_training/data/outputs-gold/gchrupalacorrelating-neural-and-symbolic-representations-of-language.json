[comment] [EOL] from setuptools import setup [EOL] [EOL] setup ( name = [string] , version = [string] , description = [string] , url = [string] , author = [string] , author_email = [string] , license = [string] , zip_safe = False ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import random [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] import torch . optim [EOL] import rsa . arithmetic as A [EOL] import rsa . correlate as C [EOL] import rsa . models as M [EOL] import rsa . kernel as K [EOL] from collections import Counter [EOL] from sklearn . linear_model import LogisticRegression , Ridge [EOL] from sklearn . model_selection import cross_val_score [EOL] from scipy . stats import pearsonr [EOL] from rsa . correlate import corr_cv [EOL] [EOL] def accuracy ( tgt , pred ) : [EOL] return torch . mean ( ( tgt == pred ) . float ( ) ) . item ( ) [EOL] [EOL] [EOL] def score ( U , V ) : [EOL] u = C . triu ( U ) . double ( ) [EOL] v = C . triu ( V ) . double ( ) [EOL] return C . pearson ( u , v , dim = [number] ) . item ( ) [EOL] [EOL] def summary ( x ) : [EOL] return dict ( mean = x . mean ( ) , std = x . std ( ) ) [EOL] [EOL] class Scorer : [EOL] [EOL] def __init__ ( self , data , reference = None , depth_f = lambda e : e . depth ( ) , value_f = lambda e : e . evaluate ( [number] ) , kernel_f = lambda E , ** kwargs : K . pairwise ( E , normalize = True , ignore_leaves = True , ** kwargs ) , alphas = ( [number] , [number] ) , device = [string] ) : [EOL] self . data = data [EOL] self . alphas = alphas [EOL] if reference is not None : [EOL] self . reference = reference [EOL] else : [EOL] _ , self . reference = A . generate_batch ( p = [number] , decay = [number] , size = [number] ) [EOL] self . depth = torch . tensor ( [ depth_f ( e ) for e in data ] , dtype = torch . float32 , device = device ) [EOL] self . value = torch . tensor ( [ value_f ( e ) for e in data ] , dtype = torch . float32 , device = device ) [EOL] self . D_tree = { alpha : [number] - kernel_f ( self . data , alpha = alpha ) . to ( device = device ) for alpha in self . alphas } [EOL] self . D_depth = C . abs_diff ( self . depth ) [EOL] self . D_value = C . abs_diff ( self . value ) [EOL] self . tree_depth = { alpha : score ( self . D_tree [ alpha ] , self . D_depth ) for alpha in self . alphas } [EOL] self . tree_value = { alpha : score ( self . D_tree [ alpha ] , self . D_value ) for alpha in self . alphas } [EOL] self . depth_value = score ( self . D_depth , self . D_value ) [EOL] self . emb_tree = { alpha : [number] - kernel_f ( self . data , self . reference , alpha = alpha ) . detach ( ) . cpu ( ) . numpy ( ) for alpha in self . alphas } [EOL] self . emb_depth = C . abs_diff_ ( self . depth , torch . tensor ( [ depth_f ( e ) for e in self . reference ] , dtype = torch . float32 , device = device ) ) . detach ( ) . cpu ( ) . numpy ( ) [EOL] self . emb_value = C . abs_diff_ ( self . value , torch . tensor ( [ value_f ( e ) for e in self . reference ] , dtype = torch . float32 , device = device ) ) . detach ( ) . cpu ( ) . numpy ( ) [EOL] [EOL] def score ( self , encode_f ) : [EOL] rep = encode_f ( self . data ) [EOL] D_rep = [number] - C . cosine_matrix ( rep , rep ) [EOL] emb = [number] - C . cosine_matrix ( rep , encode_f ( self . reference ) ) . detach ( ) . cpu ( ) . numpy ( ) [EOL] X_rep = rep . detach ( ) . cpu ( ) . numpy ( ) [EOL] y_value = self . value . detach ( ) . cpu ( ) . numpy ( ) [EOL] y_depth = self . depth . detach ( ) . cpu ( ) . numpy ( ) [EOL] r_value = corr_cv ( Ridge ( alpha = [number] ) , X_rep , y_value , cv = [number] ) [EOL] r_depth = corr_cv ( Ridge ( alpha = [number] ) , X_rep , y_depth , cv = [number] ) [EOL] acc_value = cross_val_score ( LogisticRegression ( C = [number] , solver = [string] , multi_class = [string] ) , X_rep , y_value , cv = [number] ) [EOL] acc_depth = cross_val_score ( LogisticRegression ( C = [number] , solver = [string] , multi_class = [string] ) , X_rep , y_depth , cv = [number] ) [EOL] return dict ( rsa = { alpha : dict ( rep_tree = score ( D_rep , self . D_tree [ alpha ] ) , rep_depth = score ( D_rep , self . D_depth ) , rep_value = score ( D_rep , self . D_value ) , tree_depth = self . tree_depth [ alpha ] , tree_value = self . tree_value [ alpha ] , depth_value = self . depth_value ) for alpha in self . alphas } , diagnostic = dict ( r_depth = summary ( r_depth ) , r_value = summary ( r_value ) , acc_depth = summary ( acc_depth ) , acc_value = summary ( acc_value ) ) , rsa_regress = { alpha : dict ( tree = rsa_regress ( emb , self . emb_tree [ alpha ] , cv = [number] ) , value = rsa_regress ( emb , self . emb_value , cv = [number] ) , depth = rsa_regress ( emb , self . emb_depth , cv = [number] ) ) for alpha in self . alphas } ) [EOL] [EOL] def rsa_regress ( emb1 , emb2 , cv = [number] ) : [EOL] return dict ( r_12 = summary ( corr_cv ( Ridge ( alpha = [number] ) , emb1 , emb2 , cv = cv ) ) , r_21 = summary ( corr_cv ( Ridge ( alpha = [number] ) , emb2 , emb1 , cv = cv ) ) ) [EOL] [EOL] def get_device ( net ) : [EOL] return list ( net . parameters ( ) ) [ [number] ] . device [EOL] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] from functools import reduce [EOL] import rsa . arithmetic as A [EOL] import logging [EOL] [EOL] [EOL] def deleaf ( t ) : [EOL] if t . is_leaf ( ) : [EOL] return A . Expression ( None ) [EOL] else : [EOL] return A . Expression ( t . op , left = deleaf ( t . left ) , right = deleaf ( t . right ) ) [EOL] [EOL] def pairwise ( T1 , T2 = None , normalize = False , ignore_leaves = False , device = [string] , alpha = [number] ) : [EOL] import torch [EOL] import numpy as np [EOL] from ursa . kernel import Kernel [EOL] if ignore_leaves : [EOL] T1 = [ deleaf ( t ) for t in T1 ] [EOL] if T2 is not None : [EOL] T2 = [ deleaf ( t ) for t in T2 ] [EOL] T1 = [ etree ( t ) for t in T1 ] [EOL] if T2 is not None : [EOL] T2 = [ etree ( t ) for t in T2 ] [EOL] K = Kernel ( alpha = alpha ) [EOL] return torch . tensor ( K . pairwise ( T1 , T2 , normalize = normalize , dtype = np . float64 ) ) [EOL] [EOL] def etree ( e ) : [EOL] [docstring] [EOL] from nltk . tree import Tree [EOL] if e . is_leaf ( ) : [EOL] return Tree ( [string] , [ Tree ( [string] , [ str ( e . value ) ] ) ] ) [EOL] else : [EOL] return Tree ( [string] , [ Tree ( [string] , [ [string] ] ) , etree ( e . left ) , Tree ( [string] , [ e . op ] ) , etree ( e . right ) , Tree ( [string] , [ [string] ] ) ] ) [EOL] [EOL] def cosine ( A ) : [EOL] M = A @ A . t ( ) [EOL] diag = torch . diag ( M ) . unsqueeze ( dim = [number] ) [EOL] denom = diag ** [number] * diag . t ( ) ** [number] [EOL] return M / denom [EOL] [EOL] def node_overlap ( a , b ) : [EOL] s = [number] [EOL] for n1 in a . nodes ( ) : [EOL] for n2 in b . nodes ( ) : [EOL] if n1 == n2 : [EOL] s += [number] [EOL] return s [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import numpy as np [EOL] [EOL] def rsa ( A , B ) : [EOL] [docstring] [EOL] M_A = cosine_matrix ( A , A ) [EOL] M_B = cosine_matrix ( B , B ) [EOL] return pearson ( triu ( M_A ) , triu ( M_B ) , dim = [number] ) [EOL] [EOL] [EOL] def cosine_matrix ( U , V ) : [EOL] [docstring] [EOL] U_norm = U / U . norm ( [number] , dim = [number] , keepdim = True ) [EOL] V_norm = V / V . norm ( [number] , dim = [number] , keepdim = True ) [EOL] return U_norm @ V_norm . t ( ) [EOL] [EOL] [EOL] def triu ( x ) : [EOL] [docstring] [EOL] ones = torch . ones_like ( x ) [EOL] return x [ torch . triu ( ones , diagonal = [number] ) == [number] ] [EOL] [EOL] [EOL] def pearson ( x , y , dim = [number] , eps = [number] ) : [EOL] [docstring] [EOL] x1 = x - torch . mean ( x , dim ) [EOL] x2 = y - torch . mean ( y , dim ) [EOL] w12 = torch . sum ( x1 * x2 , dim ) [EOL] w1 = torch . norm ( x1 , [number] , dim ) [EOL] w2 = torch . norm ( x2 , [number] , dim ) [EOL] return w12 / ( w1 * w2 ) . clamp ( min = eps ) [EOL] [EOL] def pearsonr ( x , y , axis = [number] , eps = [number] ) : [EOL] [docstring] [EOL] from numpy . linalg import norm [EOL] x1 = x - x . mean ( axis = axis ) [EOL] x2 = y - y . mean ( axis = axis ) [EOL] w12 = np . sum ( x1 * x2 , axis = axis ) [EOL] w1 = norm ( x1 , [number] , axis = axis ) [EOL] w2 = norm ( x2 , [number] , axis = axis ) [EOL] return w12 / np . maximum ( eps , ( w1 * w2 ) ) [EOL] [EOL] def abs_diff ( x ) : [EOL] x = x . unsqueeze ( dim = [number] ) [EOL] return torch . abs ( x - x . t ( ) ) [EOL] [EOL] def abs_diff_ ( x , y ) : [EOL] x = x . unsqueeze ( dim = [number] ) [EOL] y = y . unsqueeze ( dim = [number] ) [EOL] return torch . abs ( x . t ( ) - y ) [EOL] [EOL] def shuffled ( x ) : [EOL] i = torch . randperm ( x . size ( [number] ) ) [EOL] return x [ i ] [EOL] [EOL] class LinearRegression ( ) : [EOL] [EOL] def __init__ ( self ) : [EOL] self . coef = None [EOL] self . intercept = None [EOL] [EOL] def fit ( X , Y ) : [EOL] const = torch . ones_like ( X [ : , [number] : [number] ] ) [EOL] X = torch . cat ( [ const , X ] , dim = [number] ) [EOL] coef = torch . inverse ( X . t ( ) @ X ) @ X . t ( ) @ Y [EOL] self . intercept = coef [ [number] ] [EOL] self . coef = coef [ [number] : ] [EOL] [EOL] def predict ( X ) : [EOL] return X @ coef + intercept [EOL] [EOL] def RSA_regress ( reference , data , sim1 , sim2 , cv = [number] ) : [EOL] [docstring] [EOL] from sklearn . model_selection import cross_val_score [EOL] from sklearn . linear_model import Ridge [EOL] from scipy . stats import pearsonr [EOL] emb1 = sim1 ( data , reference ) [EOL] emb2 = sim2 ( data , reference ) [EOL] result = dict ( pearsonr = pearsonr ( emb1 . reshape ( - [number] ) , emb2 . reshape ( - [number] ) ) , r2_12 = cross_val_score ( Ridge ( alpha = [number] ) , emb1 , emb2 , cv = cv ) , r2_21 = cross_val_score ( Ridge ( alpha = [number] ) , emb2 , emb1 , cv = cv ) ) [EOL] return result [EOL] [EOL] [EOL] def pairwise ( f , data1 , data2 = None , normalize = False , device = [string] , dtype = torch . float32 ) : [EOL] [docstring] [EOL] symmetric = False [EOL] if data2 is None : [EOL] data2 = data1 [EOL] symmetric = True [EOL] M = torch . zeros ( len ( data1 ) , len ( data2 ) , dtype = dtype ) [EOL] if normalize : [EOL] self1 = torch . tensor ( [ f ( d , d ) for d in data1 ] , dtype = dtype , device = device ) [EOL] self2 = self1 if symmetric else torch . tensor ( [ f ( d , d ) for d in data2 ] , dtype = dtype , device = device ) [EOL] for i , d1 in enumerate ( data1 ) : [EOL] for j , d2 in enumerate ( data2 ) : [EOL] denom = ( self1 [ i ] * self2 [ j ] ) ** [number] if normalize else [number] [EOL] if symmetric and i > j : [comment] [EOL] M [ i , j ] = M [ j , i ] [EOL] else : [EOL] M [ i , j ] = f ( d1 , d2 ) / denom [EOL] return M [EOL] [EOL] def compute_value ( f , self1 , self2 , i , j , d1 , d2 , normalize ) : [EOL] denom = ( self1 [ i ] * self2 [ j ] ) ** [number] if normalize else [number] [EOL] return f ( d1 , d2 ) / denom [EOL] [EOL] [EOL] def pairwise_parallel ( f , data1 , data2 = None , normalize = False , device = [string] , backend = [string] , n_jobs = - [number] , dtype = torch . float32 ) : [EOL] [docstring] [EOL] from joblib import Parallel , delayed [EOL] symmetric = False [EOL] if data2 is None : [EOL] data2 = data1 [EOL] symmetric = True [EOL] if normalize : [EOL] self1 = torch . tensor ( [ f ( d , d ) for d in data1 ] , dtype = dtype , device = device ) [EOL] self2 = self1 if symmetric else torch . tensor ( [ f ( d , d ) for d in data2 ] , dtype = dtype , device = device ) [EOL] M = torch . tensor ( Parallel ( n_jobs = n_jobs , backend = backend ) ( delayed ( compute_value ) ( f , self1 , self2 , i , j , d1 , d2 , normalize ) for i , d1 in enumerate ( data1 ) for j , d2 in enumerate ( data2 ) ) , dtype = dtype , device = device ) . view ( len ( data1 ) , len ( data2 ) ) [EOL] return M [EOL] [EOL] def corr_cv ( model , X , Y , cv = [number] ) : [EOL] [docstring] [EOL] from sklearn . model_selection import cross_val_score [EOL] rs = cross_val_score ( model , X , Y , scoring = score_r , cv = cv , error_score = np . nan ) [EOL] return rs [EOL] [EOL] def score_r ( model , X , y ) : [EOL] r = pearsonr ( y , model . predict ( X ) , axis = [number] ) . mean ( ) [EOL] [comment] [EOL] [comment] [EOL] return r [EOL] [EOL] def score_r2 ( model , X , y ) : [EOL] from sklearn . metrics import r2_score [EOL] r = r2_score ( y . reshape ( - [number] ) , model . predict ( X ) . reshape ( - [number] ) ) [EOL] rmvr = r2_score ( y , model . predict ( X ) ) [EOL] print ( [string] , r , rmvr ) [EOL] [comment] [EOL] [comment] [EOL] return r [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Counter [EOL] import typing [EOL] import rsa [EOL] import collections [EOL] import torch [EOL] import torch . optim [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] from collections import Counter [EOL] import rsa . arithmetic as A [EOL] class Encoder ( nn . Module ) : [EOL] [EOL] def __init__ ( self , size_in = [number] , size = [number] , depth = [number] , size_embed = [number] ) : [EOL] super ( Encoder , self ) . __init__ ( ) [EOL] self . size_in = size_in [EOL] self . size = size [EOL] self . depth = depth [EOL] self . size_embed = size_embed [EOL] self . Embed = nn . Embedding ( self . size_in , self . size_embed ) [EOL] self . RNN = nn . LSTM ( self . size_embed , self . size , self . depth , batch_first = True ) [EOL] [EOL] [EOL] def forward ( self , text ) : [EOL] out , last = self . RNN ( self . Embed ( text ) ) [EOL] h_last , c_last = last [EOL] return h_last [EOL] [EOL] [EOL] [EOL] class Decoder ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , size_in = [number] , size = [number] , depth = [number] , size_embed = [number] ) : [EOL] super ( Decoder , self ) . __init__ ( ) [EOL] self . size_in = size_in [EOL] self . size = size [EOL] self . depth = depth [EOL] self . size_embed = size_embed [EOL] self . Embed = nn . Embedding ( self . size_in , self . size_embed ) [EOL] self . RNN = nn . LSTM ( self . size_embed , self . size , self . depth , batch_first = True ) [EOL] self . Proj = nn . Linear ( self . size , self . size_in ) [EOL] [EOL] def forward ( self , rep , prev ) : [EOL] c_0 = torch . zeros_like ( rep ) [EOL] out , last = self . RNN ( self . Embed ( prev ) , ( rep , c_0 ) ) [EOL] pred = self . Proj ( out ) [EOL] return pred [EOL] [EOL] class MLP ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , size_in , size , size_out ) : [EOL] super ( MLP , self ) . __init__ ( ) [EOL] self . L1 = nn . Linear ( size_in , size ) [EOL] self . L2 = nn . Linear ( size , size_out ) [EOL] [EOL] def forward ( self , x ) : [EOL] return self . L2 ( torch . tanh ( self . L1 ( x ) ) ) [EOL] [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self , encoder , n_classes = None , target_type = [string] , lr = [number] ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] self . n_classes = n_classes [EOL] self . target_type = target_type [EOL] self . encoder = encoder [EOL] if n_classes is not None : [EOL] self . MLP = MLP ( self . encoder . size , self . encoder . size * [number] , self . n_classes ) [EOL] else : [EOL] self . MLP = MLP ( self . encoder . size , self . encoder . size * [number] , [number] ) [EOL] [EOL] self . opt = torch . optim . Adam ( self . parameters ( ) , lr = lr ) [EOL] [EOL] def forward ( self , x ) : [EOL] rep = self . encoder ( x ) [EOL] pred = self . MLP ( rep ) [EOL] return pred . squeeze ( ) [EOL] [EOL] def target ( self , e ) : [EOL] if self . target_type == [string] : [EOL] return e . evaluate ( [number] ) [EOL] elif self . target_type == [string] : [EOL] return e . depth ( ) [EOL] else : [EOL] raise NotImplementedError ( ) [EOL] [EOL] def encode ( self , expressions ) : [EOL] return encode ( self , expressions ) [EOL] [EOL] def loss ( self , es , length = None ) : [EOL] if length is None : [EOL] length = max ( [ len ( list ( e . infix ( ) ) ) for e in es ] ) [EOL] loss_f = F . cross_entropy if self . n_classes is not None else F . mse_loss [EOL] target_dtype = torch . long if self . n_classes is not None else torch . float32 [EOL] data_in = torch . tensor ( [ list ( A . vocabify ( e . infix ( ) , length = length ) ) for e in es ] , dtype = torch . long , device = get_device ( self ) ) [EOL] tgt = torch . tensor ( [ self . target ( e ) for e in es ] , dtype = target_dtype , device = get_device ( self ) ) [EOL] pred = self ( data_in ) . squeeze ( ) [EOL] loss = loss_f ( pred , tgt ) [EOL] return loss [EOL] [EOL] def experiment ( net , batch_size = [number] , n = [number] , interval = [number] , decay = [number] , prefix = [string] ) : [EOL] loss_f = F . cross_entropy if net . n_classes is not None else F . mse_loss [EOL] cost = Counter ( dict ( cost = [number] , N = [number] ) ) [EOL] for i in range ( n ) : [EOL] L , es = A . generate_batch ( p = [number] , decay = decay , size = batch_size ) [EOL] loss = net . loss ( es , length = L ) [EOL] net . opt . zero_grad ( ) [EOL] loss . backward ( ) [EOL] net . opt . step ( ) [EOL] cost += Counter ( dict ( cost = loss . data . item ( ) , N = [number] ) ) [EOL] if i % interval == [number] : [EOL] print ( prefix , i , loss . data . item ( ) , cost [ [string] ] / cost [ [string] ] ) [EOL] [EOL] class Net_struct ( nn . Module ) : [EOL] def __init__ ( self , encoder , lr = [number] ) : [EOL] super ( Net_struct , self ) . __init__ ( ) [EOL] self . encoder = encoder [EOL] self . decoder = Decoder ( size_in = encoder . size_in , size = encoder . size , depth = encoder . depth ) [EOL] self . opt = torch . optim . Adam ( self . parameters ( ) , lr = lr ) [EOL] [EOL] def forward ( self , x , prev ) : [EOL] rep = self . encoder ( x ) [EOL] pred = self . decoder ( rep , prev ) [EOL] return pred . squeeze ( ) [EOL] [EOL] def encode ( self , expressions ) : [EOL] return encode ( self , expressions ) [EOL] [EOL] def loss ( net , es , pad = [number] , length = None ) : [EOL] if length is None : [EOL] length = max ( [ len ( list ( e . infix ( ) ) ) for e in es ] ) [EOL] data_in = torch . tensor ( [ list ( A . vocabify ( e . infix ( ) , length = length ) ) for e in es ] , dtype = torch . long , device = get_device ( net ) ) [EOL] tgt = torch . tensor ( [ list ( A . vocabify ( e . prefix ( ) , length = length , pad_end = True ) ) for e in es ] , dtype = torch . long , device = get_device ( net ) ) [EOL] prev = make_prev ( tgt , pad = pad ) [EOL] pred = net ( data_in , prev ) [EOL] loss = F . cross_entropy ( pred . view ( pred . size ( [number] ) * pred . size ( [number] ) , - [number] ) , tgt . view ( tgt . size ( [number] ) * tgt . size ( [number] ) ) , ignore_index = pad ) [EOL] return loss [EOL] [EOL] def experiment ( net , batch_size = [number] , n = [number] , interval = [number] , decay = [number] , pad = [number] , prefix = [string] ) : [EOL] cost = Counter ( dict ( cost = [number] , N = [number] ) ) [EOL] for i in range ( [number] , n + [number] ) : [EOL] L , es = A . generate_batch ( p = [number] , decay = decay , size = batch_size ) [EOL] loss = net . loss ( es , pad = pad , length = L ) [EOL] net . opt . zero_grad ( ) [EOL] loss . backward ( ) [EOL] net . opt . step ( ) [EOL] cost += Counter ( dict ( cost = loss . data . item ( ) , N = [number] ) ) [EOL] if i % interval == [number] : [EOL] print ( prefix , i , loss . data . item ( ) , cost [ [string] ] / cost [ [string] ] ) [EOL] [EOL] def get_device ( net ) : [EOL] return list ( net . parameters ( ) ) [ [number] ] . device [EOL] [EOL] def make_prev ( x , pad = [number] ) : [EOL] return torch . cat ( [ torch . zeros_like ( x [ : , [number] : [number] ] ) + pad , x ] , dim = [number] ) [ : , : - [number] ] [EOL] [EOL] def encode_MLP ( self , x ) : [EOL] rep = self . encoder ( x ) [EOL] pred = torch . tanh ( self . MLP . L1 ( rep ) ) [EOL] return pred . squeeze ( ) [EOL] [EOL] def encode ( net , expressions ) : [EOL] L = max ( len ( list ( e . infix ( ) ) ) for e in expressions ) [EOL] data_in = torch . tensor ( [ list ( A . vocabify ( e . infix ( ) , length = L ) ) for e in expressions ] , dtype = torch . long , device = get_device ( net ) ) [EOL] return net . encoder ( data_in ) . squeeze ( ) [EOL] [EOL] def predict ( net , expressions ) : [EOL] L = max ( len ( list ( e . infix ( ) ) ) for e in expressions ) [EOL] data_in = torch . tensor ( [ list ( A . vocabify ( e . infix ( ) , length = L ) ) for e in expressions ] , dtype = torch . long , device = get_device ( net ) ) [EOL] return net ( data_in ) . squeeze ( ) [EOL] [EOL] def borrow_weights ( source , target ) : [EOL] for s , t in zip ( source . parameters ( ) , target . parameters ( ) ) : [EOL] t . data = s . data [EOL] [EOL] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $rsa.models.Decoder$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 $collections.Counter[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Union , List , Any [EOL] import typing [EOL] import torch [EOL] from pytorch_pretrained_bert import BertTokenizer , BertModel [EOL] import logging [EOL] logging . basicConfig ( level = logging . INFO ) [EOL] import numpy as np [EOL] import nltk [EOL] from infersent . models import InferSent [EOL] import os . path [EOL] [EOL] [EOL] def encode_bert ( texts , trained = True , large = False ) : [EOL] [comment] [EOL] tokenizer = BertTokenizer . from_pretrained ( [string] ) \ [EOL] if large else BertTokenizer . from_pretrained ( [string] ) [EOL] if trained : [EOL] model = BertModel . from_pretrained ( [string] ) \ [EOL] if large else BertModel . from_pretrained ( [string] ) [EOL] else : [EOL] model = BertModel ( BertModel . from_pretrained ( [string] ) . config \ [EOL] if large else BertModel . from_pretrained ( [string] ) . config ) [EOL] model . eval ( ) [EOL] for text in texts : [EOL] text = [string] . format ( text . lower ( ) ) [EOL] tokenized_text = tokenizer . tokenize ( text ) [EOL] indexed_tokens = tokenizer . convert_tokens_to_ids ( tokenized_text ) [EOL] tokens_tensor = torch . tensor ( [ indexed_tokens ] ) [EOL] with torch . no_grad ( ) : [EOL] encoded_layers , pooled = model ( tokens_tensor ) [EOL] yield encoded_layers , pooled [EOL] [EOL] def prepare_bert ( params , texts ) : [EOL] [comment] [EOL] tokenizer = BertTokenizer . from_pretrained ( [string] ) \ [EOL] if params [ [string] ] [ [string] ] else BertTokenizer . from_pretrained ( [string] ) [EOL] if params [ [string] ] [ [string] ] : [EOL] model = BertModel . from_pretrained ( [string] ) \ [EOL] if params [ [string] ] [ [string] ] else BertModel . from_pretrained ( [string] ) [EOL] else : [EOL] model = BertModel ( BertModel . from_pretrained ( [string] ) . config \ [EOL] if params [ [string] ] [ [string] ] else BertModel . from_pretrained ( [string] ) . config ) [EOL] model . eval ( ) [EOL] params [ [string] ] = model [EOL] params [ [string] ] = tokenizer [EOL] [EOL] def batcher_bert ( params , texts ) : [EOL] model = params [ [string] ] [EOL] tokenizer = params [ [string] ] [EOL] emb = [ ] [EOL] for text in [ [string] . join ( text ) for text in texts ] : [EOL] text = [string] . format ( text . lower ( ) ) [EOL] tokenized_text = tokenizer . tokenize ( text ) [EOL] indexed_tokens = tokenizer . convert_tokens_to_ids ( tokenized_text ) [EOL] tokens_tensor = torch . tensor ( [ indexed_tokens ] ) [EOL] with torch . no_grad ( ) : [EOL] encoded_layers , _ = model ( tokens_tensor ) [EOL] emb . append ( encoded_layers [ params [ [string] ] [ [string] ] ] [ [number] , [number] , : ] . cpu ( ) . numpy ( ) ) [EOL] return np . vstack ( emb ) [EOL] [EOL] def encode_infersent ( texts , trained = True , tokenize = True ) : [EOL] from infersent . models import InferSent [EOL] V = [number] [EOL] MODEL_PATH = [string] % V [EOL] params_model = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : V } [EOL] infersent = InferSent ( params_model ) [EOL] W2V_PATH = [string] [EOL] if trained : [EOL] logging . info ( [string] ) [EOL] infersent . load_state_dict ( torch . load ( MODEL_PATH ) ) [EOL] infersent . set_w2v_path ( W2V_PATH ) [EOL] else : [EOL] RANDOM_PATH = [string] [EOL] if not os . path . isfile ( RANDOM_PATH ) : [EOL] random_embeddings ( W2V_PATH , RANDOM_PATH ) [EOL] logging . info ( [string] ) [EOL] infersent . set_w2v_path ( RANDOM_PATH ) [EOL] infersent . build_vocab ( texts , tokenize = tokenize ) [EOL] return torch . tensor ( infersent . encode ( texts , tokenize = tokenize ) ) [EOL] [EOL] def prepare_infersent ( params , texts ) : [EOL] from infersent . models import InferSent [EOL] texts = [ [string] . join ( text ) for text in texts ] [EOL] V = [number] [EOL] MODEL_PATH = [string] % V [EOL] params_model = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : V } [EOL] infersent = InferSent ( params_model ) [EOL] W2V_PATH = [string] [EOL] if params [ [string] ] [ [string] ] : [EOL] logging . info ( [string] ) [EOL] infersent . load_state_dict ( torch . load ( MODEL_PATH ) ) [EOL] infersent . set_w2v_path ( W2V_PATH ) [EOL] else : [EOL] RANDOM_PATH = [string] [EOL] if not os . path . isfile ( RANDOM_PATH ) : [EOL] random_embeddings ( W2V_PATH , RANDOM_PATH ) [EOL] logging . info ( [string] ) [EOL] infersent . set_w2v_path ( RANDOM_PATH ) [EOL] infersent . build_vocab ( texts , tokenize = False ) [EOL] params [ [string] ] = infersent [EOL] [EOL] def batcher_infersent ( params , texts ) : [EOL] sent = [ [string] . join ( text ) for text in texts ] [EOL] params [ [string] ] . encode ( sent , bsize = params [ [string] ] , tokenize = False ) [EOL] [EOL] def random_embeddings ( path , target_path ) : [EOL] D = { } [EOL] with open ( path ) as f : [EOL] for i , line in enumerate ( f ) : [EOL] if i > [number] : [EOL] word , vec = line . split ( [string] , [number] ) [EOL] D [ word ] = np . fromstring ( vec , sep = [string] ) [EOL] logging . info ( [string] ) [EOL] w = np . concatenate ( [ [ row ] for row in D . values ( ) ] , axis = [number] ) [EOL] loc = w . mean ( axis = [number] ) [EOL] scale = w . std ( axis = [number] ) [EOL] rows = np . random . normal ( loc = loc , scale = scale , size = w . shape ) [EOL] logging . info ( [string] ) [EOL] with open ( target_path , [string] ) as f : [EOL] f . write ( [string] ) [EOL] for k , r in zip ( D . keys ( ) , rows ) : [EOL] f . write ( [string] . format ( k , [string] . join ( [ [string] . format ( n ) for n in r ] ) ) ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import typing [EOL] from xml . etree import ElementTree as ET [EOL] import pickle [EOL] import subprocess [EOL] import json [EOL] import random [EOL] import torch [EOL] import rsa . correlate as C [EOL] import sys [EOL] import pandas as pd [EOL] from nltk . tree import Tree [EOL] import conllu as U [EOL] [EOL] [EOL] def ewt_json ( ) : [EOL] def id2path ( sentid , prefix = [string] ) : [EOL] cols = sentid . split ( [string] ) [EOL] return ( prefix + cols [ [number] ] + [string] + [string] . join ( cols [ [number] : - [number] ] ) + [string] , int ( cols [ - [number] ] ) - [number] ) [EOL] def get_tree ( sentid ) : [EOL] path , index = id2path ( sentid ) [EOL] return [ Tree . fromstring ( line ) for line in open ( path ) ] [ index ] [EOL] test = U . parse ( open ( [string] ) . read ( ) ) [EOL] train = U . parse ( open ( [string] ) . read ( ) ) [EOL] ref = random . sample ( train , len ( test ) // [number] ) [EOL] data_test = [ dict ( sent = datum . metadata [ [string] ] , sentid = datum . metadata [ [string] ] , tree = str ( get_tree ( datum . metadata [ [string] ] ) ) ) for datum in test ] [EOL] data_ref = [ dict ( sent = datum . metadata [ [string] ] , sentid = datum . metadata [ [string] ] , tree = str ( get_tree ( datum . metadata [ [string] ] ) ) ) for datum in ref ] [EOL] json . dump ( dict ( ref = data_ref , test = data_test ) , open ( [string] , [string] ) ) [EOL] [EOL] def ewt_TK ( alpha = [number] ) : [EOL] [docstring] [EOL] import numpy as np [EOL] from ursa . kernel import Kernel , delex [EOL] try : [EOL] data = json . load ( open ( [string] ) ) [EOL] except FileNotFoundError : [EOL] ewt_json ( ) [EOL] data = json . load ( open ( [string] ) ) [EOL] ref = [ delex ( Tree . fromstring ( datum [ [string] ] ) ) for datum in data [ [string] ] ] [EOL] test = [ delex ( Tree . fromstring ( datum [ [string] ] ) ) for datum in data [ [string] ] ] [EOL] M_test_test = Kernel ( alpha = alpha ) . pairwise ( test , normalize = True , dtype = np . float64 ) [EOL] M_test_ref = Kernel ( alpha = alpha ) . pairwise ( test , ref , normalize = True , dtype = np . float64 ) [EOL] torch . save ( dict ( test_test = torch . tensor ( M_test_test ) , test_ref = torch . tensor ( M_test_ref ) ) , [string] . format ( alpha ) ) [EOL] [EOL] def ewt_embed ( ) : [EOL] [docstring] [EOL] import rsa . pretrained as Pre [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] def container ( ) : [EOL] return dict ( test = dict ( random = dict ( ) , trained = dict ( ) ) , ref = dict ( random = dict ( ) , trained = dict ( ) ) ) [EOL] data = json . load ( open ( [string] ) ) [EOL] emb = dict ( bow = { } , bert = container ( ) , bert24 = container ( ) , infersent = container ( ) ) [EOL] [comment] [EOL] v = CountVectorizer ( tokenizer = lambda x : x . split ( ) ) [EOL] sent_ref = [ s [ [string] ] for s in data [ [string] ] ] [EOL] sent_test = [ s [ [string] ] for s in data [ [string] ] ] [EOL] v . fit ( sent_ref + sent_test ) [EOL] emb [ [string] ] [ [string] ] = torch . tensor ( v . transform ( sent_test ) . toarray ( ) , dtype = torch . float ) [EOL] emb [ [string] ] [ [string] ] = torch . tensor ( v . transform ( sent_ref ) . toarray ( ) , dtype = torch . float ) [EOL] [EOL] for split in [ [string] , [string] ] : [EOL] sent = [ datum [ [string] ] for datum in data [ split ] ] [EOL] for mode in [ [string] , [string] ] : [EOL] if mode == [string] : [EOL] rep24 = list ( Pre . encode_bert ( sent , trained = False , large = True ) ) [EOL] rep = list ( Pre . encode_bert ( sent , trained = False ) ) [EOL] emb [ [string] ] [ split ] [ mode ] = Pre . encode_infersent ( sent , trained = False ) [EOL] else : [EOL] rep24 = list ( Pre . encode_bert ( sent , trained = True , large = True ) ) [EOL] rep = list ( Pre . encode_bert ( sent , trained = True ) ) [EOL] emb [ [string] ] [ split ] [ mode ] = Pre . encode_infersent ( sent , trained = True ) [EOL] [EOL] pooled24 = torch . cat ( [ pooled for _ , pooled in rep24 ] ) [EOL] pooled = torch . cat ( [ pooled for _ , pooled in rep ] ) [EOL] emb [ [string] ] [ split ] [ mode ] [ [string] ] = pooled24 [EOL] emb [ [string] ] [ split ] [ mode ] [ [string] ] = pooled [EOL] for i in range ( len ( rep24 [ [number] ] [ [number] ] ) ) : [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] = { } [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] . sum ( dim = [number] ) for layers , _ in rep24 ] , dim = [number] ) [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] [ : , [number] , : ] for layers , _ in rep24 ] , dim = [number] ) [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] [ : , - [number] , : ] for layers , _ in rep24 ] , dim = [number] ) [EOL] [EOL] for i in range ( len ( rep [ [number] ] [ [number] ] ) ) : [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] = { } [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] . sum ( dim = [number] ) for layers , _ in rep ] , dim = [number] ) [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] [ : , [number] , : ] for layers , _ in rep ] , dim = [number] ) [EOL] emb [ [string] ] [ split ] [ mode ] [ i ] [ [string] ] = torch . cat ( [ layers [ i ] [ : , - [number] , : ] for layers , _ in rep ] , dim = [number] ) [EOL] torch . save ( emb , [string] ) [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0