from typing import Any [EOL] import allennlp [EOL] import typing [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] predictor = Predictor . from_path ( [string] ) [EOL] [EOL] print ( ) [EOL] print ( ) [EOL] print ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] json_result = predictor . predict ( question = [string] ) [EOL] print ( ) [EOL] print ( [string] + json_result [ [string] ] ) [EOL] print ( ) [EOL] print ( [string] ) [EOL] print ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] predictor = Predictor . from_path ( [string] ) [EOL] [EOL] print ( ) [EOL] print ( ) [EOL] print ( [string] ) [EOL] print ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] json_result = predictor . predict ( question = [string] ) [EOL] print ( ) [EOL] print ( [string] + json_result [ [string] ] ) [EOL] print ( ) [EOL] print ( [string] ) [EOL] print ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . state_machines . trainers . decoder_trainer import DecoderTrainer [EOL] from allennlp . state_machines . trainers . expected_risk_minimization import ExpectedRiskMinimization [EOL] from allennlp . state_machines . trainers . maximum_marginal_likelihood import MaximumMarginalLikelihood [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from allennlp . data . tokenizers import Token as _ [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . semparse . contexts . table_question_knowledge_graph import TableQuestionKnowledgeGraph [EOL] from allennlp . semparse . contexts . atis_sql_table_context import AtisSqlTableContext [EOL] from allennlp . semparse . contexts . table_question_context import TableQuestionContext [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . nn . activations import Activation [EOL] from allennlp . nn . initializers import Initializer , InitializerApplicator [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class Regularizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __call__ ( self , parameter ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . archival import archive_model , load_archive , Archive [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from allennlp . models . reading_comprehension . bidaf import BidirectionalAttentionFlow [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import torch [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import NamedTuple , Dict , Any [EOL] import atexit [EOL] import json [EOL] import logging [EOL] import os [EOL] import tempfile [EOL] import tarfile [EOL] import shutil [EOL] [EOL] from torch . nn import Module [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . common . params import Params , unflatten , with_fallback , parse_overrides [EOL] from allennlp . models . model import Model , _DEFAULT_WEIGHTS [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] class Archive ( NamedTuple ) : [EOL] [docstring] [EOL] model = ... [EOL] config = ... [EOL] [EOL] def extract_module ( self , path , freeze = True ) : [EOL] [docstring] [EOL] modules_dict = { path : module for path , module in self . model . named_modules ( ) } [EOL] module = modules_dict . get ( path , None ) [EOL] [EOL] if not module : [EOL] raise ConfigurationError ( f" [string] { path } [string] " f" [string] { type ( self . model ) } [string] " ) [EOL] if not isinstance ( module , Module ) : [EOL] raise ConfigurationError ( f" [string] { type ( self . model ) } [string] " f"{ path } [string] " ) [EOL] [EOL] for parameter in module . parameters ( ) : [comment] [EOL] parameter . requires_grad_ ( not freeze ) [EOL] return module [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] CONFIG_NAME = [string] [EOL] _WEIGHTS_NAME = [string] [EOL] _FTA_NAME = [string] [EOL] [EOL] def archive_model ( serialization_dir , weights = _DEFAULT_WEIGHTS , files_to_archive = None , archive_path = None ) : [EOL] [docstring] [EOL] weights_file = os . path . join ( serialization_dir , weights ) [EOL] if not os . path . exists ( weights_file ) : [EOL] logger . error ( [string] , weights_file ) [EOL] return [EOL] [EOL] config_file = os . path . join ( serialization_dir , CONFIG_NAME ) [EOL] if not os . path . exists ( config_file ) : [EOL] logger . error ( [string] , config_file ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if files_to_archive : [EOL] fta_filename = os . path . join ( serialization_dir , _FTA_NAME ) [EOL] with open ( fta_filename , [string] ) as fta_file : [EOL] fta_file . write ( json . dumps ( files_to_archive ) ) [EOL] [EOL] if archive_path is not None : [EOL] archive_file = archive_path [EOL] if os . path . isdir ( archive_file ) : [EOL] archive_file = os . path . join ( archive_file , [string] ) [EOL] else : [EOL] archive_file = os . path . join ( serialization_dir , [string] ) [EOL] logger . info ( [string] , archive_file ) [EOL] with tarfile . open ( archive_file , [string] ) as archive : [EOL] archive . add ( config_file , arcname = CONFIG_NAME ) [EOL] archive . add ( weights_file , arcname = _WEIGHTS_NAME ) [EOL] archive . add ( os . path . join ( serialization_dir , [string] ) , arcname = [string] ) [EOL] [EOL] [comment] [EOL] if files_to_archive : [EOL] [comment] [EOL] archive . add ( fta_filename , arcname = _FTA_NAME ) [EOL] [comment] [EOL] for key , filename in files_to_archive . items ( ) : [EOL] archive . add ( filename , arcname = f" [string] { key }" ) [EOL] [EOL] def load_archive ( archive_file , cuda_device = - [number] , overrides = [string] , weights_file = None ) : [EOL] [docstring] [EOL] [comment] [EOL] resolved_archive_file = cached_path ( archive_file ) [EOL] [EOL] if resolved_archive_file == archive_file : [EOL] logger . info ( f" [string] { archive_file }" ) [EOL] else : [EOL] logger . info ( f" [string] { archive_file } [string] { resolved_archive_file }" ) [EOL] [EOL] if os . path . isdir ( resolved_archive_file ) : [EOL] serialization_dir = resolved_archive_file [EOL] else : [EOL] [comment] [EOL] tempdir = tempfile . mkdtemp ( ) [EOL] logger . info ( f" [string] { resolved_archive_file } [string] { tempdir }" ) [EOL] with tarfile . open ( resolved_archive_file , [string] ) as archive : [EOL] archive . extractall ( tempdir ) [EOL] [comment] [EOL] [comment] [EOL] atexit . register ( _cleanup_archive_dir , tempdir ) [EOL] [EOL] serialization_dir = tempdir [EOL] [EOL] [comment] [EOL] fta_filename = os . path . join ( serialization_dir , _FTA_NAME ) [EOL] if os . path . exists ( fta_filename ) : [EOL] with open ( fta_filename , [string] ) as fta_file : [EOL] files_to_archive = json . loads ( fta_file . read ( ) ) [EOL] [EOL] [comment] [EOL] replacements_dict = { } [EOL] for key , _ in files_to_archive . items ( ) : [EOL] replacement_filename = os . path . join ( serialization_dir , f" [string] { key }" ) [EOL] replacements_dict [ key ] = replacement_filename [EOL] [EOL] overrides_dict = parse_overrides ( overrides ) [EOL] combined_dict = with_fallback ( preferred = unflatten ( replacements_dict ) , fallback = overrides_dict ) [EOL] overrides = json . dumps ( combined_dict ) [EOL] [EOL] [comment] [EOL] config = Params . from_file ( os . path . join ( serialization_dir , CONFIG_NAME ) , overrides ) [EOL] config . loading_from_archive = True [EOL] [EOL] if weights_file : [EOL] weights_path = weights_file [EOL] else : [EOL] weights_path = os . path . join ( serialization_dir , _WEIGHTS_NAME ) [EOL] [EOL] [comment] [EOL] model = Model . load ( config . duplicate ( ) , weights_file = weights_path , serialization_dir = serialization_dir , cuda_device = cuda_device ) [EOL] [EOL] return Archive ( model = model , config = config ) [EOL] [EOL] [EOL] def _cleanup_archive_dir ( path ) : [EOL] if os . path . exists ( path ) : [EOL] logger . info ( [string] , path ) [EOL] shutil . rmtree ( path ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $allennlp.common.params.Params$ 0 0 0 0 0 $torch.nn.Module$ 0 0 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 $builtins.str$ 0 $typing.Any$ 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $Archive$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import torch [EOL] [EOL] [EOL] def get_best_span ( span_start_logits , span_end_logits ) : [EOL] [docstring] [EOL] if span_start_logits . dim ( ) != [number] or span_end_logits . dim ( ) != [number] : [EOL] raise ValueError ( [string] ) [EOL] batch_size , passage_length = span_start_logits . size ( ) [EOL] device = span_start_logits . device [EOL] [comment] [EOL] span_log_probs = span_start_logits . unsqueeze ( [number] ) + span_end_logits . unsqueeze ( [number] ) [EOL] [comment] [EOL] [comment] [EOL] span_log_mask = torch . triu ( torch . ones ( ( passage_length , passage_length ) , device = device ) ) . log ( ) [EOL] valid_span_log_probs = span_log_probs + span_log_mask [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] best_spans = valid_span_log_probs . view ( batch_size , - [number] ) . argmax ( - [number] ) [EOL] span_start_indices = best_spans // passage_length [EOL] span_end_indices = best_spans % passage_length [EOL] return torch . stack ( [ span_start_indices , span_end_indices ] , dim = - [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . models . reading_comprehension . bidaf import BidirectionalAttentionFlow [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Set , Dict , Iterable [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Iterable , Dict , List [EOL] import random [EOL] from collections import defaultdict [EOL] [EOL] from allennlp . common . util import lazy_groups_of [EOL] from allennlp . data . dataset import Batch [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] [EOL] @ DataIterator . register ( [string] ) class HomogeneousBatchIterator ( DataIterator ) : [EOL] [docstring] [EOL] def __init__ ( self , batch_size = [number] , instances_per_epoch = None , max_instances_in_memory = None , cache_instances = False , track_epoch = False , partition_key = [string] ) : [EOL] super ( ) . __init__ ( batch_size , instances_per_epoch , max_instances_in_memory , cache_instances , track_epoch ) [EOL] self . _partition_key = partition_key [EOL] [EOL] def _create_batches ( self , instances , shuffle ) : [EOL] [comment] [EOL] for instance_list in self . _memory_sized_lists ( instances ) : [EOL] if shuffle : [EOL] random . shuffle ( instance_list ) [EOL] [EOL] [comment] [EOL] hoppers = defaultdict ( list ) [EOL] for instance in instance_list : [EOL] partition = instance . fields [ self . _partition_key ] . metadata [comment] [EOL] hoppers [ partition ] . append ( instance ) [EOL] [EOL] [comment] [EOL] batches = { key : lazy_groups_of ( iter ( hopper ) , self . _batch_size ) for key , hopper in hoppers . items ( ) } [EOL] [EOL] remaining = set ( batches ) [EOL] [EOL] [comment] [EOL] while remaining : [EOL] for key , lazy_batches in batches . items ( ) : [EOL] if key in remaining : [EOL] try : [EOL] batch = next ( lazy_batches ) [EOL] yield Batch ( batch ) [EOL] except StopIteration : [EOL] remaining . remove ( key ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Iterable[allennlp.data.dataset.Batch]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0
[docstring] [EOL] [EOL] [comment] [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] [EOL] from allennlp . data . dataset_readers . reading_comprehension import SquadReader [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Iterator , Iterable , Callable [EOL] import allennlp [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] from typing import Iterable , Iterator , Callable [EOL] import logging [EOL] [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . common import Tqdm [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] class _LazyInstances ( Iterable ) : [EOL] [docstring] [EOL] def __init__ ( self , instance_generator ) : [EOL] super ( ) . __init__ ( ) [EOL] self . instance_generator = instance_generator [EOL] [EOL] def __iter__ ( self ) : [EOL] instances = self . instance_generator ( ) [EOL] if isinstance ( instances , list ) : [EOL] raise ConfigurationError ( [string] ) [EOL] return instances [EOL] [EOL] class DatasetReader ( Registrable ) : [EOL] [docstring] [EOL] def __init__ ( self , lazy = False ) : [EOL] self . lazy = lazy [EOL] [EOL] def read ( self , file_path ) : [EOL] [docstring] [EOL] lazy = getattr ( self , [string] , None ) [EOL] if lazy is None : [EOL] logger . warning ( [string] [string] ) [EOL] [EOL] if lazy : [EOL] return _LazyInstances ( lambda : iter ( self . _read ( file_path ) ) ) [EOL] else : [EOL] instances = self . _read ( file_path ) [EOL] if not isinstance ( instances , list ) : [EOL] instances = [ instance for instance in Tqdm . tqdm ( instances ) ] [EOL] if not instances : [EOL] raise ConfigurationError ( [string] [string] . format ( file_path ) ) [EOL] return instances [EOL] [EOL] def _read ( self , file_path ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def text_to_instance ( self , * inputs ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def text_to_instance_one_argument ( self , * inputs ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[allennlp.data.instance.Instance]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple , List , Optional , Dict [EOL] import allennlp [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import json [EOL] import logging [EOL] from typing import Dict , List , Tuple , Optional [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . dataset_readers . reading_comprehension import util [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token , Tokenizer , WordTokenizer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class SquadReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , tokenizer = None , token_indexers = None , lazy = False , passage_length_limit = None , question_length_limit = None , skip_invalid_examples = False ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . passage_length_limit = passage_length_limit [EOL] self . question_length_limit = question_length_limit [EOL] self . skip_invalid_examples = skip_invalid_examples [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] [comment] [EOL] file_path = cached_path ( file_path ) [EOL] [EOL] logger . info ( [string] , file_path ) [EOL] with open ( file_path ) as dataset_file : [EOL] dataset_json = json . load ( dataset_file ) [EOL] dataset = dataset_json [ [string] ] [EOL] logger . info ( [string] ) [EOL] for article in dataset : [EOL] for paragraph_json in article [ [string] ] : [EOL] paragraph = paragraph_json [ [string] ] [EOL] tokenized_paragraph = self . _tokenizer . tokenize ( paragraph ) [EOL] [EOL] for question_answer in paragraph_json [ [string] ] : [EOL] question_text = question_answer [ [string] ] . strip ( ) . replace ( [string] , [string] ) [EOL] answer_texts = [ answer [ [string] ] for answer in question_answer [ [string] ] ] [EOL] span_starts = [ answer [ [string] ] for answer in question_answer [ [string] ] ] [EOL] span_ends = [ start + len ( answer ) for start , answer in zip ( span_starts , answer_texts ) ] [EOL] instance = self . text_to_instance ( question_text , paragraph , zip ( span_starts , span_ends ) , answer_texts , tokenized_paragraph ) [EOL] if instance is not None : [EOL] yield instance [EOL] [EOL] @ overrides def text_to_instance ( self , question_text , passage_text , char_spans = None , answer_texts = None , passage_tokens = None ) : [EOL] [comment] [EOL] if not passage_tokens : [EOL] passage_tokens = self . _tokenizer . tokenize ( passage_text ) [EOL] question_tokens = self . _tokenizer . tokenize ( question_text ) [EOL] if self . passage_length_limit is not None : [EOL] passage_tokens = passage_tokens [ : self . passage_length_limit ] [EOL] if self . question_length_limit is not None : [EOL] question_tokens = question_tokens [ : self . question_length_limit ] [EOL] char_spans = char_spans or [ ] [EOL] [comment] [EOL] [comment] [EOL] token_spans = [ ] [EOL] passage_offsets = [ ( token . idx , token . idx + len ( token . text ) ) for token in passage_tokens ] [EOL] for char_span_start , char_span_end in char_spans : [EOL] if char_span_end > passage_offsets [ - [number] ] [ [number] ] : [EOL] continue [EOL] ( span_start , span_end ) , error = util . char_span_to_token_span ( passage_offsets , ( char_span_start , char_span_end ) ) [EOL] if error : [EOL] logger . debug ( [string] , passage_text ) [EOL] logger . debug ( [string] , passage_tokens ) [EOL] logger . debug ( [string] , question_text ) [EOL] logger . debug ( [string] , char_span_start , char_span_end ) [EOL] logger . debug ( [string] , span_start , span_end ) [EOL] logger . debug ( [string] , passage_tokens [ span_start : span_end + [number] ] ) [EOL] logger . debug ( [string] , passage_text [ char_span_start : char_span_end ] ) [EOL] token_spans . append ( ( span_start , span_end ) ) [EOL] [comment] [EOL] if char_spans and not token_spans : [EOL] if self . skip_invalid_examples : [EOL] return None [EOL] else : [EOL] token_spans . append ( ( len ( passage_tokens ) - [number] , len ( passage_tokens ) - [number] ) ) [EOL] return util . make_reading_comprehension_instance ( question_tokens , passage_tokens , self . _token_indexers , passage_text , token_spans , answer_texts ) [EOL] @ overrides def text_to_instance_one_argument ( self , passage_text , char_spans = None , passage_tokens = None ) : [EOL] [comment] [EOL] if not passage_tokens : [EOL] passage_tokens = self . _tokenizer . tokenize ( passage_text ) [EOL] [EOL] if self . passage_length_limit is not None : [EOL] passage_tokens = passage_tokens [ : self . passage_length_limit ] [EOL] [EOL] char_spans = char_spans or [ ] [EOL] [comment] [EOL] [comment] [EOL] token_spans = [ ] [EOL] passage_offsets = [ ( token . idx , token . idx + len ( token . text ) ) for token in passage_tokens ] [EOL] for char_span_start , char_span_end in char_spans : [EOL] if char_span_end > passage_offsets [ - [number] ] [ [number] ] : [EOL] continue [EOL] ( span_start , span_end ) , error = util . char_span_to_token_span ( passage_offsets , ( char_span_start , char_span_end ) ) [EOL] if error : [EOL] logger . debug ( [string] , passage_text ) [EOL] logger . debug ( [string] , passage_tokens ) [EOL] [EOL] logger . debug ( [string] , char_span_start , char_span_end ) [EOL] logger . debug ( [string] , span_start , span_end ) [EOL] logger . debug ( [string] , passage_tokens [ span_start : span_end + [number] ] ) [EOL] logger . debug ( [string] , passage_text [ char_span_start : char_span_end ] ) [EOL] token_spans . append ( ( span_start , span_end ) ) [EOL] [comment] [EOL] if char_spans and not token_spans : [EOL] if self . skip_invalid_examples : [EOL] return None [EOL] else : [EOL] token_spans . append ( ( len ( passage_tokens ) - [number] , len ( passage_tokens ) - [number] ) ) [EOL] return util . make_reading_comprehension_instance_one_argument ( passage_tokens , self . _token_indexers , passage_text , token_spans ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[allennlp.data.instance.Instance]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] [comment] [EOL] from allennlp . data . dataset_readers . reading_comprehension . squad import SquadReader [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , TypeVar , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Dict , List , TypeVar , Generic [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] TokenType = TypeVar ( [string] , int , List [ int ] ) [comment] [EOL] [EOL] class TokenIndexer ( Generic [ TokenType ] , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def count_vocab_items ( self , token , counter ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def tokens_to_indices ( self , tokens , vocabulary , index_name ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_token ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_lengths ( self , token ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def pad_token_sequence ( self , tokens , desired_num_tokens , padding_lengths ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_keys ( self , index_name ) : [EOL] [docstring] [EOL] [comment] [EOL] return [ index_name ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0
import builtins [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] [EOL] [EOL] class SequenceField ( Field [ DataArray ] ) : [EOL] [docstring] [EOL] def sequence_length ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def empty_field ( self ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $'SequenceField'$ 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Dict , Any [EOL] import torch [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Dict [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . data . fields . field import Field [EOL] from allennlp . data . fields . sequence_field import SequenceField [EOL] [EOL] [EOL] class SpanField ( Field [ torch . Tensor ] ) : [EOL] [docstring] [EOL] def __init__ ( self , span_start , span_end , sequence_field ) : [EOL] self . span_start = span_start [EOL] self . span_end = span_end [EOL] self . sequence_field = sequence_field [EOL] [EOL] if not isinstance ( span_start , int ) or not isinstance ( span_end , int ) : [EOL] raise TypeError ( f" [string] " f" [string] { span_start } [string] { span_end } [string] " f" [string] { type ( span_start ) } [string] { type ( span_end ) } [string] " ) [EOL] if span_start > span_end : [EOL] raise ValueError ( f" [string] " f" [string] { span_start } [string] { span_end } [string] " ) [EOL] [EOL] if span_end > self . sequence_field . sequence_length ( ) - [number] : [EOL] raise ValueError ( f" [string] " f"{ span_end } [string] { self . sequence_field . sequence_length ( ) - [number] } [string] " ) [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] [comment] [EOL] return { } [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] [comment] [EOL] tensor = torch . LongTensor ( [ self . span_start , self . span_end ] ) [EOL] return tensor [EOL] [EOL] @ overrides def empty_field ( self ) : [EOL] return SpanField ( - [number] , - [number] , self . sequence_field . empty_field ( ) ) [EOL] [EOL] def __str__ ( self ) : [EOL] return f" [string] { self . span_start } [string] { self . span_end } [string] " [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( other , tuple ) and len ( other ) == [number] : [EOL] return other == ( self . span_start , self . span_end ) [EOL] else : [EOL] return id ( self ) == id ( other ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $allennlp.data.fields.sequence_field.SequenceField$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $allennlp.data.fields.sequence_field.SequenceField$ 0 $allennlp.data.fields.sequence_field.SequenceField$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.fields.sequence_field.SequenceField$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $allennlp.data.fields.sequence_field.SequenceField$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . training . trainer import Trainer [EOL] from allennlp . training . trainer_base import TrainerBase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Optional , Set , Dict , Callable [EOL] import typing [EOL] import allennlp [EOL] import torch [EOL] import logging [EOL] import builtins [EOL] from typing import Any , Set , Optional , Callable [EOL] import logging [EOL] import os [EOL] [EOL] from tensorboardX import SummaryWriter [EOL] import torch [EOL] [EOL] from allennlp . models . model import Model [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class TensorboardWriter : [EOL] [docstring] [EOL] def __init__ ( self , get_batch_num_total , serialization_dir = None , summary_interval = [number] , histogram_interval = None , should_log_parameter_statistics = True , should_log_learning_rate = False ) : [EOL] if serialization_dir is not None : [EOL] self . _train_log = SummaryWriter ( os . path . join ( serialization_dir , [string] , [string] ) ) [EOL] self . _validation_log = SummaryWriter ( os . path . join ( serialization_dir , [string] , [string] ) ) [EOL] else : [EOL] self . _train_log = self . _validation_log = None [EOL] [EOL] self . _summary_interval = summary_interval [EOL] self . _histogram_interval = histogram_interval [EOL] self . _should_log_parameter_statistics = should_log_parameter_statistics [EOL] self . _should_log_learning_rate = should_log_learning_rate [EOL] self . _get_batch_num_total = get_batch_num_total [EOL] [EOL] @ staticmethod def _item ( value ) : [EOL] if hasattr ( value , [string] ) : [EOL] val = value . item ( ) [EOL] else : [EOL] val = value [EOL] return val [EOL] [EOL] def should_log_this_batch ( self ) : [EOL] return self . _get_batch_num_total ( ) % self . _summary_interval == [number] [EOL] [EOL] def should_log_histograms_this_batch ( self ) : [EOL] return self . _histogram_interval is not None and self . _get_batch_num_total ( ) % self . _histogram_interval == [number] [EOL] [EOL] def add_train_scalar ( self , name , value , timestep = None ) : [EOL] timestep = timestep or self . _get_batch_num_total ( ) [EOL] [comment] [EOL] if self . _train_log is not None : [EOL] self . _train_log . add_scalar ( name , self . _item ( value ) , timestep ) [EOL] [EOL] def add_train_histogram ( self , name , values ) : [EOL] if self . _train_log is not None : [EOL] if isinstance ( values , torch . Tensor ) : [EOL] values_to_write = values . cpu ( ) . data . numpy ( ) . flatten ( ) [EOL] self . _train_log . add_histogram ( name , values_to_write , self . _get_batch_num_total ( ) ) [EOL] [EOL] def add_validation_scalar ( self , name , value , timestep = None ) : [EOL] timestep = timestep or self . _get_batch_num_total ( ) [EOL] if self . _validation_log is not None : [EOL] self . _validation_log . add_scalar ( name , self . _item ( value ) , timestep ) [EOL] [EOL] def log_parameter_and_gradient_statistics ( self , model , batch_grad_norm ) : [EOL] [docstring] [EOL] if self . _should_log_parameter_statistics : [EOL] [comment] [EOL] for name , param in model . named_parameters ( ) : [EOL] self . add_train_scalar ( [string] + name , param . data . mean ( ) ) [EOL] self . add_train_scalar ( [string] + name , param . data . std ( ) ) [EOL] if param . grad is not None : [EOL] if param . grad . is_sparse : [EOL] [comment] [EOL] grad_data = param . grad . data . _values ( ) [EOL] else : [EOL] grad_data = param . grad . data [EOL] [EOL] [comment] [EOL] if torch . prod ( torch . tensor ( grad_data . shape ) ) . item ( ) > [number] : [comment] [EOL] self . add_train_scalar ( [string] + name , grad_data . mean ( ) ) [EOL] self . add_train_scalar ( [string] + name , grad_data . std ( ) ) [EOL] else : [EOL] [comment] [EOL] logger . info ( [string] , name ) [EOL] [comment] [EOL] if batch_grad_norm is not None : [EOL] self . add_train_scalar ( [string] , batch_grad_norm ) [EOL] [EOL] def log_learning_rates ( self , model , optimizer ) : [EOL] [docstring] [EOL] if self . _should_log_learning_rate : [EOL] [comment] [EOL] [comment] [EOL] names = { param : name for name , param in model . named_parameters ( ) } [EOL] for group in optimizer . param_groups : [EOL] if [string] not in group : [EOL] continue [EOL] rate = group [ [string] ] [EOL] for param in group [ [string] ] : [EOL] [comment] [EOL] effective_rate = rate * float ( param . requires_grad ) [EOL] self . add_train_scalar ( [string] + names [ param ] , effective_rate ) [EOL] [EOL] def log_histograms ( self , model , histogram_parameters ) : [EOL] [docstring] [EOL] for name , param in model . named_parameters ( ) : [EOL] if name in histogram_parameters : [EOL] self . add_train_histogram ( [string] + name , param ) [EOL] [EOL] def log_metrics ( self , train_metrics , val_metrics = None , epoch = None , log_to_console = False ) : [EOL] [docstring] [EOL] metric_names = set ( train_metrics . keys ( ) ) [EOL] if val_metrics is not None : [EOL] metric_names . update ( val_metrics . keys ( ) ) [EOL] val_metrics = val_metrics or { } [EOL] [EOL] [comment] [EOL] if log_to_console : [EOL] dual_message_template = [string] [EOL] no_val_message_template = [string] [EOL] no_train_message_template = [string] [EOL] header_template = [string] [EOL] name_length = max ( [ len ( x ) for x in metric_names ] ) [EOL] logger . info ( header_template , [string] . rjust ( name_length + [number] ) , [string] ) [EOL] [EOL] for name in metric_names : [EOL] [comment] [EOL] train_metric = train_metrics . get ( name ) [EOL] if train_metric is not None : [EOL] self . add_train_scalar ( name , train_metric , timestep = epoch ) [EOL] val_metric = val_metrics . get ( name ) [EOL] if val_metric is not None : [EOL] self . add_validation_scalar ( name , val_metric , timestep = epoch ) [EOL] [EOL] [comment] [EOL] if log_to_console and val_metric is not None and train_metric is not None : [EOL] logger . info ( dual_message_template , name . ljust ( name_length ) , train_metric , val_metric ) [EOL] elif log_to_console and val_metric is not None : [EOL] logger . info ( no_train_message_template , name . ljust ( name_length ) , [string] , val_metric ) [EOL] elif log_to_console and train_metric is not None : [EOL] logger . info ( no_val_message_template , name . ljust ( name_length ) , train_metric , [string] ) [EOL] [EOL] def enable_activation_logging ( self , model ) : [EOL] if self . _histogram_interval is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for _ , module in model . named_modules ( ) : [EOL] if not getattr ( module , [string] , False ) : [EOL] [comment] [EOL] continue [EOL] [EOL] def hook ( module_ , inputs , outputs ) : [EOL] [comment] [EOL] log_prefix = [string] . format ( module_ . __class__ ) [EOL] if self . should_log_histograms_this_batch ( ) : [EOL] self . log_activation_histogram ( outputs , log_prefix ) [EOL] module . register_forward_hook ( hook ) [EOL] [EOL] def log_activation_histogram ( self , outputs , log_prefix ) : [EOL] if isinstance ( outputs , torch . Tensor ) : [EOL] log_name = log_prefix [EOL] self . add_train_histogram ( log_name , outputs ) [EOL] elif isinstance ( outputs , ( list , tuple ) ) : [EOL] for i , output in enumerate ( outputs ) : [EOL] log_name = [string] . format ( log_prefix , i ) [EOL] self . add_train_histogram ( log_name , output ) [EOL] elif isinstance ( outputs , dict ) : [EOL] for k , tensor in outputs . items ( ) : [EOL] log_name = [string] . format ( log_prefix , k ) [EOL] self . add_train_histogram ( log_name , tensor ) [EOL] else : [EOL] [comment] [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Union , Any [EOL] import allennlp [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] [docstring] [EOL] [comment] [EOL] [EOL] import logging [EOL] from typing import Dict , List , Union , Any [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] from allennlp . common . checks import ConfigurationError , check_for_gpu [EOL] from allennlp . models . model import Model [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class TrainerBase ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __init__ ( self , serialization_dir , cuda_device = - [number] ) : [EOL] check_for_gpu ( cuda_device ) [EOL] [EOL] self . _serialization_dir = serialization_dir [EOL] [EOL] [comment] [EOL] if not isinstance ( cuda_device , int ) and not isinstance ( cuda_device , list ) : [EOL] raise ConfigurationError ( [string] . format ( cuda_device ) ) [EOL] [EOL] if isinstance ( cuda_device , list ) : [EOL] logger . warning ( f" [string] " [string] ) [EOL] self . _multiple_gpu = True [EOL] self . _cuda_devices = cuda_device [EOL] else : [EOL] self . _multiple_gpu = False [EOL] self . _cuda_devices = [ cuda_device ] [EOL] [EOL] def _move_to_gpu ( self , model ) : [EOL] if self . _cuda_devices [ [number] ] != - [number] : [EOL] return model . cuda ( self . _cuda_devices [ [number] ] ) [EOL] else : [EOL] return model [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params , serialization_dir , recover = False ) : [EOL] [comment] [EOL] typ3 = params . get ( [string] , { } ) . pop ( [string] , [string] ) [EOL] [EOL] if typ3 == [string] : [EOL] [comment] [EOL] from allennlp . training . trainer import Trainer , TrainerPieces [EOL] [EOL] pieces = TrainerPieces . from_params ( params , serialization_dir , recover ) [comment] [EOL] return Trainer . from_params ( model = pieces . model , serialization_dir = serialization_dir , iterator = pieces . iterator , train_data = pieces . train_dataset , validation_data = pieces . validation_dataset , params = pieces . params , validation_iterator = pieces . validation_iterator ) [EOL] else : [EOL] return TrainerBase . by_name ( typ3 ) . from_params ( params , serialization_dir , recover ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.Params$ 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0
from typing import Any [EOL] import torch [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import torch [EOL] [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . training . scheduler import Scheduler [EOL] [EOL] [EOL] class MomentumScheduler ( Scheduler , Registrable ) : [EOL] [EOL] def __init__ ( self , optimizer , last_epoch = - [number] ) : [EOL] super ( ) . __init__ ( optimizer , [string] , last_epoch ) [EOL] [EOL] def get_values ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , optimizer , params ) : [comment] [EOL] [comment] [EOL] scheduler_type = params . pop_choice ( [string] , MomentumScheduler . list_available ( ) ) [EOL] scheduler = MomentumScheduler . by_name ( scheduler_type ) ( optimizer , ** params . as_dict ( ) ) [EOL] return scheduler [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 0 0 $builtins.int$ 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0 0 0 0 0 $typing.Any$ 0 0 $torch.optim.Optimizer$ 0 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0
from allennlp . training . momentum_schedulers . momentum_scheduler import MomentumScheduler [EOL] from allennlp . training . momentum_schedulers . inverted_triangular import InvertedTriangular [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . training . learning_rate_schedulers . learning_rate_scheduler import LearningRateScheduler [EOL] from allennlp . training . learning_rate_schedulers . cosine import CosineWithRestarts [EOL] from allennlp . training . learning_rate_schedulers . noam import NoamLR [EOL] from allennlp . training . learning_rate_schedulers . slanted_triangular import SlantedTriangular [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , List , Any [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] from typing import Optional [EOL] [EOL] import sys [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class UnigramRecall ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] self . correct_count = [number] [EOL] self . total_count = [number] [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , end_index = sys . maxsize ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . unwrap_to_tensors ( predictions , gold_labels , mask ) [EOL] [EOL] [comment] [EOL] if gold_labels . dim ( ) != predictions . dim ( ) - [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( gold_labels . size ( ) ) ) [EOL] if mask is not None and mask . size ( ) != gold_labels . size ( ) : [EOL] raise ConfigurationError ( [string] [string] . format ( mask . size ( ) ) ) [EOL] [EOL] batch_size = predictions . size ( ) [ [number] ] [EOL] correct = [number] [EOL] for i in range ( batch_size ) : [EOL] beams = predictions [ i ] [EOL] cur_gold = gold_labels [ i ] [EOL] [EOL] if mask is not None : [EOL] masked_gold = cur_gold * mask [ i ] [EOL] else : [EOL] masked_gold = cur_gold [EOL] cleaned_gold = [ x for x in masked_gold if x != [number] and x != end_index ] [EOL] [EOL] retval = [number] [EOL] for word in cleaned_gold : [EOL] stillsearch = True [EOL] for beam in beams : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if stillsearch and ( word in beam ) : [EOL] retval += [number] / float ( len ( cleaned_gold ) ) [EOL] stillsearch = False [EOL] correct += retval [EOL] [EOL] self . correct_count += correct [EOL] self . total_count += predictions . size ( ) [ [number] ] [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] recall = float ( self . correct_count ) / float ( self . total_count ) if self . total_count > [number] else [number] [EOL] if reset : [EOL] self . reset ( ) [EOL] return recall [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . correct_count = [number] [EOL] self . total_count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $builtins.str$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0
import builtins [EOL] from overrides import overrides [EOL] [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Average ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] self . _total_value = [number] [EOL] self . _count = [number] [EOL] [EOL] @ overrides def __call__ ( self , value ) : [EOL] [docstring] [EOL] self . _total_value += list ( self . unwrap_to_tensors ( value ) ) [ [number] ] [EOL] self . _count += [number] [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] average_value = self . _total_value / self . _count if self . _count > [number] else [number] [EOL] if reset : [EOL] self . reset ( ) [EOL] return average_value [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _total_value = [number] [EOL] self . _count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0
from typing import Callable , Any [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Callable [EOL] [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] [EOL] class Highway ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , input_dim , num_layers = [number] , activation = torch . nn . functional . relu ) : [EOL] super ( Highway , self ) . __init__ ( ) [EOL] self . _input_dim = input_dim [EOL] self . _layers = torch . nn . ModuleList ( [ torch . nn . Linear ( input_dim , input_dim * [number] ) for _ in range ( num_layers ) ] ) [EOL] self . _activation = activation [EOL] for layer in self . _layers : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] layer . bias [ input_dim : ] . data . fill_ ( [number] ) [EOL] [EOL] @ overrides def forward ( self , inputs ) : [comment] [EOL] current_input = inputs [EOL] for layer in self . _layers : [EOL] projected_input = layer ( current_input ) [EOL] linear_part = current_input [EOL] [comment] [EOL] [comment] [EOL] nonlinear_part , gate = projected_input . chunk ( [number] , dim = - [number] ) [EOL] nonlinear_part = self . _activation ( nonlinear_part ) [EOL] gate = torch . sigmoid ( gate ) [EOL] current_input = gate * linear_part + ( [number] - gate ) * nonlinear_part [EOL] return current_input [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Callable[[torch.Tensor],torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Callable[[torch.Tensor],torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0
import builtins [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class TokenEmbedder ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def get_output_dim ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . modules . matrix_attention . matrix_attention import MatrixAttention [EOL] [EOL] [EOL] @ MatrixAttention . register ( [string] ) class CosineMatrixAttention ( MatrixAttention ) : [EOL] [docstring] [EOL] [EOL] @ overrides def forward ( self , matrix_1 , matrix_2 ) : [EOL] a_norm = matrix_1 / ( matrix_1 . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] b_norm = matrix_2 / ( matrix_2 . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] return torch . bmm ( a_norm , b_norm . transpose ( - [number] , - [number] ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . span_extractors . endpoint_span_extractor import EndpointSpanExtractor [EOL] from allennlp . modules . span_extractors . self_attentive_span_extractor import SelfAttentiveSpanExtractor [EOL] from allennlp . modules . span_extractors . bidirectional_endpoint_span_extractor import BidirectionalEndpointSpanExtractor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . text_field_embedders . basic_text_field_embedder import BasicTextFieldEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Callable , List [EOL] import torch [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [comment] [EOL] from typing import Tuple , Callable [EOL] import math [EOL] import warnings [EOL] [EOL] import torch [EOL] import torch . nn . functional as F [EOL] [EOL] from allennlp . common . checks import ExperimentalFeatureWarning [EOL] from allennlp . modules . layer_norm import LayerNorm [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] def attention ( query , key , value , mask = None , dropout = None ) : [EOL] [docstring] [EOL] d_k = query . size ( - [number] ) [EOL] scores = torch . matmul ( query , key . transpose ( - [number] , - [number] ) ) / math . sqrt ( d_k ) [EOL] if mask is not None : [EOL] scores = scores . masked_fill ( mask == [number] , - [number] ) [EOL] p_attn = F . softmax ( scores , dim = - [number] ) [EOL] if dropout is not None : [EOL] p_attn = dropout ( p_attn ) [EOL] return torch . matmul ( p_attn , value ) , p_attn [EOL] [EOL] [EOL] def subsequent_mask ( size , device = [string] ) : [EOL] [docstring] [EOL] mask = torch . tril ( torch . ones ( size , size , device = device , dtype = torch . int32 ) ) . unsqueeze ( [number] ) [EOL] return mask [EOL] [EOL] [EOL] class PositionalEncoding ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_dim , max_len = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] positional_encoding = torch . zeros ( max_len , input_dim , requires_grad = False ) [EOL] position = torch . arange ( [number] , max_len ) . unsqueeze ( [number] ) . float ( ) [EOL] div_term = torch . exp ( torch . arange ( [number] , input_dim , [number] ) . float ( ) * - ( math . log ( [number] ) / input_dim ) ) [EOL] positional_encoding [ : , [number] : : [number] ] = torch . sin ( position * div_term ) [EOL] positional_encoding [ : , [number] : : [number] ] = torch . cos ( position * div_term ) [EOL] positional_encoding = positional_encoding . unsqueeze ( [number] ) [EOL] self . register_buffer ( [string] , positional_encoding ) [EOL] [EOL] def forward ( self , x ) : [EOL] [comment] [EOL] return x + self . positional_encoding [ : , : x . size ( [number] ) ] [EOL] [EOL] [EOL] class PositionwiseFeedForward ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_dim , ff_dim , dropout = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] self . w_1 = torch . nn . Linear ( input_dim , ff_dim ) [EOL] self . w_2 = torch . nn . Linear ( ff_dim , input_dim ) [EOL] self . dropout = torch . nn . Dropout ( dropout ) [EOL] [EOL] def forward ( self , x ) : [EOL] [comment] [EOL] return self . w_2 ( self . dropout ( F . relu ( self . w_1 ( x ) ) ) ) [EOL] [EOL] [EOL] class TransformerEncoder ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , layer , num_layers , return_all_layers = False ) : [EOL] super ( ) . __init__ ( ) [EOL] self . layers = util . clone ( layer , num_layers ) [EOL] self . norm = LayerNorm ( layer . size ) [EOL] self . return_all_layers = return_all_layers [EOL] [EOL] def forward ( self , x , mask ) : [EOL] [docstring] [EOL] all_layers = [ ] [EOL] for layer in self . layers : [EOL] x = layer ( x , mask ) [EOL] if self . return_all_layers : [EOL] all_layers . append ( x ) [EOL] [EOL] if self . return_all_layers : [EOL] all_layers [ - [number] ] = self . norm ( all_layers [ - [number] ] ) [EOL] return all_layers [EOL] return self . norm ( x ) [EOL] [EOL] [EOL] class SublayerConnection ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , size , dropout ) : [EOL] super ( ) . __init__ ( ) [EOL] self . norm = LayerNorm ( size ) [EOL] self . dropout = torch . nn . Dropout ( dropout ) [EOL] [EOL] def forward ( self , x , sublayer ) : [EOL] [docstring] [EOL] return x + self . dropout ( sublayer ( self . norm ( x ) ) ) [EOL] [EOL] [EOL] class EncoderLayer ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , size , self_attn , feed_forward , dropout ) : [EOL] super ( ) . __init__ ( ) [EOL] self . self_attn = self_attn [EOL] self . feed_forward = feed_forward [EOL] self . sublayer = util . clone ( SublayerConnection ( size , dropout ) , [number] ) [EOL] self . size = size [EOL] [EOL] def forward ( self , x , mask ) : [EOL] [docstring] [EOL] x = self . sublayer [ [number] ] ( x , lambda x : self . self_attn ( x , x , x , mask ) ) [EOL] return self . sublayer [ [number] ] ( x , self . feed_forward ) [EOL] [EOL] [EOL] class MultiHeadedAttention ( torch . nn . Module ) : [EOL] def __init__ ( self , num_heads , input_dim , dropout = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] assert input_dim % num_heads == [number] , [string] [EOL] [comment] [EOL] self . d_k = input_dim // num_heads [EOL] self . num_heads = num_heads [EOL] [comment] [EOL] [comment] [EOL] self . linears = util . clone ( torch . nn . Linear ( input_dim , input_dim ) , [number] ) [EOL] self . dropout = torch . nn . Dropout ( p = dropout ) [EOL] [EOL] def forward ( self , query , key , value , mask = None ) : [EOL] if mask is not None : [EOL] [comment] [EOL] [comment] [EOL] mask = mask . unsqueeze ( [number] ) . expand ( [ - [number] , self . num_heads , - [number] , - [number] ] ) [EOL] [EOL] nbatches = query . size ( [number] ) [EOL] [EOL] [comment] [EOL] query , key , value = [ layer ( x ) . view ( nbatches , - [number] , self . num_heads , self . d_k ) . transpose ( [number] , [number] ) for layer , x in zip ( self . linears , ( query , key , value ) ) ] [EOL] [EOL] [comment] [EOL] x , _ = attention ( query , key , value , mask = mask , dropout = self . dropout ) [EOL] [EOL] [comment] [EOL] x = x . transpose ( [number] , [number] ) . contiguous ( ) . view ( nbatches , - [number] , self . num_heads * self . d_k ) [EOL] return self . linears [ - [number] ] ( x ) [EOL] [EOL] [EOL] def make_model ( num_layers = [number] , input_size = [number] , hidden_size = [number] , heads = [number] , dropout = [number] , return_all_layers = False ) : [EOL] [docstring] [EOL] attn = MultiHeadedAttention ( heads , input_size , dropout ) [EOL] ff = PositionwiseFeedForward ( input_size , hidden_size , dropout ) [EOL] model = TransformerEncoder ( EncoderLayer ( input_size , attn , ff , dropout ) , num_layers , return_all_layers = return_all_layers ) [EOL] [EOL] [comment] [EOL] for p in model . parameters ( ) : [EOL] if p . dim ( ) > [number] : [EOL] torch . nn . init . xavier_uniform_ ( p ) [EOL] return model [EOL] [EOL] [EOL] class BidirectionalLanguageModelTransformer ( Seq2SeqEncoder ) : [EOL] def __init__ ( self , input_dim , hidden_dim , num_layers , dropout = [number] , input_dropout = None , return_all_layers = False ) : [EOL] [EOL] warnings . warn ( [string] [string] [string] , ExperimentalFeatureWarning ) [EOL] [EOL] super ( ) . __init__ ( ) [EOL] [EOL] self . _return_all_layers = return_all_layers [EOL] self . transformer_layers = num_layers [EOL] self . num_layers = num_layers [EOL] [EOL] self . _forward_transformer = make_model ( input_size = input_dim , hidden_size = hidden_dim , num_layers = num_layers , dropout = dropout , return_all_layers = return_all_layers ) [EOL] self . _backward_transformer = make_model ( input_size = input_dim , hidden_size = hidden_dim , num_layers = num_layers , dropout = dropout , return_all_layers = return_all_layers ) [EOL] self . _position = PositionalEncoding ( input_dim ) [EOL] [EOL] self . input_dim = input_dim [EOL] self . output_dim = [number] * input_dim [EOL] [EOL] if input_dropout : [EOL] self . _dropout = torch . nn . Dropout ( input_dropout ) [EOL] else : [EOL] self . _dropout = lambda x : x [EOL] [EOL] self . should_log_activations = False [EOL] [EOL] def get_attention_masks ( self , mask ) : [EOL] [docstring] [EOL] device = mask . device [EOL] [comment] [EOL] timesteps = mask . size ( [number] ) [EOL] [comment] [EOL] subsequent = subsequent_mask ( timesteps , device ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] forward_mask = mask . unsqueeze ( - [number] ) & subsequent [EOL] [comment] [EOL] backward_mask = forward_mask . transpose ( [number] , [number] ) [EOL] [EOL] return forward_mask , backward_mask [EOL] [EOL] def forward ( self , token_embeddings , mask ) : [EOL] forward_mask , backward_mask = self . get_attention_masks ( mask . int ( ) ) [EOL] token_embeddings = self . _position ( token_embeddings ) [EOL] token_embeddings = self . _dropout ( token_embeddings ) [EOL] forward_output = self . _forward_transformer ( token_embeddings , forward_mask ) [EOL] backward_output = self . _backward_transformer ( token_embeddings , backward_mask ) [EOL] [EOL] if self . _return_all_layers : [EOL] to_return = [ ] [EOL] for forward , backward in zip ( forward_output , backward_output ) : [EOL] to_return . append ( torch . cat ( [ forward , backward ] , - [number] ) ) [EOL] return to_return [EOL] [EOL] return torch . cat ( [ forward_output , backward_output ] , - [number] ) [EOL] [EOL] def get_regularization_penalty ( self ) : [EOL] return [number] [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . input_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] def is_bidirectional ( self ) : [EOL] return True [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] import json [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . models import Model [EOL] from allennlp . models . archival import Archive , load_archive [EOL] [EOL] [comment] [EOL] DEFAULT_PREDICTORS = { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] } [EOL] [EOL] class Predictor ( Registrable ) : [EOL] [docstring] [EOL] def __init__ ( self , model , dataset_reader ) : [EOL] self . _model = model [EOL] self . _dataset_reader = dataset_reader [EOL] [EOL] def load_line ( self , line ) : [comment] [EOL] [docstring] [EOL] return json . loads ( line ) [EOL] [EOL] def dump_line ( self , outputs ) : [comment] [EOL] [docstring] [EOL] return json . dumps ( outputs ) + [string] [EOL] [EOL] def predict_json ( self , inputs ) : [EOL] instance = self . _json_to_instance ( inputs ) [EOL] [comment] [EOL] return self . predict_instance ( instance ) [EOL] [EOL] def predict_instance ( self , instance ) : [EOL] outputs = self . _model . forward_on_instance ( instance ) [EOL] [comment] [EOL] return sanitize ( outputs ) [EOL] [EOL] def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def predict_batch_json ( self , inputs ) : [EOL] instances = self . _batch_json_to_instances ( inputs ) [EOL] return self . predict_batch_instance ( instances ) [EOL] [EOL] def predict_batch_instance ( self , instances ) : [EOL] outputs = self . _model . forward_on_instances ( instances ) [EOL] return sanitize ( outputs ) [EOL] [EOL] def _batch_json_to_instances ( self , json_dicts ) : [EOL] [docstring] [EOL] instances = [ ] [EOL] for json_dict in json_dicts : [EOL] instances . append ( self . _json_to_instance ( json_dict ) ) [EOL] return instances [EOL] [EOL] @ classmethod def from_path ( cls , archive_path , predictor_name = None ) : [EOL] [docstring] [EOL] return Predictor . from_archive ( load_archive ( archive_path ) , predictor_name ) [EOL] [EOL] @ classmethod def from_archive ( cls , archive , predictor_name = None ) : [EOL] [docstring] [EOL] [comment] [EOL] config = archive . config . duplicate ( ) [EOL] [EOL] if not predictor_name : [EOL] model_type = config . get ( [string] ) . get ( [string] ) [EOL] if not model_type in DEFAULT_PREDICTORS : [EOL] raise ConfigurationError ( f" [string] { model_type } [string] " f" [string] " ) [EOL] predictor_name = DEFAULT_PREDICTORS [ model_type ] [EOL] [EOL] dataset_reader_params = config [ [string] ] [EOL] dataset_reader = DatasetReader . from_params ( dataset_reader_params ) [EOL] [EOL] model = archive . model [EOL] model . eval ( ) [EOL] [EOL] return Predictor . by_name ( predictor_name ) ( model , dataset_reader ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.models.Model$ 0 $allennlp.data.DatasetReader$ 0 0 0 0 0 0 0 $allennlp.models.Model$ 0 0 0 0 0 $allennlp.data.DatasetReader$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.data.Instance$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $'Predictor'$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 $'Predictor'$ 0 0 0 $allennlp.models.archival.Archive$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.models.archival.Archive$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.models.archival.Archive$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0
from typing import List , Any [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] from spacy . tokens import Doc [EOL] [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . common . util import get_spacy_model [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . models import Model [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class CorefPredictor ( Predictor ) : [EOL] [docstring] [EOL] def __init__ ( self , model , dataset_reader , language = [string] ) : [EOL] super ( ) . __init__ ( model , dataset_reader ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . _spacy = get_spacy_model ( language , pos_tags = True , parse = True , ner = False ) [EOL] [EOL] def predict ( self , document ) : [EOL] [docstring] [EOL] return self . predict_json ( { [string] : document } ) [EOL] [EOL] def predict_tokenized ( self , tokenized_document ) : [EOL] [docstring] [EOL] instance = self . _words_list_to_instance ( tokenized_document ) [EOL] return self . predict_instance ( instance ) [EOL] [EOL] def _words_list_to_instance ( self , words ) : [EOL] [docstring] [EOL] spacy_document = Doc ( self . _spacy . vocab , words = words ) [EOL] for pipe in filter ( None , self . _spacy . pipeline ) : [EOL] pipe [ [number] ] ( spacy_document ) [EOL] [EOL] sentences = [ [ token . text for token in sentence ] for sentence in spacy_document . sents ] [comment] [EOL] instance = self . _dataset_reader . text_to_instance ( sentences ) [EOL] return instance [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] document = json_dict [ [string] ] [EOL] spacy_document = self . _spacy ( document ) [EOL] sentences = [ [ token . text for token in sentence ] for sentence in spacy_document . sents ] [EOL] instance = self . _dataset_reader . text_to_instance ( sentences ) [EOL] return instance [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
[docstring] [EOL] from allennlp . predictors . predictor import Predictor [EOL] from allennlp . predictors . atis_parser import AtisParserPredictor [EOL] from allennlp . predictors . biaffine_dependency_parser import BiaffineDependencyParserPredictor [EOL] from allennlp . predictors . bidaf import BidafPredictor [EOL] from allennlp . predictors . constituency_parser import ConstituencyParserPredictor [EOL] from allennlp . predictors . coref import CorefPredictor [EOL] from allennlp . predictors . decomposable_attention import DecomposableAttentionPredictor [EOL] from allennlp . predictors . dialog_qa import DialogQAPredictor [EOL] from allennlp . predictors . event2mind import Event2MindPredictor [EOL] from allennlp . predictors . nlvr_parser import NlvrParserPredictor [EOL] from allennlp . predictors . open_information_extraction import OpenIePredictor [EOL] [comment] [EOL] from allennlp . predictors . semantic_role_labeler import SemanticRoleLabelerPredictor [EOL] from allennlp . predictors . sentence_tagger import SentenceTaggerPredictor [EOL] from allennlp . predictors . seq2seq import Seq2SeqPredictor [EOL] from allennlp . predictors . simple_seq2seq import SimpleSeq2SeqPredictor [EOL] from allennlp . predictors . wikitables_parser import WikiTablesParserPredictor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] @ Predictor . register ( [string] ) class BidafPredictor ( Predictor ) : [EOL] [docstring] [EOL] [EOL] def predict ( self , question = None , passage = None ) : [EOL] [docstring] [EOL] return self . predict_json ( { [string] : passage , [string] : question } ) [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] question_text = None [EOL] passage_text = None [EOL] if json_dict [ [string] ] != None : [EOL] question_text = json_dict [ [string] ] [EOL] if json_dict [ [string] ] != None : [EOL] passage_text = json_dict [ [string] ] [EOL] [EOL] if question_text and passage_text : [EOL] [comment] [EOL] return self . _dataset_reader . text_to_instance ( question_text = question_text , passage_text = passage_text ) [EOL] elif question_text and passage_text is None : [EOL] [comment] [EOL] tokenized_question = self . _dataset_reader . text_to_instance_one_argument ( passage_text = question_text ) [EOL] [comment] [EOL] return tokenized_question [EOL] elif passage_text and question_text is None : [EOL] print ( passage_text ) [EOL] return self . _dataset_reader . text_to_instance_one_argument ( passage_text = passage_text ) [EOL] else : [EOL] raise ValueError ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
from allennlp . common . params import Params [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . common . tee_logger import TeeLogger [EOL] from allennlp . common . tqdm import Tqdm [EOL] from allennlp . common . util import JsonDict [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0