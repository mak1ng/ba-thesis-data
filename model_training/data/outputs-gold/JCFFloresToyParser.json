import parser [EOL] from lexer import Token , TokenType [EOL] from parser import parse , TokenList , IncorrectTokenException , NoMoreTokensException , ExcedingTokensException [EOL] from collections import deque [EOL] import pytest [EOL] [EOL] [EOL] def test_basic_expression ( ) : [EOL] token_list = deque ( [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] ) [EOL] parse ( token_list ) [EOL] [EOL] [EOL] def test_nested_expression ( ) : [EOL] token_list = deque ( [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] ) [EOL] parse ( token_list ) [EOL] [EOL] [EOL] def test_incomplete_expression ( ) : [EOL] token_list = deque ( [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . VARIABLE , [string] ) , ] ) [EOL] with pytest . raises ( NoMoreTokensException ) : [EOL] parse ( token_list ) [EOL] [EOL] [EOL] def test_bad_expression ( ) : [EOL] token_list = deque ( [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] ) [EOL] with pytest . raises ( IncorrectTokenException ) : [EOL] parse ( token_list ) [EOL] [EOL] [EOL] def test_consume_all ( ) : [EOL] token_list = deque ( [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) ] ) [EOL] with pytest . raises ( ExcedingTokensException ) : [EOL] parse ( token_list ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import builtins [EOL] import lexer [EOL] from lexer import tokenize , Token , TokenType , InvalidTokenException , TokenGenerator [EOL] from typing import List [EOL] import pytest [EOL] [EOL] [EOL] def assert_tokenize ( token_list , input_string ) : [EOL] assert token_list == list ( tokenize ( input_string ) ) [EOL] [EOL] [EOL] def test_sum ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL] [EOL] [EOL] def test_long_token ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL] [EOL] [EOL] def test_invalid_variable ( ) : [EOL] input_string = [string] [EOL] with pytest . raises ( InvalidTokenException ) : [EOL] list ( tokenize ( input_string ) ) [EOL] [EOL] [EOL] def test_invalid_number ( ) : [EOL] input_string = [string] [EOL] with pytest . raises ( InvalidTokenException ) : [EOL] list ( tokenize ( input_string ) ) [EOL] [EOL] [EOL] def test_nested_expression ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , Token ( TokenType . NUMBER , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL] [EOL] [EOL] def test_incomplete_variable ( ) : [EOL] input_string = [string] [EOL] with pytest . raises ( InvalidTokenException ) : [EOL] list ( tokenize ( input_string ) ) [EOL] [EOL] [EOL] def test_expression_with_variables ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL] [EOL] [EOL] def test_white_spaces ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . VARIABLE , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL] [EOL] [EOL] def test_no_floating_point_numbers ( ) : [EOL] input_string = [string] [EOL] with pytest . raises ( InvalidTokenException ) : [EOL] list ( tokenize ( input_string ) ) [EOL] [EOL] [EOL] def test_no_white_spaces_expression ( ) : [EOL] input_string = [string] [EOL] token_list = [ Token ( TokenType . LEFT_PARENTHESIS , [string] ) , Token ( TokenType . OPERATOR , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . VARIABLE , [string] ) , Token ( TokenType . RIGHT_PARENTHESIS , [string] ) , ] [EOL] assert_tokenize ( token_list , input_string ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import parser [EOL] import builtins [EOL] from lexer import tokenize , InvalidTokenException [EOL] from parser import TokenList , parse , IncorrectTokenException , ExcedingTokensException , NoMoreTokensException [EOL] from collections import deque [EOL] import click [EOL] [EOL] [EOL] @ click . group ( ) def cli ( ) : [EOL] pass [EOL] [EOL] [EOL] @ cli . command ( ) @ click . argument ( [string] ) def scanner ( input_string ) : [EOL] try : [EOL] for token in tokenize ( input_string ) : [EOL] print ( token ) [EOL] except InvalidTokenException as e : [EOL] print ( e ) [EOL] [EOL] [EOL] @ cli . command ( ) @ click . argument ( [string] ) def parser ( input_string ) : [EOL] try : [EOL] token_list = deque ( tokenize ( input_string ) ) [EOL] parse ( token_list ) [EOL] print ( f'{ input_string } [string] ' ) [EOL] except InvalidTokenException as e : [EOL] print ( e ) [EOL] except IncorrectTokenException as e : [EOL] print ( e ) [EOL] except ExcedingTokensException : [EOL] print ( f'{ input_string } [string] ' ) [EOL] except NoMoreTokensException : [EOL] print ( f'{ input_string } [string] ' ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] cli ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . exceptions import InvalidTokenException [EOL] from . lexer import Token , tokenize , TokenType , TokenGenerator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Union , Any , Type , Generator , Callable [EOL] import typing [EOL] import builtins [EOL] import lexer [EOL] from enum import Enum [EOL] from typing import Mapping , Union , Generator , Callable [EOL] from dataclasses import dataclass [EOL] from functools import partial [EOL] from . exceptions import InvalidTokenException [EOL] from . utils import is_digit , is_operator , is_letter [EOL] [EOL] [EOL] class TokenType ( Enum ) : [EOL] NUMBER = [number] [EOL] VARIABLE = [number] [EOL] LEFT_PARENTHESIS = [number] [EOL] RIGHT_PARENTHESIS = [number] [EOL] OPERATOR = [number] [EOL] [EOL] [EOL] class ComplexCharacter ( Enum ) : [EOL] DIGIT = [number] [EOL] LETTER = [number] [EOL] OPERATOR = [number] [EOL] [EOL] [EOL] class NonTerminalState ( Enum ) : [EOL] START = [number] [EOL] VARIABLE_INTERMEDIATE = [number] [EOL] [EOL] [EOL] @ dataclass class Token : [EOL] token_type = ... [EOL] value = ... [EOL] [EOL] def __str__ ( self ) : [EOL] return f' [string] { self . token_type . name } [string] { self . value }' [EOL] [EOL] [EOL] State = Union [ TokenType , NonTerminalState ] [EOL] Character = Union [ str , ComplexCharacter ] [EOL] Automata = Mapping [ State , Mapping [ Character , State ] ] [EOL] TokenGenerator = Generator [ Token , None , None ] [EOL] [EOL] automata = { NonTerminalState . START : { ComplexCharacter . DIGIT : TokenType . NUMBER , [string] : NonTerminalState . VARIABLE_INTERMEDIATE , [string] : TokenType . LEFT_PARENTHESIS , [string] : TokenType . RIGHT_PARENTHESIS , ComplexCharacter . OPERATOR : TokenType . OPERATOR , [string] : NonTerminalState . START , } , TokenType . NUMBER : { ComplexCharacter . DIGIT : TokenType . NUMBER } , NonTerminalState . VARIABLE_INTERMEDIATE : { ComplexCharacter . LETTER : TokenType . VARIABLE } , TokenType . VARIABLE : { ComplexCharacter . LETTER : TokenType . VARIABLE , ComplexCharacter . DIGIT : TokenType . VARIABLE } } [EOL] [EOL] [EOL] def convert_character ( s ) : [EOL] if is_digit ( s ) : [EOL] return ComplexCharacter . DIGIT [EOL] if is_letter ( s ) : [EOL] return ComplexCharacter . LETTER [EOL] if is_operator ( s ) : [EOL] return ComplexCharacter . OPERATOR [EOL] return s [EOL] [EOL] [EOL] def token_or_exception ( state , token ) : [EOL] if isinstance ( state , TokenType ) : [EOL] return Token ( state , token ) [EOL] raise InvalidTokenException ( token ) from None [EOL] [EOL] [EOL] def tokenize_helper ( automata , text ) : [EOL] token = [string] [EOL] current_state = NonTerminalState . START [EOL] for current_char , next_char in zip ( text , text [ [number] : ] + [string] ) : [EOL] current_character = convert_character ( current_char ) [EOL] next_character = convert_character ( next_char ) [EOL] try : [EOL] token += current_char [EOL] next_state = automata [ current_state ] [ current_character ] [EOL] current_state = next_state [EOL] _ = automata [ current_state ] [ next_character ] [EOL] except KeyError : [EOL] yield token_or_exception ( current_state , token . strip ( ) ) [EOL] token = [string] [EOL] current_state = NonTerminalState . START [EOL] [EOL] [EOL] tokenize = partial ( tokenize_helper , automata ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $TokenType$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Pattern [EOL] import typing [EOL] import builtins [EOL] import re [EOL] [EOL] [EOL] def is_digit ( s ) : [EOL] try : [EOL] int ( s ) [EOL] return True [EOL] except ValueError : [EOL] return False [EOL] [EOL] [EOL] def is_operator ( s ) : [EOL] return s in [string] [EOL] [EOL] [EOL] regex = re . compile ( [string] ) [EOL] [EOL] [EOL] def is_letter ( s ) : [EOL] return regex . fullmatch ( s ) is not None [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0
import builtins [EOL] class InvalidTokenException ( Exception ) : [EOL] def __init__ ( self , token ) : [EOL] super ( ) . __init__ ( f' [string] { token } [string] ' ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0
from . exceptions import IncorrectTokenException , NoMoreTokensException , ExcedingTokensException [EOL] from . parser import parse , TokenList [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import lexer [EOL] from lexer import Token [EOL] [EOL] [EOL] class NoMoreTokensException ( Exception ) : [EOL] pass [EOL] [EOL] [EOL] class IncorrectTokenException ( Exception ) : [EOL] [EOL] def __init__ ( self , token ) : [EOL] super ( ) . __init__ ( f' [string] { token . value } [string] ' ) [EOL] [EOL] [EOL] class ExcedingTokensException ( Exception ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $lexer.Token$ 0 0 0 0 0 0 0 0 0 0 0 0 $lexer.Token$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0