[comment] [EOL] [comment] [EOL] [EOL] [EOL] import setuptools [EOL] [EOL] setuptools . setup ( package_dir = { [string] : [string] } , packages = setuptools . find_packages ( [string] ) , package_data = { [string] : [ [string] , [string] ] } , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import List [EOL] import typing [EOL] __version__ = [string] [EOL] [EOL] __all__ = [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0
[comment] [EOL] from typing import Any , Optional , Dict , List , Tuple [EOL] import src [EOL] import builtins [EOL] import typing [EOL] import csv [EOL] import os [EOL] from datetime import datetime , timedelta [EOL] from pathlib import Path [EOL] from typing import List , NamedTuple , Optional [EOL] [EOL] import airflow [EOL] from airflow import DAG [EOL] from airflow . hooks . postgres_plugin import PostgresHook [EOL] from airflow . models import Variable [EOL] from airflow . operators . python_operator import PythonOperator [EOL] from airflow . utils . log . logging_mixin import LoggingMixin [EOL] from airflow_postgres_plugin . operators import ( PandasToPostgresBulkOperator , PandasToPostgresTableOperator , ) [EOL] [EOL] from airflow_sync . sync import ( _cleanup , _run_sql , _sync_interval , _upsert_table , _cleanup , get_s3_files , ) [EOL] [EOL] [EOL] class S3File ( NamedTuple ) : [EOL] uri = [string] [EOL] filename = [string] [EOL] table = [string] [EOL] [EOL] [EOL] log = LoggingMixin ( ) . log [EOL] [EOL] dag_default_args = { [string] : [string] , [string] : False , [string] : airflow . utils . dates . days_ago ( [number] ) , [string] : [number] , [string] : timedelta ( minutes = [number] ) , [string] : [number] , [string] : [number] , } [EOL] [EOL] [EOL] def create_dag ( dag_name , pg_conn_id = None , s3_conn_id = None , owner = None , variable_key = None , tables = None , pool = None , ** dag_defaults , ) : [EOL] if variable_key is None : [EOL] variable_key = dag_name [EOL] CONSTANTS = Variable . get ( variable_key , deserialize_json = True ) [EOL] S3_CONNECTION = CONSTANTS . get ( [string] ) [EOL] PG_CONN_ID = CONSTANTS . get ( [string] ) [EOL] S3_BUCKET = CONSTANTS . get ( [string] ) [EOL] SYNC_DELTA = CONSTANTS . get ( [string] , { [string] : - [number] } ) [EOL] SYNC_INTERVAL = CONSTANTS . get ( [string] , [string] ) [EOL] default_owner = owner if owner is not None else dag_default_args [ [string] ] [EOL] default_pool = pool if pool is not None else dag_default_args [ [string] ] [EOL] dag_default_args . update ( { [string] : default_owner , [string] : default_pool } ) [EOL] dag_default_args . update ( dag_defaults ) [EOL] dag = DAG ( dag_name , default_args = dag_default_args , schedule_interval = SYNC_INTERVAL , catchup = False , ) [EOL] [EOL] if not tables : [EOL] tables = [ fn . split ( [string] ) [ [number] ] for fn in get_s3_files ( S3_CONNECTION , S3_BUCKET ) ] [EOL] [EOL] def get_s3_uri ( filename , ** context ) : [EOL] new_uri = f" [string] { S3_BUCKET } [string] { filename }" [EOL] log . debug ( f" [string] { filename } [string] { new_uri }" ) [EOL] return new_uri [EOL] [EOL] def convert_table_to_file ( table ) : [EOL] if [string] in table : [EOL] table = Path ( table ) . stem [EOL] formatted_date = ( datetime . today ( ) + timedelta ( days = - [number] ) ) . strftime ( [string] ) [EOL] return f"{ table } [string] { formatted_date } [string] " [EOL] [EOL] def get_s3_uris ( tables ) : [EOL] if not tables : [EOL] tables = [ fn . split ( [string] ) [ [number] ] for fn in get_s3_files ( S3_CONNECTION , S3_BUCKET ) ] [comment] [EOL] s3_uris = [ get_s3_uri ( fn ) for fn in tables ] [EOL] return s3_uris [EOL] [EOL] files = [ ( table , convert_table_to_file ( table ) ) for table in tables ] [EOL] uris = [ S3File ( uri = get_s3_uri ( fn ) , filename = fn , table = table ) for table , fn in files ] [EOL] [EOL] def _sync_join ( ** context ) : [EOL] join_results = [ ] [EOL] instance = context [ [string] ] [EOL] for fn in uris : [EOL] result = instance . xcom_pull ( task_ids = f" [string] { fn . table }" ) [EOL] join_results . append ( ( result , fn ) ) [EOL] return join_results [EOL] [EOL] def _cleanup_temp_table ( context ) : [EOL] instance = context [ [string] ] [EOL] table_name = instance . task_id . split ( [string] ) [ - [number] ] [EOL] temp_table = context [ [string] ] . xcom_pull ( task_ids = f" [string] { table_name }" ) [EOL] templates_dict = context . get ( [string] , { } ) . update ( { [string] : temp_table } ) [EOL] context [ [string] ] = templates_dict [EOL] _cleanup ( PG_CONN_ID , schema = [string] , context = context ) [EOL] [EOL] sync_interval = PythonOperator ( task_id = [string] , python_callable = _sync_interval , op_kwargs = { [string] : SYNC_DELTA } , dag = dag , provide_context = True , templates_dict = { [string] : ( [string] ) } , ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] sync_join = PythonOperator ( task_id = [string] , python_callable = _sync_join , trigger_rule = [string] , provide_context = True , dag = dag , ) [EOL] [EOL] [comment] [EOL] for uri in uris : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] load_s3_file = PandasToPostgresTableOperator ( task_id = f" [string] { uri . table }" , conn_id = PG_CONN_ID , schema = [string] , table = uri . table , sep = [string] , compression = [string] , filepath = uri . uri , pool = pool if pool else [string] , s3_conn_id = S3_CONNECTION , include_index = False , quoting = csv . QUOTE_NONE , temp_table = True , on_failure_callback = _cleanup_temp_table , on_retry_callback = _cleanup_temp_table , templates_dict = { [string] : ( [string] ) , [string] : ( [string] ) , } , dag = dag , ) [EOL] [EOL] upsert_table = PythonOperator ( task_id = f" [string] { uri . table }" , python_callable = _upsert_table , op_kwargs = { [string] : uri . table , [string] : PG_CONN_ID , [string] : [string] } , provide_context = True , pool = pool if pool else [string] , templates_dict = { [string] : ( [string] ) , [string] : ( [string] ) , [string] : ( [string] f" [string] { uri . table } [string] " [string] ) , } , on_failure_callback = _cleanup_temp_table , dag = dag , ) [EOL] [EOL] cleanup = PythonOperator ( task_id = f" [string] { uri . table }" , python_callable = _cleanup , op_kwargs = { [string] : PG_CONN_ID , [string] : [string] } , provide_context = True , pool = pool if pool else [string] , templates_dict = { [string] : ( [string] f" [string] { uri . table } [string] " [string] ) } , dag = dag , trigger_rule = [string] , ) [EOL] [EOL] sync_interval . set_downstream ( load_s3_file ) [EOL] load_s3_file . set_downstream ( upsert_table ) [EOL] upsert_table . set_downstream ( cleanup ) [EOL] sync_join . set_upstream ( cleanup ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] return dag [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any [EOL] import datetime [EOL] import builtins [EOL] import typing [EOL] from datetime import datetime , timedelta [EOL] from typing import List [EOL] [EOL] from airflow import DAG [EOL] from airflow . hooks . S3_hook import S3Hook [EOL] from airflow . models import Variable [EOL] from airflow . operators . python_operator import PythonOperator [EOL] from airflow_salesforce_plugin . hooks import SalesforceHook [EOL] from airflow_salesforce_plugin . operators import SalesforceAttachmentToS3Operator [EOL] [EOL] from airflow_sync . sync import _cleanup , _sync_interval , _upsert_table , query_salesforce [EOL] from airflow_sync . utils import get_sql_dir [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def create_dag ( dag_name , sf_conn_id , s3_conn_id , dag_base = None , owner = [string] , ** dag_defaults , ) : [EOL] if dag_base is not None : [EOL] CONSTANTS = Variable . get ( dag_base , deserialize_json = True ) [EOL] else : [EOL] CONSTANTS = Variable . get ( dag_name , deserialize_json = True ) [EOL] ATTACHMENT_SQL_PATH = get_sql_dir ( ) / [string] / [string] [EOL] S3_CONN_ID = CONSTANTS [ [string] ] [EOL] SF_CONN_ID = CONSTANTS [ [string] ] [EOL] S3_BUCKET = CONSTANTS [ [string] ] [EOL] DEPENDS_ON_PAST = CONSTANTS . get ( [string] , False ) [EOL] SYNC_INTERVAL = CONSTANTS . get ( [string] , [string] ) [EOL] SYNC_DELTA = CONSTANTS . get ( [string] , { [string] : - [number] } ) [EOL] START_DATE = datetime ( [number] , [number] , [number] ) [EOL] UPLOAD_TIMEOUT = int ( CONSTANTS [ [string] ] ) [EOL] CONCURRENT_UPLOADS = int ( CONSTANTS [ [string] ] ) [EOL] CATCHUP = False [EOL] dag_default_args = { [string] : DEPENDS_ON_PAST , [string] : START_DATE , [string] : [number] , [string] : timedelta ( seconds = [number] ) , [string] : owner , [string] : [string] , } [EOL] dag_default_args . update ( dag_defaults ) [EOL] [EOL] dag = DAG ( dag_name , schedule_interval = SYNC_INTERVAL , catchup = CATCHUP , default_args = dag_default_args , ) [EOL] [EOL] [comment] [EOL] [EOL] sync_interval = PythonOperator ( task_id = [string] , python_callable = _sync_interval , op_kwargs = { [string] : SYNC_DELTA } , dag = dag , provide_context = True , templates_dict = { [string] : [string] } , ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] attachment_ids = PythonOperator ( task_id = [string] , python_callable = query_salesforce , provide_context = True , op_kwargs = { [string] : SF_CONN_ID , [string] : ATTACHMENT_SQL_PATH . read_text ( ) } , templates_dict = { [string] : ( [string] ) } , dag = dag , ) [EOL] [EOL] get_attachments = SalesforceAttachmentToS3Operator ( task_id = [string] , concurrent_uploads = CONCURRENT_UPLOADS , attachment_ids = [string] , conn_id = SF_CONN_ID , s3_conn_id = S3_CONN_ID , s3_bucket = S3_BUCKET , upload_timeout = UPLOAD_TIMEOUT , api_version = [string] , provide_context = True , dag = dag , ) [EOL] [EOL] sync_interval >> attachment_ids >> get_attachments [EOL] return dag [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] from typing import Generator , Iterator , Any , Optional , Dict , List , Tuple , Union [EOL] import airflow [EOL] import airflow_postgres_plugin [EOL] import pathlib [EOL] import typing [EOL] import datetime [EOL] import builtins [EOL] import sqlalchemy [EOL] import concurrent [EOL] import concurrent . futures [EOL] import fnmatch [EOL] import gzip [EOL] import hashlib [EOL] import json [EOL] import os [EOL] import shutil [EOL] import tempfile [EOL] from datetime import date , datetime , timedelta [EOL] from functools import partial [EOL] from pathlib import Path [EOL] from typing import Any , Dict , Iterator , List , Optional , Tuple , Union [EOL] [EOL] import dateutil . parser [EOL] import sqlalchemy [EOL] from airflow import DAG [EOL] from airflow . hooks . S3_hook import S3Hook [EOL] from airflow . models import Variable [EOL] from airflow . operators . dummy_operator import DummyOperator [EOL] from airflow . operators . python_operator import PythonOperator [EOL] from airflow . utils . log . logging_mixin import LoggingMixin [EOL] from airflow_postgres_plugin . hooks import PostgresHook [EOL] from airflow_postgres_plugin . operators import FileToPostgresTableOperator [EOL] from airflow_salesforce_plugin . hooks import SalesforceHook [EOL] from airflow_salesforce_plugin . operators import SalesforceToFileOperator [EOL] from sqlalchemy . dialects . postgresql import insert [EOL] [EOL] log = LoggingMixin ( ) . log [EOL] [EOL] [EOL] def _sync_interval ( delta , ** context ) : [EOL] if not isinstance ( delta , dict ) : [EOL] delta = { [string] : - [number] } [EOL] templates_dict = context . get ( [string] , { } ) [EOL] if [string] in templates_dict : [EOL] start_datetime = dateutil . parser . isoparse ( templates_dict [ [string] ] ) . replace ( tzinfo = None ) [EOL] else : [EOL] start_datetime = datetime . now ( ) [EOL] return ( start_datetime + timedelta ( ** delta ) ) . isoformat ( ) [EOL] [EOL] [EOL] def _run_sql ( pg_conn_id , query , schema = [string] , returns_rows = True , ** context , ) : [EOL] templates_dict = context . get ( [string] , { } ) [EOL] start_datetime = templates_dict [ [string] ] [EOL] end_datetime = templates_dict [ [string] ] [EOL] pg_hook = PostgresHook ( pg_conn_id , schema = schema ) [EOL] results = list ( pg_hook . query ( query , [ start_datetime , end_datetime ] , returns_rows = returns_rows ) ) [EOL] return results [EOL] [EOL] [EOL] def _upsert_table ( pg_conn_id , table , schema , constraints = None , returns_rows = True , ** context , ) : [EOL] if not constraints : [EOL] constraints = [ ] [EOL] [EOL] templates_dict = context . get ( [string] , { } ) [EOL] start_datetime = templates_dict [ [string] ] [EOL] end_datetime = templates_dict [ [string] ] [EOL] table_name = templates_dict [ [string] ] [EOL] pg_hook = PostgresHook ( pg_conn_id , schema = schema ) [EOL] from_table = pg_hook . get_sqlalchemy_table ( table_name ) [EOL] to_table = pg_hook . get_sqlalchemy_table ( table ) [EOL] [EOL] insert_statement = insert ( to_table ) . from_select ( from_table . columns . keys ( ) , from_table . select ( ) ) [EOL] [EOL] inspected = sqlalchemy . inspect ( to_table ) [EOL] primary_keys = [ _ . name for _ in inspected . primary_key ] [EOL] if isinstance ( constraints , list ) and len ( constraints ) > [number] : [EOL] primary_keys = constraints [EOL] [EOL] upsert_statement = insert_statement . on_conflict_do_update ( index_elements = primary_keys , set_ = { column . name : getattr ( insert_statement . excluded , column . name ) for column in inspected . columns } , ) [EOL] [EOL] with pg_hook . sqlalchemy_session ( ) as session : [EOL] session . execute ( sqlalchemy . text ( [string] [string] ) , dict ( start_datetime = start_datetime , end_datetime = end_datetime ) , ) [EOL] session . execute ( sqlalchemy . text ( str ( upsert_statement ) ) ) [EOL] return None [EOL] [EOL] [EOL] def _cleanup ( pg_conn_id , schema = [string] , ** context ) : [EOL] pg_hook = PostgresHook ( pg_conn_id , schema = schema ) [EOL] templates_dict = context . get ( [string] , { } ) [EOL] if [string] not in templates_dict : [EOL] return None [EOL] temp_table_name = templates_dict [ [string] ] [EOL] temp_table = pg_hook . get_sqlalchemy_table ( temp_table_name ) [EOL] temp_table . drop ( pg_hook . get_sqlalchemy_engine ( ) ) [EOL] return None [EOL] [EOL] [EOL] def query_salesforce ( sf_conn_id , soql , ** context ) : [EOL] sf_hook = SalesforceHook ( sf_conn_id ) [EOL] templates_dict = context . get ( [string] , { } ) [EOL] if [string] not in templates_dict : [EOL] raise ValueError ( [string] ) [EOL] soql_args = templates_dict [ [string] ] [EOL] query = sf_hook . query ( soql , soql_args . split ( [string] ) , include_headers = True ) [EOL] column_index = next ( query ) . index ( [string] ) [EOL] log . info ( f" [string] { query !r} [string] { sf_hook !r}" ) [EOL] id_list = [string] . join ( [ _ [ column_index ] for _ in query if _ [ column_index ] is not None ] ) [EOL] return id_list [EOL] [EOL] [EOL] def upload_file_to_s3 ( s3_bucket , path = None , s3_conn_id = None , s3_key = None , s3_hook = None , ** context , ) : [EOL] templates_dict = context . get ( [string] , { } ) [EOL] path = templates_dict . get ( [string] , path ) [EOL] if path is None : [EOL] raise TypeError ( f" [string] { path !r}" ) [EOL] filepath = Path ( path ) [EOL] if s3_key is None : [EOL] s3_key = f"{ filepath . stem }" [EOL] record_key = f"{ s3_key } [string] { date . today ( ) . isoformat ( ) } [string] " [EOL] if s3_hook is None : [EOL] if s3_conn_id is None : [EOL] raise TypeError ( f" [string] { s3_conn_id !r}" ) [EOL] s3_hook = S3Hook ( s3_conn_id , verify = False ) [EOL] if s3_hook . check_for_key ( record_key , bucket_name = s3_bucket ) : [EOL] log . warning ( f" [string] { record_key !r} [string] { s3_hook !r}" ) [EOL] else : [EOL] log . info ( f" [string] { filepath !r} [string] { record_key !r} [string] { s3_hook !r}" ) [EOL] [EOL] log . info ( f" [string] { record_key } [string] { s3_bucket }" ) [EOL] try : [EOL] s3_hook . load_bytes ( filepath . read_bytes ( ) , record_key , bucket_name = s3_bucket ) [EOL] except Exception as exc : [EOL] log . error ( f" [string] { filepath !r}" f" [string] { record_key !r} [string] { s3_hook !r} [string] { exc }" ) [EOL] raise exc [EOL] log . info ( f" [string] { record_key }" ) [EOL] return record_key [EOL] [EOL] [EOL] def gzip_file ( filepath ) : [EOL] f_out = None [EOL] path = Path ( filepath ) [EOL] log . info ( f" [string] { path . as_posix ( ) }" ) [EOL] temp_path = Path ( tempfile . gettempdir ( ) ) / f"{ path . name } [string] " [EOL] temp_filename = temp_path . as_posix ( ) [EOL] with open ( path . as_posix ( ) , [string] ) as f_in : [EOL] with gzip . open ( temp_filename , [string] ) as f_out : [EOL] log . info ( f" [string] { temp_filename }" ) [EOL] shutil . copyfileobj ( f_in , f_out ) [EOL] return temp_filename [EOL] [EOL] [EOL] def gzip_files ( filepaths , max_workers = [number] , ** context ) : [EOL] results = [ ] [EOL] result_map = [ ] [EOL] [EOL] if filepaths : [EOL] with concurrent . futures . ThreadPoolExecutor ( max_workers = max_workers ) as executor : [EOL] result_map = executor . map ( gzip_file , filepaths ) [EOL] results = [ r for r in result_map if r is not None ] [EOL] return [string] . join ( results ) [EOL] [EOL] [EOL] def _upload_file ( path_and_hook , bucket ) : [EOL] path , s3_hook = path_and_hook [EOL] filepath = Path ( path ) [EOL] s3_key = f"{ filepath . stem }" [EOL] record_key = f"{ s3_key } [string] { date . today ( ) . isoformat ( ) } [string] " [EOL] if s3_hook . check_for_key ( record_key , bucket_name = bucket ) : [EOL] log . warning ( f" [string] { record_key !r} [string] { s3_hook !r}" ) [EOL] else : [EOL] log . info ( f" [string] { filepath !r} [string] { record_key !r} [string] { s3_hook !r}" ) [EOL] [EOL] log . info ( f" [string] { record_key } [string] { bucket }" ) [EOL] try : [EOL] s3_hook . load_bytes ( filepath . read_bytes ( ) , record_key , bucket_name = bucket ) [EOL] except Exception as exc : [EOL] log . error ( f" [string] { filepath !r}" f" [string] { record_key !r} [string] { s3_hook !r} [string] { exc }" ) [EOL] raise exc [EOL] log . info ( f" [string] { record_key }" ) [EOL] return record_key [EOL] [EOL] [EOL] def upload_files_to_s3 ( s3_conn_id , s3_bucket , max_connections = [number] , ** context ) : [EOL] results = [ ] [EOL] result_map = [ ] [EOL] templates_dict = context . get ( [string] , { } ) [EOL] filepaths = templates_dict . get ( [string] , [string] ) . strip ( ) [EOL] [EOL] def upload_file ( filepath_and_hook , bucket = s3_bucket ) : [EOL] return _upload_file ( filepath_and_hook , bucket ) [EOL] [EOL] if filepaths : [EOL] log . info ( f" [string] { s3_conn_id }" ) [EOL] hook = S3Hook ( s3_conn_id , verify = False ) [EOL] filepath_list = filepaths . split ( [string] ) [EOL] paths = [ ( os . path . abspath ( fp ) , hook ) for fp in filepath_list ] [EOL] with concurrent . futures . ThreadPoolExecutor ( max_workers = max_connections ) as executor : [EOL] result_map = executor . map ( upload_file , paths ) [EOL] results = [ r for r in result_map if r is not None ] [EOL] return [string] . join ( results ) [EOL] [EOL] [EOL] def get_s3_files ( s3_conn_id , s3_bucket , prefix = [string] ) : [EOL] hook = S3Hook ( s3_conn_id , verify = False ) [EOL] formatted_date = ( datetime . today ( ) + timedelta ( days = - [number] ) ) . strftime ( [string] ) [EOL] file_format = f" [string] { formatted_date } [string] " [EOL] matches = ( key for key in hook . list_keys ( s3_bucket , prefix = prefix ) if fnmatch . fnmatch ( key , file_format ) ) [EOL] rv = list ( matches ) [EOL] return rv [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0