from typing import Union , Callable , Any , Dict , List [EOL] import typing [EOL] import src [EOL] import warnings [EOL] [EOL] warnings . filterwarnings ( [string] ) [EOL] import sys [EOL] [EOL] sys . path . append ( [string] ) [EOL] import time [EOL] from src . data_loading import load_mnist , load_fashion_mnist , load_sat_image_data , load_forest_cover_data [EOL] from src . experiment import Experiment [EOL] import tensorflow as tf [EOL] import pandas as pd [EOL] import numpy as np [EOL] from tqdm import tqdm_notebook as tqdm [EOL] [EOL] SEEDS = [number] [EOL] EPOCHS = [number] [EOL] [EOL] [EOL] def main ( ) : [EOL] datasets = [ [ [string] , load_sat_image_data , [ [number] ] , [ [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , [ [string] , load_forest_cover_data , [ [number] ] , [ [number] ] , [ [number] , [number] , [number] ] ] , [ [string] , load_mnist , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , [ [string] , load_fashion_mnist , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , ] [EOL] algorithms = [ [string] , [string] , [string] ] [EOL] dataset_names = [ x [ [number] ] for x in datasets ] [EOL] [EOL] df_runs = None [EOL] timestamp = time . strftime ( [string] ) [EOL] [EOL] seeds = np . random . randint ( np . iinfo ( np . uint32 ) . max , size = SEEDS , dtype = np . uint32 ) [EOL] for seed in tqdm ( seeds ) : [EOL] for dataset in tqdm ( datasets ) : [EOL] name , load_data_func , inliers , outliers , comp_hidden = dataset [EOL] [EOL] df , df_test = load_data_func ( ) [EOL] y_train = df [ [string] ] [EOL] del df [ [string] ] [EOL] y_test = df_test [ [string] ] [EOL] del df_test [ [string] ] [EOL] [EOL] x_train , x_test = df . values , df_test . values [EOL] clusters = len ( inliers ) [EOL] [EOL] df = df [ ~ y_train . isin ( outliers ) ] [EOL] [EOL] for algorithm in algorithms : [EOL] config = { [string] : comp_hidden , [string] : EPOCHS , [string] : [number] , [string] : seed , [string] : False , [string] : algorithm , [string] : clusters , [string] : name , [string] : None } [EOL] [EOL] tf . reset_default_graph ( ) [EOL] model = Experiment ( ** config ) [EOL] scores = model . fit ( df . values , y_train , x_test = x_test , y_test = y_test , outlier_classes = outliers ) [EOL] df_run = pd . DataFrame ( scores , columns = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] df_run [ [string] ] = seed [EOL] df_run [ [string] ] = algorithm [EOL] df_run [ [string] ] = name [EOL] if df_runs is None : [EOL] df_runs = df_run [EOL] else : [EOL] df_runs = df_runs . append ( df_run , ignore_index = True ) [EOL] try : [EOL] df_runs . to_csv ( [string] . format ( timestamp ) ) [EOL] except : [EOL] pass [EOL] [EOL] max_scores = df_runs . groupby ( [ [string] , [string] , [string] ] ) . max ( ) [EOL] for score_name in max_scores . keys ( ) : [EOL] if score_name == [string] : [EOL] continue [EOL] print ( score_name ) [EOL] df_result = pd . DataFrame ( columns = algorithms , index = dataset_names ) [EOL] for dataset_name in dataset_names : [EOL] for algorithm in algorithms : [EOL] dataset_algorithm_result = max_scores . loc [ dataset_name ] . loc [ algorithm ] [ score_name ] [EOL] df_result . loc [ dataset_name , algorithm ] = [string] . format ( dataset_algorithm_result . mean ( ) , dataset_algorithm_result . std ( ) ) [EOL] print ( df_result ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import pathlib [EOL] from os . path import realpath , dirname [EOL] from pathlib import Path [EOL] [EOL] REPO_NAME = [string] [EOL] REPO_PATH = Path ( dirname ( realpath ( __file__ ) ) ) [EOL] DATA_PATH = REPO_PATH / [string] [EOL] if Path ( DATA_PATH / REPO_NAME ) . exists ( ) : [EOL] DATA_PATH = DATA_PATH / REPO_NAME / [string] [EOL] [EOL] OUTPUT_PATH = DATA_PATH / [string] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 0 0 0 $pathlib.Path$ 0 $builtins.str$ 0 0 0 0 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 $builtins.str$ 0 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import tensorflow as tf [EOL] import tensorflow_probability as tfd [EOL] [EOL] from . algorithm import Algorithm [EOL] [EOL] tfd = tfd . distributions [EOL] [EOL] [EOL] class VAE ( Algorithm ) : [EOL] [EOL] def __init__ ( self , hidden_layers , n_clusters ) : [EOL] super ( ) . __init__ ( hidden_layers , n_clusters ) [EOL] self . prior , self . posterior , self . likelihood , self . divergence = [ None ] * [number] [EOL] [EOL] def make_encoder ( self , data , code_size ) : [EOL] x = tf . layers . flatten ( data ) [EOL] for layer in self . hidden_layers [ : - [number] ] : [EOL] x = tf . layers . dense ( x , layer , tf . nn . relu ) [EOL] loc = tf . layers . dense ( x , code_size ) [EOL] scale = tf . layers . dense ( x , code_size , tf . nn . softplus ) [EOL] return tfd . MultivariateNormalDiag ( loc , scale ) [EOL] [EOL] def make_prior ( self , code_size ) : [EOL] loc = tf . zeros ( code_size ) [EOL] scale = tf . ones ( code_size ) [EOL] return tfd . MultivariateNormalDiag ( loc , scale ) [EOL] [EOL] def make_decoder ( self , code , data_shape ) : [EOL] x = code [EOL] for layer in self . hidden_layers [ : - [number] ] [ : : - [number] ] : [EOL] x = tf . layers . dense ( x , layer , tf . nn . relu ) [EOL] logit = tf . layers . dense ( x , np . prod ( data_shape ) ) [EOL] logit = tf . reshape ( logit , [ - [number] ] + data_shape ) [EOL] return tfd . Independent ( tfd . Bernoulli ( logit ) , [number] ) [EOL] [EOL] def build_graph ( self , x ) : [EOL] n_features = x . shape [ [number] ] [EOL] make_encoder = tf . make_template ( [string] , self . make_encoder ) [EOL] make_decoder = tf . make_template ( [string] , self . make_decoder ) [EOL] self . prior = self . make_prior ( code_size = self . latent_size ) [EOL] self . posterior = make_encoder ( x , code_size = self . latent_size ) [EOL] self . z = self . posterior . sample ( ) [EOL] self . x_dash = make_decoder ( self . z , [ n_features ] ) . mean ( ) [EOL] [EOL] self . likelihood = make_decoder ( self . z , [ n_features ] ) . log_prob ( x ) [EOL] self . divergence = tfd . kl_divergence ( self . posterior , self . prior ) [EOL] self . loss = - tf . reduce_mean ( self . likelihood - self . divergence ) [EOL] self . minimizer = tf . train . AdamOptimizer ( [number] ) . minimize ( self . loss ) [EOL] return self . z , self . x_dash , self . loss , self . minimizer [EOL] [EOL] def get_decoded_centroids ( self ) : [EOL] centers = self . get_kmeans_centroids ( ) [EOL] return self . sess . run ( self . x_dash , feed_dict = { self . z : centers } ) [EOL] [EOL] def get_custom_anomaly_scores ( self , x ) : [EOL] return [ self . sess . run ( [ self . loss ] , feed_dict = { [string] : row . reshape ( [number] , - [number] ) } ) [ [number] ] for row in x ] [EOL] [EOL] def plot_custom_test_set ( self , x , y , outlier = None ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import numpy as np [EOL] import tensorflow_probability as tfd [EOL] from sklearn . cluster import KMeans [EOL] [EOL] tfd = tfd . distributions [EOL] [EOL] [EOL] class Algorithm ( object ) : [EOL] [EOL] def __init__ ( self , hidden_layers , n_clusters ) : [EOL] self . hidden_layers = hidden_layers [EOL] self . latent_size = hidden_layers [ - [number] ] [EOL] self . n_clusters = n_clusters [EOL] self . z , self . x_dash , self . loss , self . minimizer , self . sess , self . kmeans = [ None ] * [number] [EOL] [EOL] def build_graph ( self , x ) : [EOL] raise NotImplementedError ( ) [EOL] [EOL] def set_session ( self , sess ) : [EOL] [docstring] [EOL] self . sess = sess [EOL] [EOL] [docstring] [EOL] [EOL] def get_reconstruction ( self , x ) : [EOL] [docstring] [EOL] return self . sess . run ( self . x_dash , feed_dict = { [string] : x } ) [EOL] [EOL] def get_reconstruction_loss ( self , x ) : [EOL] [docstring] [EOL] x_dash = self . get_reconstruction ( x ) [EOL] return np . mean ( np . sum ( ( x - x_dash ) ** [number] , axis = [number] ) ) [EOL] [EOL] def get_reconstruction_anomaly_scores ( self , x ) : [EOL] [docstring] [EOL] x_dash = self . get_reconstruction ( x ) [EOL] anomaly_scores = np . sum ( ( x - x_dash ) ** [number] , axis = [number] ) [EOL] return anomaly_scores [EOL] [EOL] def get_custom_anomaly_scores ( self , x ) : [EOL] [docstring] [EOL] return self . get_reconstruction_anomaly_scores ( x ) [EOL] [EOL] [docstring] [EOL] [EOL] def fit_kmeans ( self , x ) : [EOL] [docstring] [EOL] z = self . sess . run ( self . z , feed_dict = { [string] : x } ) [EOL] self . kmeans = KMeans ( n_clusters = self . n_clusters ) . fit ( np . nan_to_num ( z ) ) [EOL] [EOL] def get_kmeans_assignments ( self ) : [EOL] [docstring] [EOL] return self . kmeans . labels_ [EOL] [EOL] def get_kmeans_centroids ( self ) : [EOL] [docstring] [EOL] return self . kmeans . cluster_centers_ [EOL] [EOL] def get_custom_assignments ( self , x ) : [EOL] [docstring] [EOL] return self . get_kmeans_assignments ( ) [EOL] [EOL] def get_decoded_centroids ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError ( ) [EOL] [EOL] def get_closest_decoded_centroids ( self , x ) : [EOL] decoded_centroids = self . get_decoded_centroids ( ) [EOL] assignments = self . kmeans . predict ( self . sess . run ( self . z , feed_dict = { [string] : x } ) ) [EOL] closest_centroids = [ ] [EOL] for i in range ( x . shape [ [number] ] ) : [EOL] closest_centroids . append ( decoded_centroids [ assignments [ i ] ] ) [EOL] return np . asarray ( closest_centroids ) [EOL] [EOL] def plot_custom_test_set ( self , x , y , outlier = None ) : [EOL] pass [EOL] [EOL] [EOL] class Encoding ( object ) : [EOL] AE = [string] [EOL] VAE = [string] [EOL] GMVAE = [string]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0
from typing import Any [EOL] import typing [EOL] import src [EOL] import tensorflow as tf [EOL] import tensorflow_probability as tfd [EOL] [EOL] from src . compression_net import CompressionNet [EOL] [EOL] tfd = tfd . distributions [EOL] from . algorithm import Algorithm [EOL] [EOL] [EOL] class AE ( Algorithm ) : [EOL] [EOL] def __init__ ( self , hidden_layers , n_clusters , comp_activation = tf . nn . tanh , use_error_functions = False , use_cnn = False , learning_rate = [number] ) : [EOL] super ( ) . __init__ ( hidden_layers , n_clusters ) [EOL] self . learning_rate = learning_rate [EOL] self . comp_net = CompressionNet ( hidden_layers , activation = comp_activation , use_error_functions = use_error_functions , use_cnn = use_cnn ) [EOL] self . reconstruction_loss , self . minimizer = [ None ] * [number] [EOL] [EOL] def build_graph ( self , x ) : [EOL] self . z , self . x_dash = self . comp_net . inference ( x ) [EOL] self . reconstruction_loss = CompressionNet . reconstruction_error ( x , self . x_dash ) [EOL] self . minimizer = tf . train . AdamOptimizer ( self . learning_rate ) . minimize ( self . reconstruction_loss ) [EOL] return self . z , self . x_dash , self . reconstruction_loss , self . minimizer [EOL] [EOL] def get_decoded_centroids ( self ) : [EOL] centers = self . get_kmeans_centroids ( ) [EOL] z_placeholder = tf . placeholder ( dtype = tf . float32 , shape = [ self . n_clusters , self . latent_size ] , name = [string] ) [EOL] return self . sess . run ( self . comp_net . reverse_tmpl ( z_placeholder ) , feed_dict = { z_placeholder : centers } ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.compression_net.CompressionNet$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0
from typing import List , Any [EOL] import typing [EOL] import glob [EOL] from pathlib import Path [EOL] [EOL] import numpy as np [EOL] import pandas as pd [EOL] import tensorflow as tf [EOL] import scipy [EOL] from scipy . io import loadmat [EOL] from sklearn . datasets import make_blobs [EOL] [EOL] from config import DATA_PATH [EOL] [EOL] [EOL] def load_gaussian_blobs ( ) : [EOL] data , _ = make_blobs ( n_samples = [number] , n_features = [number] , centers = [number] , random_state = [number] ) [EOL] df = pd . DataFrame ( data = data ) [EOL] df [ [string] ] = [number] [EOL] df . iloc [ [number] , : ] = [ - [number] , - [number] , [number] ] [EOL] df . iloc [ [number] , : ] = [ [number] , [number] , [number] ] [EOL] y = df [ [string] ] [EOL] df . drop ( [string] , axis = [number] , inplace = True ) [EOL] df -= df . mean ( ) [EOL] df /= df . std ( ) [EOL] df [ [string] ] = y [EOL] return df [EOL] [EOL] [EOL] def load_mnist ( ) : [EOL] mnist = tf . keras . datasets . mnist [EOL] ( x_train , y_train ) , ( x_test , y_test ) = mnist . load_data ( ) [EOL] x_train = x_train / [number] [EOL] x_test = x_test / [number] [EOL] df = pd . DataFrame ( data = x_train . reshape ( - [number] , [number] ) ) [EOL] df [ [string] ] = y_train [EOL] df_test = pd . DataFrame ( data = x_test . reshape ( - [number] , [number] ) ) [EOL] df_test [ [string] ] = y_test [EOL] return df , df_test [EOL] [EOL] [EOL] def load_fashion_mnist ( ) : [EOL] mnist = tf . keras . datasets . fashion_mnist [EOL] ( x_train , y_train ) , ( x_test , y_test ) = mnist . load_data ( ) [EOL] x_train = x_train / [number] [EOL] x_test = x_test / [number] [EOL] df = pd . DataFrame ( data = x_train . reshape ( - [number] , [number] ) ) [EOL] df [ [string] ] = y_train [EOL] df_test = pd . DataFrame ( data = x_test . reshape ( - [number] , [number] ) ) [EOL] df_test [ [string] ] = y_test [EOL] return df , df_test [EOL] [EOL] [EOL] def load_gas_data ( outlier_class = [number] ) : [EOL] df = pd . DataFrame ( ) [EOL] for file in glob . glob ( str ( Path ( DATA_PATH / [string] ) ) + [string] ) : [EOL] df = df . append ( pd . read_csv ( file , sep = [string] , header = None ) ) [EOL] df . columns = [ [string] ] + list ( df . columns ) [ [number] : ] [EOL] for c in df . columns [ [number] : ] : [EOL] df [ c ] = df [ c ] . map ( lambda x : x . split ( [string] ) [ [number] ] ) [EOL] y = df [ [string] ] [EOL] del df [ [string] ] [EOL] df = df . astype ( [string] ) [EOL] df = ( df - df . mean ( ) ) / ( df . max ( ) - df . min ( ) ) [EOL] df [ [string] ] = y == outlier_class [EOL] return df [EOL] [EOL] [EOL] def load_forest_cover_data ( ) : [EOL] columns = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] columns . extend ( [ [string] + str ( i ) for i in range ( [number] ) ] ) [EOL] columns . extend ( [ [string] + str ( i ) for i in range ( [number] ) ] ) [EOL] columns . append ( [string] ) [EOL] df = pd . read_csv ( str ( Path ( DATA_PATH / [string] / [string] ) ) , sep = [string] , header = None , names = columns , dtype = float ) [EOL] df = df [ df [ [string] ] . isin ( [ [number] , [number] ] ) ] [EOL] y = df [ [string] ] [EOL] del df [ [string] ] [EOL] df = df - df . min ( ) [EOL] df = df / df . max ( ) [EOL] df . fillna ( [number] , inplace = True ) [EOL] df [ [string] ] = y [EOL] [EOL] df_test = df [ y == [number] ] [EOL] df = df [ y == [number] ] [EOL] df = df . append ( df_test , ignore_index = True ) [EOL] [EOL] n = len ( df ) [EOL] return df . loc [ : int ( [number] * n ) ] , df . loc [ int ( [number] * n ) : ] [EOL] [EOL] [EOL] def load_sat_image_data ( ) : [EOL] file = glob . glob ( str ( DATA_PATH / [string] / [string] ) ) [ [number] ] [EOL] mat = loadmat ( file ) [EOL] df = pd . DataFrame ( mat [ [string] ] ) [EOL] y = mat [ [string] ] [EOL] df = df - df . min ( ) [EOL] df = df / df . max ( ) [EOL] df [ [string] ] = y [EOL] n = len ( df ) [EOL] df_test = df . loc [ int ( [number] * n ) : ] [EOL] df = df . loc [ : int ( [number] * n ) ] [EOL] return df , df_test [EOL] [EOL] [EOL] def load_kdd_cup ( seed = [number] ) : [EOL] [docstring] [EOL] np . random . seed ( seed ) [EOL] data = np . load ( str ( Path ( DATA_PATH / [string] ) ) ) [EOL] [EOL] labels = data [ [string] ] [ : , - [number] ] [EOL] features = data [ [string] ] [ : , : - [number] ] [EOL] [EOL] normal_data = features [ labels == [number] ] [EOL] normal_labels = labels [ labels == [number] ] [EOL] [EOL] attack_data = features [ labels == [number] ] [EOL] attack_labels = labels [ labels == [number] ] [EOL] [EOL] n_attack = attack_data . shape [ [number] ] [EOL] [EOL] rand_idx = np . arange ( n_attack ) [EOL] np . random . shuffle ( rand_idx ) [EOL] n_train = n_attack // [number] [EOL] [EOL] train = attack_data [ rand_idx [ : n_train ] ] [EOL] train_labels = attack_labels [ rand_idx [ : n_train ] ] [EOL] [EOL] test = attack_data [ rand_idx [ n_train : ] ] [EOL] test_labels = attack_labels [ rand_idx [ n_train : ] ] [EOL] [EOL] test = np . concatenate ( ( test , normal_data ) , axis = [number] ) [EOL] test_labels = np . concatenate ( ( test_labels , normal_labels ) , axis = [number] ) [EOL] [EOL] return ( train , train_labels ) , ( test , test_labels ) [EOL] [EOL] [EOL] def save_stl10_images ( ) : [EOL] data = loadmat ( str ( Path ( DATA_PATH / [string] / [string] ) ) ) [EOL] X = data [ [string] ] [EOL] X = X . reshape ( - [number] , [number] , [number] , [number] ) [EOL] X = np . transpose ( X , ( [number] , [number] , [number] , [number] ) ) [EOL] for index in range ( len ( X ) ) : [EOL] scipy . misc . imsave ( str ( Path ( DATA_PATH / [string] / [string] . format ( index ) ) ) , X [ index , : , : , : ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] import tensorflow as tf [EOL] import tensorflow_probability as tfd [EOL] from tensorflow . layers import dense [EOL] from tensorflow . python . ops . nn_ops import softmax_cross_entropy_with_logits_v2 as softmax_xent [EOL] [EOL] from src . plotting import plot_latent_code [EOL] from . algorithm import Algorithm [EOL] [EOL] tfd = tfd . distributions [EOL] [EOL] [EOL] class GMVAE ( Algorithm ) : [EOL] [EOL] def __init__ ( self , hidden_layers , n_clusters , img_dims , encoder , decode_activation = None , use_deconvs = False ) : [EOL] super ( ) . __init__ ( hidden_layers , n_clusters ) [EOL] self . img_dims = img_dims [EOL] self . decode_activation = decode_activation [EOL] self . encoder = encoder [EOL] self . use_deconvs = use_deconvs [EOL] self . xb , self . y_ , self . qy_logit , self . qy , self . zm , self . zv , self . zm_prior , self . zv_prior , self . px_logit , self . z_representative , self . nent , self . zs , self . decode , self . n_features , self . losses = [ None ] * [number] [EOL] [EOL] def infer_y ( self , x , k = [number] ) : [EOL] [docstring] [EOL] reuse = len ( tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = [string] ) ) > [number] [EOL] with tf . variable_scope ( [string] ) : [EOL] for i , layer in enumerate ( self . hidden_layers [ : - [number] ] ) : [EOL] x = dense ( x , layer , activation = tf . nn . relu , name = [string] + str ( i + [number] ) , reuse = reuse ) [EOL] qy_logit = dense ( x , k , name = [string] , reuse = reuse ) [EOL] qy = tf . nn . softmax ( qy_logit , name = [string] ) [EOL] return qy_logit , qy [EOL] [EOL] def encode ( self , x , y , encoder = None , latent_size = [number] ) : [EOL] [docstring] [EOL] reuse = len ( tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = [string] ) ) > [number] [EOL] [comment] [EOL] with tf . variable_scope ( [string] ) : [EOL] if encoder is None : [EOL] encoder = tf . concat ( ( x , y ) , [number] , name = [string] ) [EOL] for i , layer in enumerate ( self . hidden_layers [ : - [number] ] ) : [EOL] encoder = dense ( encoder , layer , activation = tf . nn . relu , name = [string] + str ( i + [number] ) , reuse = reuse ) [EOL] else : [EOL] if len ( encoder . shape ) != [number] : [EOL] encoder = tf . reshape ( encoder , [ - [number] , np . product ( encoder . shape [ [number] : ] ) ] ) [EOL] encoder = dense ( encoder , [number] , name = [string] , activation = tf . nn . elu , reuse = reuse ) [EOL] encoder = dense ( encoder , [number] , name = [string] , activation = tf . nn . relu , reuse = reuse ) [EOL] zm = dense ( encoder , latent_size , name = [string] , reuse = reuse ) [EOL] zv = dense ( encoder , latent_size , name = [string] , activation = tf . nn . softplus , reuse = reuse ) [EOL] z = gaussian_sample ( zm , zv , scope = [string] , vary_z = self . vary_z ) [EOL] return z , zm , zv [EOL] [EOL] def decoder ( self , z , y , latent_size = [number] ) : [EOL] [docstring] [EOL] [EOL] reuse = len ( tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = [string] ) ) > [number] [EOL] [comment] [EOL] with tf . variable_scope ( [string] ) : [EOL] zm = dense ( y , latent_size , name = [string] , reuse = reuse ) [EOL] zv = dense ( y , latent_size , name = [string] , activation = tf . nn . softplus , reuse = reuse ) [EOL] [comment] [EOL] with tf . variable_scope ( [string] ) : [EOL] if self . use_deconvs : [EOL] h = tf . layers . conv2d_transpose ( tf . reshape ( z , [ - [number] , [number] , [number] , latent_size ] ) , [number] , ( [number] , [number] ) ) [EOL] h = tf . layers . conv2d_transpose ( h , [number] , ( [number] , [number] ) , reuse = reuse ) [EOL] h = tf . layers . conv2d_transpose ( h , [number] , ( [number] , [number] ) , reuse = reuse ) [EOL] h = tf . layers . conv2d_transpose ( h , [number] , ( [number] , [number] ) , reuse = reuse ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] h = tf . layers . Flatten ( ) ( h ) [EOL] else : [EOL] h = dense ( z , self . hidden_layers [ - [number] ] , activation = tf . nn . relu , name = [string] , reuse = reuse ) [EOL] for i , layer in enumerate ( self . hidden_layers [ : - [number] ] [ : : - [number] ] ) : [EOL] h = dense ( h , layer , activation = tf . nn . relu , name = [string] + str ( i + [number] ) , reuse = reuse ) [EOL] px_logit = dense ( h , self . n_features , activation = self . decode_activation , name = [string] , reuse = reuse ) [EOL] return zm , zv , px_logit [EOL] [EOL] def build_graph ( self , input ) : [EOL] self . input = input [EOL] self . vary_z = tf . placeholder_with_default ( [ [number] ] , shape = [number] , name = [string] ) [EOL] self . n_features = np . prod ( self . input . shape [ [number] : ] ) [EOL] [EOL] self . xb = self . input [EOL] assert len ( self . xb . shape ) == [number] , self . xb [EOL] [EOL] with tf . name_scope ( [string] ) : [EOL] self . y_ = tf . fill ( tf . stack ( [ tf . shape ( self . input ) [ [number] ] , self . n_clusters ] ) , [number] ) [EOL] [EOL] self . qy_logit , self . qy = self . infer_y ( self . xb , self . n_clusters ) [comment] [EOL] [EOL] for label_variable in [ self . y_ , self . qy , self . qy_logit ] : [EOL] assert len ( label_variable . shape ) == [number] and label_variable . shape [ - [number] ] == self . n_clusters [EOL] [EOL] self . zs , self . zm , self . zv , self . zm_prior , self . zv_prior , self . px_logit , self . z_representative = [ [ None ] * self . n_clusters for _ in range ( [number] ) ] [EOL] self . decode = tf . make_template ( [string] , self . decoder ) [EOL] for i in range ( self . n_clusters ) : [EOL] with tf . name_scope ( [string] . format ( i ) ) : [EOL] y = tf . add ( self . y_ , tf . constant ( np . eye ( self . n_clusters ) [ i ] , dtype = [string] , name = [string] . format ( i ) ) ) [EOL] self . zs [ i ] , self . zm [ i ] , self . zv [ i ] = self . encode ( self . xb , y , encoder = self . encoder , latent_size = self . latent_size ) [EOL] self . zm_prior [ i ] , self . zv_prior [ i ] , self . px_logit [ i ] = self . decode ( self . zs [ i ] , y , self . latent_size ) [EOL] _ , _ , self . z_representative [ i ] = self . decode ( self . zm_prior [ i ] , y , self . latent_size ) [EOL] [EOL] for latent_encoding_variable in [ self . zm , self . zs , self . zv , self . zm_prior , self . zv_prior ] : [EOL] for dimension_variable in latent_encoding_variable : [EOL] assert len ( dimension_variable . shape ) == [number] and dimension_variable . shape [ - [number] ] == self . latent_size [EOL] for decoded_variables in [ self . px_logit , self . z_representative ] : [EOL] for dimension_variable in decoded_variables : [EOL] assert len ( dimension_variable . shape ) == [number] and dimension_variable . shape [ - [number] ] == self . n_features [EOL] [EOL] with tf . name_scope ( [string] ) : [EOL] with tf . name_scope ( [string] ) : [EOL] self . nent = softmax_cross_entropy_with_two_logits ( self . qy_logit , self . qy ) [EOL] self . losses = [ None ] * self . n_clusters [EOL] for i in range ( self . n_clusters ) : [EOL] with tf . name_scope ( [string] . format ( i ) ) : [EOL] self . losses [ i ] = labeled_loss ( self . xb , self . px_logit [ i ] , self . zs [ i ] , self . zm [ i ] , self . zv [ i ] , self . zm_prior [ i ] , self . zv_prior [ i ] ) [EOL] with tf . name_scope ( [string] ) : [EOL] self . loss = tf . add_n ( [ self . nent ] + [ self . qy [ : , i ] * self . losses [ i ] for i in range ( self . n_clusters ) ] ) [EOL] [EOL] self . x_dash = self . px_logit [ [number] ] [EOL] self . z = self . zs [ [number] ] [EOL] self . minimizer = tf . train . AdamOptimizer ( [number] ) . minimize ( self . loss ) [EOL] return self . z , self . x_dash , self . loss , self . minimizer [EOL] [EOL] def get_reconstruction ( self , x ) : [EOL] x_dash_gmvae , x_dash_y = self . sess . run ( [ self . px_logit , self . qy ] , feed_dict = { [string] : x } ) [EOL] gaussian_assignments = x_dash_y . argmax ( [number] ) [EOL] [comment] [EOL] [comment] [EOL] x_dash = np . zeros_like ( x ) [EOL] for sample_index , sample_assignment in enumerate ( gaussian_assignments ) : [EOL] x_dash [ sample_index ] = x_dash_gmvae [ sample_assignment ] [ sample_index ] [EOL] return x_dash [EOL] [EOL] def get_custom_anomaly_scores ( self , x ) : [EOL] return self . sess . run ( self . loss , feed_dict = { [string] : x } ) [EOL] [EOL] def get_custom_assignments ( self , x ) : [EOL] return self . sess . run ( self . qy_logit , feed_dict = { [string] : x } ) . argmax ( [number] ) [EOL] [EOL] def get_decoded_centroids ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return np . asarray ( [ self . sess . run ( self . z_representative [ i ] , feed_dict = { [string] : np . zeros ( ( [number] , self . n_features ) ) , [string] : [ [number] ] } ) for i in range ( self . n_clusters ) ] ) [EOL] [EOL] def get_closest_decoded_centroids ( self , x ) : [EOL] decoded_centroids = self . get_decoded_centroids ( ) [EOL] assignments = self . sess . run ( self . qy , feed_dict = { [string] : x . reshape ( - [number] , self . n_features ) } ) . argmax ( [number] ) [EOL] closest_centroids = np . zeros ( ( x . shape [ [number] ] , self . n_features ) ) [EOL] for i in range ( x . shape [ [number] ] ) : [EOL] closest_centroids [ i , : ] = decoded_centroids [ assignments [ i ] ] [EOL] return closest_centroids [EOL] [EOL] def plot_custom_test_set ( self , x , y , outlier = None ) : [EOL] fig , ax = plt . subplots ( nrows = [number] , ncols = self . n_clusters , figsize = ( [number] * self . n_clusters , [number] * self . n_clusters ) ) [EOL] z_results , qy_result = self . sess . run ( [ self . zs , self . qy ] , feed_dict = { [string] : x . reshape ( - [number] , self . n_features ) } ) [EOL] representatives = self . get_decoded_centroids ( ) [EOL] if self . img_dims is not None : [EOL] plot_latent_code ( fig , ax , self . n_clusters , [number] , z_results , qy_result , representatives , y , self . img_dims , outlier = outlier ) [EOL] [EOL] [EOL] def softmax_cross_entropy_with_two_logits ( logits = None , labels = None ) : [EOL] return softmax_xent ( labels = tf . nn . softmax ( labels ) , logits = logits ) [EOL] [EOL] [EOL] def gaussian_sample ( mean , var , vary_z = None , scope = None ) : [EOL] [docstring] [EOL] [EOL] with tf . variable_scope ( scope , [string] ) : [EOL] sample = tf . random_normal ( tf . shape ( mean ) , mean , vary_z * tf . sqrt ( var ) ) [EOL] sample . set_shape ( mean . get_shape ( ) ) [EOL] return sample [EOL] [EOL] [EOL] def log_bernoulli ( x , logits , eps = [number] , axis = - [number] ) : [EOL] return log_bernoulli_with_logits ( x , logits , eps , axis ) [EOL] [EOL] [EOL] def log_bernoulli_with_logits ( x , logits , eps = [number] , axis = - [number] ) : [EOL] if eps > [number] : [EOL] max_val = np . log ( [number] - eps ) - np . log ( eps ) [EOL] logits = tf . clip_by_value ( logits , - max_val , max_val , name = [string] ) [EOL] return - tf . reduce_sum ( tf . nn . sigmoid_cross_entropy_with_logits ( logits = logits , labels = x ) , axis ) [EOL] [EOL] [EOL] def log_normal ( x , mu , var , eps = [number] , axis = - [number] ) : [EOL] if eps > [number] : [EOL] var = tf . add ( var , eps , name = [string] ) [EOL] return - [number] * tf . reduce_sum ( tf . log ( [number] * np . pi ) + tf . log ( var ) + tf . square ( x - mu ) / var , axis ) [EOL] [EOL] [EOL] def labeled_loss ( x , px_logit , z , zm , zv , zm_prior , zv_prior ) : [EOL] xy_loss = - log_bernoulli_with_logits ( x , px_logit ) [EOL] xy_loss += log_normal ( z , zm , zv ) - log_normal ( z , zm_prior , zv_prior ) [EOL] return xy_loss - np . log ( [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import DefaultDict , Tuple , Union , Any , Dict , Literal , List [EOL] import typing [EOL] import typing_extensions [EOL] import sys [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] import pandas as pd [EOL] from deepexplain . tensorflow import DeepExplain [EOL] import tensorflow as tf [EOL] import matplotlib [EOL] from matplotlib import offsetbox [EOL] from matplotlib import ticker [EOL] from matplotlib . cm import get_cmap [EOL] from matplotlib . lines import Line2D [EOL] from scipy . spatial . distance import cosine [EOL] from sklearn . metrics import auc [EOL] from scipy . stats import multivariate_normal [EOL] from sklearn . manifold import TSNE [EOL] [EOL] [EOL] def plot_energy_distribution ( energy ) : [EOL] plt . figure ( figsize = [ [number] , [number] ] ) [EOL] plt . hist ( energy , bins = [number] , log = True ) [EOL] plt . xlabel ( [string] ) [EOL] plt . ylabel ( [string] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_energy_per_sample ( energy ) : [EOL] plt . figure ( figsize = [ [number] , [number] ] ) [EOL] plt . plot ( energy , [string] ) [EOL] plt . xlabel ( [string] ) [EOL] plt . ylabel ( [string] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_2d_outliers_by_energy ( df , energy ) : [EOL] plt . plot ( df . iloc [ : , [number] ] , df . iloc [ : , [number] ] , [string] ) [EOL] ano_index = np . arange ( len ( energy ) ) [ energy > np . percentile ( energy , [number] ) ] [EOL] plt . plot ( df . iloc [ ano_index , [number] ] , df . iloc [ ano_index , [number] ] , [string] , c = [string] , markersize = [number] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_2d_scatter_by_energy ( df , energy ) : [EOL] plt . scatter ( df . iloc [ : , [number] ] , df . iloc [ : , [number] ] , s = energy + [number] , cmap = plt . cm . YlOrRd , c = energy + [number] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_representatives ( mu , z , x ) : [EOL] n_gaussians = mu . shape [ [number] ] [EOL] n_samples = z . shape [ [number] ] [EOL] idx_mins = np . zeros ( n_gaussians , dtype = int ) [EOL] for gauss_index in range ( n_gaussians ) : [EOL] min_distance = [number] [EOL] idx_min = - [number] [EOL] for sample_idx in range ( n_samples ) : [EOL] cosine_distance = cosine ( z [ sample_idx , : ] , mu [ gauss_index , : ] ) . sum ( ) [EOL] if cosine_distance < min_distance : [EOL] min_distance = cosine_distance [EOL] idx_min = sample_idx [EOL] idx_mins [ gauss_index ] = idx_min [EOL] fig , axes = plt . subplots ( nrows = [number] , ncols = n_gaussians , figsize = ( [number] * n_gaussians , [number] ) ) [EOL] for i in range ( n_gaussians ) : [EOL] plot_sub_explanation ( x [ idx_mins [ i ] , : ] . reshape ( [number] , [number] ) , axis = axes [ i ] ) . set_title ( [string] + str ( i ) ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_representatives_with_gamma ( x , gamma , title = None ) : [EOL] idx_mins = np . argmax ( gamma , axis = [number] ) [EOL] n_gaussians = gamma . shape [ [number] ] [EOL] assert idx_mins . shape [ [number] ] == n_gaussians [EOL] fig , axes = plt . subplots ( nrows = [number] , ncols = n_gaussians , figsize = ( [number] * n_gaussians , [number] ) ) [EOL] if title is not None : [EOL] fig . suptitle ( title ) [EOL] for i in range ( n_gaussians ) : [EOL] plot_sub_explanation ( x [ idx_mins [ i ] , : ] . reshape ( [number] , [number] ) , axis = axes [ i ] ) . set_title ( [string] + str ( i ) ) [EOL] return plt [EOL] [EOL] [EOL] def plot_parallel_coordinates ( df ) : [EOL] [comment] [EOL] [comment] [EOL] [EOL] df = df . copy ( ) [EOL] df [ [string] ] = pd . Categorical ( df . y ) . as_ordered ( ) [EOL] cols = list ( df . columns ) [EOL] cols . remove ( [string] ) [EOL] x = [ i for i , _ in enumerate ( cols ) ] [EOL] n_classes = len ( df [ [string] ] . unique ( ) ) [EOL] cmap = plt . get_cmap ( [string] , n_classes ) [EOL] [EOL] [comment] [EOL] colours = { df [ [string] ] . cat . categories [ i ] : cmap ( i / n_classes ) for i , _ in enumerate ( df [ [string] ] . cat . categories ) } [EOL] [EOL] [comment] [EOL] fig , axes = plt . subplots ( [number] , len ( x ) - [number] , sharey = False , figsize = ( [number] , [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] min_max_range = { } [EOL] for col in cols : [EOL] min_max_range [ col ] = [ df [ col ] . min ( ) , df [ col ] . max ( ) , np . ptp ( df [ col ] ) ] [EOL] df [ col ] = np . true_divide ( df [ col ] - df [ col ] . min ( ) , np . ptp ( df [ col ] ) ) [EOL] [EOL] [comment] [EOL] for i , ax in enumerate ( axes ) : [EOL] for idx in df . index : [EOL] y_category = df . loc [ idx , [string] ] [EOL] ax . plot ( x , df . loc [ idx , cols ] , colours [ y_category ] , alpha = [number] ) [EOL] ax . set_xlim ( [ x [ i ] , x [ i + [number] ] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def set_ticks_for_axis ( dim , ax , ticks ) : [EOL] min_val , max_val , val_range = min_max_range [ cols [ dim ] ] [EOL] step = val_range / float ( ticks - [number] ) [EOL] tick_labels = [ round ( min_val + step * i , [number] ) for i in range ( ticks ) ] [EOL] norm_min = df [ cols [ dim ] ] . min ( ) [EOL] norm_range = np . ptp ( df [ cols [ dim ] ] ) [EOL] norm_step = norm_range / float ( ticks - [number] ) [EOL] ticks = [ round ( norm_min + norm_step * i , [number] ) for i in range ( ticks ) ] [EOL] ax . yaxis . set_ticks ( ticks ) [EOL] ax . set_yticklabels ( tick_labels ) [EOL] [EOL] for dim , ax in enumerate ( axes ) : [EOL] ax . xaxis . set_major_locator ( ticker . FixedLocator ( [ dim ] ) ) [EOL] set_ticks_for_axis ( dim , ax , ticks = [number] ) [EOL] ax . set_xticklabels ( [ cols [ dim ] ] ) [EOL] [EOL] [comment] [EOL] ax = plt . twinx ( axes [ - [number] ] ) [EOL] dim = len ( axes ) [EOL] ax . xaxis . set_major_locator ( ticker . FixedLocator ( [ x [ - [number] ] , x [ - [number] ] ] ) ) [EOL] set_ticks_for_axis ( dim , ax , ticks = [number] ) [EOL] ax . set_xticklabels ( [ cols [ - [number] ] , cols [ - [number] ] ] ) [EOL] [EOL] [comment] [EOL] plt . subplots_adjust ( wspace = [number] ) [EOL] [EOL] [comment] [EOL] plt . legend ( [ plt . Line2D ( ( [number] , [number] ) , ( [number] , [number] ) , color = colours [ cat ] ) for cat in df [ [string] ] . cat . categories ] , df [ [string] ] . cat . categories , bbox_to_anchor = ( [number] , [number] ) , loc = [number] , borderaxespad = [number] ) [EOL] [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_error_by_energy ( error , energy , y = None , title = None ) : [EOL] [docstring] [EOL] plt . plot ( energy , error , [string] ) [EOL] plt . xlabel ( [string] ) [EOL] plt . ylabel ( [string] ) [EOL] if title is not None : [EOL] plt . title ( title ) [EOL] if y is not None : [EOL] for en , er in zip ( energy [ y == [number] ] , error [ y == [number] ] ) : [EOL] plt . plot ( en , er , [string] , color = [string] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_embedding ( z , digits , title = None , xlabel = [string] ) : [EOL] [docstring] [EOL] n_samples , n_features = z . shape [EOL] reconstruction_error_index = - [number] [EOL] digits = digits . reshape ( - [number] , [number] , [number] ) [EOL] error = z [ : , reconstruction_error_index ] [EOL] for latent_index in range ( n_features - [number] ) : [EOL] latent = z [ : , latent_index ] [EOL] plt . figure ( figsize = ( [number] , [number] ) ) [EOL] ax = plt . subplot ( [number] ) [EOL] if hasattr ( offsetbox , [string] ) : [EOL] shown_images = np . array ( [ [ [number] , [number] ] ] ) [comment] [EOL] for i in range ( n_samples ) : [EOL] X_instance = z [ i , ( latent_index , reconstruction_error_index ) ] [EOL] dist = np . sum ( ( X_instance - shown_images ) ** [number] , [number] ) [EOL] if np . min ( dist ) < [number] : [EOL] [comment] [EOL] continue [EOL] shown_images = np . r_ [ shown_images , [ X_instance ] ] [EOL] imagebox = offsetbox . AnnotationBbox ( offsetbox . OffsetImage ( digits [ i ] , cmap = plt . cm . gray_r ) , X_instance ) [EOL] ax . add_artist ( imagebox ) [EOL] plt . xlim ( ( latent . min ( ) , [number] * latent . max ( ) ) ) [EOL] plt . ylim ( ( [number] * error . min ( ) , [number] * error . max ( ) ) ) [EOL] plt . xlabel ( xlabel + [string] + str ( latent_index + [number] ) ) [EOL] plt . ylabel ( [string] ) [EOL] if title is not None : [EOL] plt . title ( title ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_embedding_with_gaussians ( z , digits , gamma , title = None , xlabel = [string] ) : [EOL] [docstring] [EOL] n_samples , n_features = z . shape [EOL] n_gaussians = gamma . shape [ [number] ] [EOL] reconstruction_error_index = - [number] [EOL] digits = digits . reshape ( - [number] , [number] , [number] ) [EOL] error = z [ : , reconstruction_error_index ] [EOL] cmap = get_cmap ( [string] ) [EOL] from collections import defaultdict [EOL] for latent_index in range ( n_features ) : [EOL] used_gaussian_indexes = defaultdict ( lambda : [number] ) [EOL] latent = z [ : , latent_index ] [EOL] plt . figure ( figsize = ( [number] , [number] ) ) [EOL] ax = plt . subplot ( [number] ) [EOL] if hasattr ( offsetbox , [string] ) : [EOL] shown_images = np . array ( [ [ [number] , [number] ] ] ) [comment] [EOL] for i in range ( n_samples ) : [EOL] X_instance = z [ i , ( latent_index , reconstruction_error_index ) ] [EOL] gaussian_index = np . argmin ( gamma [ i , : ] ) [EOL] used_gaussian_indexes [ gaussian_index ] += [number] [EOL] dist = np . sum ( ( X_instance - shown_images ) ** [number] , [number] ) [EOL] if np . min ( dist ) < [number] : [EOL] [comment] [EOL] continue [EOL] shown_images = np . r_ [ shown_images , [ X_instance ] ] [EOL] color = cmap ( gaussian_index / n_gaussians ) [EOL] offset_image = offsetbox . OffsetImage ( digits [ i ] , cmap = plt . cm . gray_r ) [EOL] imagebox = offsetbox . AnnotationBbox ( offset_image , X_instance , bboxprops = dict ( edgecolor = color , linewidth = [number] ) ) [EOL] ax . add_artist ( imagebox ) [EOL] [EOL] [comment] [EOL] elements = [ ] [EOL] for index in range ( n_gaussians ) : [EOL] elements . append ( Line2D ( [ [number] ] , [ [number] ] , color = cmap ( index / n_gaussians ) , lw = [number] , label = [string] . format ( index , used_gaussian_indexes [ index ] / n_samples ) ) ) [EOL] plt . legend ( handles = elements ) [EOL] [EOL] plt . xlim ( ( latent . min ( ) , [number] * latent . max ( ) ) ) [EOL] plt . ylim ( ( [number] * error . min ( ) , [number] * error . max ( ) ) ) [EOL] plt . xlabel ( xlabel + [string] + str ( latent_index + [number] ) ) [EOL] plt . ylabel ( [string] ) [EOL] if title is not None : [EOL] plt . title ( title ) [EOL] return plt [EOL] [EOL] [EOL] def plot_latent_distributions ( mu , sigma , z , y = None ) : [EOL] [docstring] [EOL] [EOL] class nf ( float ) : [EOL] def __repr__ ( self ) : [EOL] str = [string] % ( self . __float__ ( ) , ) [EOL] if str [ - [number] ] == [string] : [EOL] return [string] % self . __float__ ( ) [EOL] else : [EOL] return [string] % self . __float__ ( ) [EOL] [EOL] num_latent_dimensions = mu . shape [ [number] ] - [number] [comment] [EOL] num_gaussians = mu . shape [ [number] ] [EOL] reconstruction_error_index = - [number] [EOL] [EOL] for latent_index in range ( num_latent_dimensions ) : [EOL] for gaussian_index in range ( num_gaussians ) : [EOL] latent = z [ : , ( latent_index , reconstruction_error_index ) ] [EOL] mu_small = [ mu [ gaussian_index , latent_index ] , mu [ gaussian_index , reconstruction_error_index ] ] [EOL] sigma_small = np . zeros ( ( [number] , [number] ) ) [comment] [EOL] sigma_small [ [number] , : ] = np . array ( [ sigma [ gaussian_index , latent_index , latent_index ] , sigma [ gaussian_index , latent_index , reconstruction_error_index ] ] ) [EOL] sigma_small [ [number] , : ] = np . array ( [ sigma [ gaussian_index , reconstruction_error_index , latent_index ] , sigma [ gaussian_index , reconstruction_error_index , reconstruction_error_index ] ] ) [EOL] [EOL] x_line = np . linspace ( latent [ : , [number] ] . min ( ) , latent [ : , [number] ] . max ( ) , [number] ) [EOL] y_line = np . linspace ( latent [ : , [number] ] . min ( ) , latent [ : , [number] ] . max ( ) , [number] ) [EOL] X , Y = np . meshgrid ( x_line , y_line ) [EOL] pos = np . dstack ( ( X , Y ) ) [EOL] [EOL] rv = multivariate_normal ( mu_small , sigma_small ) [EOL] Z = rv . pdf ( pos ) [EOL] [EOL] [comment] [EOL] fig , ax = plt . subplots ( ) [EOL] CS = ax . contour ( X , Y , Z ) [EOL] [EOL] [comment] [EOL] CS . levels = [ nf ( val ) for val in CS . levels ] [EOL] [EOL] [comment] [EOL] if plt . rcParams [ [string] ] : [EOL] fmt = [string] [EOL] else : [EOL] fmt = [string] [EOL] [EOL] ax . clabel ( CS , CS . levels , inline = True , fmt = fmt , fontsize = [number] ) [EOL] plt . plot ( latent [ : , [number] ] , latent [ : , [number] ] , [string] , alpha = [number] ) [EOL] plt . title ( [string] + str ( gaussian_index + [number] ) + [string] + str ( latent_index + [number] ) ) [EOL] plt . xlabel ( [string] + str ( latent_index + [number] ) ) [EOL] plt . ylabel ( [string] ) [EOL] if y is not None : [EOL] for anomaly in latent [ y == [number] ] : [EOL] plt . plot ( anomaly [ [number] ] , anomaly [ [number] ] , [string] , color = [string] ) [EOL] plt . show ( ) [EOL] [EOL] [EOL] def plot_sub_explanation ( data , cmap = [string] , axis = plt , percentile = [number] ) : [EOL] [docstring] [EOL] dx , dy = [number] , [number] [EOL] xx = np . arange ( [number] , data . shape [ [number] ] , dx ) [EOL] yy = np . arange ( [number] , data . shape [ [number] ] , dy ) [EOL] xmin , xmax , ymin , ymax = np . amin ( xx ) , np . amax ( xx ) , np . amin ( yy ) , np . amax ( yy ) [EOL] extent = xmin , xmax , ymin , ymax [EOL] cmap_xi = plt . get_cmap ( [string] ) [EOL] cmap_xi . set_bad ( alpha = [number] ) [EOL] [EOL] abs_max = np . percentile ( np . abs ( data ) , percentile ) [EOL] abs_min = abs_max [EOL] [EOL] if len ( data . shape ) == [number] : [EOL] data = np . mean ( data , [number] ) [EOL] data [ data == [number] ] = None [EOL] axis . imshow ( data , extent = extent , interpolation = [string] , cmap = cmap )[comment] [EOL] axis . axis ( [string] ) [EOL] return axis [EOL] [EOL] [EOL] def plot_explanations ( data , input_ , output , model ) : [EOL] assert tf . gradients ( output , [ input_ ] ) [ [number] ] is not None , [string] . format ( output , input_ ) [EOL] with DeepExplain ( session = model . sess , graph = model . graph ) as de : [EOL] attributions = { [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data , window_shape = ( [number] , ) ) } [EOL] [EOL] n_cols = len ( attributions ) + [number] [EOL] xi = data . reshape ( [number] , - [number] ) [EOL] fig , axes = plt . subplots ( nrows = [number] , ncols = n_cols , figsize = ( [number] * n_cols , [number] ) ) [EOL] plot_sub_explanation ( xi . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) . set_title ( [string] ) [EOL] for i , method_name in enumerate ( sorted ( attributions . keys ( ) ) ) : [EOL] plot_sub_explanation ( attributions [ method_name ] . reshape ( [number] , [number] ) , axis = axes [ [number] + i ] ) . set_title ( method_name ) [EOL] [EOL] [EOL] def plot_tsne ( z , y_test ) : [EOL] X_embedded = TSNE ( n_components = [number] ) . fit_transform ( z ) [EOL] cmap = matplotlib . cm . get_cmap ( [string] ) [EOL] fig , ax = plt . subplots ( figsize = ( [number] , [number] ) ) [EOL] for label in np . unique ( y_test ) : [EOL] ix = np . where ( y_test == label ) [EOL] ax . scatter ( X_embedded [ ix , [number] ] , X_embedded [ ix , [number] ] , c = [ cmap ( label / [number] ) ] , label = label , s = [number] ) [EOL] ax . legend ( ) [EOL] [EOL] [EOL] def plot_latent_code ( fig , ax , n_clusters , epoch , z_result , qy_result , representatives , y_true , img_dims , outlier = None ) : [EOL] [docstring] [EOL] [EOL] for k in range ( n_clusters ) : [EOL] plot_z_with_labels ( np . asarray ( z_result [ k ] ) , y_true , qy_result [ : , k ] , ax = ax [ [number] ] [ k ] , outlier = outlier ) [EOL] ax [ [number] ] [ k ] . imshow ( representatives [ k ] . reshape ( * img_dims ) ) [EOL] fig . savefig ( [string] . format ( str ( epoch ) . zfill ( [number] ) ) , dpi = [number] ) [EOL] [EOL] [EOL] def plot_z_with_labels ( z , y_test , prob , ax , outlier = None ) : [EOL] if z . shape [ [number] ] != [number] : [EOL] z = z [ : , : [number] ] [EOL] [comment] [EOL] [EOL] cmap = matplotlib . cm . get_cmap ( [string] ) [EOL] for label in np . unique ( y_test ) : [EOL] if label != outlier : [EOL] s , c , m = [number] , [ cmap ( label / [number] ) ] , [string] [EOL] else : [EOL] s , c , m = [number] , [ [string] ] , [string] [EOL] ix = np . where ( y_test == label ) [EOL] for ix in ix : [EOL] ax . scatter ( z [ ix , [number] ] , z [ ix , [number] ] , c = c , label = label , s = s , alpha = prob [ ix ] [ [number] ] , marker = m ) [EOL] ax . legend ( ) [EOL] return ax [EOL] [EOL] [EOL] def plot_interpretation ( data , model , output ) : [EOL] input_ = model . input [EOL] [EOL] with DeepExplain ( session = model . sess , graph = model . graph ) as de : [EOL] attributions = { [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data , window_shape = ( [number] , ) ) } [EOL] [EOL] n_cols = len ( attributions ) + [number] [EOL] xi = data . reshape ( [number] , - [number] ) [EOL] [EOL] x_dash = model . sess . run ( model . x_dash , feed_dict = { [string] : xi } ) [EOL] centroid = model . algorithm . get_closest_decoded_centroids ( xi ) [EOL] [EOL] fig , axes = plt . subplots ( nrows = [number] , ncols = n_cols , figsize = ( [number] * n_cols , [number] ) ) [EOL] plot_sub_explanation ( xi . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) . set_title ( [string] ) [EOL] plot_sub_explanation ( x_dash . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) . set_title ( [string] ) [EOL] plot_sub_explanation ( centroid . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) . set_title ( [string] ) [EOL] for i , method_name in enumerate ( sorted ( attributions . keys ( ) ) ) : [EOL] plot_sub_explanation ( attributions [ method_name ] . reshape ( [number] , [number] ) , axis = axes [ [number] + i ] ) . set_title ( method_name ) [EOL] [EOL] [EOL] def plot_interpretation_paper ( data , model , output , model_ae , set_title = True ) : [EOL] input_ = model . input [EOL] [EOL] with DeepExplain ( session = model . sess , graph = model . graph ) as de : [EOL] attributions = { [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data ) , [string] : de . explain ( [string] , output , input_ , data , window_shape = ( [number] , ) ) } [EOL] [EOL] n_cols = len ( attributions ) + [number] [EOL] xi = data . reshape ( [number] , - [number] ) [EOL] [EOL] x_dash = model_ae . sess . run ( model_ae . x_dash , feed_dict = { [string] : xi } ) [EOL] fig , axes = plt . subplots ( nrows = [number] , ncols = n_cols , figsize = ( [number] * n_cols , [number] ) ) [EOL] plot_sub_explanation ( xi . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) [EOL] plot_sub_explanation ( data . reshape ( [number] , [number] ) - x_dash . reshape ( [number] , [number] ) , cmap = [string] , axis = axes [ [number] ] ) [EOL] [EOL] if set_title : [EOL] axes [ [number] ] . set_title ( [string] , fontsize = [number] ) [EOL] axes [ [number] ] . set_title ( [string] , fontsize = [number] ) [EOL] [EOL] for i , method_name in enumerate ( sorted ( attributions . keys ( ) ) ) : [EOL] plot_sub_explanation ( attributions [ method_name ] . reshape ( [number] , [number] ) , axis = axes [ [number] + i ] ) [EOL] if set_title : [EOL] axes [ [number] + i ] . set_title ( method_name , fontsize = [number] ) [EOL] [EOL] [EOL] def get_optimal_pixel_value ( sample , model , pixel_index ) : [EOL] min_loss = [number] [EOL] min_value = - [number] [EOL] for pixel_value in [ [number] , [number] ] : [EOL] sample [ [number] , pixel_index ] = pixel_value / [number] [EOL] loss = model . sess . run ( model . algorithm . loss , feed_dict = { [string] : sample , [string] : [ [number] ] } ) [ [number] ] [EOL] if loss < min_loss : [EOL] min_loss = loss [EOL] min_value = pixel_value / [number] [EOL] return min_value [EOL] [EOL] [EOL] def get_flipped_image ( data , model , output , attribution_method = [string] , n_pixels = [number] , intelligent = True , use_mask = False ) : [EOL] [docstring] [EOL] input_ = model . input [EOL] if use_mask : [EOL] y_pred = model . sess . run ( model . algorithm . qy_logit , feed_dict = { [string] : data } ) . argmax ( ) [EOL] mask = np . zeros ( ( [number] , [number] ) ) [EOL] mask [ [number] , y_pred ] = [number] [EOL] output *= mask [EOL] [EOL] with DeepExplain ( session = model . sess , graph = model . graph ) as de : [EOL] if attribution_method == [string] : [EOL] res = np . expand_dims ( np . random . permutation ( data . shape [ [number] ] ) , axis = [number] ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data ) [EOL] elif attribution_method == [string] : [EOL] res = de . explain ( [string] , output , input_ , data , window_shape = ( [number] , ) ) [EOL] else : [EOL] raise Exception ( [string] + attribution_method ) [EOL] scores = [ ] [EOL] important_pixels = ( - res ) . argsort ( ) [ [number] ] [EOL] test_sample_flipping = data . copy ( ) [EOL] for i in range ( n_pixels ) : [EOL] new_value = get_optimal_pixel_value ( test_sample_flipping . copy ( ) , model , important_pixels [ i ] ) if intelligent else [number] - test_sample_flipping [ [number] ] [ important_pixels [ i ] ] [EOL] test_sample_flipping [ [number] ] [ important_pixels [ i ] ] = new_value [EOL] scores . append ( model . algorithm . get_custom_anomaly_scores ( test_sample_flipping ) ) [EOL] [EOL] return test_sample_flipping , res , scores [EOL] [EOL] [EOL] def get_flipping_auc_scores ( data , model , output , n_pixels = [number] ) : [EOL] methods = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] auc_scores = [ ] [EOL] for sample in data : [EOL] sample_scores = [ ] [EOL] for method_index , method in enumerate ( methods ) : [EOL] method_image , res , scores = get_flipped_image ( np . expand_dims ( sample , axis = [number] ) , model , output , attribution_method = method , n_pixels = n_pixels ) [EOL] auc_score = auc ( list ( range ( len ( scores ) ) ) , scores ) [EOL] sample_scores . append ( auc_score ) [EOL] auc_scores . append ( sample_scores ) [EOL] return auc_scores [EOL] [EOL] [EOL] def plot_pixel_flipping ( data , model , output , n_pixels = [number] , intelligent = True , use_mask = False , model_ae = None , plot_scores = False ) : [EOL] [docstring] [EOL] [EOL] methods = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] n_rows = [number] if plot_scores else [number] [EOL] n_cols = len ( methods ) if model_ae is None else len ( methods ) + [number] [EOL] plt . subplots_adjust ( wspace = [number] , hspace = [number] ) [EOL] fig , axes = plt . subplots ( nrows = n_rows , ncols = n_cols , figsize = ( [number] * len ( methods ) , [number] ) , gridspec_kw = { [string] : [number] , [string] : [number] } ) [EOL] y_min , y_max = sys . maxsize , - sys . maxsize [EOL] if model_ae is not None : [EOL] baseline = model_ae . sess . run ( model_ae . x_dash , { [string] : data . reshape ( [number] , - [number] ) } ) . reshape ( [number] , [number] ) [EOL] [comment] [EOL] axes [ [number] ] [ [number] ] . imshow ( data . reshape ( [number] , [number] ) - baseline , cmap = [string] ) [EOL] axes [ [number] ] [ [number] ] . axis ( [string] ) [EOL] axes [ [number] ] [ [number] ] . axis ( [string] ) [EOL] axes [ [number] ] [ [number] ] . imshow ( baseline , cmap = [string] ) [EOL] [EOL] for method_index , method in enumerate ( methods ) : [EOL] if model_ae is not None : [EOL] method_index += [number] [EOL] [EOL] method_image , res , scores = get_flipped_image ( data , model , output , attribution_method = method , n_pixels = n_pixels , intelligent = intelligent , use_mask = use_mask ) [EOL] y_min = min ( y_min , min ( scores ) ) [EOL] y_max = max ( y_max , max ( scores ) ) [EOL] plot_sub_explanation ( res . reshape ( [number] , [number] ) , axis = axes [ [number] ] [ method_index ] ) . set_title ( method , fontsize = [number] ) [EOL] axes [ [number] ] [ method_index ] . imshow ( method_image . reshape ( [number] , [number] ) , cmap = [string] ) [EOL] axes [ [number] ] [ method_index ] . axis ( [string] ) [EOL] if plot_scores : [EOL] axes [ [number] ] [ method_index ] . set_title ( [string] % auc ( list ( range ( len ( scores ) ) ) , scores ) ) [EOL] axes [ [number] ] [ method_index ] . plot ( scores ) [EOL] if method_index == [number] and plot_scores : [EOL] axes [ [number] ] [ method_index ] . set_xlabel ( [string] ) [EOL] axes [ [number] ] [ method_index ] . set_ylabel ( [string] ) [EOL] [EOL] if plot_scores : [EOL] for method_index , _ in enumerate ( methods ) : [EOL] axes [ [number] ] [ method_index ] . set_ylim ( ( y_min * [number] , y_max * [number] ) ) [EOL] [EOL] plt . subplots_adjust ( wspace = [number] , hspace = [number] ) [EOL] plt . savefig ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import tensorflow as tf [EOL] import math [EOL] import numpy as np [EOL] [EOL] [EOL] def lrelu ( x , leak = [number] , name = [string] ) : [EOL] [docstring] [EOL] with tf . variable_scope ( name ) : [EOL] f1 = [number] * ( [number] + leak ) [EOL] f2 = [number] * ( [number] - leak ) [EOL] return f1 * x + f2 * abs ( x ) [EOL] [EOL] [EOL] class CompressionNet : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , hidden_layer_sizes , activation = tf . nn . tanh , use_error_functions = True , use_cnn = False ) : [EOL] [docstring] [EOL] self . hidden_layer_sizes = hidden_layer_sizes [EOL] self . activation = activation [EOL] self . n_filters = [ [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] self . filter_sizes = [ [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] self . use_error_functions = use_error_functions [EOL] self . use_cnn = use_cnn [EOL] [EOL] def compress_cnn ( self , x ) : [EOL] [comment] [EOL] if len ( x . get_shape ( ) ) == [number] : [EOL] x_dim = np . sqrt ( x . get_shape ( ) . as_list ( ) [ [number] ] ) [EOL] if x_dim != int ( x_dim ) : [EOL] raise ValueError ( [string] ) [EOL] x_dim = int ( x_dim ) [EOL] x_tensor = tf . reshape ( x , [ - [number] , x_dim , x_dim , self . n_filters [ [number] ] ] ) [EOL] elif len ( x . get_shape ( ) ) == [number] : [EOL] x_tensor = x [EOL] else : [EOL] raise ValueError ( [string] ) [EOL] current_input = x_tensor [EOL] [EOL] self . encoder = [ ] [EOL] self . shapes = [ ] [EOL] for layer_i , n_output in enumerate ( self . n_filters [ [number] : ] ) : [EOL] n_input = current_input . get_shape ( ) . as_list ( ) [ [number] ] [EOL] self . shapes . append ( current_input . get_shape ( ) . as_list ( ) ) [EOL] W = tf . Variable ( tf . random_uniform ( [ self . filter_sizes [ layer_i ] , self . filter_sizes [ layer_i ] , n_input , n_output ] , - [number] / math . sqrt ( n_input ) , [number] / math . sqrt ( n_input ) ) ) [EOL] b = tf . Variable ( tf . zeros ( [ n_output ] ) ) [EOL] self . encoder . append ( W ) [EOL] output = lrelu ( tf . add ( tf . nn . conv2d ( current_input , W , strides = [ [number] , [number] , [number] , [number] ] , padding = [string] ) , b ) ) [EOL] current_input = output [EOL] [EOL] z = current_input [EOL] z = tf . layers . Flatten ( ) ( z ) [EOL] return z [EOL] [EOL] def compress ( self , x ) : [EOL] self . input_size = x . shape [ [number] ] [EOL] z = tf . layers . flatten ( x ) [EOL] for layer in self . hidden_layer_sizes [ : - [number] ] : [EOL] z = tf . layers . dense ( z , layer , tf . nn . relu ) [EOL] z = tf . layers . dense ( z , self . hidden_layer_sizes [ - [number] ] ) [EOL] return z [EOL] [EOL] def reverse_cnn ( self , z ) : [EOL] assert len ( z . shape ) == [number] , [string] + str ( z . shape ) [EOL] current_input = tf . reshape ( z , ( - [number] , [number] , [number] , z . shape [ - [number] ] ) ) [EOL] self . encoder . reverse ( ) [EOL] self . shapes . reverse ( ) [EOL] for layer_i , shape in enumerate ( self . shapes ) : [EOL] W = self . encoder [ layer_i ] [EOL] b = tf . Variable ( tf . zeros ( [ W . get_shape ( ) . as_list ( ) [ [number] ] ] ) ) [EOL] output = lrelu ( tf . add ( tf . nn . conv2d_transpose ( current_input , W , tf . stack ( [ tf . shape ( z ) [ [number] ] , shape [ [number] ] , shape [ [number] ] , shape [ [number] ] ] ) , strides = [ [number] , [number] , [number] , [number] ] , padding = [string] ) , b ) ) [EOL] current_input = output [EOL] [EOL] output = tf . reshape ( current_input , ( - [number] , [number] ) ) [EOL] return output [EOL] [EOL] def reverse ( self , z ) : [EOL] x_dash = z [EOL] for layer in self . hidden_layer_sizes [ : - [number] ] [ : : - [number] ] : [EOL] x_dash = tf . layers . dense ( x_dash , layer , tf . nn . relu ) [EOL] x_dash = tf . layers . dense ( x_dash , self . input_size ) [EOL] return x_dash [EOL] [EOL] def loss ( self , x , x_dash ) : [EOL] def euclid_norm ( x ) : [EOL] return tf . sqrt ( tf . reduce_sum ( tf . square ( x ) , axis = [number] ) ) [EOL] [EOL] [comment] [EOL] norm_x = euclid_norm ( x ) [EOL] norm_x_dash = euclid_norm ( x_dash ) [EOL] dist_x = euclid_norm ( x - x_dash ) [EOL] dot_x = tf . reduce_sum ( x * x_dash , axis = [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] min_val = [number] [EOL] loss_E = dist_x / ( norm_x + min_val ) [EOL] loss_C = [number] * ( [number] - dot_x / ( norm_x * norm_x_dash + min_val ) ) [EOL] return tf . concat ( [ loss_E [ : , None ] , loss_C [ : , None ] ] , axis = [number] ) [EOL] [EOL] def extract_feature ( self , x , x_dash , z_c ) : [EOL] if self . use_error_functions : [EOL] z_r = self . loss ( x , x_dash ) [EOL] z_sum = tf . concat ( [ z_c , z_r ] , axis = [number] ) [EOL] else : [EOL] z_sum = z_c [EOL] return z_sum [EOL] [EOL] def inference ( self , x ) : [EOL] [docstring] [EOL] [EOL] with tf . variable_scope ( [string] ) : [EOL] [EOL] if self . use_cnn : [EOL] z_c = self . compress_cnn ( x ) [EOL] x_dash = self . reverse_cnn ( z_c ) [EOL] else : [EOL] z_c = self . compress ( x ) [EOL] self . reverse_tmpl = tf . make_template ( [string] , self . reverse ) [EOL] x_dash = self . reverse_tmpl ( z_c ) [EOL] [EOL] [comment] [EOL] z = self . extract_feature ( x , x_dash , z_c ) [EOL] [EOL] return z , x_dash [EOL] [EOL] @ staticmethod def reconstruction_error ( x , x_dash ) : [EOL] return tf . reduce_mean ( tf . reduce_sum ( tf . square ( x - x_dash ) , axis = [number] ) , axis = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $builtins.float$ 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any , Dict [EOL] import typing [EOL] import os [EOL] import time [EOL] [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] import tensorflow as tf [EOL] import tensorflow_probability as tfd [EOL] from scipy . stats import mode [EOL] from sklearn . externals import joblib [EOL] from sklearn . metrics import adjusted_rand_score , roc_auc_score [EOL] from sklearn . preprocessing import StandardScaler [EOL] from tqdm import tqdm_notebook as tqdm [EOL] [EOL] from config import OUTPUT_PATH [EOL] from src . ae import AE [EOL] from src . algorithm import Encoding [EOL] from src . gmvae import GMVAE [EOL] from src . vae import VAE [EOL] [EOL] tfd = tfd . distributions [EOL] [EOL] [EOL] def accuracy ( y_pred , y_true ) : [EOL] [docstring] [EOL] real_pred = np . zeros_like ( y_pred ) [comment] [EOL] for i in np . unique ( y_pred ) : [EOL] cluster_samples_idx = y_pred == i [comment] [EOL] true_classes_of_samples = y_true [ cluster_samples_idx ] [EOL] if sum ( cluster_samples_idx ) == [number] : [comment] [EOL] continue [EOL] real_pred [ cluster_samples_idx ] = mode ( true_classes_of_samples ) . mode [ [number] ] [EOL] return np . mean ( real_pred == y_true ) [EOL] [EOL] [EOL] class Experiment : [EOL] MODEL_FILENAME = [string] [EOL] SCALER_FILENAME = [string] [EOL] [EOL] def __init__ ( self , comp_hiddens , n_clusters , comp_activation = tf . nn . tanh , use_cnn = False , minibatch_size = [number] , epoch_size = [number] , learning_rate = [number] , name = [string] , normalize = True , random_seed = [number] , sess = None , use_error_functions = False , binarize = False , gmvae_decode_activation = None , encoding = Encoding . AE , img_dims = ( [number] , [number] ) , encoder = None , graph = None , encoder_input = None , encoder_input_data = None , use_deconvs = False , log_performance = True ) : [EOL] [docstring] [EOL] print ( name , encoding ) [EOL] self . hidden_layers = comp_hiddens [EOL] self . use_cnn = use_cnn [EOL] [EOL] self . comp_activation = comp_activation [EOL] [EOL] self . n_clusters = n_clusters [EOL] [EOL] self . minibatch_size = minibatch_size [EOL] self . epoch_size = epoch_size [EOL] self . learning_rate = learning_rate [EOL] self . name = name [EOL] self . img_dims = img_dims [EOL] self . normalize = normalize [EOL] self . scaler = None [EOL] self . seed = random_seed [EOL] self . encoding = encoding [EOL] self . use_error_functions = use_error_functions [EOL] self . gmvae_decode_activation = gmvae_decode_activation [EOL] self . encoder = encoder [EOL] self . encoder_input = encoder_input [EOL] self . encoder_input_data = encoder_input_data [EOL] self . use_deconvs = use_deconvs [EOL] self . log_performance = log_performance [EOL] self . input , self . x_dash , self . z , self . likelihood , self . loss , self . minimizer , self . saver , self . algorithm = [ None ] * [number] [EOL] [EOL] if graph is None : [EOL] self . graph = tf . get_default_graph ( ) [EOL] else : [EOL] self . graph = graph [EOL] self . sess = sess [EOL] self . initialized = False [EOL] self . binarize = binarize [EOL] [EOL] def __del__ ( self ) : [EOL] if self . sess is not None : [EOL] self . sess . close ( ) [EOL] [EOL] def fit ( self , x , y = None , x_test = None , y_test = None , outlier_classes = None ) : [EOL] n_samples , n_features = x . shape [EOL] timestamp = time . strftime ( [string] ) [EOL] [EOL] if self . normalize : [EOL] self . scaler = scaler = StandardScaler ( ) [EOL] x = scaler . fit_transform ( x ) [EOL] [EOL] with self . graph . as_default ( ) : [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] self . input = tf . placeholder ( dtype = tf . float32 , shape = [ None , n_features ] , name = [string] ) [EOL] [EOL] if self . encoding == Encoding . AE : [EOL] self . algorithm = AE ( self . hidden_layers , self . n_clusters , comp_activation = self . comp_activation , use_error_functions = self . use_error_functions , use_cnn = self . use_cnn ) [EOL] elif self . encoding == Encoding . VAE : [EOL] self . algorithm = VAE ( self . hidden_layers , self . n_clusters ) [EOL] elif self . encoding == Encoding . GMVAE : [EOL] self . algorithm = GMVAE ( self . hidden_layers , self . n_clusters , self . img_dims , self . encoder , decode_activation = self . gmvae_decode_activation , use_deconvs = self . use_deconvs ) [EOL] else : [EOL] raise Exception ( [string] + str ( self . encoding ) ) [EOL] [EOL] self . z , self . x_dash , self . loss , self . minimizer = self . algorithm . build_graph ( self . input ) [EOL] [EOL] init = tf . global_variables_initializer ( ) [EOL] [EOL] if self . sess is None : [EOL] self . sess = tf . Session ( graph = self . graph ) [EOL] if not self . initialized : [EOL] self . sess . run ( init ) [EOL] self . initialized = True [EOL] [EOL] self . algorithm . set_session ( self . sess ) [EOL] [EOL] [comment] [EOL] reconstruction_loss_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , reconstruction_loss_placeholder ) [EOL] auc_reconstruction_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , auc_reconstruction_placeholder ) [EOL] auc_custom_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , auc_custom_placeholder ) [EOL] ari_kmeans_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , ari_kmeans_placeholder ) [EOL] ari_custom_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , ari_custom_placeholder ) [EOL] accuracy_kmeans_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , accuracy_kmeans_placeholder ) [EOL] accuracy_custom_placeholder = tf . placeholder ( tf . float32 , shape = ( ) , name = [string] ) [EOL] tf . summary . scalar ( [string] , accuracy_custom_placeholder ) [EOL] [EOL] if self . img_dims is not None : [EOL] cluster_centroids_placeholder = tf . placeholder ( tf . float32 , shape = ( self . n_clusters , ) + self . img_dims + ( [number] , ) , name = [string] ) [EOL] tf . summary . image ( [string] , cluster_centroids_placeholder , max_outputs = self . n_clusters ) [EOL] [EOL] if outlier_classes is not None : [EOL] reconstruction_img_in_placeholder = tf . placeholder ( tf . float32 , shape = ( [number] , ) + self . img_dims + ( [number] , ) , name = [string] ) [EOL] tf . summary . image ( [string] , reconstruction_img_in_placeholder , max_outputs = [number] ) [EOL] reconstruction_img_out_placeholder = tf . placeholder ( tf . float32 , shape = ( [number] , ) + self . img_dims + ( [number] , ) , name = [string] ) [EOL] tf . summary . image ( [string] , reconstruction_img_out_placeholder , max_outputs = [number] ) [EOL] closest_centroid_placeholder = tf . placeholder ( tf . float32 , shape = ( [number] , ) + self . img_dims + ( [number] , ) , name = [string] ) [EOL] tf . summary . image ( [string] , closest_centroid_placeholder , max_outputs = [number] ) [EOL] [EOL] merged_summary_op = tf . summary . merge_all ( ) [EOL] summary_writer = tf . summary . FileWriter ( OUTPUT_PATH / [string] . format ( timestamp , self . name , str ( self . encoding ) , str ( self . hidden_layers ) , str ( self . n_clusters ) , str ( outlier_classes ) , str ( self . gmvae_decode_activation ) ) , graph = tf . get_default_graph ( ) ) [EOL] [EOL] [comment] [EOL] n_batch = ( n_samples - [number] ) // self . minibatch_size + [number] [EOL] [EOL] [comment] [EOL] idx = np . arange ( x . shape [ [number] ] ) [EOL] np . random . shuffle ( idx ) [EOL] [EOL] scores = [ ] [EOL] for epoch in tqdm ( range ( self . epoch_size ) ) : [EOL] [EOL] for batch in range ( n_batch ) : [EOL] i_start = batch * self . minibatch_size [EOL] i_end = ( batch + [number] ) * self . minibatch_size [EOL] x_batch = x [ idx [ i_start : i_end ] ] [EOL] [EOL] feed_dict = { self . input : x_batch } [EOL] if self . encoder is not None : [EOL] feed_dict . update ( { self . encoder : self . encoder_input_data [ idx [ i_start : i_end ] ] } ) [EOL] self . sess . run ( [ self . minimizer ] , feed_dict = feed_dict ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if self . log_performance and ( epoch % [number] == [number] or epoch == self . epoch_size - [number] ) : [EOL] self . algorithm . fit_kmeans ( x_test ) [EOL] auc_reconstruction , auc_custom , auc_custom = [number] , [number] , [number] [EOL] if outlier_classes is not None : [EOL] auc_reconstruction = roc_auc_score ( y_test . isin ( outlier_classes ) , np . nan_to_num ( self . algorithm . get_reconstruction_anomaly_scores ( x_test ) ) ) [EOL] auc_custom = roc_auc_score ( y_test . isin ( outlier_classes ) , np . nan_to_num ( self . algorithm . get_custom_anomaly_scores ( x_test ) ) ) [EOL] [EOL] if self . img_dims is not None : [EOL] test_sample = x_test [ y_test . isin ( outlier_classes ) ] [ [number] ] . reshape ( [number] , - [number] ) [EOL] reconstruction_img_out = self . algorithm . get_reconstruction ( test_sample ) . reshape ( [number] , * self . img_dims , [number] ) [EOL] self . algorithm . fit_kmeans ( x ) [EOL] closest_centroid = self . algorithm . get_closest_decoded_centroids ( test_sample ) . reshape ( [number] , * self . img_dims , [number] ) [EOL] [EOL] self . algorithm . fit_kmeans ( x_test ) [EOL] y_prediction_kmeans = self . algorithm . get_kmeans_assignments ( ) [EOL] y_prediction_custom = self . algorithm . get_custom_assignments ( x_test ) [EOL] ari_kmeans_score = adjusted_rand_score ( y_test , y_prediction_kmeans ) [EOL] ari_custom_score = adjusted_rand_score ( y_test , y_prediction_custom ) [EOL] accuracy_kmeans = accuracy ( y_prediction_kmeans , y_test ) [EOL] accuracy_custom = accuracy ( y_prediction_custom , y_test ) [EOL] [EOL] if self . img_dims is not None : [EOL] self . algorithm . fit_kmeans ( x ) [EOL] cluster_centroids = self . algorithm . get_decoded_centroids ( ) . reshape ( - [number] , * self . img_dims , [number] ) [EOL] [EOL] [comment] [EOL] [EOL] print ( [string] . format ( auc_reconstruction , auc_custom , accuracy_kmeans , accuracy_custom , ari_kmeans_score , ari_custom_score ) ) [EOL] [EOL] scores . append ( [ auc_reconstruction , auc_custom , accuracy_kmeans , accuracy_custom , ari_kmeans_score , ari_custom_score , epoch ] ) [EOL] [EOL] summary_feed = { self . input : x_batch , reconstruction_loss_placeholder : self . algorithm . get_reconstruction_loss ( x_test ) , auc_reconstruction_placeholder : auc_reconstruction , auc_custom_placeholder : auc_custom , ari_kmeans_placeholder : ari_kmeans_score , ari_custom_placeholder : ari_custom_score , accuracy_kmeans_placeholder : ari_kmeans_score , accuracy_custom_placeholder : accuracy_custom } [EOL] [EOL] if outlier_classes is not None and self . img_dims is not None : [EOL] summary_feed . update ( { reconstruction_img_in_placeholder : test_sample . reshape ( - [number] , * self . img_dims , [number] ) , reconstruction_img_out_placeholder : reconstruction_img_out , cluster_centroids_placeholder : cluster_centroids , closest_centroid_placeholder : closest_centroid } ) [EOL] [EOL] summary = self . sess . run ( [ merged_summary_op ] , feed_dict = summary_feed ) [ [number] ] [EOL] summary_writer . add_summary ( summary , epoch ) [EOL] [EOL] [comment] [EOL] [EOL] tf . add_to_collection ( [string] , self . input ) [EOL] tf . add_to_collection ( [string] , self . z ) [EOL] [EOL] self . saver = tf . train . Saver ( ) [EOL] return scores [EOL] [EOL] def predict ( self , x ) : [EOL] return self . algorithm . get_custom_anomaly_scores ( x ) [EOL] [EOL] def save ( self , fdir ) : [EOL] [docstring] [EOL] if self . sess is None : [EOL] raise Exception ( [string] ) [EOL] [EOL] if not os . path . exists ( fdir ) : [EOL] os . makedirs ( fdir ) [EOL] [EOL] model_path = os . path . join ( fdir , self . MODEL_FILENAME ) [EOL] self . saver . save ( self . sess , model_path ) [EOL] [EOL] if self . normalize : [EOL] scaler_path = os . path . join ( fdir , self . SCALER_FILENAME ) [EOL] joblib . dump ( self . scaler , scaler_path ) [EOL] [EOL] def restore ( self , fdir ) : [EOL] [docstring] [EOL] if not os . path . exists ( fdir ) : [EOL] raise Exception ( [string] ) [EOL] [EOL] model_path = os . path . join ( fdir , self . MODEL_FILENAME ) [EOL] meta_path = model_path + [string] [EOL] [EOL] with tf . Graph ( ) . as_default ( ) as graph : [EOL] self . graph = graph [EOL] self . sess = tf . Session ( graph = graph ) [EOL] self . saver = tf . train . import_meta_graph ( meta_path ) [EOL] self . saver . restore ( self . sess , model_path ) [EOL] print ( tf . get_collection ( ) ) [EOL] [EOL] self . input , self . z = tf . get_collection ( [string] ) [EOL] [EOL] if self . normalize : [EOL] scaler_path = os . path . join ( fdir , self . SCALER_FILENAME ) [EOL] self . scaler = joblib . load ( scaler_path ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[unknown,typing.Any]$ 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0