from typing import Any [EOL] import typing [EOL] import gym [EOL] import gym_tictactoe [comment] [EOL] import numpy as np [EOL] [EOL] [comment] [EOL] from agent import MCTSAgent [EOL] [EOL] [EOL] def set_seed ( seed ) : [EOL] np . random . seed ( seed ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] set_seed ( [number] ) [EOL] [EOL] env = gym . make ( [string] ) [EOL] obs = env . reset ( ) [EOL] mcts_agent = MCTSAgent ( ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] actions = [ [number] , [number] , [number] ] [comment] [EOL] [EOL] for action in actions : [EOL] obs , _ , _ , _ = env . step ( action ) [EOL] [EOL] env . render ( ) [EOL] print ( obs ) [EOL] action = mcts_agent . get_action ( env , obs ) [EOL] print ( action ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] from agent . AlphaZero import AlphaZero [EOL] [EOL] [EOL] def main ( ) : [EOL] alphazero = AlphaZero ( ) [EOL] alphazero . train ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] import logging [EOL] import logging [EOL] import logging . handlers [EOL] import os [EOL] [EOL] LOG_ROTATE_SIZE = [number] * [number] * [number] [comment] [EOL] LOG_ROTATE_NUM = [number] [comment] [EOL] [EOL] [EOL] def setup_logger ( name , level = logging . DEBUG , filename = None ) : [EOL] logger = logging . getLogger ( name ) [EOL] logger . setLevel ( level ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] formatter = logging . Formatter ( [string] ) [EOL] [EOL] [comment] [EOL] ch = logging . StreamHandler ( ) [EOL] ch . setLevel ( level ) [EOL] ch . setFormatter ( formatter ) [EOL] logger . addHandler ( ch ) [EOL] [EOL] [comment] [EOL] if filename is not None : [EOL] fh = logging . handlers . RotatingFileHandler ( os . path . join ( [string] , filename ) , maxBytes = LOG_ROTATE_SIZE , backupCount = LOG_ROTATE_NUM ) [EOL] fh . setLevel ( level ) [EOL] fh . setFormatter ( formatter ) [EOL] logger . addHandler ( fh ) [EOL] [EOL] return logger [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] import gym [EOL] import gym_tictactoe [comment] [EOL] [EOL] from agent import RandomAgent , MCTSAgent , AlphaZeroAgent [EOL] from agent . AlphaZero import AlphaZeroConfig , AlphaZeroNetwork [EOL] [EOL] [EOL] def set_seed ( seed ) : [EOL] np . random . seed ( seed ) [EOL] torch . manual_seed ( seed ) [EOL] [EOL] [EOL] def play_game ( env , agent_b , agent_w ) : [EOL] obs = env . reset ( ) [EOL] done = False [EOL] while not done : [EOL] [comment] [EOL] [comment] [EOL] player = obs [ [string] ] [EOL] if player == [number] : [EOL] action = agent_b . get_action ( env , obs ) [EOL] else : [EOL] action = agent_w . get_action ( env , obs ) [EOL] obs , _ , done , _ = env . step ( action ) [EOL] [comment] [EOL] env . render ( ) [EOL] return obs [ [string] ] [EOL] [EOL] [EOL] def match ( num , env , agent_b , agent_w ) : [EOL] win_b_cnt = [number] [EOL] win_w_cnt = [number] [EOL] draw_cnt = [number] [EOL] for i in range ( num ) : [EOL] print ( f" [string] { i + [number] } [string] " ) [EOL] winner = play_game ( env , agent_b , agent_w ) [EOL] print ( f" [string] { winner if winner is not None else [string] } [string] " ) [EOL] if winner is None : [EOL] draw_cnt += [number] [EOL] elif winner == env . BLACK : [EOL] win_b_cnt += [number] [EOL] else : [EOL] win_w_cnt += [number] [EOL] return win_b_cnt , win_w_cnt , draw_cnt [EOL] [EOL] [EOL] def build_agent ( name ) : [EOL] if name == [string] : [EOL] return RandomAgent ( ) [EOL] elif name == [string] : [EOL] return MCTSAgent ( ) [EOL] elif name == [string] : [EOL] config = AlphaZeroConfig ( ) [EOL] network = AlphaZeroNetwork ( config ) [EOL] agent = AlphaZeroAgent ( config , network ) [EOL] agent . load_model ( [string] ) [EOL] return agent [EOL] else : [EOL] raise RuntimeError [EOL] [EOL] [EOL] def main ( ) : [EOL] [comment] [EOL] [EOL] env = gym . make ( [string] ) [EOL] agent_b = build_agent ( [string] ) [EOL] agent_w = build_agent ( [string] ) [EOL] [EOL] win_agent_b = [number] [EOL] win_agent_w = [number] [EOL] draw_cnt_total = [number] [EOL] [EOL] win_b_cnt , win_w_cnt , draw_cnt = match ( [number] , env , agent_b , agent_w ) [EOL] win_agent_b += win_b_cnt [EOL] win_agent_w += win_w_cnt [EOL] draw_cnt_total += draw_cnt [EOL] [EOL] win_b_cnt , win_w_cnt , draw_cnt = match ( [number] , env , agent_w , agent_b ) [comment] [EOL] win_agent_b += win_w_cnt [EOL] win_agent_w += win_b_cnt [EOL] draw_cnt_total += draw_cnt [EOL] [EOL] print ( f" [string] { win_agent_b } [string] { win_agent_w } [string] { draw_cnt_total } [string] " ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . Agent import Agent [comment] [EOL] from . Random import RandomAgent [comment] [EOL] from . MCTS import MCTSAgent [comment] [EOL] from . AlphaZero import AlphaZeroAgent [comment] [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict [EOL] import gym [EOL] import typing [EOL] from typing import Dict [EOL] from abc import ABC , abstractmethod [EOL] import gym [EOL] [EOL] [EOL] class Agent ( ABC ) : [EOL] @ abstractmethod def get_action ( self , env , obs ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gym.Env$ 0 $typing.Dict$ 0 0 0 0 0 0
from . MCTSAgent import MCTSAgent [comment] [EOL]	0 0 0 0 0 0 0
from typing import Any [EOL] import RL [EOL] import typing [EOL] import gym [EOL] import numpy as np [EOL] import torch [EOL] from torch . utils . tensorboard import SummaryWriter [EOL] [EOL] import gym_tictactoe [comment] [EOL] import radam [comment] [EOL] [EOL] from . config import AlphaZeroConfig [EOL] from . network import AlphaZeroNetwork [EOL] from . replay import ReplayBuffer [EOL] from . self_play import SelfPlay [EOL] from . trainer import Trainer [EOL] from . AlphaZeroAgent import AlphaZeroAgent [EOL] [EOL] [EOL] class AlphaZero : [EOL] def __init__ ( self ) : [EOL] self . config = AlphaZeroConfig ( ) [EOL] np . random . seed ( self . config . seed ) [EOL] torch . random . manual_seed ( self . config . seed ) [EOL] [EOL] self . env = gym . make ( [string] ) [EOL] self . env . seed ( self . config . seed ) [EOL] [EOL] network = AlphaZeroNetwork ( self . config ) [EOL] [comment] [EOL] network . to ( self . config . device ) [EOL] optimizer = radam . RAdam ( network . parameters ( ) , lr = self . config . lr , weight_decay = self . config . weight_decay ) [EOL] [EOL] writer = SummaryWriter ( [string] ) [EOL] [EOL] self . agent = AlphaZeroAgent ( self . config , network ) [EOL] replay = ReplayBuffer ( self . config ) [EOL] self . self_play = SelfPlay ( self . config , self . env , self . agent , replay ) [EOL] self . trainer = Trainer ( self . config , network , optimizer , replay , writer ) [EOL] [EOL] self . model_file_path = [string] + self . config . model_file [EOL] self . agent . load_model ( self . model_file_path ) [EOL] [EOL] def train ( self ) : [EOL] try : [EOL] for i in range ( self . config . max_training_step ) : [EOL] self . self_play . run ( ) [EOL] self . trainer . run ( i ) [EOL] [EOL] if ( i > [number] ) and ( i % self . config . validate_interval ) == [number] : [EOL] self . validate ( ) [EOL] self . agent . save_model ( self . model_file_path ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] self . validate ( True ) [EOL] self . agent . save_model ( self . model_file_path ) [EOL] [EOL] def validate ( self , is_render = False ) : [EOL] obs = self . env . reset ( ) [EOL] done = False [EOL] while not done : [EOL] if is_render : [EOL] self . env . render ( ) [EOL] action , root = self . agent . get_action ( self . env , obs , True ) [EOL] if is_render : [EOL] root . print_node ( limit_depth = [number] ) [EOL] obs , _ , done , _ = self . env . step ( action ) [EOL] self . env . render ( ) [EOL] print ( f" [string] { obs [ [string] ] } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.self_play.SelfPlay$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.trainer.Trainer$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 $typing.Any$ 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] from typing import Tuple , Any [EOL] import torch [EOL] import RL [EOL] import typing [EOL] import config [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] [EOL] from . config import AlphaZeroConfig [EOL] [EOL] [EOL] def weight_init ( m ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . kaiming_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] [EOL] def conv3x3 ( in_channels , out_channels ) : [EOL] return nn . Conv2d ( in_channels , out_channels , kernel_size = [number] , stride = [number] , padding = [number] , bias = False ) [EOL] [EOL] [EOL] class SwishImpl ( torch . autograd . Function ) : [EOL] @ staticmethod def forward ( ctx , i ) : [EOL] result = i * torch . sigmoid ( i ) [EOL] ctx . save_for_backward ( i ) [EOL] return result [EOL] [EOL] @ staticmethod def backward ( ctx , grad_output ) : [EOL] i = ctx . saved_variables [ [number] ] [EOL] sigmoid_i = torch . sigmoid ( i ) [EOL] return grad_output * ( sigmoid_i * ( [number] + i * ( [number] - sigmoid_i ) ) ) [EOL] [EOL] [EOL] class Swish ( nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( Swish , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] return SwishImpl . apply ( x ) [EOL] [EOL] [EOL] class Mish ( nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( Mish , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] return x * torch . tanh ( F . softplus ( x ) ) [EOL] [EOL] [EOL] class ResNetBlock ( nn . Module ) : [EOL] def __init__ ( self , num_channels , activation = Swish ) : [EOL] super ( ResNetBlock , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( conv3x3 ( num_channels , num_channels ) , nn . BatchNorm2d ( num_channels ) , activation ( ) , conv3x3 ( num_channels , num_channels ) , nn . BatchNorm2d ( num_channels ) , ) [EOL] self . relu = activation ( ) [EOL] [EOL] def forward ( self , x ) : [comment] [EOL] return self . relu ( x + self . layers ( x ) ) [EOL] [EOL] [EOL] class ResNet ( nn . Module ) : [EOL] def __init__ ( self , in_channels , num_channels , activation = Swish ) : [EOL] super ( ResNet , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( conv3x3 ( in_channels , num_channels ) , nn . BatchNorm2d ( num_channels ) , activation ( ) , ResNetBlock ( num_channels , activation ) , ResNetBlock ( num_channels , activation ) , ) [EOL] [EOL] def forward ( self , x ) : [comment] [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class MLP ( nn . Module ) : [EOL] def __init__ ( self , input_Num , hid_num , output_num , activation = Swish ) : [EOL] super ( MLP , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( nn . Linear ( input_Num , hid_num ) , activation ( ) , nn . Linear ( hid_num , hid_num ) , activation ( ) , nn . Linear ( hid_num , output_num ) , ) [EOL] self . layers . apply ( weight_init ) [EOL] [EOL] def forward ( self , x ) : [comment] [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class AlphaZeroNetwork ( nn . Module ) : [EOL] def __init__ ( self , config , activation = Swish ) : [EOL] super ( AlphaZeroNetwork , self ) . __init__ ( ) [EOL] [EOL] self . num_channels = config . num_channels [EOL] in_channels = config . obs_space [ [number] ] [EOL] self . resnet = ResNet ( in_channels , config . num_channels , activation ) [EOL] [EOL] ch_h = config . obs_space [ [number] ] [EOL] ch_w = config . obs_space [ [number] ] [EOL] self . fc_input_num = config . num_channels * ch_h * ch_w [EOL] self . policy_layers = MLP ( self . fc_input_num , config . fc_hid_num , config . fc_output_num , activation ) [EOL] self . value_layers = MLP ( self . fc_input_num , config . fc_hid_num , config . atoms , activation ) [EOL] [EOL] def inference ( self , x ) : [EOL] h = self . resnet ( x ) [EOL] h = h . view ( - [number] , self . fc_input_num ) [EOL] return self . policy_layers ( h ) , self . value_layers ( h ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] config = AlphaZeroConfig ( ) [EOL] config . obs_space = ( [number] , [number] , [number] ) [EOL] config . num_channels = [number] [EOL] config . fc_hid_num = [number] [EOL] config . fc_output_num = [number] [EOL] config . support_size = [number] [EOL] config . atoms = config . support_size * [number] + [number] [EOL] batch_size = [number] [EOL] [EOL] network = AlphaZeroNetwork ( config ) [EOL] print ( network ) [EOL] x = torch . randn ( ( batch_size , config . obs_space [ [number] ] , config . obs_space [ [number] ] , config . obs_space [ [number] ] ) ) [EOL] print ( x ) [EOL] policy_logits , value_logits = network . inference ( x ) [EOL] print ( policy_logits ) [EOL] print ( value_logits ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $config.AlphaZeroConfig$ 0 0 0 $builtins.int$ 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.ResNet$ 0 0 0 $builtins.int$ 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 $builtins.int$ 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 $builtins.int$ 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 $config.AlphaZeroConfig$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.MLP$ 0 0 0 0 0 0 0 $config.AlphaZeroConfig$ 0 0 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.MLP$ 0 0 0 0 0 0 0 $config.AlphaZeroConfig$ 0 0 0 $config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , List , Any [EOL] import RL [EOL] import typing [EOL] import builtins [EOL] import config [EOL] import logging [EOL] from typing import List [EOL] [EOL] import numpy as np [EOL] [EOL] import gym_tictactoe [comment] [EOL] from logger import setup_logger [EOL] [EOL] from . config import AlphaZeroConfig [EOL] [EOL] logger = setup_logger ( __name__ , logging . INFO ) [EOL] [EOL] [EOL] class GameBuffer : [EOL] def __init__ ( self , obs , player , action_space ) : [EOL] self . observations = [ obs ] [EOL] self . players = [ player ] [EOL] self . actions = [ ] [EOL] self . values = [ ] [EOL] self . child_visits = [ ] [EOL] self . winner = None [EOL] self . action_space = action_space [EOL] [EOL] def append ( self , next_obs , next_player , action ) : [EOL] self . observations . append ( next_obs ) [EOL] self . players . append ( next_player ) [EOL] self . actions . append ( action ) [EOL] [EOL] def store_search_statistics ( self , root ) : [EOL] total_visit = sum ( [ edge . visit_count for edge in root . edges ] ) [EOL] edge_map = { edge . action : edge . visit_count for edge in root . edges } [EOL] child_visit = [ edge_map [ action ] / total_visit if action in edge_map else [number] for action in range ( self . action_space ) ] [EOL] self . child_visits . append ( child_visit ) [EOL] [EOL] def set_winner ( self , winner , discount , reward ) : [EOL] self . child_visits . append ( [ [number] / self . action_space for _ in range ( self . action_space ) ] ) [EOL] self . winner = winner [EOL] for i in range ( len ( self . observations ) ) : [EOL] if self . winner is not None : [EOL] player = self . players [ i ] [EOL] value = reward * ( discount ** ( len ( self . observations ) - i - [number] ) ) [EOL] value = value if self . winner == player else - value [EOL] else : [EOL] value = [number] [EOL] self . values . append ( value ) [EOL] [EOL] def make_target ( self , state_index ) : [EOL] return self . values [ state_index ] , np . array ( self . child_visits [ state_index ] ) [EOL] [EOL] def print_buffer ( self ) : [EOL] print ( [string] ) [EOL] print ( [string] , self . observations ) [EOL] print ( [string] , self . players ) [EOL] print ( [string] , self . actions ) [EOL] print ( [string] , self . values ) [EOL] print ( [string] , self . child_visits ) [EOL] print ( [string] , self . winner ) [EOL] print ( [string] ) [EOL] [EOL] [EOL] class ReplayBuffer : [EOL] def __init__ ( self , config ) : [EOL] self . config = config [EOL] self . buffer = [ ] [EOL] [EOL] def append ( self , game ) : [EOL] if len ( self . buffer ) >= self . config . replay_buffer_size : [EOL] self . buffer . pop ( [number] ) [EOL] self . buffer . append ( game ) [EOL] [EOL] def sample_batch ( self ) : [EOL] games = np . random . choice ( self . buffer , self . config . batch_size ) [EOL] game_pos = [ ( g , np . random . randint ( len ( g . observations ) ) ) for g in games ] [EOL] return [ ( g . observations [ i ] , g . make_target ( i ) ) for ( g , i ) in game_pos ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.dict$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 $typing.List[GameBuffer]$ 0 0 0 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0
from typing import List , Any [EOL] import typing [EOL] import replay [EOL] import torch [EOL] import RL [EOL] import network [EOL] import builtins [EOL] import config [EOL] import logging [EOL] import numpy as np [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from torch . optim . optimizer import Optimizer [EOL] from torch . utils . tensorboard import SummaryWriter [EOL] [EOL] from logger import setup_logger [EOL] [EOL] from . config import AlphaZeroConfig [EOL] from . network import AlphaZeroNetwork [EOL] from . replay import ReplayBuffer [EOL] [EOL] logger = setup_logger ( __name__ , logging . INFO ) [EOL] [EOL] [EOL] class Trainer : [EOL] def __init__ ( self , config , network , optimizer , replay , writer , ) : [EOL] self . config = config [EOL] self . network = network [EOL] self . optimizer = optimizer [EOL] self . replay = replay [EOL] self . writer = writer [EOL] [EOL] def run ( self , i ) : [EOL] p_loss , v_loss = self . _train ( ) [EOL] [EOL] win_b_rate , win_w_rate , draw_rate = self . _calc_win_rate ( ) [EOL] logger . info ( f"{ i } [string] { p_loss : [string] } [string] { v_loss : [string] } [string] { win_b_rate : [string] } [string] { win_w_rate : [string] } [string] { draw_rate : [string] } [string] " ) [EOL] self . writer . add_scalar ( [string] , p_loss , i ) [EOL] self . writer . add_scalar ( [string] , v_loss , i ) [EOL] self . writer . add_scalar ( [string] , win_b_rate , i ) [EOL] self . writer . add_scalar ( [string] , win_w_rate , i ) [EOL] self . writer . add_scalar ( [string] , draw_rate , i ) [EOL] [EOL] def _train ( self ) : [EOL] self . network . train ( ) [EOL] [EOL] batch = self . replay . sample_batch ( ) [EOL] [EOL] observations , targets = zip ( * batch ) [EOL] observations = torch . from_numpy ( np . array ( observations ) ) . float ( ) [EOL] target_values , target_policies = zip ( * targets ) [EOL] target_values = torch . from_numpy ( np . array ( target_values ) ) . unsqueeze ( [number] ) . float ( ) [EOL] target_policies = torch . from_numpy ( np . array ( target_policies ) ) . float ( ) [EOL] [EOL] target_values = self . _scalar_to_support ( target_values ) [EOL] [EOL] policy_logits , value_logits = self . network . inference ( observations ) [EOL] [comment] [EOL] [EOL] p_loss = ( - ( target_policies * F . log_softmax ( policy_logits , dim = [number] ) ) ) . sum ( dim = [number] ) . mean ( ) [EOL] v_loss = ( - ( target_values * F . log_softmax ( value_logits , dim = [number] ) ) ) . sum ( dim = [number] ) . mean ( ) [EOL] [EOL] self . optimizer . zero_grad ( ) [EOL] total_loss = p_loss + v_loss [EOL] total_loss . backward ( ) [EOL] [comment] [EOL] self . optimizer . step ( ) [EOL] [EOL] return p_loss . item ( ) , v_loss . item ( ) [EOL] [EOL] def _scalar_to_support ( self , x ) : [EOL] batch_size = x . shape [ [number] ] [EOL] [EOL] [comment] [EOL] eps = self . config . support_eps [EOL] scaled_x = x . sign ( ) * ( ( x . abs ( ) + [number] ) . sqrt ( ) - [number] ) + eps * x [EOL] scaled_x . clamp_ ( self . config . min_v , self . config . max_v ) [EOL] [EOL] b = ( scaled_x - self . config . min_v ) / ( self . config . delta_z ) [comment] [EOL] lower_index , upper_index = b . floor ( ) . long ( ) , b . ceil ( ) . long ( ) [comment] [EOL] [comment] [EOL] lower_index [ ( upper_index > [number] ) * ( lower_index == upper_index ) ] -= [number] [comment] [EOL] upper_index [ ( lower_index < ( self . config . atoms - [number] ) ) * ( lower_index == upper_index ) ] += [number] [comment] [EOL] lower_probs = upper_index - b [EOL] upper_probs = b - lower_index [EOL] [EOL] logits = torch . zeros ( batch_size , self . config . atoms ) [EOL] logits . scatter_ ( dim = [number] , index = lower_index , src = lower_probs ) [EOL] logits . scatter_ ( dim = [number] , index = upper_index , src = upper_probs ) [EOL] return logits [EOL] [EOL] def _calc_win_rate ( self ) : [EOL] win_b_cnt = [number] [EOL] win_w_cnt = [number] [EOL] draw_cnt = [number] [EOL] [EOL] replay_temp = self . replay . buffer [ - self . config . calc_rate_size : ] [EOL] for game in replay_temp : [EOL] if game . winner is None : [EOL] draw_cnt += [number] [EOL] elif game . winner == [number] : [EOL] win_b_cnt += [number] [EOL] else : [EOL] win_w_cnt += [number] [EOL] [EOL] replay_size = len ( replay_temp ) [EOL] win_b_rate = win_b_cnt / replay_size [EOL] win_w_rate = win_w_cnt / replay_size [EOL] draw_rate = draw_cnt / replay_size [EOL] [EOL] return win_b_rate , win_w_rate , draw_rate [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 $RL.TicTacToe.agent.AlphaZero.network.AlphaZeroNetwork$ 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[RL.TicTacToe.agent.AlphaZero.replay.GameBuffer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[RL.TicTacToe.agent.AlphaZero.replay.GameBuffer]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[RL.TicTacToe.agent.AlphaZero.replay.GameBuffer]$ 0 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0
from typing import Any [EOL] import typing [EOL] import replay [EOL] import RL [EOL] import AlphaZeroAgent [EOL] import config [EOL] import copy [EOL] [EOL] from . config import AlphaZeroConfig [EOL] from . AlphaZeroAgent import AlphaZeroAgent [EOL] from . replay import GameBuffer , ReplayBuffer [EOL] [EOL] [EOL] class SelfPlay : [EOL] def __init__ ( self , config , env , agent , replay ) : [EOL] self . config = config [EOL] self . env = env [EOL] self . agent = agent [EOL] self . replay = replay [EOL] [EOL] def run ( self ) : [EOL] for i in range ( self . config . self_play_num ) : [EOL] game = self . _play_game ( ) [EOL] self . replay . append ( game ) [EOL] [EOL] def _play_game ( self ) : [EOL] obs = self . env . reset ( ) [EOL] done = False [EOL] game = GameBuffer ( copy . deepcopy ( obs [ [string] ] ) , obs [ [string] ] , self . config . action_space ) [EOL] while not done : [EOL] [comment] [EOL] action , root = self . agent . get_action ( self . env , obs , True ) [EOL] obs , _ , done , _ = self . env . step ( action ) [EOL] game . append ( copy . deepcopy ( obs [ [string] ] ) , obs [ [string] ] , action ) [EOL] game . store_search_statistics ( root ) [EOL] [comment] [EOL] game . set_winner ( obs [ [string] ] , self . config . discount , self . config . terminate_value ) [EOL] [comment] [EOL] return game [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 $RL.TicTacToe.agent.AlphaZero.config.AlphaZeroConfig$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 $RL.TicTacToe.agent.AlphaZero.replay.ReplayBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.GameBuffer$ 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $RL.TicTacToe.agent.AlphaZero.replay.GameBuffer$ 0
from typing import Tuple [EOL] import typing [EOL] import torch [EOL] [EOL] [EOL] class AlphaZeroConfig : [EOL] def __init__ ( self ) : [EOL] [comment] [EOL] self . seed = [number] [EOL] [EOL] [comment] [EOL] self . obs_space = ( [number] , [number] , [number] ) [EOL] self . action_space = [number] [EOL] self . terminate_value = [number] [EOL] [EOL] [comment] [EOL] self . simulation_num = [number] [EOL] self . root_dirichlet_alpha = [number] [EOL] self . root_exploration_fraction = [number] [EOL] [EOL] [comment] [EOL] self . pb_c_base = [number] [EOL] self . pb_c_init = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . device = torch . device ( [string] ) [comment] [EOL] self . num_channels = [number] [EOL] self . fc_hid_num = [number] [EOL] self . fc_output_num = [number] [EOL] self . model_file = [string] [EOL] [EOL] [comment] [EOL] self . min_v = - [number] [EOL] self . max_v = + [number] [EOL] self . support_size = [number] [EOL] self . support_eps = [number] [EOL] self . atoms = self . support_size * [number] + [number] [EOL] self . delta_z = ( self . max_v - self . min_v ) / ( self . atoms - [number] ) [EOL] self . support_base = torch . linspace ( self . min_v , self . max_v , self . atoms ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] self . self_play_num = [number] [EOL] self . discount = [number] [EOL] [EOL] [comment] [EOL] self . replay_buffer_size = [number] [EOL] self . batch_size = [number] [EOL] [EOL] [comment] [EOL] self . lr = [number] [EOL] self . weight_decay = [number] [EOL] self . max_training_step = [number] [EOL] self . validate_interval = [number] [EOL] [EOL] [comment] [EOL] self . calc_rate_size = [number] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0
from . config import AlphaZeroConfig [comment] [EOL] from . network import AlphaZeroNetwork [comment] [EOL] from . AlphaZeroAgent import AlphaZeroAgent [comment] [EOL] from . alphazero import AlphaZero [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . RandomAgent import RandomAgent [comment] [EOL]	0 0 0 0 0 0 0
from typing import Dict [EOL] import gym [EOL] import typing [EOL] from typing import Dict [EOL] import gym [EOL] import numpy as np [EOL] [EOL] from agent import Agent [EOL] [EOL] [EOL] class RandomAgent ( Agent ) : [comment] [EOL] def get_action ( self , env , obs ) : [EOL] return np . random . choice ( obs [ [string] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gym.Env$ 0 $typing.Dict$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict$ 0 0 0 0 0
from typing import Type , Optional , Any [EOL] import RL [EOL] import numpy [EOL] import typing [EOL] import builtins [EOL] import collections [EOL] from typing import Optional [EOL] import numpy as np [EOL] import torch [EOL] [EOL] MAXIMUM_FLOAT_VALUE = float ( [string] ) [EOL] KnownBounds = collections . namedtuple ( [string] , [ [string] , [string] ] ) [EOL] [EOL] [EOL] class MinMaxStats : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , known_bounds = None ) : [EOL] self . maximum = known_bounds . max if known_bounds else - MAXIMUM_FLOAT_VALUE [EOL] self . minimum = known_bounds . min if known_bounds else MAXIMUM_FLOAT_VALUE [EOL] [EOL] def update ( self , value ) : [EOL] if value is None : [EOL] raise ValueError [EOL] [EOL] self . maximum = max ( self . maximum , value ) [EOL] self . minimum = min ( self . minimum , value ) [EOL] [EOL] def normalize ( self , value ) : [EOL] if value is None : [EOL] return [number] [EOL] if self . maximum > self . minimum : [EOL] [comment] [EOL] return ( value - self . minimum ) / ( self . maximum - self . minimum ) [EOL] return value [EOL] [EOL] def __repr__ ( self ) : [EOL] return f" [string] { self . minimum } [string] { self . maximum } [string] " [EOL] [EOL] [EOL] def softmax ( x ) : [EOL] x_max = np . max ( x ) [EOL] exp = np . exp ( x - x_max ) [EOL] return exp / np . sum ( exp ) [EOL] [EOL] [EOL] def get_device ( use_gpu ) : [EOL] [comment] [EOL] if use_gpu : [EOL] print ( [string] ) [EOL] device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] else : [EOL] device = torch . device ( [string] ) [EOL] if device == [string] : [EOL] torch . backends . cudnn . benchmark = True [EOL] print ( f" [string] { device } [string] " ) [EOL] return device [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Type[RL.TicTacToe.agent.MuZero.utils.KnownBounds]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[KnownBounds]$ 0 0 0 0 0 0 0 0 0 $typing.Optional[KnownBounds]$ 0 0 0 $typing.Optional[KnownBounds]$ 0 0 $builtins.float$ 0 0 0 0 0 $typing.Optional[KnownBounds]$ 0 0 0 $typing.Optional[KnownBounds]$ 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . MuZeroAgent import MuZeroAgent [comment] [EOL] from . replay import GameBuffer , ReplayBuffer [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import torch [EOL] import RL [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] [EOL] [EOL] def weight_init ( m ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . kaiming_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] [EOL] class Representation ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_num , hid_num , state_num ) : [EOL] super ( Representation , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( nn . Linear ( input_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , state_num ) , ) [EOL] self . layers . apply ( weight_init ) [EOL] [EOL] def inference ( self , x ) : [EOL] h = self . layers ( x ) [EOL] [EOL] [comment] [EOL] h_scaled = h - h . min ( [number] , keepdim = True ) [ [number] ] [EOL] h_scaled = h_scaled / h_scaled . max ( [number] , keepdim = True ) [ [number] ] [EOL] [comment] [EOL] return h_scaled [EOL] [EOL] [EOL] class Prediction ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , state_num , hid_num , action_num ) : [EOL] super ( Prediction , self ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . policy_layers = nn . Sequential ( nn . Linear ( state_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , action_num ) , ) [EOL] self . value_layers = nn . Sequential ( nn . Linear ( state_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , [number] ) , nn . Tanh ( ) , ) [EOL] [EOL] [comment] [EOL] self . policy_layers . apply ( weight_init ) [EOL] self . value_layers . apply ( weight_init ) [EOL] [EOL] def inference ( self , x , mask = None ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] policy_logit = self . policy_layers ( x ) [EOL] value = self . value_layers ( x ) [EOL] return policy_logit , value [EOL] [EOL] [EOL] class Dynamics ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , state_num , action_num , hid_num ) : [EOL] super ( Dynamics , self ) . __init__ ( ) [EOL] input_num = state_num + action_num [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . state_layers = nn . Sequential ( nn . Linear ( input_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , state_num ) , ) [EOL] self . reward_layers = nn . Sequential ( nn . Linear ( input_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , hid_num ) , nn . ReLU ( inplace = True ) , nn . Linear ( hid_num , [number] ) , ) [EOL] [EOL] [comment] [EOL] self . state_layers . apply ( weight_init ) [EOL] self . reward_layers . apply ( weight_init ) [EOL] [EOL] def inference ( self , x ) : [EOL] [comment] [EOL] [comment] [EOL] h = self . state_layers ( x ) [EOL] [EOL] [comment] [EOL] h_scaled = h - h . min ( [number] , keepdim = True ) [ [number] ] [EOL] h_scaled = h_scaled / h_scaled . max ( [number] , keepdim = True ) [ [number] ] [EOL] [comment] [EOL] [EOL] return h_scaled , self . reward_layers ( x ) [EOL] [EOL] [EOL] class Network ( nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( Network , self ) . __init__ ( ) [EOL] input_num = [number] [comment] [EOL] hid_num = [number] [comment] [EOL] state_num = [number] [comment] [EOL] action_num = [number] [EOL] [EOL] self . representation = Representation ( input_num , hid_num , state_num ) [EOL] self . prediction = Prediction ( state_num , hid_num , action_num ) [EOL] self . dynamics = Dynamics ( state_num , action_num , hid_num ) [EOL] [EOL] def initial_inference ( self , x , mask = None ) : [EOL] [docstring] [EOL] [comment] [EOL] state = self . representation . inference ( x ) [EOL] policy_logit , value = self . prediction . inference ( state , mask ) [EOL] [comment] [EOL] return state , policy_logit , value [EOL] [EOL] def recurrent_inference ( self , x ) : [EOL] [docstring] [EOL] [comment] [EOL] next_state , reward = self . dynamics . inference ( x ) [EOL] policy_logit , value = self . prediction . inference ( next_state ) [EOL] [comment] [EOL] return next_state , reward , policy_logit , value [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $RL.TicTacToe.agent.MuZero.network.Representation$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $RL.TicTacToe.agent.MuZero.network.Prediction$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $RL.TicTacToe.agent.MuZero.network.Dynamics$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , List , Any [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] import numpy as np [EOL] import logging [EOL] [EOL] from TicTacToe import TicTacToeEnv [EOL] from logger import setup_logger [EOL] [EOL] logger = setup_logger ( __name__ , logging . INFO ) [EOL] [EOL] [EOL] class GameBuffer : [EOL] def __init__ ( self , obs , player , discount ) : [EOL] self . observations = [ obs ] [EOL] self . players = [ player ] [EOL] self . actions = [ ] [EOL] self . child_visits = [ ] [EOL] self . values = [ ] [EOL] self . rewards = [ ] [EOL] self . winner = TicTacToeEnv . EMPTY [EOL] self . num_actions = [number] [EOL] self . discount = discount [EOL] [EOL] def append ( self , obs , player , action ) : [EOL] self . observations . append ( obs ) [EOL] self . players . append ( player ) [EOL] self . actions . append ( action ) [EOL] self . values . append ( [number] ) [EOL] self . rewards . append ( [number] ) [EOL] [EOL] def store_search_statistics ( self , root ) : [EOL] total_visit = sum ( [ edge . visit_count for edge in root . edges ] ) [EOL] edge_map = { edge . action : edge . visit_count for edge in root . edges } [EOL] child_visit = [ edge_map [ action ] / total_visit if action in edge_map else [number] for action in range ( self . num_actions ) ] [EOL] [comment] [EOL] self . child_visits . append ( child_visit ) [EOL] [EOL] def set_winner ( self , wineer ) : [EOL] self . winner = wineer [EOL] self . values . append ( [number] ) [EOL] self . child_visits . append ( [ [number] / self . num_actions for _ in range ( self . num_actions ) ] ) [EOL] for i in range ( len ( self . observations ) ) : [EOL] if self . winner != [number] : [EOL] player = self . players [ i ] [EOL] value = + [number] if self . winner == player else - [number] [EOL] value *= self . discount ** ( len ( self . observations ) - ( i + [number] ) ) [EOL] else : [EOL] value = [number] [EOL] self . values [ i ] = value [EOL] [EOL] if wineer != [number] : [EOL] if self . players [ - [number] ] != wineer : [EOL] self . rewards [ - [number] ] = + [number] [EOL] else : [EOL] self . rewards [ - [number] ] = - [number] [EOL] [EOL] def make_target ( self , state_index , unroll_steps ) : [EOL] return list ( zip ( self . child_visits [ state_index : state_index + unroll_steps + [number] ] , self . values [ state_index : state_index + unroll_steps + [number] ] , [ [number] ] + self . rewards [ state_index : state_index + unroll_steps ] , ) ) [EOL] [EOL] def print_buffer ( self ) : [EOL] print ( [string] ) [EOL] print ( [string] , self . observations ) [EOL] print ( [string] , self . players ) [EOL] print ( [string] , self . actions ) [EOL] print ( [string] , self . values ) [EOL] print ( [string] , self . rewards ) [EOL] print ( [string] , self . child_visits ) [EOL] print ( [string] , self . winner ) [EOL] print ( [string] ) [EOL] [EOL] [EOL] class ReplayBuffer : [EOL] def __init__ ( self , window_size , batch_size , unroll_steps ) : [EOL] self . window_size = window_size [EOL] self . batch_size = batch_size [EOL] self . buffer = [ ] [EOL] self . unroll_steps = unroll_steps [EOL] [EOL] def append ( self , game ) : [EOL] if len ( self . buffer ) >= self . window_size : [EOL] self . buffer . pop ( [number] ) [EOL] self . buffer . append ( game ) [EOL] [EOL] def sample_batch ( self ) : [EOL] games = np . random . choice ( self . buffer , self . batch_size ) [EOL] game_pos = [ ( g , np . random . randint ( len ( g . observations ) ) ) for g in games ] [EOL] return [ ( g . observations [ i ] , g . actions [ i : i + self . unroll_steps ] , g . make_target ( i , self . unroll_steps ) ) for ( g , i ) in game_pos ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.dict$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[GameBuffer]$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0
from typing import Dict , Any [EOL] import typing [EOL] import builtins [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from torch . distributions import Categorical [EOL] import numpy as np [EOL] import os [EOL] [EOL] [EOL] class Agent : [EOL] def __init__ ( self , p_net , v_net , optim_p , optim_v , device ) : [EOL] super ( Agent , self ) . __init__ ( ) [EOL] self . p_net = p_net [EOL] self . v_net = v_net [EOL] self . optim_p = optim_p [EOL] self . optim_v = optim_v [EOL] self . device = device [EOL] [EOL] def get_action ( self , obs ) : [EOL] obs = obs . long ( ) [EOL] with torch . no_grad ( ) : [EOL] pi = self . p_net ( obs ) [EOL] value = self . v_net ( obs ) [EOL] c = Categorical ( pi ) [EOL] action = c . sample ( ) [EOL] [comment] [EOL] return action . squeeze ( ) . item ( ) , value . item ( ) , c . log_prob ( action ) . item ( ) , pi . detach ( ) . cpu ( ) . numpy ( ) [EOL] [EOL] def get_llhs ( self , obs , actions ) : [EOL] with torch . no_grad ( ) : [EOL] pi = self . p_net ( obs ) [EOL] llhs = Categorical ( pi ) . log_prob ( actions ) [EOL] return llhs . detach ( ) [EOL] [EOL] def update ( self , traj , batch_size , epochs , clip_param , v_loss_c , ent_c , max_grad_norm ) : [EOL] iterate = traj . iterate ( batch_size , epochs ) [EOL] loss_vals = [ ] [EOL] for batch in iterate : [EOL] loss_vals . append ( self . _update_batch ( batch , clip_param , v_loss_c , ent_c , max_grad_norm ) ) [EOL] [EOL] loss_vals = np . mean ( loss_vals , axis = [number] ) [EOL] return loss_vals [EOL] [EOL] def save_model ( self , filename , info , suffix ) : [EOL] if suffix : [EOL] filename += [string] + suffix [EOL] filename += [string] [EOL] print ( [string] . format ( filename ) ) [EOL] save_data = { [string] : self . p_net . state_dict ( ) , [string] : self . v_net . state_dict ( ) , [string] : info } [EOL] torch . save ( save_data , filename ) [EOL] [EOL] def load_model ( self , filename , suffix ) : [EOL] if suffix : [EOL] filename += [string] + suffix [EOL] filename += [string] [EOL] if not os . path . exists ( filename ) : [EOL] return None [EOL] print ( [string] . format ( filename ) ) [EOL] load_data = torch . load ( filename , map_location = self . device ) [EOL] self . p_net . load_state_dict ( load_data [ [string] ] ) [EOL] self . v_net . load_state_dict ( load_data [ [string] ] ) [EOL] return load_data [ [string] ] [EOL] [EOL] def train ( self ) : [EOL] self . p_net . train ( ) [EOL] self . v_net . train ( ) [EOL] [EOL] def eval ( self ) : [EOL] self . p_net . eval ( ) [EOL] self . v_net . eval ( ) [EOL] [EOL] def _update_batch ( self , batch , clip_param , v_loss_c , ent_c , max_grad_norm ) : [EOL] obs = batch [ [string] ] . long ( ) [EOL] actions = batch [ [string] ] [EOL] llhs = batch [ [string] ] [EOL] advantages = batch [ [string] ] [EOL] returns = batch [ [string] ] [EOL] [EOL] pol_loss , entropy = self . _update_pol ( obs , actions , llhs , advantages , clip_param , ent_c , max_grad_norm ) [EOL] v_loss = self . _update_val ( obs , returns , v_loss_c , max_grad_norm ) [EOL] [EOL] return pol_loss , v_loss , entropy [EOL] [EOL] def _update_pol ( self , obs , actions , llhs , advantages , clip_param , ent_c , max_grad_norm ) : [EOL] [comment] [EOL] pi = self . p_net ( obs ) [EOL] c = Categorical ( pi ) [EOL] new_llhs = c . log_prob ( actions ) [EOL] ratio = torch . exp ( new_llhs - llhs ) [comment] [EOL] clip_ratio = ratio . clamp ( [number] - clip_param , [number] + clip_param ) [EOL] pol_loss = ( torch . max ( - ratio * advantages , - clip_ratio * advantages ) ) . mean ( ) [EOL] [comment] [EOL] entropy = - ent_c * c . entropy ( ) . mean ( ) [EOL] [EOL] total_loss = pol_loss + entropy [EOL] self . optim_p . zero_grad ( ) [EOL] total_loss . backward ( ) [EOL] torch . nn . utils . clip_grad_norm_ ( self . p_net . parameters ( ) , max_grad_norm ) [EOL] self . optim_p . step ( ) [EOL] [EOL] return pol_loss . item ( ) , entropy . item ( ) [EOL] [EOL] def _update_val ( self , obs , returns , v_loss_c , max_grad_norm ) : [EOL] [comment] [EOL] value = self . v_net ( obs ) [EOL] v_loss = v_loss_c * F . smooth_l1_loss ( value . squeeze ( [number] ) , returns ) . mean ( ) [EOL] [EOL] self . optim_v . zero_grad ( ) [EOL] v_loss . backward ( ) [EOL] torch . nn . utils . clip_grad_norm_ ( self . v_net . parameters ( ) , max_grad_norm ) [EOL] self . optim_v . step ( ) [EOL] [EOL] return v_loss . item ( ) [EOL] [EOL] [EOL] class Discriminator : [EOL] def __init__ ( self , pseudo_rew_net , shaping_val_net , optim_discrim , device ) : [EOL] self . pseudo_rew_net = pseudo_rew_net [EOL] self . shaping_val_net = shaping_val_net [EOL] self . optim_discrim = optim_discrim [EOL] self . device = device [EOL] [EOL] def get_pseudo_reward ( self , obs ) : [EOL] obs = obs . long ( ) [EOL] with torch . no_grad ( ) : [EOL] reward = self . pseudo_rew_net ( obs ) [EOL] return reward . item ( ) [EOL] [EOL] def update ( self , agent_traj , expert_traj , batch_size , gamma , agent ) : [EOL] agent_iterate = agent_traj . iterate ( batch_size , epoch = [number] ) [EOL] expert_iterate = expert_traj . iterate ( batch_size , epoch = [number] ) [EOL] loss_vals = [ ] [EOL] for agent_batch , expert_batch in zip ( agent_iterate , expert_iterate ) : [EOL] loss_vals . append ( self . _update_batch ( agent_batch , expert_batch , gamma , agent ) ) [EOL] [EOL] loss_vals = np . mean ( loss_vals , axis = [number] ) [EOL] return loss_vals [EOL] [EOL] def save_model ( self , filename , suffix ) : [EOL] if suffix : [EOL] filename += [string] + suffix [EOL] filename += [string] [EOL] print ( [string] . format ( filename ) ) [EOL] save_data = { [string] : self . pseudo_rew_net . state_dict ( ) , [string] : self . shaping_val_net . state_dict ( ) , } [EOL] torch . save ( save_data , filename ) [EOL] [EOL] def load_model ( self , filename , suffix ) : [EOL] if suffix : [EOL] filename += [string] + suffix [EOL] filename += [string] [EOL] if not os . path . exists ( filename ) : [EOL] return None [EOL] print ( [string] . format ( filename ) ) [EOL] load_data = torch . load ( filename , map_location = self . device ) [EOL] self . pseudo_rew_net . load_state_dict ( load_data [ [string] ] ) [EOL] self . shaping_val_net . load_state_dict ( load_data [ [string] ] ) [EOL] [EOL] def train ( self ) : [EOL] self . pseudo_rew_net . train ( ) [EOL] self . shaping_val_net . train ( ) [EOL] [EOL] def eval ( self ) : [EOL] self . pseudo_rew_net . eval ( ) [EOL] self . shaping_val_net . eval ( ) [EOL] [EOL] def _update_batch ( self , agent_batch , expert_batch , gamma , agent ) : [EOL] agent_loss = self . _calc_airl_loss ( agent_batch , gamma , agent , True ) [EOL] expert_loss = self . _calc_airl_loss ( expert_batch , gamma , agent , False ) [EOL] discrim_loss = agent_loss + expert_loss [EOL] [EOL] self . optim_discrim . zero_grad ( ) [EOL] discrim_loss . backward ( ) [EOL] self . optim_discrim . step ( ) [EOL] [EOL] return agent_loss . item ( ) , expert_loss . item ( ) [EOL] [EOL] def _calc_airl_loss ( self , batch , gamma , agent , is_agent ) : [EOL] obs = batch [ [string] ] . long ( ) [EOL] actions = batch [ [string] ] [EOL] next_obs = batch [ [string] ] . long ( ) [EOL] dones = batch [ [string] ] [EOL] [EOL] value = self . shaping_val_net ( obs ) . squeeze ( [number] ) [EOL] next_value = self . shaping_val_net ( next_obs ) . squeeze ( [number] ) [EOL] reward = self . pseudo_rew_net ( obs ) . squeeze ( [number] ) [EOL] energies = reward + ( [number] - dones ) * gamma * next_value - value [EOL] llhs = agent . get_llhs ( obs , actions ) [EOL] logits = energies - llhs [EOL] [EOL] if is_agent : [EOL] target = torch . zeros ( len ( logits ) ) . to ( self . device ) [EOL] else : [EOL] target = torch . ones ( len ( logits ) ) . to ( self . device ) [EOL] discrim_loss = [number] * F . binary_cross_entropy_with_logits ( logits , target ) [EOL] return discrim_loss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.float$ 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] [EOL] [EOL] class Swish ( nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( Swish , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] return x * torch . sigmoid ( x ) [EOL] [EOL] [EOL] class PNet ( nn . Module ) : [EOL] def __init__ ( self , observation_space , action_space , hid_num ) : [EOL] super ( PNet , self ) . __init__ ( ) [EOL] [EOL] self . in_features = observation_space . n [EOL] out_features = action_space . n [EOL] [EOL] [comment] [EOL] self . layer = nn . Sequential ( nn . Linear ( self . in_features , hid_num ) , Swish ( ) , nn . Linear ( hid_num , hid_num ) , Swish ( ) , nn . Linear ( hid_num , out_features ) , nn . Softmax ( dim = - [number] ) , ) [EOL] [EOL] [comment] [EOL] for m in self . modules ( ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . kaiming_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = F . one_hot ( x , num_classes = self . in_features ) . float ( ) [comment] [EOL] return self . layer ( x ) [EOL] [EOL] [EOL] class VNet ( nn . Module ) : [EOL] def __init__ ( self , observation_space , hid_num ) : [EOL] super ( VNet , self ) . __init__ ( ) [EOL] [EOL] self . in_features = observation_space . n [EOL] [EOL] [comment] [EOL] self . layer = nn . Sequential ( nn . Linear ( self . in_features , hid_num ) , Swish ( ) , nn . Linear ( hid_num , hid_num ) , Swish ( ) , nn . Linear ( hid_num , [number] ) ) [EOL] [EOL] [comment] [EOL] for m in self . modules ( ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . kaiming_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = F . one_hot ( x , num_classes = self . in_features ) . float ( ) [comment] [EOL] return self . layer ( x ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from typing import Dict , List , Any [EOL] import typing [EOL] import torch [EOL] import numpy as np [EOL] import gym [EOL] import os [EOL] import pickle [EOL] from absl import app [EOL] from absl import flags [EOL] [EOL] [comment] [EOL] from model import PNet , VNet [EOL] import ralamb [EOL] from trajectory import Buffer , Trajectory [EOL] from agent import Agent , Discriminator [EOL] [EOL] [EOL] FLAGS = flags . FLAGS [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] flags . DEFINE_bool ( [string] , True , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_float ( [string] , [number] , [string] ) [EOL] flags . DEFINE_bool ( [string] , False , [string] ) [EOL] [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_bool ( [string] , True , [string] ) [EOL] [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] [comment] [EOL] flags . DEFINE_bool ( [string] , True , [string] ) [EOL] [EOL] [EOL] def create_directory ( dir_name ) : [EOL] if not os . path . exists ( dir_name ) : [EOL] print ( [string] . format ( dir_name ) ) [EOL] os . mkdir ( dir_name ) [EOL] [EOL] [EOL] def get_device ( use_gpu ) : [EOL] [comment] [EOL] if use_gpu : [EOL] print ( [string] ) [EOL] device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] else : [EOL] device = torch . device ( [string] ) [EOL] if device == [string] : [EOL] torch . backends . cudnn . benchmark = True [EOL] return device [EOL] [EOL] [EOL] def train ( env , agent , max_rew , model_filename_base , device , discrim , discrim_filename_base , expert_traj ) : [EOL] print ( [string] ) [EOL] try : [EOL] for epoch in range ( FLAGS . n_train_epochs ) : [EOL] [comment] [EOL] agent . eval ( ) [EOL] traj = Trajectory ( ) [EOL] episode_rewards = [ ] [EOL] if discrim is not None : [EOL] real_episode_rewards = [ ] [EOL] for episode in range ( FLAGS . n_sample_episodes ) : [EOL] ob = env . reset ( ) [EOL] done = False [EOL] buf = Buffer ( ) [EOL] while not done : [EOL] ob_tsr = torch . from_numpy ( np . array ( ob ) ) . to ( device = device ) [comment] [EOL] action , value , llh , _ = agent . get_action ( ob_tsr ) [EOL] next_ob , reward , done , info = env . step ( action ) [EOL] buf_data = dict ( obs = ob , actions = action , rewards = reward , values = value , llhs = llh ) [EOL] [EOL] if discrim is not None : [EOL] pseudo_rew = discrim . get_pseudo_reward ( ob_tsr ) [EOL] buf_data [ [string] ] = buf_data [ [string] ] [EOL] buf_data [ [string] ] = pseudo_rew [EOL] buf_data [ [string] ] = next_ob [EOL] buf_data [ [string] ] = done [EOL] [EOL] buf . append ( buf_data ) [EOL] ob = next_ob [EOL] [EOL] buf . finish_path ( FLAGS . gamma , FLAGS . lam ) [EOL] traj . append ( buf . data_map ) [EOL] episode_rewards . append ( buf . data_map [ [string] ] . sum ( ) ) [EOL] [EOL] if discrim is not None : [EOL] real_episode_rewards . append ( buf . data_map [ [string] ] . sum ( ) ) [EOL] [EOL] [comment] [EOL] agent . train ( ) [EOL] traj . finish_path ( FLAGS . eps , device ) [EOL] pol_loss , v_loss , entropy = agent . update ( traj , FLAGS . batch_size , FLAGS . n_opt_epochs , FLAGS . clip_param , FLAGS . v_loss_c , FLAGS . ent_c , FLAGS . max_grad_norm , ) [EOL] if discrim is not None : [EOL] discrim . train ( ) [EOL] discrim_a_loss , discrim_e_loss = discrim . update ( traj , expert_traj , FLAGS . batch_size , FLAGS . gamma , agent ) [EOL] [EOL] mean_rew = np . mean ( episode_rewards ) [EOL] print ( [string] . format ( epoch + [number] , FLAGS . n_train_epochs ) , end = [string] ) [EOL] if discrim is not None : [EOL] print ( [string] . format ( pol_loss , v_loss , entropy , discrim_a_loss , discrim_e_loss ) , end = [string] , ) [EOL] else : [EOL] print ( [string] . format ( pol_loss , v_loss , entropy ) , end = [string] ) [EOL] reward_format = [string] [EOL] print ( reward_format . format ( mean_rew , int ( np . min ( episode_rewards ) ) , int ( np . max ( episode_rewards ) ) ) , end = [string] ) [EOL] if discrim is not None : [EOL] mean_real_rew = np . mean ( real_episode_rewards ) [EOL] reward_format = [string] [EOL] print ( reward_format . format ( mean_real_rew , np . min ( real_episode_rewards ) , np . max ( real_episode_rewards ) ) , end = [string] , ) [EOL] print ( ) [EOL] [EOL] if ( ( ( epoch + [number] ) % [number] ) == [number] ) and ( mean_rew > max_rew ) : [EOL] print ( [string] . format ( max_rew , mean_rew ) ) [EOL] max_rew = mean_rew [EOL] print ( [string] ) [EOL] save_info = dict ( max_rew = max_rew ) [EOL] agent . save_model ( model_filename_base , save_info , [string] ) [EOL] [EOL] if discrim is not None : [EOL] discrim . save_model ( discrim_filename_base , [string] ) [EOL] [EOL] if ( ( epoch + [number] ) % [number] ) == [number] : [EOL] print ( [string] ) [EOL] save_info = dict ( max_rew = max_rew ) [EOL] agent . save_model ( model_filename_base , save_info , [string] ) [EOL] [EOL] if discrim is not None : [EOL] discrim . save_model ( discrim_filename_base , [string] ) [EOL] [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] [EOL] print ( [string] ) [EOL] save_info = dict ( max_rew = max_rew ) [EOL] agent . save_model ( model_filename_base , save_info , [string] ) [EOL] [EOL] if discrim is not None : [EOL] discrim . save_model ( discrim_filename_base , [string] ) [EOL] [EOL] [EOL] def test ( env , agent , device ) : [EOL] print ( [string] ) [EOL] try : [EOL] [comment] [EOL] agent . eval ( ) [EOL] test_reward = [ ] [EOL] for epoch in range ( FLAGS . n_test_epochs ) : [EOL] ob = env . reset ( ) [EOL] done = False [EOL] step = [number] [EOL] episode_reward = [number] [EOL] if FLAGS . render_env : [EOL] print ( [string] . format ( epoch + [number] , step ) + [string] * [number] ) [EOL] env . render ( ) [EOL] [EOL] while not done : [EOL] ob_tsr = torch . from_numpy ( np . array ( ob ) ) . to ( device = device ) [EOL] action , value , _ , pi = agent . get_action ( ob_tsr ) [EOL] next_ob , reward , done , info = env . step ( action ) [EOL] episode_reward += reward [EOL] ob = next_ob [EOL] [EOL] step += [number] [EOL] if FLAGS . render_env : [EOL] print ( [string] . format ( epoch + [number] , step ) + [string] * [number] ) [EOL] env . render ( ) [EOL] print ( info , ( pi * [number] ) . astype ( int ) , [string] . format ( value ) ) [EOL] [EOL] episode_reward = np . mean ( episode_reward ) [EOL] test_reward . append ( episode_reward ) [EOL] print ( [string] . format ( epoch + [number] ) , end = [string] ) [EOL] print ( [string] . format ( episode_reward ) , end = [string] ) [EOL] print ( ) [EOL] print ( [string] . format ( np . mean ( test_reward ) ) ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] [EOL] [EOL] def main ( _ ) : [EOL] device = get_device ( FLAGS . use_gpu ) [EOL] print ( [string] . format ( device ) ) [EOL] [EOL] [comment] [EOL] data_dir = FLAGS . data_dir [EOL] create_directory ( data_dir ) [EOL] create_directory ( os . path . join ( data_dir , [string] ) ) [EOL] [EOL] env = gym . make ( FLAGS . env ) [EOL] [EOL] p_net = PNet ( env . observation_space , env . action_space , FLAGS . hid_num ) [EOL] v_net = VNet ( env . observation_space , FLAGS . hid_num ) [EOL] print ( p_net ) [EOL] print ( v_net ) [EOL] p_net . to ( device ) [EOL] v_net . to ( device ) [EOL] optim_p = ralamb . Ralamb ( p_net . parameters ( ) , lr = FLAGS . lr , weight_decay = FLAGS . weight_decay ) [EOL] optim_v = ralamb . Ralamb ( v_net . parameters ( ) , lr = FLAGS . lr , weight_decay = FLAGS . weight_decay ) [EOL] agent = Agent ( p_net , v_net , optim_p , optim_v , device ) [EOL] [EOL] if FLAGS . use_discrim : [EOL] expert_filename = os . path . join ( FLAGS . data_dir , [string] , [string] ) [EOL] print ( [string] , expert_filename ) [EOL] with open ( expert_filename , [string] ) as f : [EOL] expert_traj = Trajectory ( ) [EOL] expert_epis = pickle . load ( f ) [EOL] for epi in expert_epis : [EOL] epi [ [string] ] = np . append ( epi [ [string] ] [ [number] : ] , epi [ [string] ] [ [number] ] ) [EOL] expert_traj . append ( epi ) [EOL] expert_traj . to_tensor ( device ) [EOL] [EOL] pseudo_rew_net = VNet ( env . observation_space , FLAGS . hid_num ) [EOL] shaping_val_net = VNet ( env . observation_space , FLAGS . hid_num ) [EOL] print ( pseudo_rew_net ) [EOL] print ( shaping_val_net ) [EOL] pseudo_rew_net . to ( device ) [EOL] shaping_val_net . to ( device ) [EOL] optim_discrim = ralamb . Ralamb ( list ( pseudo_rew_net . parameters ( ) ) + list ( shaping_val_net . parameters ( ) ) , lr = FLAGS . lr , weight_decay = FLAGS . weight_decay , ) [EOL] discrim = Discriminator ( pseudo_rew_net , shaping_val_net , optim_discrim , device ) [EOL] else : [EOL] discrim = None [EOL] expert_traj = None [EOL] [EOL] [comment] [EOL] max_rew = - [number] [EOL] model_filename_base = os . path . join ( FLAGS . data_dir , [string] , [string] + FLAGS . env + [string] + str ( FLAGS . hid_num ) ) [EOL] discrim_filename_base = None [EOL] if FLAGS . resume : [EOL] print ( [string] ) [EOL] load_info = agent . load_model ( model_filename_base , [string] ) [EOL] if load_info : [EOL] max_rew = load_info [ [string] ] [EOL] print ( [string] . format ( max_rew ) ) [EOL] else : [EOL] print ( [string] ) [EOL] [EOL] if FLAGS . use_discrim : [EOL] discrim_filename_base = os . path . join ( FLAGS . data_dir , [string] , [string] + FLAGS . env + [string] + str ( FLAGS . hid_num ) ) [EOL] discrim . load_model ( discrim_filename_base , [string] ) [EOL] [EOL] train ( env , agent , max_rew , model_filename_base , device , discrim , discrim_filename_base , expert_traj ) [EOL] test ( env , agent , device ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] app . run ( main ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import typing [EOL] import builtins [EOL] import torch [EOL] from torch . utils . data . sampler import BatchSampler , SubsetRandomSampler [EOL] import numpy as np [EOL] [EOL] [EOL] class Buffer : [EOL] def __init__ ( self ) : [EOL] self . _data_map = dict ( ) [EOL] [EOL] [comment] [EOL] def append ( self , data ) : [EOL] for key in data . keys ( ) : [EOL] if key not in self . _data_map . keys ( ) : [EOL] self . _data_map [ key ] = [ data [ key ] ] [EOL] else : [EOL] self . _data_map [ key ] . append ( data [ key ] ) [EOL] [EOL] def finish_path ( self , gamma , lam ) : [EOL] self . to_ndarray ( ) [EOL] values = np . append ( self . data_map [ [string] ] , [number] ) [EOL] last_delta = [number] [EOL] last_rew = [number] [EOL] rewards = self . _data_map [ [string] ] [EOL] reward_size = len ( rewards ) [EOL] returns = np . zeros ( reward_size ) [EOL] advantages = np . zeros ( reward_size ) [EOL] for t in reversed ( range ( reward_size ) ) : [EOL] [comment] [EOL] [comment] [EOL] delta = rewards [ t ] + gamma * values [ t + [number] ] - values [ t ] [EOL] last_delta = delta + ( gamma * lam ) * last_delta [EOL] advantages [ t ] = last_delta [EOL] [EOL] [comment] [EOL] last_rew = rewards [ t ] + gamma * last_rew [EOL] returns [ t ] = last_rew [EOL] self . _data_map [ [string] ] = advantages [EOL] self . _data_map [ [string] ] = returns [EOL] [EOL] def to_ndarray ( self ) : [EOL] [comment] [EOL] for key in self . _data_map . keys ( ) : [EOL] self . _data_map [ key ] = np . array ( self . _data_map [ key ] , dtype = float ) [EOL] [EOL] @ property def data_map ( self ) : [EOL] return self . _data_map [EOL] [EOL] [EOL] class Trajectory : [EOL] def __init__ ( self ) : [EOL] self . _data_map = dict ( ) [EOL] self . total_step = [number] [EOL] [EOL] def append ( self , data_map ) : [EOL] for key in data_map . keys ( ) : [EOL] if key not in self . _data_map . keys ( ) : [EOL] self . _data_map [ key ] = data_map [ key ] [EOL] else : [EOL] self . _data_map [ key ] = np . concatenate ( [ self . _data_map [ key ] , data_map [ key ] ] ) [EOL] [EOL] self . total_step += len ( data_map [ [string] ] ) [EOL] [EOL] def finish_path ( self , eps , device ) : [EOL] self . to_tensor ( device ) [EOL] [comment] [EOL] advs = self . _data_map [ [string] ] [EOL] self . _data_map [ [string] ] = ( advs - advs . mean ( ) ) / ( advs . std ( ) + eps ) [EOL] [EOL] def iterate ( self , batch_size , epoch , drop_last = False ) : [EOL] sampler = BatchSampler ( SubsetRandomSampler ( range ( self . total_step ) ) , batch_size , drop_last = drop_last ) [EOL] for _ in range ( epoch ) : [EOL] for indices in sampler : [EOL] batch = dict ( ) [EOL] for k in self . _data_map . keys ( ) : [EOL] batch [ k ] = self . _data_map [ k ] [ indices ] [EOL] yield batch [EOL] [EOL] def to_tensor ( self , device ) : [EOL] [comment] [EOL] for key in self . _data_map . keys ( ) : [EOL] self . _data_map [ key ] = torch . tensor ( self . _data_map [ key ] , dtype = torch . float , device = device ) [EOL] [EOL] @ property def data_map ( self ) : [EOL] return self . _data_map [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import gym [EOL] import numpy as np [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from torch . distributions import Categorical [EOL] import pickle [EOL] import os [EOL] import math [EOL] import time [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def calc_temperature ( step , max_t , min_t , tau ) : [EOL] return ( max_t - min_t ) * math . exp ( - step / tau ) + min_t [EOL] [EOL] [EOL] def sarsa ( q_value , ob , action , reward , next_ob , next_action , eta , gamma ) : [EOL] q_value [ ob , action ] = ( [number] - eta ) * q_value [ ob , action ] + eta * ( reward + gamma * q_value [ next_ob , next_action ] ) [EOL] return q_value [EOL] [EOL] [EOL] def get_action ( q_value , ob , step , max_t , min_t , tau ) : [EOL] t = calc_temperature ( step , max_t , min_t , tau ) [EOL] pi = F . softmax ( torch . from_numpy ( q_value [ ob , : ] ) / t , dim = [number] ) [EOL] action = Categorical ( pi ) . sample ( ) . item ( ) [EOL] return action [EOL] [EOL] [EOL] def get_max_action ( q_value , ob ) : [EOL] return np . argmax ( q_value [ ob ] ) [EOL] [EOL] [EOL] def one_epi ( env , q_value , eta , gamma , start_step , max_t , min_t , tau ) : [EOL] done = False [EOL] episode_reward = [number] [EOL] rewards = [ ] [EOL] step = start_step [EOL] [EOL] ob = env . reset ( ) [EOL] action = get_action ( q_value , ob , step , max_t , min_t , tau ) [EOL] while not done : [EOL] next_ob , reward , done , info = env . step ( action ) [EOL] next_action = get_action ( q_value , next_ob , step , max_t , min_t , tau ) [EOL] [comment] [EOL] q_value = sarsa ( q_value , ob , action , reward , next_ob , next_action , eta , gamma ) [EOL] [EOL] rewards . append ( reward ) [EOL] episode_reward += reward [EOL] ob = next_ob [EOL] action = next_action [EOL] step += [number] [EOL] return q_value , episode_reward , len ( rewards ) [EOL] [EOL] [EOL] def one_epi_test ( env , q_value ) : [EOL] done = False [EOL] episode_reward = [number] [EOL] obs = [ ] [EOL] actions = [ ] [EOL] rewards = [ ] [EOL] dones = [ ] [EOL] [EOL] ob = env . reset ( ) [EOL] while not done : [EOL] action = get_max_action ( q_value , ob ) [EOL] next_ob , reward , done , info = env . step ( action ) [EOL] [EOL] obs . append ( ob ) [EOL] actions . append ( action ) [EOL] rewards . append ( reward ) [EOL] dones . append ( done ) [EOL] episode_reward += reward [EOL] ob = next_ob [EOL] return ( episode_reward , len ( obs ) , dict ( obs = obs , actions = actions , rewards = rewards , dones = dones ) ) [EOL] [EOL] [EOL] def train ( env , n_episode , q_value , eta , gamma , max_t , min_t , tau , log_interval ) : [EOL] try : [EOL] start_step = [number] [EOL] start_time = time . perf_counter ( ) [EOL] total_time = [number] [EOL] for episode in range ( n_episode ) : [EOL] q_value , episode_reward , epi_len = one_epi ( env , q_value , eta , gamma , start_step , max_t , min_t , tau ) [EOL] start_step += epi_len [EOL] if ( ( episode + [number] ) % log_interval ) == [number] : [EOL] elapsed_time = time . perf_counter ( ) - start_time [EOL] total_time += elapsed_time [EOL] start_time = time . perf_counter ( ) [EOL] print ( [string] . format ( episode + [number] , n_episode , episode_reward , epi_len , elapsed_time ) ) [EOL] print ( [string] . format ( total_time , total_time / n_episode ) ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] [EOL] return q_value [EOL] [EOL] [EOL] def test ( env , n_test_episode , q_value ) : [EOL] try : [EOL] epis = [ ] [EOL] start_time = time . perf_counter ( ) [EOL] total_time = [number] [EOL] for episode in range ( n_test_episode ) : [EOL] episode_reward , epi_len , epi = one_epi_test ( env , q_value ) [EOL] [EOL] if ( episode_reward > [number] ) and ( epi_len < [number] ) : [EOL] epis . append ( epi ) [EOL] [EOL] elapsed_time = time . perf_counter ( ) - start_time [EOL] total_time += elapsed_time [EOL] start_time = time . perf_counter ( ) [EOL] print ( [string] . format ( episode + [number] , n_test_episode , episode_reward , epi_len , elapsed_time ) ) [EOL] print ( [string] . format ( total_time , total_time / n_test_episode ) ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] [EOL] return epis [EOL] [EOL] [EOL] def main ( ) : [EOL] is_train = True [EOL] is_test = True [EOL] [EOL] env = gym . make ( [string] ) [EOL] ob_num = env . observation_space . n [EOL] ac_num = env . action_space . n [EOL] [EOL] eta = [number] [comment] [EOL] gamma = [number] [comment] [EOL] n_episode = [number] [EOL] n_test_episode = [number] [EOL] log_interval = [number] [EOL] [EOL] [comment] [EOL] max_t = [number] [EOL] min_t = [number] [EOL] tau = ( n_episode / [number] ) * [number] [EOL] [EOL] q_value = np . zeros ( ( ob_num , ac_num ) ) [EOL] filename = [string] [EOL] if os . path . exists ( filename ) : [EOL] with open ( filename , [string] ) as f : [EOL] print ( [string] ) [EOL] q_value = pickle . load ( f ) [EOL] [EOL] if is_train : [EOL] q_value = train ( env , n_episode , q_value , eta , gamma , max_t , min_t , tau , log_interval ) [EOL] [EOL] with open ( filename , [string] ) as f : [EOL] print ( [string] ) [EOL] pickle . dump ( q_value , f ) [EOL] [EOL] for ob in range ( ob_num ) : [EOL] print ( [string] . format ( ob ) , end = [string] ) [EOL] for ac in range ( ac_num ) : [EOL] print ( [string] . format ( q_value [ ob , ac ] ) , end = [string] ) [EOL] print ( ) [EOL] [EOL] if is_test : [EOL] [comment] [EOL] [comment] [EOL] expert_filename = [string] [EOL] [EOL] expert_epis = test ( env , n_test_episode , q_value ) [EOL] [EOL] if len ( expert_epis ) > ( n_test_episode * [number] ) : [EOL] with open ( expert_filename , [string] ) as f : [EOL] print ( [string] . format ( len ( expert_epis ) ) ) [EOL] pickle . dump ( expert_epis , f ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import ctypes [EOL] import typing [EOL] def _windows_enable_ANSI ( std_id = [number] ) : [EOL] [comment] [EOL] from ctypes import byref , POINTER , windll , WINFUNCTYPE [EOL] from ctypes . wintypes import BOOL , DWORD , HANDLE [EOL] [EOL] GetStdHandle = WINFUNCTYPE ( HANDLE , DWORD ) ( ( [string] , windll . kernel32 ) ) [EOL] GetFileType = WINFUNCTYPE ( DWORD , HANDLE ) ( ( [string] , windll . kernel32 ) ) [EOL] GetConsoleMode = WINFUNCTYPE ( BOOL , HANDLE , POINTER ( DWORD ) ) ( ( [string] , windll . kernel32 ) ) [EOL] SetConsoleMode = WINFUNCTYPE ( BOOL , HANDLE , DWORD ) ( ( [string] , windll . kernel32 ) ) [EOL] [EOL] if std_id == [number] : [comment] [EOL] h = GetStdHandle ( - [number] ) [EOL] elif std_id == [number] : [comment] [EOL] h = GetStdHandle ( - [number] ) [EOL] else : [EOL] return False [EOL] [EOL] if h is None or h == HANDLE ( - [number] ) : [EOL] return False [EOL] [EOL] FILE_TYPE_CHAR = [number] [EOL] if ( GetFileType ( h ) & [number] ) != FILE_TYPE_CHAR : [EOL] return False [EOL] [EOL] mode = DWORD ( ) [EOL] if not GetConsoleMode ( h , byref ( mode ) ) : [EOL] return False [EOL] [EOL] ENABLE_VIRTUAL_TERMINAL_PROCESSING = [number] [EOL] if ( mode . value & ENABLE_VIRTUAL_TERMINAL_PROCESSING ) == [number] : [EOL] SetConsoleMode ( h , mode . value | ENABLE_VIRTUAL_TERMINAL_PROCESSING ) [EOL] return True [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import torch [EOL] import numpy as np [EOL] import gym [EOL] import os [EOL] from absl import app [EOL] from absl import flags [EOL] [EOL] from model import PNet , VNet [EOL] from agent import Agent [EOL] [EOL] FLAGS = flags . FLAGS [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_integer ( [string] , [number] , [string] ) [EOL] flags . DEFINE_bool ( [string] , True , [string] ) [EOL] flags . DEFINE_string ( [string] , [string] , [string] ) [EOL] [EOL] [EOL] def test ( env , agent , device ) : [EOL] print ( [string] ) [EOL] try : [EOL] [comment] [EOL] agent . eval ( ) [EOL] test_reward = [ ] [EOL] for epoch in range ( FLAGS . n_test_epochs ) : [EOL] ob = env . reset ( ) [EOL] done = False [EOL] step = [number] [EOL] episode_reward = [number] [EOL] if FLAGS . render_env : [EOL] print ( [string] . format ( epoch + [number] , step ) + [string] * [number] ) [EOL] env . render ( ) [EOL] [EOL] while not done : [EOL] ob_tsr = torch . from_numpy ( np . array ( ob ) ) . to ( device = device ) [EOL] action , value , _ , pi = agent . get_action ( ob_tsr ) [EOL] next_ob , reward , done , info = env . step ( action ) [EOL] episode_reward += reward [EOL] ob = next_ob [EOL] [EOL] step += [number] [EOL] if FLAGS . render_env : [EOL] print ( [string] . format ( epoch + [number] , step ) + [string] * [number] ) [EOL] env . render ( ) [EOL] print ( info , ( pi * [number] ) . astype ( int ) , [string] . format ( value ) ) [EOL] [EOL] episode_reward = np . mean ( episode_reward ) [EOL] test_reward . append ( episode_reward ) [EOL] print ( [string] . format ( epoch + [number] ) , end = [string] ) [EOL] print ( [string] . format ( episode_reward ) , end = [string] ) [EOL] print ( ) [EOL] print ( [string] . format ( np . mean ( test_reward ) ) ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] [EOL] [EOL] def main ( _ ) : [EOL] device = [string] [EOL] print ( [string] . format ( device ) ) [EOL] [EOL] env = gym . make ( FLAGS . env ) [EOL] [EOL] p_net = PNet ( env . observation_space , env . action_space , FLAGS . hid_num ) [EOL] v_net = VNet ( env . observation_space , FLAGS . hid_num ) [EOL] p_net . to ( device ) [EOL] v_net . to ( device ) [EOL] agent = Agent ( p_net , v_net , None , None , device ) [EOL] [EOL] [comment] [EOL] max_rew = - [number] [EOL] model_filename_base = os . path . join ( FLAGS . data_dir , [string] , [string] + FLAGS . env + [string] + str ( FLAGS . hid_num ) ) [EOL] print ( [string] . format ( model_filename_base ) ) [EOL] load_info = agent . load_model ( model_filename_base , [string] ) [EOL] if load_info : [EOL] max_rew = load_info [ [string] ] [EOL] print ( [string] . format ( max_rew ) ) [EOL] else : [EOL] print ( [string] ) [EOL] exit ( [number] ) [EOL] [EOL] test ( env , agent , device ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] app . run ( main ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Union , Iterable , Any , Tuple , Dict [EOL] import typing [EOL] import builtins [EOL] from typing import Tuple , Iterable [comment] [EOL] [EOL] import math [EOL] [EOL] import torch [EOL] from torch . optim . optimizer import Optimizer [EOL] [EOL] [EOL] class Ralamb ( Optimizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , params , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , weight_decay = [number] , ) : [EOL] [docstring] [EOL] defaults = dict ( lr = lr , betas = betas , eps = eps , weight_decay = weight_decay ) [EOL] self . buffer = [ [ None , None , None ] for ind in range ( [number] ) ] [EOL] super ( Ralamb , self ) . __init__ ( params , defaults ) [comment] [EOL] [EOL] def __setstate__ ( self , state ) : [EOL] [docstring] [EOL] super ( Ralamb , self ) . __setstate__ ( state ) [EOL] [EOL] def step ( self , closure = None ) : [EOL] [docstring] [EOL] loss = None [EOL] if closure is not None : [EOL] loss = closure ( ) [EOL] [EOL] for group in self . param_groups : [EOL] [EOL] for p in group [ [string] ] : [EOL] if p . grad is None : [EOL] continue [EOL] grad = p . grad . data . float ( ) [EOL] if grad . is_sparse : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] p_data_fp32 = p . data . float ( ) [EOL] [EOL] state = self . state [ p ] [EOL] [EOL] if len ( state ) == [number] : [EOL] state [ [string] ] = [number] [EOL] state [ [string] ] = torch . zeros_like ( p_data_fp32 ) [EOL] state [ [string] ] = torch . zeros_like ( p_data_fp32 ) [EOL] else : [EOL] state [ [string] ] = state [ [string] ] . type_as ( p_data_fp32 ) [EOL] state [ [string] ] = state [ [string] ] . type_as ( p_data_fp32 ) [EOL] [EOL] exp_avg , exp_avg_sq = state [ [string] ] , state [ [string] ] [EOL] beta1 , beta2 = group [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] exp_avg . mul_ ( beta1 ) . add_ ( [number] - beta1 , grad ) [EOL] [comment] [EOL] exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( [number] - beta2 , grad , grad ) [EOL] [EOL] state [ [string] ] += [number] [EOL] buffered = self . buffer [ int ( state [ [string] ] % [number] ) ] [EOL] [EOL] if state [ [string] ] == buffered [ [number] ] : [EOL] N_sma , radam_step_size = buffered [ [number] ] , buffered [ [number] ] [EOL] else : [EOL] buffered [ [number] ] = state [ [string] ] [EOL] beta2_t = beta2 ** state [ [string] ] [EOL] N_sma_max = [number] / ( [number] - beta2 ) - [number] [EOL] N_sma = N_sma_max - [number] * state [ [string] ] * beta2_t / ( [number] - beta2_t ) [EOL] buffered [ [number] ] = N_sma [EOL] [EOL] [comment] [EOL] if N_sma >= [number] : [EOL] radam_step_size = math . sqrt ( ( [number] - beta2_t ) * ( N_sma - [number] ) / ( N_sma_max - [number] ) * ( N_sma - [number] ) / N_sma * N_sma_max / ( N_sma_max - [number] ) ) / ( [number] - beta1 ** state [ [string] ] ) [EOL] else : [EOL] radam_step_size = [number] / ( [number] - beta1 ** state [ [string] ] ) [EOL] buffered [ [number] ] = radam_step_size [EOL] [EOL] if group [ [string] ] != [number] : [EOL] p_data_fp32 . add_ ( - group [ [string] ] * group [ [string] ] , p_data_fp32 ) [EOL] [EOL] [comment] [EOL] radam_step = p_data_fp32 . clone ( ) [EOL] if N_sma >= [number] : [EOL] denom = exp_avg_sq . sqrt ( ) . add_ ( group [ [string] ] ) [EOL] radam_step . addcdiv_ ( - radam_step_size * group [ [string] ] , exp_avg , denom ) [EOL] else : [EOL] radam_step . add_ ( - radam_step_size * group [ [string] ] , exp_avg ) [EOL] [EOL] radam_norm = radam_step . pow ( [number] ) . sum ( ) . sqrt ( ) [EOL] weight_norm = p . data . pow ( [number] ) . sum ( ) . sqrt ( ) . clamp ( [number] , [number] ) [EOL] if weight_norm == [number] or radam_norm == [number] : [EOL] trust_ratio = [number] [EOL] else : [EOL] trust_ratio = weight_norm / radam_norm [EOL] [EOL] state [ [string] ] = weight_norm [EOL] state [ [string] ] = radam_norm [EOL] state [ [string] ] = trust_ratio [EOL] [EOL] if N_sma >= [number] : [EOL] p_data_fp32 . addcdiv_ ( - radam_step_size * group [ [string] ] * trust_ratio , exp_avg , denom ) [EOL] else : [EOL] p_data_fp32 . add_ ( - radam_step_size * group [ [string] ] * trust_ratio , exp_avg ) [EOL] [EOL] p . data . copy_ ( p_data_fp32 ) [EOL] [EOL] return loss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable$ 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.float,typing.Tuple[builtins.float,builtins.float]]]$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable$ 0 $typing.Dict[builtins.str,typing.Union[builtins.float,typing.Tuple[builtins.float,builtins.float]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] import gym [EOL] import numpy as np [EOL] [EOL] [EOL] class TaxiObservationWrapper ( gym . ObservationWrapper ) : [EOL] def __init__ ( self , env ) : [EOL] super ( TaxiObservationWrapper , self ) . __init__ ( env ) [EOL] self . observation_space = gym . spaces . MultiDiscrete ( [ [number] , [number] , [number] , [number] ] ) [EOL] [EOL] def observation ( self , observation ) : [EOL] return self . _decode ( observation ) [EOL] [EOL] def _decode ( self , obs ) : [EOL] dest_idx = obs % [number] [EOL] obs = obs // [number] [EOL] pass_idx = obs % [number] [EOL] obs = obs // [number] [EOL] taxi_col = obs % [number] [EOL] obs = obs // [number] [EOL] taxi_row = obs [EOL] return np . array ( [ taxi_row , taxi_col , pass_idx , dest_idx ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0
from typing import Tuple , List , Any [EOL] import numpy [EOL] import typing [EOL] import builtins [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] [EOL] [EOL] class GameBuffer : [EOL] def __init__ ( self ) : [EOL] self . observations = [ ] [EOL] self . actions = [ ] [EOL] self . rewards = [ ] [EOL] self . values = [ ] [EOL] self . policy = [ ] [EOL] [EOL] def append ( self , obs , action , reward , values , policy ) : [EOL] self . observations . append ( obs ) [EOL] self . actions . append ( action ) [EOL] self . rewards . append ( reward ) [EOL] self . values . append ( values ) [EOL] self . policy . append ( policy ) [EOL] [EOL] def print_buffer ( self ) : [EOL] for i , ( obs , action , reward , value , policy ) in enumerate ( zip ( self . observations , self . actions , self . rewards , self . values , self . policy ) ) : [EOL] print ( f" [string] { i + [number] : [string] } [string] { obs } [string] { action } [string] { reward : [string] } [string] { value : [string] } [string] { policy } [string] " ) [EOL] [EOL] def sample_target ( self , num_unroll_steps , td_steps , discount ) : [EOL] pos = np . random . randint ( [number] , len ( self . rewards ) - num_unroll_steps ) [EOL] return ( self . observations [ pos ] , self . actions [ pos : pos + num_unroll_steps ] , self . _make_target ( pos , num_unroll_steps , td_steps , discount ) , ) [EOL] [EOL] def _make_target ( self , pos , num_unroll_steps , td_steps , discount ) : [EOL] target_values = [ ] [EOL] target_rewards = [ ] [EOL] target_policies = [ ] [EOL] for current_pos in range ( pos , pos + num_unroll_steps + [number] ) : [EOL] bootstrap_index = current_pos + td_steps [EOL] if bootstrap_index < len ( self . values ) : [EOL] value = self . values [ bootstrap_index ] * ( discount ** td_steps ) [EOL] else : [EOL] value = [number] [EOL] [EOL] for i , reward in enumerate ( self . rewards [ current_pos : bootstrap_index ] ) : [EOL] value += reward * ( discount ** i ) [EOL] [EOL] if current_pos < len ( self . rewards ) : [EOL] [comment] [EOL] target_values . append ( [ value ] ) [EOL] target_rewards . append ( [ self . rewards [ current_pos ] ] ) [EOL] target_policies . append ( self . policy [ current_pos ] ) [EOL] else : [EOL] [comment] [EOL] target_values . append ( [ [number] ] ) [EOL] target_rewards . append ( [ [number] ] ) [EOL] target_policies . append ( [ ] ) [EOL] return target_values , target_rewards , target_policies [EOL] [EOL] [EOL] class ReplayBuffer : [EOL] def __init__ ( self , window_size , batch_size ) : [EOL] self . window_size = window_size [EOL] self . games = [ ] [EOL] self . batch_size = batch_size [EOL] [EOL] def append ( self , game ) : [EOL] if len ( self . games ) > self . window_size : [EOL] self . games . pop ( [number] ) [EOL] self . games . append ( game ) [EOL] [EOL] def sample_batch ( self , num_unroll_steps , td_steps , discount ) : [EOL] batch = [ ] [EOL] games = np . random . choice ( self . games , self . batch_size ) [EOL] batch = [ game . sample_target ( num_unroll_steps , td_steps , discount ) for game in games ] [EOL] return batch [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[numpy.ndarray]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.float]]$ 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.float$ 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 0 0 0 $typing.Tuple[typing.List[typing.List[builtins.float]],typing.List[typing.List[builtins.float]],typing.List[typing.List[builtins.float]]]$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[GameBuffer]$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $GameBuffer$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0
from typing import Any [EOL] import typing [EOL] import os [EOL] import gym [EOL] from gym . wrappers import RecordEpisodeStatistics [EOL] import numpy as np [EOL] import torch [EOL] [EOL] from env_wrapper import TaxiObservationWrapper [EOL] from network import Network [EOL] from utils import get_device [EOL] from ralamb import Ralamb [EOL] from mcts import MCTS [EOL] from agent import Agent [EOL] from trainer import Trainer [EOL] [EOL] [EOL] def main ( ) : [EOL] seed = [number] [EOL] env_name = [string] [EOL] state_units = [number] [EOL] hid_units = [number] [EOL] dirichlet_alpha = [number] [EOL] exploration_fraction = [number] [EOL] pb_c_base = [number] [EOL] pb_c_init = [number] [EOL] discount = [number] [EOL] num_simulations = [number] [EOL] window_size = [number] [EOL] nb_self_play = [number] [EOL] num_unroll_steps = [number] [EOL] td_steps = [number] [EOL] batch_size = [number] [EOL] lr = [number] [EOL] nb_train_update = [number] [EOL] nb_train_epochs = [number] [EOL] max_grad_norm = [number] [EOL] filename = [string] [EOL] ent_c = [number] [EOL] [EOL] device = get_device ( True ) [EOL] [EOL] env = gym . make ( env_name ) [EOL] env = RecordEpisodeStatistics ( env ) [EOL] env = TaxiObservationWrapper ( env ) [EOL] [EOL] np . random . seed ( seed ) [EOL] torch . manual_seed ( seed ) [EOL] env . seed ( seed ) [EOL] np . set_printoptions ( formatter = { [string] : [string] . format } ) [EOL] [EOL] network = Network ( env . observation_space . nvec . sum ( ) , env . action_space . n , state_units , hid_units ) [EOL] mcts = MCTS ( dirichlet_alpha , exploration_fraction , pb_c_base , pb_c_init , discount , num_simulations ) [EOL] agent = Agent ( network , mcts ) [EOL] trainer = Trainer ( ) [EOL] optimizer = Ralamb ( network . parameters ( ) , lr = lr ) [EOL] [EOL] if os . path . exists ( filename ) : [EOL] agent . load_model ( filename , device ) [EOL] [EOL] print ( [string] ) [EOL] try : [EOL] trainer . train ( env , agent , network , optimizer , window_size , nb_self_play , num_unroll_steps , td_steps , discount , batch_size , nb_train_update , nb_train_epochs , max_grad_norm , filename , ent_c , ) [EOL] except KeyboardInterrupt : [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] [EOL] agent . save_model ( filename ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , List , Any [EOL] import node [EOL] import typing [EOL] import numpy [EOL] import network [EOL] import utils [EOL] import builtins [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] import torch [EOL] [EOL] from node import Node [EOL] from network import Network [EOL] from utils import MinMaxStats [EOL] [EOL] [EOL] class MCTS : [EOL] def __init__ ( self , dirichlet_alpha , exploration_fraction , pb_c_base , pb_c_init , discount , num_simulations , ) : [EOL] self . dirichlet_alpha = dirichlet_alpha [EOL] self . exploration_fraction = exploration_fraction [EOL] self . pb_c_base = pb_c_base [EOL] self . pb_c_init = pb_c_init [EOL] self . discount = discount [EOL] self . num_simulations = num_simulations [EOL] [EOL] def run_mcts ( self , obs , network ) : [EOL] [comment] [EOL] root = Node ( [number] ) [EOL] state , policy , value = network . initial_inference ( obs ) [EOL] root . expand_node ( [number] , state . squeeze ( ) . detach ( ) . numpy ( ) , [number] , policy . squeeze ( ) . detach ( ) . numpy ( ) ) [EOL] root . add_exploration_noise ( self . dirichlet_alpha , self . exploration_fraction ) [comment] [EOL] [EOL] min_max_stats = MinMaxStats ( None ) [EOL] for _ in range ( self . num_simulations ) : [EOL] node = root [EOL] search_path = [ node ] [EOL] [EOL] while node . expanded : [EOL] [comment] [EOL] action , node = self . _select_child ( node , min_max_stats ) [EOL] search_path . append ( node ) [EOL] [EOL] [comment] [EOL] parent = search_path [ - [number] ] [EOL] next_state , reward , policy , value = network . recurrent_inference ( torch . from_numpy ( parent . hidden_state ) . unsqueeze ( [number] ) , np . array ( [ action ] ) ) [EOL] node . expand_node ( reward . item ( ) , next_state . squeeze ( ) . detach ( ) . numpy ( ) , [number] , policy . squeeze ( ) . detach ( ) . numpy ( ) ) [EOL] [EOL] [comment] [EOL] self . _backpropagate ( search_path , value . item ( ) , [number] , min_max_stats ) [EOL] [EOL] return root [EOL] [EOL] def _select_child ( self , node , min_max_stats ) : [EOL] [docstring] [EOL] ucb = [ self . _ucb_score ( node , child , min_max_stats ) for child in node . children ] [EOL] action = np . argmax ( ucb ) [EOL] return action , node . children [ action ] [EOL] [EOL] def _ucb_score ( self , parent , child , min_max_stats ) : [EOL] [docstring] [EOL] pb_c = np . log ( ( parent . visit_count + self . pb_c_base + [number] ) / self . pb_c_base ) + self . pb_c_init [EOL] pb_c *= np . sqrt ( parent . visit_count ) / ( child . visit_count + [number] ) [EOL] [EOL] prior_score = pb_c * child . prior [EOL] value_score = min_max_stats . normalize ( child . value ) [EOL] return prior_score + value_score [EOL] [EOL] def _backpropagate ( self , search_path , value , player , min_max_stats ) : [EOL] for node in reversed ( search_path ) : [EOL] node . value_sum += value if node . player == player else - value [EOL] node . visit_count += [number] [EOL] min_max_stats . update ( node . value ) [EOL] value = node . reward + self . discount * value [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $node.Node$ 0 0 0 $numpy.ndarray$ 0 $network.Network$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $network.Network$ 0 0 0 $numpy.ndarray$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $network.Network$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Tuple[builtins.int,node.Node]$ 0 0 0 $node.Node$ 0 $utils.MinMaxStats$ 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 $node.Node$ 0 0 0 $utils.MinMaxStats$ 0 0 0 0 $node.Node$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 $typing.Any$ 0 $node.Node$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 $node.Node$ 0 $node.Node$ 0 $utils.MinMaxStats$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $node.Node$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $node.Node$ 0 0 0 0 0 $node.Node$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $node.Node$ 0 0 0 $typing.Any$ 0 $utils.MinMaxStats$ 0 0 0 $node.Node$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[node.Node]$ 0 $builtins.float$ 0 $builtins.int$ 0 $utils.MinMaxStats$ 0 0 0 0 0 0 0 0 $typing.List[node.Node]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 $utils.MinMaxStats$ 0 0 0 0 0 $builtins.float$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0
from typing import Dict , Tuple , Any [EOL] import node [EOL] import typing [EOL] import numpy [EOL] import network [EOL] import mcts [EOL] import builtins [EOL] from typing import Tuple [EOL] import numpy as np [EOL] import torch [EOL] from torch . distributions import Categorical [EOL] [EOL] [EOL] from node import Node [EOL] from network import Network [EOL] from mcts import MCTS [EOL] [EOL] [EOL] class Agent : [EOL] def __init__ ( self , network , mcts ) : [EOL] self . network = network [EOL] self . mcts = mcts [EOL] [EOL] def get_action ( self , obs ) : [EOL] root = self . mcts . run_mcts ( obs , self . network ) [EOL] [comment] [EOL] action = self . _select_action ( root ) [EOL] return action , root [EOL] [EOL] def load_model ( self , filename , device ) : [EOL] print ( f" [string] { filename }" ) [EOL] load_data = torch . load ( filename , map_location = device ) [EOL] self . network . load_state_dict ( load_data [ [string] ] ) [EOL] [EOL] def save_model ( self , filename ) : [EOL] print ( f" [string] { filename }" ) [EOL] save_data = { [string] : self . network . state_dict ( ) } [EOL] torch . save ( save_data , filename ) [EOL] [EOL] def _select_action ( self , node ) : [EOL] return Categorical ( logits = torch . Tensor ( [ child . visit_count for child in node . children ] ) ) . sample ( ) . item ( ) [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,node.Node]$ 0 0 0 $numpy.ndarray$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $node.Node$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $node.Node$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import os [EOL] import gym [EOL] from gym . wrappers import RecordEpisodeStatistics [EOL] [EOL] from network import Network [EOL] from env_wrapper import TaxiObservationWrapper [EOL] from utils import get_device [EOL] from mcts import MCTS [EOL] from agent import Agent [EOL] from trainer import Trainer [EOL] [EOL] [EOL] def main ( ) : [EOL] env_name = [string] [EOL] state_units = [number] [EOL] hid_units = [number] [EOL] dirichlet_alpha = [number] [EOL] exploration_fraction = [number] [EOL] pb_c_base = [number] [EOL] pb_c_init = [number] [EOL] discount = [number] [EOL] num_simulations = [number] [EOL] filename = [string] [EOL] [EOL] device = get_device ( True ) [EOL] [EOL] env = gym . make ( env_name ) [EOL] env = RecordEpisodeStatistics ( env ) [EOL] env = TaxiObservationWrapper ( env ) [EOL] [EOL] network = Network ( env . observation_space . nvec . sum ( ) , env . action_space . n , state_units , hid_units ) [EOL] mcts = MCTS ( dirichlet_alpha , exploration_fraction , pb_c_base , pb_c_init , discount , num_simulations ) [EOL] agent = Agent ( network , mcts ) [EOL] trainer = Trainer ( ) [EOL] [EOL] if os . path . exists ( filename ) : [EOL] agent . load_model ( filename , device ) [EOL] [comment] [EOL] [EOL] trainer . validate ( env , agent , network ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import unittest [EOL] import sys [EOL] [EOL] sys . path . append ( [string] ) [EOL] from reversi . board import Board [comment] [EOL] [EOL] [EOL] class TestBoardMask ( unittest . TestCase ) : [EOL] def test_4x4 ( self ) : [EOL] board = Board ( size = [number] ) [EOL] mask_bb = board . mask_bb [EOL] self . assertEqual ( [number] , mask_bb . H ) [EOL] self . assertEqual ( [number] , mask_bb . V ) [EOL] self . assertEqual ( [number] , mask_bb . D ) [EOL] self . assertEqual ( [number] , mask_bb . U ) [EOL] self . assertEqual ( [number] , mask_bb . UR ) [EOL] self . assertEqual ( [number] , mask_bb . R ) [EOL] self . assertEqual ( [number] , mask_bb . BR ) [EOL] self . assertEqual ( [number] , mask_bb . B ) [EOL] self . assertEqual ( [number] , mask_bb . BL ) [EOL] self . assertEqual ( [number] , mask_bb . L ) [EOL] self . assertEqual ( [number] , mask_bb . UL ) [EOL] [EOL] def test_8x8 ( self ) : [EOL] board = Board ( size = [number] ) [EOL] mask_bb = board . mask_bb [EOL] self . assertEqual ( [number] , mask_bb . H ) [EOL] self . assertEqual ( [number] , mask_bb . V ) [EOL] self . assertEqual ( [number] , mask_bb . D ) [EOL] self . assertEqual ( [number] , mask_bb . U ) [EOL] self . assertEqual ( [number] , mask_bb . UR ) [EOL] self . assertEqual ( [number] , mask_bb . R ) [EOL] self . assertEqual ( [number] , mask_bb . BR ) [EOL] self . assertEqual ( [number] , mask_bb . B ) [EOL] self . assertEqual ( [number] , mask_bb . BL ) [EOL] self . assertEqual ( [number] , mask_bb . L ) [EOL] self . assertEqual ( [number] , mask_bb . UL ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] unittest . main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import sys [EOL] import random [EOL] [EOL] sys . path . append ( [string] ) [EOL] from reversi . board import Board [comment] [EOL] [EOL] size = [number] [EOL] board = Board ( size ) [EOL] [EOL] done = False [EOL] pass_num = [number] [EOL] while not done : [EOL] print ( [string] * [number] ) [EOL] print ( board ) [EOL] board_info = board . get_board_info ( ) [EOL] legal_moves = board . get_legal_moves ( ) [EOL] print ( [string] , board_info ) [EOL] print ( [string] , legal_moves ) [EOL] [EOL] if len ( legal_moves ) == [number] : [EOL] print ( [string] ) [EOL] board . pass_move ( ) [EOL] pass_num += [number] [EOL] else : [EOL] action = random . choice ( legal_moves ) [EOL] print ( [string] , action ) [EOL] reversibles = board . get_flippable_discs ( action ) [EOL] print ( [string] , reversibles ) [EOL] board . move ( action ) [EOL] pass_num = [number] [EOL] [EOL] if ( pass_num == [number] ) or ( board . get_blank_num ( ) == [number] ) : [EOL] done = True [EOL] [EOL] print ( [string] * [number] ) [EOL] print ( board ) [EOL] board_info = board . get_board_info ( ) [EOL] if board_info [ [string] ] > board_info [ [string] ] : [EOL] print ( [string] ) [EOL] elif board_info [ [string] ] < board_info [ [string] ] : [EOL] print ( [string] ) [EOL] else : [EOL] print ( [string] ) [EOL] print ( [string] * [number] ) [EOL] print ( [string] ) [EOL] print ( board . get_history ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
	0
from typing import List , Any [EOL] import RL [EOL] import typing [EOL] import builtins [EOL] from typing import NamedTuple , List [EOL] from enum import IntEnum [EOL] [EOL] [EOL] [comment] [EOL] class BitMask ( NamedTuple ) : [EOL] H = ... [comment] [EOL] V = ... [comment] [EOL] D = ... [comment] [EOL] U = ... [comment] [EOL] UR = ... [comment] [EOL] R = ... [comment] [EOL] BR = ... [comment] [EOL] B = ... [comment] [EOL] BL = ... [comment] [EOL] L = ... [comment] [EOL] UL = ... [comment] [EOL] [EOL] [EOL] [comment] [EOL] class Position ( NamedTuple ) : [EOL] X = ... [EOL] Y = ... [EOL] [EOL] [EOL] class Board : [EOL] EMPTY = [number] [EOL] BLACK = [number] [EOL] WHITE = [number] [EOL] PIECE_STRS = [ [string] , [string] , [string] ] [EOL] [EOL] def __init__ ( self , size = [number] ) : [EOL] self . size = size [comment] [EOL] [EOL] [comment] [EOL] center = size // [number] [EOL] total_size = size * size [EOL] black_bb = [number] << ( ( total_size - [number] ) - ( size * ( center - [number] ) + center ) ) [EOL] black_bb |= [number] << ( ( total_size - [number] ) - ( size * center + ( center - [number] ) ) ) [EOL] white_bb = [number] << ( ( total_size - [number] ) - ( size * ( center - [number] ) + ( center - [number] ) ) ) [EOL] white_bb |= [number] << ( ( total_size - [number] ) - ( size * center + center ) ) [EOL] self . black_bb = black_bb [EOL] self . white_bb = white_bb [EOL] [EOL] [comment] [EOL] self . black_score = [number] [EOL] self . white_score = [number] [EOL] self . blank_num = total_size - [number] [comment] [EOL] [EOL] [comment] [EOL] self . mask_bb = BitMask ( H = int ( [string] . join ( ( [ [string] ] + ( [ [string] ] * ( size - [number] ) ) + [ [string] ] ) * size ) , [number] ) , V = int ( [string] . join ( ( [ [string] ] * size ) + ( ( [ [string] ] * size ) * ( size - [number] ) ) + ( [ [string] ] * size ) ) , [number] ) , D = int ( [string] . join ( ( [ [string] ] * size ) + ( ( [ [string] ] + ( [ [string] ] * ( size - [number] ) ) + [ [string] ] ) * ( size - [number] ) ) + ( [ [string] ] * size ) ) , [number] ) , U = int ( [string] . join ( ( ( [ [string] ] * size ) * ( size - [number] ) ) + ( [ [string] ] * size ) ) , [number] ) , UR = int ( [string] . join ( ( ( [ [string] ] + ( [ [string] ] * ( size - [number] ) ) ) * ( size - [number] ) ) + ( [ [string] ] * size ) ) , [number] ) , R = int ( [string] . join ( ( [ [string] ] + ( [ [string] ] * ( size - [number] ) ) ) * size ) , [number] ) , BR = int ( [string] . join ( ( [ [string] ] * size ) + ( ( [ [string] ] + ( [ [string] ] * ( size - [number] ) ) ) * ( size - [number] ) ) ) , [number] ) , B = int ( [string] . join ( ( [ [string] ] * size ) + ( ( [ [string] ] * size ) * ( size - [number] ) ) ) , [number] ) , BL = int ( [string] . join ( ( [ [string] ] * size ) + ( ( ( [ [string] ] * ( size - [number] ) ) + [ [string] ] ) * ( size - [number] ) ) ) , [number] ) , L = int ( [string] . join ( ( ( [ [string] ] * ( size - [number] ) ) + [ [string] ] ) * size ) , [number] ) , UL = int ( [string] . join ( ( ( ( [ [string] ] * ( size - [number] ) ) + [ [string] ] ) * ( size - [number] ) ) + ( [ [string] ] * size ) ) , [number] ) , ) [EOL] [EOL] [comment] [EOL] self . history = [ ] [EOL] [EOL] self . current_color = self . BLACK [EOL] self . turn = [number] [EOL] [EOL] def get_board_info ( self ) : [EOL] return { [string] : self . turn , [string] : self . blank_num , [string] : self . current_color , [string] : self . black_bb , [string] : self . white_bb , [string] : self . black_score , [string] : self . white_score , } [EOL] [EOL] def get_legal_moves ( self ) : [EOL] if self . current_color == self . BLACK : [EOL] player_bb , opponent_bb = self . black_bb , self . white_bb [EOL] else : [EOL] player_bb , opponent_bb = self . white_bb , self . black_bb [EOL] [EOL] horizontal_bb = opponent_bb & self . mask_bb . H [EOL] vertical_bb = opponent_bb & self . mask_bb . V [EOL] diagonal_bb = opponent_bb & self . mask_bb . D [EOL] blank_bb = ~ ( player_bb | opponent_bb ) [comment] [EOL] [EOL] legal_moves_bb = [number] [EOL] legal_moves_bb |= self . _get_legal_moves_lshift ( horizontal_bb , player_bb , blank_bb , [number] ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_rshift ( horizontal_bb , player_bb , blank_bb , [number] ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_lshift ( vertical_bb , player_bb , blank_bb , self . size ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_rshift ( vertical_bb , player_bb , blank_bb , self . size ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_lshift ( diagonal_bb , player_bb , blank_bb , self . size + [number] ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_lshift ( diagonal_bb , player_bb , blank_bb , self . size - [number] ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_rshift ( diagonal_bb , player_bb , blank_bb , self . size - [number] ) [comment] [EOL] legal_moves_bb |= self . _get_legal_moves_rshift ( diagonal_bb , player_bb , blank_bb , self . size + [number] ) [comment] [EOL] [EOL] legal_moves = [ ] [EOL] mask = [number] << ( self . size * self . size - [number] ) [EOL] for y in range ( self . size ) : [EOL] for x in range ( self . size ) : [EOL] if legal_moves_bb & mask : [EOL] legal_moves . append ( Position ( X = x , Y = y ) ) [EOL] mask >>= [number] [EOL] [EOL] return legal_moves [EOL] [EOL] def move ( self , pos ) : [EOL] size = self . size [EOL] total_size = size * size [EOL] pos_index = ( total_size - [number] ) - ( ( pos . Y * size ) + pos . X ) [EOL] if ( pos_index < [number] ) or ( pos_index > ( total_size - [number] ) ) : [EOL] return [number] [EOL] [EOL] flippable_discs = self . get_flippable_discs ( pos ) [EOL] if len ( flippable_discs ) == [number] : [EOL] return [number] [EOL] [EOL] flippable_discs_bb = [number] [EOL] for flip_pos in flippable_discs : [EOL] flippable_discs_bb |= [number] << ( ( total_size - [number] ) - ( ( flip_pos . Y * size ) + flip_pos . X ) ) [EOL] [EOL] put_bb = [number] << pos_index [EOL] if self . current_color == self . BLACK : [EOL] self . black_bb ^= put_bb | flippable_discs_bb [EOL] self . white_bb ^= flippable_discs_bb [EOL] self . black_score += [number] + len ( flippable_discs ) [EOL] self . white_score -= len ( flippable_discs ) [EOL] self . current_color = self . WHITE [EOL] else : [EOL] self . white_bb ^= put_bb | flippable_discs_bb [EOL] self . black_bb ^= flippable_discs_bb [EOL] self . white_score += [number] + len ( flippable_discs ) [EOL] self . black_score -= len ( flippable_discs ) [EOL] self . current_color = self . BLACK [EOL] self . turn += [number] [EOL] self . blank_num -= [number] [EOL] [EOL] self . history . append ( { [string] : pos , [string] : flippable_discs } ) [EOL] [EOL] return flippable_discs_bb [EOL] [EOL] def pass_move ( self ) : [EOL] self . history . append ( None ) [EOL] if self . current_color == self . BLACK : [EOL] self . current_color = self . WHITE [EOL] else : [EOL] self . current_color = self . BLACK [EOL] self . turn += [number] [EOL] [EOL] def undo ( self ) : [EOL] his_data = self . history . pop ( ) [EOL] if his_data : [EOL] size = self . size [EOL] total_size = size * size [EOL] [EOL] pos = his_data [ [string] ] [EOL] flippable_discs = his_data [ [string] ] [EOL] [EOL] flippable_discs_bb = [number] [EOL] for flip_pos in flippable_discs : [EOL] flippable_discs_bb |= [number] << ( ( total_size - [number] ) - ( ( flip_pos . Y * size ) + flip_pos . X ) ) [EOL] [EOL] pos_index = ( total_size - [number] ) - ( ( pos . Y * size ) + pos . X ) [EOL] put_bb = [number] << pos_index [EOL] if self . current_color == self . BLACK : [EOL] self . white_bb ^= put_bb | flippable_discs_bb [EOL] self . black_bb ^= flippable_discs_bb [EOL] self . white_score -= [number] + len ( flippable_discs ) [EOL] self . black_score += len ( flippable_discs ) [EOL] else : [EOL] self . black_bb ^= put_bb | flippable_discs_bb [EOL] self . white_bb ^= flippable_discs_bb [EOL] self . black_score -= [number] + len ( flippable_discs ) [EOL] self . white_score += len ( flippable_discs ) [EOL] self . blank_num += [number] [EOL] [EOL] if self . current_color == self . BLACK : [EOL] self . current_color = self . WHITE [EOL] else : [EOL] self . current_color = self . BLACK [EOL] self . turn -= [number] [EOL] [EOL] def get_flippable_discs ( self , pos ) : [EOL] size = self . size [EOL] [EOL] if self . current_color == self . BLACK : [EOL] player_bb , opponent_bb = self . black_bb , self . white_bb [EOL] else : [EOL] player_bb , opponent_bb = self . white_bb , self . black_bb [EOL] [EOL] [comment] [EOL] pos_index = [number] << ( ( ( size * size ) - [number] ) - ( ( pos . Y * size ) + pos . X ) ) [EOL] [EOL] reversibles_bb = [number] [EOL] for direction in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] : [EOL] check_bb = self . _get_next_put ( pos_index , direction ) [EOL] [EOL] [comment] [EOL] temp_bb = [number] [EOL] while check_bb & opponent_bb : [EOL] temp_bb |= check_bb [EOL] check_bb = self . _get_next_put ( check_bb , direction ) [EOL] [EOL] [comment] [EOL] if check_bb & player_bb : [EOL] reversibles_bb |= temp_bb [EOL] [EOL] reversibles = [ ] [EOL] mask = [number] << ( self . size * self . size - [number] ) [EOL] for y in range ( self . size ) : [EOL] for x in range ( self . size ) : [EOL] if reversibles_bb & mask : [EOL] reversibles . append ( Position ( X = x , Y = y ) ) [EOL] mask >>= [number] [EOL] [EOL] return reversibles [EOL] [EOL] def get_history ( self ) : [EOL] return self . history [EOL] [EOL] def get_blank_num ( self ) : [EOL] return self . blank_num [EOL] [EOL] def __str__ ( self ) : [EOL] size = self . size [EOL] total_size = size * size [EOL] mask = [number] << ( total_size - [number] ) [EOL] [EOL] out_str = [string] [EOL] out_str += [string] + [string] . join ( [ chr ( [number] + i ) for i in range ( size ) ] ) + [string] [EOL] for y in range ( size ) : [EOL] out_str += [string] . format ( y + [number] ) [EOL] for x in range ( size ) : [EOL] if self . black_bb & mask : [EOL] out_str += self . PIECE_STRS [ self . BLACK ] [EOL] elif self . white_bb & mask : [EOL] out_str += self . PIECE_STRS [ self . WHITE ] [EOL] else : [EOL] out_str += self . PIECE_STRS [ self . EMPTY ] [EOL] mask >>= [number] [EOL] out_str += [string] [EOL] black_score_str = self . PIECE_STRS [ self . BLACK ] + [string] . format ( self . black_score ) [EOL] white_score_str = self . PIECE_STRS [ self . WHITE ] + [string] . format ( self . white_score ) [EOL] out_str += [string] + black_score_str + [string] + white_score_str + [string] [EOL] out_str += [string] . format ( self . turn ) + [string] [EOL] out_str += [string] . format ( self . PIECE_STRS [ self . current_color ] ) [EOL] [EOL] return out_str [EOL] [EOL] def _get_legal_moves_lshift ( self , mask_bb , player_bb , blank_bb , shift_size ) : [EOL] temp_bb = mask_bb & ( player_bb << shift_size ) [EOL] for _ in range ( self . size - [number] ) : [EOL] temp_bb |= mask_bb & ( temp_bb << shift_size ) [EOL] return blank_bb & ( temp_bb << shift_size ) [EOL] [EOL] def _get_legal_moves_rshift ( self , mask_bb , player_bb , blank_bb , shift_size ) : [EOL] temp_bb = mask_bb & ( player_bb >> shift_size ) [EOL] for _ in range ( self . size - [number] ) : [EOL] temp_bb |= mask_bb & ( temp_bb >> shift_size ) [EOL] return blank_bb & ( temp_bb >> shift_size ) [EOL] [EOL] def _get_next_put ( self , index , direction ) : [EOL] if direction == [string] : [EOL] return ( index << self . size ) & self . mask_bb . U [EOL] elif direction == [string] : [EOL] return ( index << ( self . size - [number] ) ) & self . mask_bb . UR [EOL] elif direction == [string] : [EOL] return ( index >> [number] ) & self . mask_bb . R [EOL] elif direction == [string] : [EOL] return ( index >> ( self . size + [number] ) ) & self . mask_bb . BR [EOL] elif direction == [string] : [EOL] return ( index >> self . size ) & self . mask_bb . B [EOL] elif direction == [string] : [EOL] return ( index >> ( self . size - [number] ) ) & self . mask_bb . BL [EOL] elif direction == [string] : [EOL] return ( index << [number] ) & self . mask_bb . L [EOL] elif direction == [string] : [EOL] return ( index << ( self . size + [number] ) ) & self . mask_bb . UL [EOL] else : [EOL] return [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $RL.Reversi.reversi.board.BitMask$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[Position]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $builtins.int$ 0 0 0 $Position$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $Position$ 0 0 0 0 0 0 $Position$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $Position$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $Position$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $Position$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $Position$ 0 0 0 0 0 0 $Position$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import zipfile [EOL] import tarfile [EOL] import os [EOL] import urllib . request [EOL] import tarfile [EOL] import zipfile [EOL] [EOL] [EOL] def report_hook ( block_num , block_size , total_size ) : [EOL] if total_size <= [number] : [EOL] return [EOL] [EOL] part_size = block_size * block_num [EOL] if part_size > total_size : [EOL] part_size = total_size [EOL] progress = ( part_size / total_size ) * [number] [EOL] print ( [string] . format ( progress , part_size , total_size ) , end = [string] ) [EOL] [EOL] [EOL] data_dir = [string] [EOL] if not os . path . exists ( data_dir ) : [EOL] os . mkdir ( data_dir ) [EOL] [EOL] [comment] [EOL] url = [string] [EOL] save_path = [string] [EOL] extract_path = [string] [EOL] if not os . path . exists ( save_path ) : [EOL] print ( [string] . format ( url ) ) [EOL] urllib . request . urlretrieve ( url , save_path , report_hook ) [EOL] print ( ) [EOL] if not os . path . exists ( extract_path ) : [EOL] print ( [string] . format ( save_path ) ) [EOL] tar = tarfile . open ( save_path , [string] ) [EOL] tar . extractall ( [string] ) [EOL] tar . close ( ) [EOL] [EOL] [comment] [EOL] url = [string] [EOL] save_path = [string] [EOL] extract_path = [string] [EOL] if not os . path . exists ( save_path ) : [EOL] print ( [string] . format ( url ) ) [EOL] urllib . request . urlretrieve ( url , save_path , report_hook ) [EOL] print ( ) [EOL] if not os . path . exists ( extract_path ) : [EOL] print ( [string] . format ( save_path ) ) [EOL] zip = zipfile . ZipFile ( save_path ) [EOL] zip . extractall ( extract_path ) [EOL] zip . close ( ) [EOL] [EOL] [comment] [EOL] url = [string] [EOL] save_path = [string] [EOL] extract_path = [string] [EOL] if not os . path . exists ( save_path ) : [EOL] print ( [string] . format ( url ) ) [EOL] urllib . request . urlretrieve ( url , save_path , report_hook ) [EOL] print ( ) [EOL] if not os . path . exists ( extract_path ) : [EOL] print ( [string] . format ( save_path ) ) [EOL] tar = tarfile . open ( save_path ) [EOL] tar . extractall ( [string] ) [EOL] tar . close ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] save_path = [string] [EOL] extract_path = [string] [EOL] if not os . path . exists ( extract_path ) : [EOL] print ( [string] . format ( save_path ) ) [EOL] zip = zipfile . ZipFile ( save_path ) [EOL] zip . extractall ( extract_path ) [EOL] zip . close ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $tarfile.TarFile$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $tarfile.TarFile$ 0 0 0 0 0 0 $tarfile.TarFile$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $zipfile.ZipFile$ 0 0 0 0 0 $builtins.str$ 0 0 $zipfile.ZipFile$ 0 0 0 $builtins.str$ 0 0 $zipfile.ZipFile$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $tarfile.TarFile$ 0 0 0 0 0 $builtins.str$ 0 0 $tarfile.TarFile$ 0 0 0 0 0 0 $tarfile.TarFile$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $zipfile.ZipFile$ 0 0 0 0 0 $builtins.str$ 0 0 $zipfile.ZipFile$ 0 0 0 $builtins.str$ 0 0 $zipfile.ZipFile$ 0 0 0 0 0
from typing import Any [EOL] import nlp [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] import math [EOL] [EOL] [comment] [EOL] class Embedder ( nn . Module ) : [EOL] def __init__ ( self , text_embedding_vectors ) : [EOL] super ( Embedder , self ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] self . emb = nn . Embedding . from_pretrained ( embeddings = text_embedding_vectors , freeze = True ) [EOL] [EOL] def forward ( self , x ) : [EOL] return self . emb ( x ) [EOL] [EOL] [EOL] [comment] [EOL] class PositionEncoder ( nn . Module ) : [EOL] def __init__ ( self , d_model = [number] , max_seq_len = [number] , device = [string] ) : [EOL] super ( PositionEncoder , self ) . __init__ ( ) [EOL] [EOL] self . d_model = d_model [comment] [EOL] self . pe = torch . zeros ( max_seq_len , d_model ) . to ( device ) [comment] [EOL] [EOL] for pos in range ( max_seq_len ) : [EOL] for i in range ( [number] , d_model , [number] ) : [EOL] self . pe [ pos , i ] = math . sin ( pos / ( [number] ** ( ( [number] * i ) / d_model ) ) ) [EOL] self . pe [ pos , i + [number] ] = math . cos ( pos / ( [number] ** ( ( [number] * i ) / d_model ) ) ) [EOL] self . pe = self . pe . unsqueeze ( [number] ) [comment] [EOL] self . pe . requires_grad = False [comment] [EOL] [EOL] def forward ( self , x ) : [EOL] return ( math . sqrt ( self . d_model ) * x ) + self . pe [EOL] [EOL] [EOL] [comment] [EOL] class Attention ( nn . Module ) : [EOL] def __init__ ( self , d_model = [number] ) : [EOL] super ( Attention , self ) . __init__ ( ) [EOL] [EOL] self . d_model = d_model [EOL] self . q_linear = nn . Linear ( d_model , d_model , bias = False ) [EOL] self . k_linear = nn . Linear ( d_model , d_model , bias = False ) [EOL] self . v_linear = nn . Linear ( d_model , d_model , bias = False ) [EOL] self . out = nn . Linear ( d_model , d_model , bias = False ) [EOL] [EOL] def forward ( self , x , mask ) : [EOL] q = self . q_linear ( x ) [EOL] k = self . k_linear ( x ) [EOL] v = self . v_linear ( x ) [EOL] [EOL] [comment] [EOL] weights = torch . matmul ( q , k . transpose ( [number] , [number] ) ) / math . sqrt ( self . d_model ) [EOL] weights = weights . masked_fill ( mask . unsqueeze ( [number] ) == [number] , - [number] ) [comment] [EOL] normalized_weights = F . softmax ( weights , dim = - [number] ) [EOL] h = torch . matmul ( normalized_weights , v ) [EOL] return self . out ( h ) , normalized_weights [EOL] [EOL] [EOL] class FeedForward ( nn . Module ) : [EOL] def __init__ ( self , d_model = [number] , d_hidden = [number] , drop_ratio = [number] ) : [EOL] super ( FeedForward , self ) . __init__ ( ) [EOL] [EOL] self . layers = nn . Sequential ( nn . Linear ( d_model , d_hidden ) , nn . ReLU ( ) , nn . Dropout ( drop_ratio ) , nn . Linear ( d_hidden , d_model ) ) [EOL] [EOL] def forward ( self , x ) : [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class TransformerBlock ( nn . Module ) : [EOL] def __init__ ( self , d_model = [number] , d_hidden = [number] , drop_ratio = [number] ) : [EOL] super ( TransformerBlock , self ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] self . norm1 = nn . LayerNorm ( d_model ) [EOL] self . attn = Attention ( d_model ) [EOL] self . dropout1 = nn . Dropout ( drop_ratio ) [EOL] [EOL] [comment] [EOL] self . norm2 = nn . LayerNorm ( d_model ) [EOL] self . ff = FeedForward ( d_model , d_hidden , drop_ratio ) [EOL] self . dropout2 = nn . Dropout ( drop_ratio ) [EOL] [EOL] def forward ( self , x , mask ) : [EOL] [comment] [EOL] h , normalized_weights = self . attn ( self . norm1 ( x ) , mask ) [EOL] h = x + self . dropout1 ( h ) [EOL] [EOL] [comment] [EOL] h = h + self . dropout2 ( self . ff ( self . norm2 ( h ) ) ) [EOL] [EOL] return h , normalized_weights [EOL] [EOL] [EOL] class ClassificationHead ( nn . Module ) : [EOL] def __init__ ( self , d_model = [number] , d_out = [number] ) : [EOL] super ( ClassificationHead , self ) . __init__ ( ) [EOL] [EOL] self . layer = nn . Linear ( d_model , d_out ) [EOL] nn . init . normal_ ( self . layer . weight , std = [number] ) [EOL] nn . init . normal_ ( self . layer . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] return F . softmax ( self . layer ( x [ : , [number] , : ] ) , dim = [number] ) [comment] [EOL] [EOL] [EOL] class TransformerClassification ( nn . Module ) : [EOL] def __init__ ( self , emb_vectors , d_model = [number] , max_seq_len = [number] , d_hidden = [number] , d_out = [number] , drop_ratio = [number] , device = [string] ) : [EOL] super ( TransformerClassification , self ) . __init__ ( ) [EOL] [EOL] self . emb = Embedder ( emb_vectors ) [EOL] self . pe = PositionEncoder ( d_model , max_seq_len , device ) [EOL] self . trm1 = TransformerBlock ( d_model , d_hidden , drop_ratio ) [EOL] self . trm2 = TransformerBlock ( d_model , d_hidden , drop_ratio ) [EOL] self . norm = nn . LayerNorm ( d_model ) [EOL] self . head = ClassificationHead ( d_model , d_out ) [EOL] [EOL] def forward ( self , x , mask ) : [EOL] h = self . pe ( self . emb ( x ) ) [EOL] h , attn_w1 = self . trm1 ( h , mask ) [EOL] h , attn_w2 = self . trm2 ( h , mask ) [EOL] h = self . norm ( h ) [EOL] h = self . head ( h ) [EOL] return h , attn_w1 , attn_w2 [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.Attention$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.FeedForward$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.Embedder$ 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.PositionEncoder$ 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.TransformerBlock$ 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.TransformerBlock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp.Transformer.utils.transformer.ClassificationHead$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import re [EOL] import string [EOL] import random [EOL] import torchtext [EOL] from torchtext . vocab import Vectors [EOL] [EOL] def get_IMDb_DataLoaders_and_TEXT ( max_length = [number] , batch_size = [number] ) : [EOL] [comment] [EOL] def preprocessing_text ( text ) : [EOL] text = re . sub ( [string] , [string] , text ) [comment] [EOL] [EOL] for p in string . punctuation : [EOL] if ( p == [string] ) or ( p == [string] ) : [comment] [EOL] continue [EOL] else : [EOL] text = text . replace ( p , [string] ) [EOL] [EOL] [comment] [EOL] text = text . replace ( [string] , [string] ) [EOL] text = text . replace ( [string] , [string] ) [EOL] return text [EOL] [EOL] [comment] [EOL] def tokenizer_janome ( text ) : [EOL] [comment] [EOL] return text . strip ( ) . split ( ) [EOL] [EOL] [comment] [EOL] def tokenizer_with_preprocessing ( text ) : [EOL] return tokenizer_janome ( preprocessing_text ( text ) ) [EOL] [EOL] [comment] [EOL] TEXT = torchtext . data . Field ( sequential = True , tokenize = tokenizer_with_preprocessing , use_vocab = True , lower = True , include_lengths = True , batch_first = True , fix_length = max_length , init_token = [string] , eos_token = [string] ) [EOL] LABEL = torchtext . data . Field ( sequential = False , use_vocab = False , ) [EOL] train_val_ds , test_ds = torchtext . data . TabularDataset . splits ( path = [string] , train = [string] , test = [string] , format = [string] , fields = [ ( [string] , TEXT ) , ( [string] , LABEL ) ] ) [EOL] [comment] [EOL] train_ds , val_ds = train_val_ds . split ( split_ratio = [number] , random_state = random . seed ( [number] ) ) [EOL] [EOL] [comment] [EOL] english_fasttext_vectors = Vectors ( [string] ) [EOL] TEXT . build_vocab ( train_ds , vectors = english_fasttext_vectors , min_freq = [number] ) [EOL] [EOL] [comment] [EOL] train_dl = torchtext . data . Iterator ( train_ds , batch_size = batch_size , train = True ) [EOL] val_dl = torchtext . data . Iterator ( val_ds , batch_size = batch_size , train = False , sort = False ) [EOL] test_dl = torchtext . data . Iterator ( test_ds , batch_size = batch_size , train = False , sort = False ) [EOL] [EOL] return train_dl , val_dl , test_dl , TEXT [EOL] [EOL] [EOL] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import mnist [EOL] import typing [EOL] import argparse [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] import torchvision . datasets as datasets [EOL] import torchvision . transforms as transforms [EOL] import argparse [EOL] import time [EOL] [EOL] [EOL] class AverageMeter : [EOL] [comment] [EOL] def __init__ ( self ) : [EOL] self . reset ( ) [EOL] [EOL] def reset ( self ) : [EOL] self . sum = [number] [EOL] self . count = [number] [EOL] [EOL] def update ( self , val , n = [number] ) : [EOL] self . sum += val * n [EOL] self . count += n [EOL] [EOL] @ property def avg ( self ) : [EOL] assert self . count > [number] [EOL] return self . sum / self . count [EOL] [EOL] [EOL] def calc_accuracy ( output_data , target_data ) : [EOL] batch_size = target_data . size ( [number] ) [EOL] _ , pred_idx = output_data . topk ( [number] , [number] , True , True ) [comment] [EOL] pred_idx = pred_idx . t ( ) . squeeze ( ) . cpu ( ) [EOL] correct = pred_idx . eq ( target_data ) [comment] [EOL] return ( correct . sum ( ) . float ( ) / batch_size ) * [number] [EOL] [EOL] [EOL] class Conv_BN_Act ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_channels , kernel_size , stride = [number] , padding = [number] , bias = False ) : [EOL] super ( Conv_BN_Act , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( nn . Conv2d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , bias = bias ) , nn . BatchNorm2d ( out_channels ) , nn . ReLU ( inplace = True ) , ) [EOL] [EOL] def forward ( self , x ) : [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class SimpleConvNet ( nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( SimpleConvNet , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , Conv_BN_Act ( [number] , [number] , [number] ) , nn . MaxPool2d ( [number] , stride = [number] , padding = [number] ) , nn . AdaptiveAvgPool2d ( ( [number] , [number] ) ) , ) [EOL] self . fc = nn . Linear ( [number] , [number] ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] for m in self . modules ( ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . xavier_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = self . layers ( x ) [EOL] x = x . view ( x . size ( [number] ) , - [number] ) [EOL] x = self . fc ( x ) [EOL] return F . softmax ( x , dim = - [number] ) [EOL] [EOL] [EOL] def train ( args , data_loader , model , loss_func , optimizer , epoch ) : [EOL] [comment] [EOL] model . train ( ) [EOL] [EOL] loss_avg = AverageMeter ( ) [EOL] acc_avg = AverageMeter ( ) [EOL] batch_time_avg = AverageMeter ( ) [EOL] [EOL] for i , ( input_data , target_data ) in enumerate ( data_loader ) : [EOL] start_time = time . perf_counter ( ) [EOL] [EOL] output_data = model ( input_data ) [EOL] loss = loss_func ( output_data , target_data ) [EOL] loss_avg . update ( loss . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] acc = calc_accuracy ( output_data . data , target_data ) [EOL] acc_avg . update ( acc . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] optimizer . zero_grad ( ) [EOL] loss . backward ( ) [EOL] optimizer . step ( ) [EOL] [EOL] batch_time = time . perf_counter ( ) - start_time [EOL] batch_time_avg . update ( batch_time ) [EOL] [EOL] if ( ( i + [number] ) % args . log_interval ) == [number] : [EOL] print ( [string] . format ( epoch + [number] , args . max_epoch , i + [number] , len ( data_loader ) , loss . item ( ) , loss_avg . avg , acc . item ( ) , acc_avg . avg , batch_time * args . log_interval , batch_time_avg . avg * args . log_interval , ) ) [EOL] print ( [string] . format ( acc_avg . avg , batch_time_avg . sum ) ) [EOL] [EOL] [EOL] def validate ( args , data_loader , model , loss_func , epoch ) : [EOL] [comment] [EOL] model . eval ( ) [EOL] [EOL] loss_avg = AverageMeter ( ) [EOL] acc_avg = AverageMeter ( ) [EOL] batch_time_avg = AverageMeter ( ) [EOL] [EOL] for i , ( input_data , target_data ) in enumerate ( data_loader ) : [EOL] start_time = time . perf_counter ( ) [EOL] [EOL] output_data = model ( input_data ) [EOL] loss = loss_func ( output_data , target_data ) [EOL] loss_avg . update ( loss . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] acc = calc_accuracy ( output_data . data , target_data ) [EOL] acc_avg . update ( acc . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] batch_time = time . perf_counter ( ) - start_time [EOL] batch_time_avg . update ( batch_time ) [EOL] [EOL] if ( ( i + [number] ) % args . log_interval ) == [number] : [EOL] print ( [string] . format ( epoch + [number] , args . max_epoch , i + [number] , len ( data_loader ) , loss . item ( ) , loss_avg . avg , acc . item ( ) , acc_avg . avg , batch_time * args . log_interval , batch_time_avg . avg * args . log_interval , ) ) [EOL] print ( [string] . format ( acc_avg . avg , batch_time_avg . sum ) ) [EOL] return acc_avg . avg [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = str , default = [string] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] [comment] [EOL] transform_train = transforms . Compose ( [ transforms . ToTensor ( ) ] ) [EOL] [comment] [EOL] transform_validate = transforms . Compose ( [ transforms . ToTensor ( ) ] ) [EOL] [EOL] [comment] [EOL] train_data_set = datasets . MNIST ( args . datasets_dir , train = True , transform = transform_train , download = True ) [EOL] validate_data_set = datasets . MNIST ( args . datasets_dir , train = False , transform = transform_validate , download = True ) [EOL] [EOL] [comment] [EOL] train_data_loader = torch . utils . data . DataLoader ( train_data_set , batch_size = args . batch_size , shuffle = True , num_workers = [number] , pin_memory = True ) [EOL] validate_data_loader = torch . utils . data . DataLoader ( validate_data_set , batch_size = args . batch_size , shuffle = True , num_workers = [number] , pin_memory = True ) [EOL] [EOL] [comment] [EOL] model = SimpleConvNet ( ) [EOL] loss_func = nn . CrossEntropyLoss ( ) [EOL] optimizer = torch . optim . Adam ( model . parameters ( ) , lr = [number] ) [EOL] [comment] [EOL] [EOL] best_acc = [number] [EOL] for epoch in range ( args . max_epoch ) : [EOL] train ( args , train_data_loader , model , loss_func , optimizer , epoch ) [EOL] epoch_acc = validate ( args , validate_data_loader , model , loss_func , epoch ) [EOL] [EOL] [comment] [EOL] best_acc = max ( epoch_acc , best_acc ) [EOL] [EOL] print ( [string] . format ( best_acc ) ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import cifar10 [EOL] import argparse [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] import torchvision . datasets as datasets [EOL] import torchvision . transforms as transforms [EOL] from torch . utils . tensorboard import SummaryWriter [EOL] import argparse [EOL] import time [EOL] [EOL] from models . simple_net import SimpleConvNet [EOL] from models . simple_net import SimpleOctConvNet [EOL] [EOL] [EOL] class AverageMeter : [EOL] [comment] [EOL] def __init__ ( self ) : [EOL] self . reset ( ) [EOL] [EOL] def reset ( self ) : [EOL] self . sum = [number] [EOL] self . count = [number] [EOL] [EOL] def update ( self , val , n = [number] ) : [EOL] self . sum += val * n [EOL] self . count += n [EOL] [EOL] @ property def avg ( self ) : [EOL] assert self . count > [number] [EOL] return self . sum / self . count [EOL] [EOL] [EOL] def calc_accuracy ( output_data , target_data ) : [EOL] batch_size = target_data . size ( [number] ) [EOL] _ , pred_idx = output_data . topk ( [number] , [number] , True , True ) [comment] [EOL] pred_idx = pred_idx . t ( ) . squeeze ( ) . cpu ( ) [EOL] correct = pred_idx . eq ( target_data ) [comment] [EOL] return ( correct . sum ( ) . float ( ) / batch_size ) * [number] [EOL] [EOL] [EOL] def train ( args , data_loader , model , loss_func , optimizer , epoch , writer , log_step , device ) : [EOL] [comment] [EOL] model . train ( ) [EOL] [EOL] loss_avg = AverageMeter ( ) [EOL] acc_avg = AverageMeter ( ) [EOL] batch_time_avg = AverageMeter ( ) [EOL] [EOL] for i , ( input_data , target_data ) in enumerate ( data_loader ) : [EOL] start_time = time . perf_counter ( ) [EOL] [EOL] output_data = model ( input_data . to ( device ) ) [EOL] loss = loss_func ( output_data , target_data . to ( device ) ) [EOL] loss_avg . update ( loss . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] acc = calc_accuracy ( output_data . data , target_data ) [EOL] acc_avg . update ( acc . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] optimizer . zero_grad ( ) [EOL] loss . backward ( ) [EOL] optimizer . step ( ) [EOL] [EOL] batch_time = time . perf_counter ( ) - start_time [EOL] batch_time_avg . update ( batch_time ) [EOL] [EOL] if ( ( i + [number] ) % args . log_interval ) == [number] : [EOL] print ( [string] . format ( epoch + [number] , args . max_epoch , i + [number] , len ( data_loader ) , loss . item ( ) , loss_avg . avg , acc . item ( ) , acc_avg . avg , batch_time * args . log_interval , batch_time_avg . avg * args . log_interval , ) ) [EOL] if writer : [EOL] writer . add_scalar ( [string] , loss . item ( ) , log_step ) [EOL] writer . add_scalar ( [string] , acc . item ( ) , log_step ) [EOL] log_step += [number] [EOL] print ( [string] . format ( acc_avg . avg , batch_time_avg . sum ) ) [EOL] return log_step [EOL] [EOL] [EOL] def validate ( args , data_loader , model , loss_func , epoch , writer , log_step , device ) : [EOL] [comment] [EOL] model . eval ( ) [EOL] [EOL] loss_avg = AverageMeter ( ) [EOL] acc_avg = AverageMeter ( ) [EOL] batch_time_avg = AverageMeter ( ) [EOL] [EOL] for i , ( input_data , target_data ) in enumerate ( data_loader ) : [EOL] start_time = time . perf_counter ( ) [EOL] [EOL] output_data = model ( input_data . to ( device ) ) [EOL] loss = loss_func ( output_data , target_data . to ( device ) ) [EOL] loss_avg . update ( loss . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] acc = calc_accuracy ( output_data . data , target_data ) [EOL] acc_avg . update ( acc . item ( ) , target_data . size ( [number] ) ) [EOL] [EOL] batch_time = time . perf_counter ( ) - start_time [EOL] batch_time_avg . update ( batch_time ) [EOL] [EOL] if ( ( i + [number] ) % args . log_interval ) == [number] : [EOL] print ( [string] . format ( epoch + [number] , args . max_epoch , i + [number] , len ( data_loader ) , loss . item ( ) , loss_avg . avg , acc . item ( ) , acc_avg . avg , batch_time * args . log_interval , batch_time_avg . avg * args . log_interval , ) ) [EOL] if writer : [EOL] writer . add_scalar ( [string] , loss . item ( ) , log_step ) [EOL] writer . add_scalar ( [string] , acc . item ( ) , log_step ) [EOL] log_step += [number] [EOL] print ( [string] . format ( acc_avg . avg , batch_time_avg . sum ) ) [EOL] return acc_avg . avg , log_step [EOL] [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = str , default = [string] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = bool , default = True ) [EOL] parser . add_argument ( [string] , type = bool , default = True ) [EOL] parser . add_argument ( [string] , type = bool , default = True ) [EOL] args = parser . parse_args ( args = [ ] ) [EOL] [EOL] if args . use_logfile : [EOL] if args . use_octconv : [EOL] writer = SummaryWriter ( [string] ) [EOL] else : [EOL] writer = SummaryWriter ( [string] ) [EOL] else : [EOL] writer = None [EOL] [EOL] [comment] [EOL] if args . use_gpu : [EOL] print ( [string] ) [EOL] device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] else : [EOL] device = torch . device ( [string] ) [EOL] if device == [string] : [EOL] torch . backends . cudnn . benchmark = True [EOL] print ( [string] . format ( device ) ) [EOL] [EOL] [comment] [EOL] transform_train = transforms . Compose ( [ transforms . ToTensor ( ) , transforms . Lambda ( lambda x : F . pad ( x . unsqueeze ( [number] ) , ( [number] , [number] , [number] , [number] ) , mode = [string] ) . squeeze ( ) ) , transforms . ToPILImage ( ) , transforms . RandomCrop ( [number] ) , transforms . RandomHorizontalFlip ( ) , transforms . ToTensor ( ) , transforms . Normalize ( ( [number] , ) , ( [number] , ) ) , ] ) [EOL] [comment] [EOL] transform_validate = transforms . Compose ( [ transforms . ToTensor ( ) , transforms . Normalize ( ( [number] , ) , ( [number] , ) ) ] ) [EOL] [EOL] [comment] [EOL] train_data_set = datasets . CIFAR10 ( args . datasets_dir , train = True , transform = transform_train , download = True ) [EOL] validate_data_set = datasets . CIFAR10 ( args . datasets_dir , train = False , transform = transform_validate , download = True ) [EOL] [EOL] [comment] [EOL] train_data_loader = torch . utils . data . DataLoader ( train_data_set , batch_size = args . batch_size , shuffle = True , num_workers = [number] , pin_memory = True ) [EOL] validate_data_loader = torch . utils . data . DataLoader ( validate_data_set , batch_size = args . batch_size , shuffle = True , num_workers = [number] , pin_memory = True ) [EOL] [EOL] [comment] [EOL] if args . use_octconv : [EOL] model = SimpleOctConvNet ( args . innter_channnels ) [EOL] else : [EOL] model = SimpleConvNet ( args . innter_channnels ) [EOL] print ( model ) [EOL] model . to ( device ) [EOL] loss_func = nn . CrossEntropyLoss ( ) [EOL] optimizer = torch . optim . Adam ( model . parameters ( ) , lr = [number] ) [EOL] [EOL] best_acc = [number] [EOL] train_log_step = [number] [EOL] validate_log_step = [number] [EOL] for epoch in range ( args . max_epoch ) : [EOL] train_log_step = train ( args , train_data_loader , model , loss_func , optimizer , epoch , writer , train_log_step , device ) [EOL] epoch_acc , validate_log_step = validate ( args , validate_data_loader , model , loss_func , epoch , writer , validate_log_step , device ) [EOL] best_acc = max ( epoch_acc , best_acc ) [EOL] [EOL] print ( [string] . format ( best_acc ) ) [EOL] if writer : [EOL] writer . close ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] [EOL] from models . octave_conv import OctaveConv [EOL] [EOL] [EOL] class Conv_BN_Act ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_channels , kernel_size , stride = [number] , padding = [number] , bias = False ) : [EOL] super ( Conv_BN_Act , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( nn . Conv2d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , bias = bias ) , nn . BatchNorm2d ( out_channels ) , nn . ReLU ( inplace = True ) , ) [EOL] [EOL] def forward ( self , x ) : [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class OctConv_BN_Act ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_channels , kernel_size , alpha_in , alpha_out , stride = [number] , padding = [number] , bias = False ) : [EOL] super ( OctConv_BN_Act , self ) . __init__ ( ) [EOL] [EOL] self . conv = OctaveConv ( in_channels , out_channels , kernel_size , alpha_in , alpha_out , stride , padding ) [EOL] [EOL] l_out_channels = int ( alpha_out * out_channels ) [EOL] h_out_channels = out_channels - l_out_channels [EOL] self . bn_h = nn . BatchNorm2d ( h_out_channels ) [EOL] self . bn_l = nn . BatchNorm2d ( l_out_channels ) [EOL] self . act = nn . ReLU ( inplace = True ) [EOL] [EOL] def forward ( self , x ) : [EOL] x_h , x_l = self . conv ( x ) [EOL] x_h = self . act ( self . bn_h ( x_h ) ) [EOL] x_l = self . act ( self . bn_l ( x_l ) ) if x_l is not None else None [EOL] return x_h , x_l [EOL] [EOL] [EOL] class SimpleConvNet ( nn . Module ) : [EOL] def __init__ ( self , innter_channnels ) : [EOL] super ( SimpleConvNet , self ) . __init__ ( ) [EOL] self . layers = nn . Sequential ( Conv_BN_Act ( [number] , innter_channnels , [number] ) , Conv_BN_Act ( innter_channnels , innter_channnels , [number] ) , Conv_BN_Act ( innter_channnels , innter_channnels , [number] ) , nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) , Conv_BN_Act ( innter_channnels , innter_channnels * [number] , [number] ) , Conv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] ) , Conv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] ) , nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) , Conv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] ) , Conv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] ) , Conv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] ) , nn . MaxPool2d ( [number] , stride = [number] , padding = [number] ) , nn . AdaptiveAvgPool2d ( ( [number] , [number] ) ) , ) [EOL] self . fc = nn . Linear ( innter_channnels * [number] , [number] ) [EOL] [EOL] [comment] [EOL] for m in self . modules ( ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . xavier_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] elif isinstance ( m , nn . Conv2d ) : [EOL] nn . init . kaiming_normal_ ( m . weight , mode = [string] , nonlinearity = [string] ) [EOL] elif isinstance ( m , nn . BatchNorm2d ) : [EOL] nn . init . constant_ ( m . weight , [number] ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = self . layers ( x ) [EOL] x = x . view ( x . size ( [number] ) , - [number] ) [EOL] x = self . fc ( x ) [EOL] return F . softmax ( x , dim = - [number] ) [EOL] [EOL] [EOL] class SimpleOctConvNet ( nn . Module ) : [EOL] def __init__ ( self , innter_channnels ) : [EOL] super ( SimpleOctConvNet , self ) . __init__ ( ) [EOL] alpha = [number] [EOL] self . layer1 = nn . Sequential ( OctConv_BN_Act ( [number] , innter_channnels , [number] , [number] , alpha ) , OctConv_BN_Act ( innter_channnels , innter_channnels , [number] , alpha , alpha ) , OctConv_BN_Act ( innter_channnels , innter_channnels , [number] , alpha , alpha ) , ) [EOL] self . layer2 = nn . Sequential ( OctConv_BN_Act ( innter_channnels , innter_channnels * [number] , [number] , alpha , alpha ) , OctConv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] , alpha , alpha ) , OctConv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] , alpha , alpha ) , ) [EOL] self . layer3 = nn . Sequential ( OctConv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] , alpha , alpha ) , OctConv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] , alpha , alpha ) , OctConv_BN_Act ( innter_channnels * [number] , innter_channnels * [number] , [number] , alpha , [number] ) , ) [EOL] self . layer4 = nn . Sequential ( nn . MaxPool2d ( [number] , stride = [number] , padding = [number] ) , nn . AdaptiveAvgPool2d ( ( [number] , [number] ) ) ) [EOL] self . downsample = nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) [EOL] self . fc = nn . Linear ( innter_channnels * [number] , [number] ) [EOL] [EOL] [comment] [EOL] for m in self . modules ( ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] nn . init . xavier_normal_ ( m . weight ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] elif isinstance ( m , nn . Conv2d ) : [EOL] nn . init . kaiming_normal_ ( m . weight , mode = [string] , nonlinearity = [string] ) [EOL] elif isinstance ( m , nn . BatchNorm2d ) : [EOL] nn . init . constant_ ( m . weight , [number] ) [EOL] nn . init . constant_ ( m . bias , [number] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x_h , x_l = self . layer1 ( x ) [EOL] x_h , x_l = self . downsample ( x_h ) , self . downsample ( x_l ) [EOL] x_h , x_l = self . layer2 ( ( x_h , x_l ) ) [EOL] x_h , x_l = self . downsample ( x_h ) , self . downsample ( x_l ) [EOL] x_h , _ = self . layer3 ( ( x_h , x_l ) ) [EOL] x = self . layer4 ( x_h ) [EOL] x = x . view ( x . size ( [number] ) , - [number] ) [EOL] x = self . fc ( x ) [EOL] return F . softmax ( x , dim = - [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import cifar10 [EOL] import torch . nn as nn [EOL] [EOL] [EOL] class OctaveConv ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_channels , kernel_size , alpha_in = [number] , alpha_out = [number] , stride = [number] , padding = [number] , dilation = [number] , groups = [number] , bias = False , ) : [EOL] super ( OctaveConv , self ) . __init__ ( ) [EOL] assert stride == [number] or stride == [number] , [string] [EOL] assert [number] <= alpha_in < [number] and [number] <= alpha_out < [number] , [string] [EOL] [EOL] self . alpha_in = alpha_in [EOL] self . alpha_out = alpha_out [EOL] self . stride = stride [EOL] [EOL] l_in_channels = int ( alpha_in * in_channels ) [EOL] h_in_channels = in_channels - l_in_channels [EOL] l_out_channels = int ( alpha_out * out_channels ) [EOL] h_out_channels = out_channels - l_out_channels [EOL] assert h_in_channels > [number] and h_out_channels > [number] , [string] [EOL] [EOL] self . conv_h2h = nn . Conv2d ( h_in_channels , h_out_channels , kernel_size , [number] , padding , dilation , groups , bias ) [EOL] self . conv_h2l = ( None [EOL] if ( l_out_channels == [number] ) [EOL] else nn . Conv2d ( h_in_channels , l_out_channels , kernel_size , [number] , padding , dilation , groups , bias ) ) [EOL] self . conv_l2h = ( None [EOL] if ( l_in_channels == [number] ) [EOL] else nn . Conv2d ( l_in_channels , h_out_channels , kernel_size , [number] , padding , dilation , groups , bias ) ) [EOL] self . conv_l2l = ( None [EOL] if ( l_in_channels == [number] ) or ( l_out_channels == [number] ) [EOL] else nn . Conv2d ( l_in_channels , l_out_channels , kernel_size , [number] , padding , dilation , groups , bias ) ) [EOL] [EOL] self . downsample = nn . AvgPool2d ( kernel_size = ( [number] , [number] ) , stride = [number] ) [EOL] self . upsample = nn . Upsample ( scale_factor = [number] , mode = [string] ) [EOL] [EOL] def forward ( self , x ) : [EOL] x_h , x_l = x if type ( x ) is tuple else ( x , None ) [EOL] [comment] [EOL] if x_h is not None : [EOL] x_h = self . downsample ( x_h ) if self . stride == [number] else x_h [EOL] [EOL] [comment] [EOL] x_h2h = self . conv_h2h ( x_h ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] if self . conv_h2l is not None : [EOL] x_h2l = self . conv_h2l ( self . downsample ( x_h ) ) [EOL] [comment] [EOL] else : [EOL] x_h2l = None [EOL] [comment] [EOL] if x_l is not None : [EOL] [comment] [EOL] if self . conv_l2h is not None : [EOL] x_l2h = self . conv_l2h ( x_l ) [EOL] x_l2h = self . upsample ( x_l2h ) if self . stride == [number] else x_l2h [EOL] [comment] [EOL] else : [EOL] [comment] [EOL] x_l2h = None [EOL] [EOL] [comment] [EOL] if self . conv_l2l is not None : [EOL] x_l2l = self . downsample ( x_l ) if self . stride == [number] else x_l [EOL] x_l2l = self . conv_l2l ( x_l2l ) [EOL] [comment] [EOL] else : [EOL] x_l2l = None [EOL] [EOL] x_h = x_h2h + x_l2h [EOL] x_l = x_h2l + x_l2l if x_h2l is not None and x_l2l is not None else None [EOL] return x_h , x_l [EOL] else : [EOL] return x_h2h , x_h2l [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import torch [EOL] [EOL] octconv = OctaveConv ( [number] , [number] , [number] , alpha_in = [number] , alpha_out = [number] , stride = [number] , padding = [number] ) [EOL] print ( octconv ) [EOL] input_tsr = torch . randn ( [number] , [number] , [number] , [number] ) [EOL] print ( [string] , input_tsr . shape ) [EOL] output_tsr_h , output_tsr_l = octconv ( input_tsr ) [EOL] print ( [string] , output_tsr_h . shape ) [EOL] print ( [string] , output_tsr_l . shape ) [EOL] [EOL] print ( [string] * [number] ) [EOL] octconv_2 = OctaveConv ( [number] , [number] , [number] , alpha_in = [number] , alpha_out = [number] , stride = [number] , padding = [number] ) [EOL] print ( octconv_2 ) [EOL] output_tsr_h , output_tsr_l = octconv_2 ( ( output_tsr_h , output_tsr_l ) ) [EOL] print ( [string] , output_tsr_h . shape ) [EOL] print ( [string] , output_tsr_l . shape ) [EOL] [EOL] print ( [string] * [number] ) [EOL] octconv_2_1 = OctaveConv ( [number] , [number] , [number] , alpha_in = [number] , alpha_out = [number] , stride = [number] , padding = [number] ) [EOL] print ( octconv_2_1 ) [EOL] output_tsr_h , output_tsr_l = octconv_2_1 ( ( output_tsr_h , output_tsr_l ) ) [EOL] print ( [string] , output_tsr_h . shape ) [EOL] print ( [string] , output_tsr_l . shape ) [EOL] [EOL] print ( [string] * [number] ) [EOL] octconv_3 = OctaveConv ( [number] , [number] , [number] , alpha_in = [number] , alpha_out = [number] , stride = [number] , padding = [number] ) [EOL] print ( octconv_3 ) [EOL] output_tsr_h , _ = octconv_3 ( ( output_tsr_h , output_tsr_l ) ) [EOL] print ( [string] , output_tsr_h . shape ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 0 $None$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $None$ 0 $typing.Any$ 0 $None$ 0 $None$ 0 $None$ 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 $cifar10.models.octave_conv.OctaveConv$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0