from typing import List , Any [EOL] import typing [EOL] import apiclient [EOL] import builtins [EOL] from apiclient import discovery [EOL] import spacy [EOL] [EOL] API_KEY = [string] [EOL] VERSION = [string] [EOL] API = [string] [EOL] FACTCHECK = discovery . build ( API , VERSION , developerKey = API_KEY ) [EOL] [EOL] [EOL] nlp = spacy . load ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def is_claim_a_quote ( claim , quote , similarity_threshold ) : [EOL] [docstring] [EOL] parsed_claim = nlp ( claim ) [EOL] parsed_quote = nlp ( quote ) [EOL] [EOL] [comment] [EOL] parsed_claim_no_stop_words = nlp ( [string] . join ( [ str ( t ) for t in parsed_claim if not t . is_stop ] ) ) [EOL] parsed_quote_no_stop_words = nlp ( [string] . join ( [ str ( t ) for t in parsed_quote if not t . is_stop ] ) ) [EOL] [EOL] return ( parsed_claim_no_stop_words . similarity ( parsed_quote_no_stop_words ) >= similarity_threshold ) [EOL] [EOL] [EOL] def fact_check_claim ( claim , FACTCHECK ) : [EOL] [docstring] [EOL] [comment] [EOL] items = FACTCHECK . claims ( ) . search ( query = claim ) . execute ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] quotes = [ claim [ [string] ] for claim in items [ [string] ] ] [EOL] is_quote = is_claim_a_quote ( claim , quotes [ [number] ] , similarity_threshold = [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if is_quote : [EOL] claim_reviews = [ claim [ [string] ] [ [number] ] for claim in items [ [string] ] ] [EOL] return claim_reviews [ [number] ] [EOL] else : [EOL] return [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import builtins [EOL] import glob [EOL] [EOL] [EOL] def append_files_from_path ( path ) : [EOL] [docstring] [EOL] paths = glob . glob ( path + [string] ) [EOL] text = [string] [EOL] for path in paths : [EOL] with open ( path , [string] ) as f : [EOL] read_text = f . read ( ) [EOL] text += read_text [EOL] return text [EOL] [EOL] [EOL] text = append_files_from_path ( [string] ) [EOL] [EOL] [EOL] from typing import List [EOL] [EOL] [EOL] def flatten_list_of_lists ( l ) : [EOL] [docstring] [EOL] return [string] . join ( l ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] import random [EOL] import string [EOL] [EOL] import spacy [EOL] from sklearn . base import TransformerMixin [EOL] from sklearn . feature_extraction . stop_words import ENGLISH_STOP_WORDS as stopwords [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] from sklearn . metrics import accuracy_score [EOL] from sklearn . pipeline import Pipeline [EOL] from sklearn . svm import LinearSVC [EOL] [EOL] [EOL] def listen_for_textual_claims ( text_stream ) : [EOL] [docstring] [EOL] claim = None [EOL] return claim [EOL] [EOL] [EOL] punctuations = string . punctuation [EOL] [EOL] parser = spacy . load ( [string] ) [EOL] [EOL] [comment] [EOL] class predictors ( TransformerMixin ) : [EOL] def transform ( self , X , ** transform_params ) : [EOL] return [ clean_text ( text ) for text in X ] [EOL] [EOL] def fit ( self , X , y = None , ** fit_params ) : [EOL] return self [EOL] [EOL] def get_params ( self , deep = True ) : [EOL] return { } [EOL] [EOL] [EOL] [comment] [EOL] def clean_text ( text ) : [EOL] return text . strip ( ) . lower ( ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] def spacy_tokenizer ( sentence ) : [EOL] tokens = parser ( sentence ) [EOL] tokens = [ tok . lemma_ . lower ( ) . strip ( ) if tok . lemma_ != [string] else tok . lower_ for tok in tokens ] [EOL] tokens = [ tok for tok in tokens if ( tok not in stopwords and tok not in punctuations ) ] [EOL] return tokens [EOL] [EOL] [EOL] [comment] [EOL] vectorizer = CountVectorizer ( tokenizer = spacy_tokenizer , ngram_range = ( [number] , [number] ) ) [EOL] classifier = LinearSVC ( ) [EOL] [EOL] [comment] [EOL] pipe = Pipeline ( [ ( [string] , predictors ( ) ) , ( [string] , vectorizer ) , ( [string] , classifier ) ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] data = [ ] [EOL] random . shuffle ( data ) [EOL] [EOL] n_train , n_test = int ( len ( data ) * [number] ) , len ( data ) - int ( len ( data ) * [number] ) [EOL] train , test = data [ : n_train ] , data [ n_train : ] [EOL] [EOL] [comment] [EOL] pipe . fit ( [ x [ [number] ] for x in train ] , [ x [ [number] ] for x in train ] ) [EOL] pred_data = pipe . predict ( [ x [ [number] ] for x in test ] ) [EOL] for ( sample , pred ) in zip ( test , pred_data ) : [EOL] print ( sample , pred ) [EOL] print ( [string] , accuracy_score ( [ x [ [number] ] for x in test ] , pred_data ) ) [EOL] [EOL] [EOL] def find_numerical_claims ( ) : [EOL] [docstring] [EOL] [EOL] [EOL] def find_entity_and_event_properties ( ) : [EOL] [docstring] [EOL] [EOL] [EOL] def find_position_statements ( ) : [EOL] [docstring] [EOL] [EOL] [EOL] def find_quote_verification_assessments ( ) : [EOL] [docstring] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 $builtins.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import builtins [EOL] import requests [EOL] import time [EOL] import urllib . request [EOL] [EOL] import requests [EOL] [EOL] from bs4 import BeautifulSoup [EOL] [EOL] url = [string] [EOL] response = requests . get ( url ) [EOL] soup = BeautifulSoup ( response . text , [string] ) [EOL] [EOL] [comment] [EOL] paragraphs = [ p . text for p in soup . findAll ( [string] ) if len ( p . text ) > [number] ] [EOL] [EOL] [EOL] [comment] [EOL] def paragraph_to_summary ( paragraph , summary_len ) : [EOL] [docstring] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0