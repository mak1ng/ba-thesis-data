from setuptools import setup [EOL] [EOL] setup ( name = [string] , version = [string] , description = [string] , url = [string] , author = [string] , license = [string] , packages = [ [string] , [string] , [string] ] , install_requires = [ [string] , [string] , [string] , [string] , [string] ] , zip_safe = False ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import gym [EOL] from gym import logger [EOL] from core . states import StateSerializer [EOL] from core . runner import Runner [EOL] from agents . dqn_agent import DQNAgent [EOL] [EOL] logger . set_level ( logger . INFO ) [EOL] [EOL] env_name = [string] [EOL] env = gym . make ( env_name ) [EOL] env . _max_episode_steps = [number] [EOL] [EOL] serializer = StateSerializer ( env . observation_space . shape ) [EOL] [EOL] agent = DQNAgent . from_h5 ( file_path = env_name + [string] ) [EOL] [EOL] runner = Runner ( env , serializer , agent , epsilon_policy = lambda e : [number] , max_episode_steps = [number] ) [EOL] [EOL] runner . render ( ) [EOL] [EOL] runner . demonstrate ( num_episodes = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import gym [EOL] from gym import logger [EOL] from agents import ddqn_agent [EOL] from core . states import StateSerializer [EOL] from core . runner import constant_decay_epsilon , Runner [EOL] from core . visualization import rolling_mean [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] logger . set_level ( logger . INFO ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] env_name = [string] [EOL] env = gym . make ( env_name ) [EOL] env . seed ( [number] ) [EOL] [EOL] [comment] [EOL] serializer = StateSerializer . from_num_states ( env . observation_space . n ) [EOL] [EOL] [comment] [EOL] agent = ddqn_agent . DDQNAgent ( env . action_space . n , serializer . shape , gamma = [number] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] epochs = [number] [EOL] episodes = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] runner = Runner ( env , serializer , agent , epsilon_policy = lambda e : constant_decay_epsilon ( e , initial_epsilon = [number] , decay_rate = [number] , min_epsilon = [number] ) , training_period = [number] , max_episode_steps = [number] ) [EOL] [EOL] runner . warm_up ( ) [EOL] history = runner . train ( epochs , episodes ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] results = runner . demonstrate ( num_episodes = [number] ) [EOL] [EOL] rolling_mean ( [ history [ [string] ] , history [ [string] ] ] ) [EOL] [EOL] agent . save ( env_name ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0
from typing import Any , List [EOL] import typing [EOL] import numpy as np [EOL] import random [EOL] import time [EOL] [EOL] x = [number] [comment] [EOL] y = [number] [comment] [EOL] z = [number] [comment] [EOL] [EOL] m1 = np . zeros ( ( x , y , z ) ) [EOL] m2 = np . zeros ( ( x , y , z ) ) [EOL] m3 = np . zeros ( ( x , y , z ) ) [EOL] [EOL] time1 = [ ] [EOL] time2 = [ ] [EOL] time3 = [ ] [EOL] [EOL] vals = np . ones ( ( y , z ) ) [EOL] [EOL] for i in range ( [number] ) : [EOL] act = np . array ( [ random . randint ( [number] , x - [number] ) for _ in range ( y ) ] ) [EOL] m = np . array ( [ random . randint ( [number] , z - [number] ) for _ in range ( y * z ) ] ) . reshape ( ( y , z ) ) [EOL] [EOL] ii = np . arange ( y ) [EOL] jj = np . arange ( z ) [EOL] [EOL] start = time . time ( ) [EOL] for i in range ( y ) : [EOL] for j in range ( z ) : [EOL] m1 [ act [ i ] , i , m [ i , j ] ] += vals [ i , j ] [EOL] time1 . append ( time . time ( ) - start ) [EOL] [EOL] start = time . time ( ) [EOL] for i in range ( y ) : [EOL] m2 [ act [ i ] , i , m [ i , jj ] ] += vals [ i ] [EOL] time2 . append ( time . time ( ) - start ) [EOL] [EOL] start = time . time ( ) [EOL] t = np . zeros ( m3 [ act , ii ] . shape ) [EOL] np . put_along_axis ( t , m , vals , axis = [number] ) [EOL] m3 [ act , ii ] += t [EOL] time3 . append ( time . time ( ) - start ) [EOL] [EOL] if not np . array_equal ( m1 , m2 ) : [EOL] print ( m1 , [string] , m2 ) [EOL] raise AssertionError ( ) [EOL] [EOL] if not np . array_equal ( m2 , m3 ) : [EOL] print ( m2 , [string] , m3 ) [EOL] raise AssertionError ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] mean1 = np . mean ( time1 ) [EOL] mean2 = np . mean ( time2 ) [EOL] mean3 = np . mean ( time3 ) [EOL] tmax = max ( mean1 , mean2 , mean3 ) [EOL] print ( [string] ) [EOL] print ( [string] . format ( mean1 , [number] * mean1 / tmax ) ) [EOL] print ( [string] . format ( mean2 , [number] * mean2 / tmax ) ) [EOL] print ( [string] . format ( mean3 , [number] * mean3 / tmax ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import random [EOL] import time [EOL] import numpy as np [EOL] import pandas as pd [EOL] from gym import logger [EOL] [EOL] [docstring] [EOL] [EOL] [EOL] def constant_decay_epsilon ( epoch , initial_epsilon = [number] , decay_rate = [number] , min_epsilon = [number] ) : [EOL] epsilon = initial_epsilon * decay_rate ** epoch [EOL] return max ( epsilon , min_epsilon ) [EOL] [EOL] [EOL] class Runner : [EOL] [EOL] def __init__ ( self , env , serializer , agent , epsilon_policy = lambda e : constant_decay_epsilon ( e ) , training_period = [number] , max_episode_steps = [number] ) : [EOL] self . _env = env [EOL] self . _serializer = serializer [EOL] self . _agent = agent [EOL] self . _epsilon_policy = epsilon_policy [EOL] self . _max_episode_steps = max_episode_steps [EOL] self . _history = pd . DataFrame ( columns = [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] self . _epochs_trained = [number] [EOL] self . _train_period = training_period [EOL] self . _train_steps = [number] [EOL] [EOL] def warm_up ( self , num_steps = [number] ) : [EOL] [docstring] [EOL] state = self . _env . reset ( ) [EOL] for s in range ( num_steps ) : [EOL] action = self . _env . action_space . sample ( ) [EOL] next_state , reward , done , _ = self . _env . step ( action ) [EOL] self . _agent . process_observation ( self . _serializer . serialize ( state ) , action , reward , self . _serializer . serialize ( next_state ) , done ) [EOL] state = next_state [EOL] if done : [EOL] state = self . _env . reset ( ) [EOL] [EOL] def train ( self , num_epochs , num_episodes , render_frequency = [number] ) : [EOL] for _ in range ( num_epochs ) : [EOL] epsilon = self . _epsilon_policy ( self . _epochs_trained ) [EOL] t , rewards , steps , aborted_episodes = self . run_epoch ( epsilon , num_episodes , training = True , render_frequency = render_frequency ) [EOL] logger . info ( [string] [string] . format ( t , t * [number] / num_episodes , self . _epochs_trained , epsilon , np . mean ( rewards ) , np . mean ( steps ) , aborted_episodes * [number] / num_episodes , aborted_episodes ) ) [EOL] self . _epochs_trained += [number] [EOL] return self . history [EOL] [EOL] def demonstrate ( self , num_episodes ) : [EOL] t , rewards , steps , aborted_episodes = self . run_epoch ( epsilon = [number] , num_episodes = num_episodes , training = False ) [EOL] logger . info ( [string] [string] . format ( t , t * [number] / num_episodes , num_episodes , np . mean ( rewards ) , np . mean ( steps ) , aborted_episodes * [number] / num_episodes , aborted_episodes ) ) [EOL] return t , np . mean ( rewards ) , np . mean ( steps ) , aborted_episodes [EOL] [EOL] def render ( self ) : [EOL] reward , done , steps = self . run_episode ( epsilon = [number] , training = False , render = True ) [EOL] logger . info ( [string] . format ( steps , reward ) ) [EOL] return reward , done , steps [EOL] [EOL] @ property def history ( self ) : [EOL] return self . _history [EOL] [EOL] def run_episode ( self , epsilon , training = True , render = False ) : [EOL] state = self . _env . reset ( ) [EOL] if render : [EOL] self . _env . render ( ) [EOL] total_reward = [number] [EOL] h = None [EOL] for step in range ( self . _max_episode_steps ) : [EOL] action = self . _agent . act ( self . _serializer . serialize ( state ) ) [EOL] if random . random ( ) < epsilon : [EOL] action = self . _env . action_space . sample ( ) [EOL] next_state , reward , done , _ = self . _env . step ( action ) [EOL] total_reward += reward [EOL] if training : [EOL] self . _agent . process_observation ( self . _serializer . serialize ( state ) , action , reward , self . _serializer . serialize ( next_state ) , done ) [EOL] self . _train_steps += [number] [EOL] if self . _train_steps % self . _train_period == [number] : [EOL] h = self . _agent . train ( self . _train_steps ) [EOL] if render : [EOL] self . _env . render ( ) [EOL] if done : [EOL] break [EOL] state = next_state [EOL] if training and h is not None : [EOL] self . _history = self . _history . append ( { [string] : epsilon , [string] : total_reward , [string] : step + [number] , [string] : not done , [string] : np . mean ( h . history [ [string] ] ) } , ignore_index = True ) [EOL] return total_reward , done , step + [number] [EOL] [EOL] def run_epoch ( self , epsilon , num_episodes , training = True , render_frequency = [number] ) : [EOL] rewards = [ ] [EOL] steps = [ ] [EOL] aborted_episodes = [number] [EOL] start = time . time ( ) [EOL] if render_frequency > num_episodes : [EOL] logger . warn ( [string] . format ( render_frequency , num_episodes ) ) [EOL] for i in range ( num_episodes ) : [EOL] render = False [EOL] if render_frequency != [number] and ( i + [number] ) % render_frequency == [number] : [EOL] render = True [EOL] r , done , step = self . run_episode ( epsilon , training , render ) [EOL] if render_frequency != [number] and ( i + [number] ) % render_frequency == [number] : [EOL] logger . info ( [string] . format ( step , r ) ) [EOL] rewards . append ( r ) [EOL] steps . append ( step ) [EOL] if not done : [EOL] aborted_episodes += [number] [EOL] end = time . time ( ) [EOL] [EOL] return end - start , rewards , steps , aborted_episodes [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 $builtins.int$ 0
from typing import Any , Tuple [EOL] import builtins [EOL] import my_agents [EOL] import typing [EOL] import numpy as np [EOL] [EOL] def one_hot ( size , idx ) : [EOL] res = np . zeros ( size ) [EOL] res [ idx ] = [number] [EOL] return res [EOL] [EOL] class StateSerializer : [EOL] [docstring] [EOL] def __init__ ( self , state_shape ) : [EOL] self . _state_shape = state_shape [EOL] self . _one_hot = False [EOL] [EOL] def serialize ( self , state ) : [EOL] if self . _one_hot : [EOL] return one_hot ( self . _state_shape [ [number] ] , state ) [EOL] return state [EOL] [EOL] def deserialize ( self , state ) : [EOL] if self . _one_hot : [EOL] return np . argmax ( state ) [EOL] return state [EOL] [EOL] @ property def shape ( self ) : [EOL] return self . _state_shape [EOL] [EOL] @ staticmethod def from_num_states ( num_states ) : [EOL] handler = StateSerializer ( state_shape = ( num_states , ) ) [EOL] handler . _one_hot = True [EOL] return handler [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $my_agents.core.states.StateSerializer$ 0 0 0 0 0 0 0 0 0 0 0 $my_agents.core.states.StateSerializer$ 0 $builtins.bool$ 0 0 0 0 $my_agents.core.states.StateSerializer$ 0 0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import pandas [EOL] import platform [EOL] if platform . system ( ) == [string] : [EOL] import matplotlib [EOL] matplotlib . use ( [string] ) [EOL] import seaborn as sns [EOL] import matplotlib . pyplot as plt [EOL] import pandas as pd [EOL] from typing import List [EOL] from math import sqrt , ceil [EOL] [EOL] [EOL] def rolling_mean ( history , window = [number] , label = None , axis = None , show = True ) : [EOL] cols = max ( ceil ( sqrt ( len ( history ) ) ) , [number] ) [EOL] rows = max ( ceil ( len ( history ) / cols ) , [number] ) [EOL] if axis is None : [EOL] fig , axis = plt . subplots ( nrows = rows , ncols = cols ) [EOL] assert len ( axis ) == len ( history ) [EOL] [EOL] for series , ax in zip ( history , axis ) : [EOL] rolling_mean = series . rolling ( window = window ) . mean ( ) [EOL] sns . lineplot ( data = rolling_mean , ax = ax ) [EOL] ax . set_xlabel ( [string] ) [EOL] ax . set_ylabel ( series . name ) [EOL] ax . grid ( b = True ) [EOL] if label is not None : [EOL] ax . legend ( [ label ] ) [EOL] [EOL] if show : [EOL] plt . show ( ) [EOL] return axis [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] import numpy as np [EOL] import pandas as pd [EOL] [EOL] [EOL] class TableAgent : [EOL] def __init__ ( self , num_actions , num_states , gamma = [number] , alpha = [number] ) : [EOL] self . _gamma = gamma [EOL] self . _alpha = alpha [EOL] self . _num_actions = num_actions [EOL] self . _num_states = num_states [EOL] self . _q_impl = { state : np . zeros ( num_actions ) for state in range ( num_states ) } [EOL] [EOL] def act ( self , state ) : [EOL] [docstring] [EOL] return self . policy ( state ) [EOL] [EOL] def process_observation ( self , state , action , reward , next_state , done ) : [EOL] [docstring] [EOL] new_q = reward [EOL] if not done : [EOL] [comment] [EOL] new_q += self . _gamma * self . V ( next_state ) [EOL] self . _q_impl [ state ] [ action ] += self . _alpha * ( new_q - self . Q ( state ) [ action ] ) [EOL] [EOL] def train ( self , step_num ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def Q ( self , state ) : [EOL] [docstring] [EOL] return self . _q_impl [ state ] [EOL] [EOL] def policy ( self , state ) : [EOL] [docstring] [EOL] return np . argmax ( self . Q ( state ) ) [EOL] [EOL] def V ( self , state ) : [EOL] [docstring] [EOL] return np . max ( self . Q ( state ) ) [EOL] [EOL] def print_q_map ( self ) : [EOL] print ( pd . DataFrame ( self . _q_impl ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.tuple$ 0 0 0 0 0 0 0 0 0 0 $builtins.tuple$ 0 0 0 0 0 0 0 0 $builtins.tuple$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.tuple$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.tuple$ 0 0 0 0 0 0 $builtins.tuple$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.tuple$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple , List [EOL] import builtins [EOL] import my_agents [EOL] import typing [EOL] import tensorflow [EOL] import numpy [EOL] from collections import deque [EOL] from typing import Tuple [EOL] import random [EOL] import numpy as np [EOL] from gym import logger [EOL] from tensorflow import keras [EOL] [EOL] [docstring] [EOL] [EOL] [EOL] def build_dense_network ( num_actions , state_shape , hidden_layers = [ [number] , [number] ] ) : [EOL] [docstring] [EOL] [EOL] model = keras . models . Sequential ( ) [EOL] [EOL] for idx , val in enumerate ( hidden_layers ) : [EOL] if idx == [number] : [EOL] model . add ( keras . layers . Dense ( val , activation = [string] , input_shape = state_shape , name = [string] ) ) [EOL] else : [EOL] model . add ( keras . layers . Dense ( val , activation = [string] , name = [string] . format ( idx ) ) ) [EOL] [EOL] model . add ( keras . layers . Dense ( num_actions , name = [string] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] model . compile ( optimizer = [string] , loss = [string] , metrics = [ [string] ] ) [comment] [EOL] [EOL] return model [EOL] [EOL] [EOL] class DQNAgent : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , num_actions , state_shape , gamma = [number] , target_update_freq = [number] , prebuilt_model = None ) : [EOL] if prebuilt_model is not None : [EOL] if num_actions is not None or state_shape is not None : [EOL] logger . warn ( [string] [string] ) [EOL] self . _q_impl = prebuilt_model [EOL] else : [EOL] self . _q_impl = build_dense_network ( num_actions , state_shape ) [EOL] [EOL] [comment] [EOL] self . _target_q_impl = keras . models . Sequential . from_config ( self . _q_impl . get_config ( ) ) [EOL] self . _update_target_model ( ) [EOL] [EOL] self . _target_update_freq = target_update_freq [EOL] self . _gamma = gamma [EOL] self . _memory = deque ( maxlen = [number] ) [EOL] [EOL] def act ( self , state ) : [EOL] [docstring] [EOL] return self . policy ( state ) [ [number] ] [EOL] [EOL] def process_observation ( self , state , action , reward , next_state , done ) : [EOL] [docstring] [EOL] self . _memory . append ( ( state , action , reward , next_state , done ) ) [EOL] [EOL] def train ( self , step_num , batch_size = [number] , epochs = [number] ) : [EOL] [docstring] [EOL] if len ( self . _memory ) <= batch_size : [EOL] logger . warning ( [string] ) [EOL] return [EOL] [EOL] minibatch = random . sample ( self . _memory , batch_size ) [EOL] states , actions , rewards , next_states , dones = zip ( * minibatch ) [EOL] states , target_qs = self . _observations_to_train_data ( np . array ( states ) , np . array ( actions ) , np . array ( rewards ) , np . array ( next_states ) , np . array ( dones ) ) [EOL] [EOL] result = self . _q_impl . fit ( states , target_qs , batch_size = batch_size , epochs = epochs , verbose = [number] ) [EOL] [EOL] if step_num % self . _target_update_freq == [number] : [EOL] self . _update_target_model ( ) [EOL] [EOL] return result [EOL] [EOL] def _update_target_model ( self ) : [EOL] self . _target_q_impl . set_weights ( self . _q_impl . get_weights ( ) ) [EOL] [EOL] def _observations_to_train_data ( self , states , actions , rewards , next_states , dones ) : [EOL] [docstring] [EOL] assert ( states . shape == next_states . shape ) [EOL] assert ( actions . shape == rewards . shape == dones . shape ) [EOL] assert ( len ( states ) == len ( actions ) ) [EOL] [EOL] batch_size = len ( actions ) [comment] [EOL] targets = rewards + np . logical_not ( dones ) * self . _gamma * self . V ( next_states ) [EOL] target_qs = self . Q ( states ) [EOL] [EOL] target_qs [ np . arange ( batch_size ) , actions ] = targets [EOL] return states , target_qs [EOL] [EOL] def Q ( self , states ) : [EOL] [docstring] [EOL] if len ( states . shape ) == [number] : [EOL] [comment] [EOL] states = states [ np . newaxis ] [EOL] [EOL] [comment] [EOL] return self . _target_q_impl . predict ( states ) [EOL] [EOL] def policy ( self , states ) : [EOL] [docstring] [EOL] return np . argmax ( self . Q ( states ) , axis = [number] ) [comment] [EOL] [EOL] def V ( self , states ) : [EOL] [docstring] [EOL] return np . max ( self . Q ( states ) , axis = [number] ) [comment] [EOL] [EOL] def save ( self , file_path = [string] ) : [EOL] [docstring] [EOL] if not file_path . endswith ( [string] ) : [EOL] file_path += [string] [EOL] logger . info ( [string] + file_path ) [EOL] self . _q_impl . save ( file_path ) [EOL] [EOL] @ staticmethod def from_h5 ( file_path = [string] , gamma = [number] , target_update_freq = [number] ) : [EOL] [docstring] [EOL] logger . info ( [string] + file_path ) [EOL] model = keras . models . load_model ( file_path ) [EOL] agent = DQNAgent ( None , None , gamma = gamma , target_update_freq = target_update_freq , prebuilt_model = model ) [EOL] return agent [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $'DQNAgent'$ 0 $builtins.str$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $my_agents.agents.dqn_agent.DQNAgent$ 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $my_agents.agents.dqn_agent.DQNAgent$ 0
from typing import Any , List [EOL] import builtins [EOL] import my_agents [EOL] import numpy [EOL] import typing [EOL] import numpy as np [EOL] [EOL] [docstring] [EOL] [EOL] [EOL] class SumTree : [EOL] def __init__ ( self , capacity ) : [EOL] self . capacity = capacity [comment] [EOL] self . tree = np . zeros ( [number] * capacity - [number] ) [EOL] self . data = np . zeros ( capacity , dtype = object ) [EOL] self . data_pointer = [number] [EOL] self . elements = [number] [EOL] [EOL] def add ( self , priority , data ) : [EOL] [docstring] [EOL] tree_index = self . data_pointer + self . capacity - [number] [EOL] self . data [ self . data_pointer ] = data [EOL] self . update ( tree_index , priority ) [EOL] self . data_pointer += [number] [EOL] self . elements += [number] [EOL] [EOL] if self . data_pointer >= self . capacity : [EOL] [comment] [EOL] [comment] [EOL] self . data_pointer = [number] [EOL] self . elements = self . capacity [EOL] [EOL] def update ( self , tree_index , priority ) : [EOL] [docstring] [EOL] change = priority - self . tree [ tree_index ] [EOL] self . tree [ tree_index ] = priority [EOL] [EOL] [comment] [EOL] while tree_index != [number] : [EOL] tree_index = ( tree_index - [number] ) // [number] [EOL] self . tree [ tree_index ] += change [EOL] [EOL] def get_leaf ( self , v ) : [EOL] [docstring] [EOL] parent_index = [number] [EOL] [EOL] while True : [EOL] left_child_index = [number] * parent_index + [number] [EOL] right_child_index = left_child_index + [number] [EOL] [EOL] if left_child_index >= len ( self . tree ) : [EOL] [comment] [EOL] leaf_index = parent_index [EOL] break [EOL] else : [EOL] [comment] [EOL] if v <= self . tree [ left_child_index ] : [EOL] parent_index = left_child_index [EOL] else : [EOL] v -= self . tree [ left_child_index ] [EOL] parent_index = right_child_index [EOL] [EOL] data_index = leaf_index - self . capacity + [number] [EOL] return leaf_index , self . tree [ leaf_index ] , self . data [ data_index ] [EOL] [EOL] @ property def total_priority ( self ) : [EOL] return self . tree [ [number] ] [EOL] [EOL] @ property def max_priority ( self ) : [EOL] return np . max ( self . tree [ - self . capacity : ] ) [EOL] [EOL] @ property def min_priority ( self ) : [EOL] return np . min ( self . tree [ - self . capacity : ] ) [EOL] [EOL] def __len__ ( self ) : [EOL] return self . elements [EOL] [EOL] [EOL] class PrioritizedMemory : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , capacity , alpha = [number] , beta = [number] , max_error = [number] ) : [EOL] self . tree = SumTree ( capacity ) [EOL] self . alpha = alpha [EOL] self . beta = beta [EOL] self . max_error = max_error [comment] [EOL] self . min_error = [number] [EOL] [EOL] def store ( self , experience ) : [EOL] [docstring] [EOL] max_priority = self . tree . max_priority [EOL] if max_priority == [number] : [EOL] max_priority = self . max_error [EOL] self . tree . add ( max_priority , experience ) [EOL] [EOL] def sample ( self , batch_size ) : [EOL] [docstring] [EOL] batch = [ ] [EOL] [EOL] tree_idx = np . zeros ( ( batch_size , ) , dtype = np . int32 ) [comment] [EOL] sample_weights = np . zeros ( ( batch_size , ) , dtype = np . float32 ) [comment] [EOL] [EOL] priority_segment = self . tree . total_priority / batch_size [EOL] [EOL] p_min = ( self . tree . min_priority + self . min_error ) / self . tree . total_priority [EOL] max_weight = ( p_min * batch_size ) ** ( - self . beta ) [EOL] [EOL] for i in range ( batch_size ) : [EOL] a , b = priority_segment * i , priority_segment * ( i + [number] ) [EOL] value = np . random . uniform ( a , b ) [EOL] index , priority , data = self . tree . get_leaf ( value ) [EOL] [EOL] sampling_probabilities = priority / self . tree . total_priority [EOL] [EOL] sample_weights [ i ] = np . power ( batch_size * sampling_probabilities , - self . beta ) / max_weight [EOL] tree_idx [ i ] = index [EOL] batch . append ( data ) [comment] [EOL] [EOL] return tree_idx , batch , sample_weights [EOL] [EOL] def batch_update ( self , tree_idx , abs_errors ) : [EOL] [docstring] [EOL] abs_errors = np . absolute ( abs_errors ) + self . min_error [comment] [EOL] clipped_errors = np . minimum ( abs_errors , self . max_error ) [EOL] ps = np . power ( clipped_errors , self . alpha ) [EOL] [EOL] for ti , p in zip ( tree_idx , ps ) : [EOL] self . tree . update ( ti , p ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . tree ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $my_agents.agents.prioritized_memory.SumTree$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 $builtins.float$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 $builtins.float$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 $numpy.ndarray$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0