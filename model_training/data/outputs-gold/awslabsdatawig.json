from typing import List [EOL] import typing [EOL] import os [EOL] import setuptools [EOL] [EOL] ROOT = os . path . dirname ( __file__ ) [EOL] [EOL] with open ( os . path . join ( ROOT , [string] , [string] ) ) as f : [EOL] required = f . read ( ) . splitlines ( ) [EOL] [EOL] with open ( [string] , [string] ) as fh : [EOL] long_description = fh . read ( ) [EOL] [EOL] setuptools . setup ( name = [string] , version = [string] , author = [string] , author_email = [string] , maintainer_email = [string] , description = [string] , long_description = long_description , long_description_content_type = [string] , url = [string] , packages = setuptools . find_packages ( ) , data_files = [ ( [string] , [ [string] ] ) ] , install_requires = required , license = [string] , python_requires = [string] , classifiers = [ [string] , [string] ] , ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import zipfile [EOL] import typing [EOL] import requests [EOL] import json [EOL] from glob import glob [EOL] import pandas as pd [EOL] from collections import defaultdict [EOL] import requests , zipfile , io [EOL] import sys [EOL] [EOL] [EOL] if len ( sys . argv ) != [number] or sys . argv [ [number] ] not in [ [string] , [string] ] : [EOL] print ( [string] . format ( sys . argv [ [number] ] ) ) [EOL] sys . exit ( [number] ) [EOL] [EOL] [EOL] def prepare_dataset ( prefix ) : [EOL] output_fn = [string] . format ( prefix ) [EOL] [EOL] def parse_and_store_dataset ( ) : [EOL] df = [ ] [EOL] for f in glob ( [string] . format ( prefix ) ) : [EOL] print ( f ) [EOL] j = json . load ( open ( f ) ) [EOL] for elem in j : [EOL] elem [ [string] ] = defaultdict ( lambda : None , elem [ [string] ] ) [EOL] df . append ( { [string] : elem [ [string] ] , [string] : elem [ [string] ] , [string] : elem [ [string] ] [ [string] ] , [string] : elem [ [string] ] [ [string] ] } ) [EOL] df = pd . DataFrame ( df ) [EOL] [EOL] df = df . loc [ ( ~ df . color . isna ( ) ) & ( ~ df . finish . isna ( ) ) ] [EOL] df . loc [ : , [string] ] = df . loc [ : , [string] ] . str . replace ( [string] , [string] ) . str . lower ( ) [EOL] df . loc [ : , [string] ] = df . loc [ : , [string] ] . str . replace ( [string] , [string] ) . str . lower ( ) [EOL] df . loc [ : , [string] ] = df . loc [ : , [string] ] . str . replace ( [string] , [string] ) [EOL] df . loc [ : , [string] ] = df . loc [ : , [string] ] . str . replace ( [string] , [string] ) [EOL] [EOL] n_distinct_categorical = [number] [EOL] [EOL] df = df . loc [ ( df . color . isin ( df . color . value_counts ( ) . index [ : n_distinct_categorical ] ) ) & ( df . finish . isin ( df . finish . value_counts ( ) . index [ : n_distinct_categorical ] ) ) ] [EOL] df [ [ [string] , [string] , [string] , [string] ] ] . to_csv ( output_fn , index = False ) [EOL] [EOL] fn = [string] . format ( prefix ) [EOL] print ( [string] . format ( fn ) ) [EOL] r = requests . get ( fn ) [EOL] z = zipfile . ZipFile ( io . BytesIO ( r . content ) ) [EOL] [EOL] print ( [string] . format ( fn ) ) [EOL] z . extractall ( ) [EOL] [EOL] print ( [string] . format ( output_fn ) ) [EOL] parse_and_store_dataset ( ) [EOL] [EOL] [EOL] prepare_dataset ( sys . argv [ [number] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import datawig [EOL] import pandas as pd [EOL] [EOL] from datawig import SimpleImputer [EOL] from datawig . utils import random_split [EOL] [EOL] import numpy as np [EOL] [EOL] [EOL] [docstring] [EOL] df = pd . read_csv ( [string] ) . sample ( n = [number] ) [EOL] df_train , df_test = random_split ( df , split_ratios = [ [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] imputer_text = SimpleImputer ( input_columns = [ [string] , [string] ] , output_column = [string] , output_path = [string] , num_hash_buckets = [number] ** [number] , tokens = [string] ) [EOL] [EOL] imputer_text . fit ( train_df = df_train , learning_rate = [number] , num_epochs = [number] , final_fc_hidden_units = [ [number] ] ) [EOL] [EOL] [comment] [EOL] imputer_text = SimpleImputer ( input_columns = [ [string] , [string] ] , output_column = [string] , output_path = [string] , ) [EOL] [EOL] imputer_text . fit_hpo ( train_df = df_train , num_epochs = [number] , num_hash_bucket_candidates = [ [number] ** [number] , [number] ** [number] ] , tokens_candidates = [ [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] [EOL] [docstring] [EOL] [comment] [EOL] n_samples = [number] [EOL] numeric_data = np . random . uniform ( - np . pi , np . pi , ( n_samples , ) ) [EOL] df = pd . DataFrame ( { [string] : numeric_data , [string] : numeric_data * [number] + np . random . normal ( [number] , [number] , ( n_samples , ) ) } ) [EOL] df_train , df_test = random_split ( df , split_ratios = [ [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] imputer_numeric = SimpleImputer ( input_columns = [ [string] ] , output_column = [string] , output_path = [string] ) [EOL] [EOL] imputer_numeric . fit ( train_df = df_train , learning_rate = [number] , num_epochs = [number] ) [EOL] [EOL] [comment] [EOL] imputer_numeric = SimpleImputer ( input_columns = [ [string] ] , output_column = [string] , output_path = [string] , ) [EOL] [EOL] imputer_numeric . fit_hpo ( train_df = df_train , num_epochs = [number] , num_hash_bucket_candidates = [ [number] ** [number] , [number] ** [number] ] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import datawig [EOL] from datawig import SimpleImputer [EOL] from datawig . utils import random_split [EOL] from sklearn . metrics import f1_score , classification_report [EOL] import pandas as pd [EOL] [EOL] [docstring] [EOL] df = pd . read_csv ( [string] ) . sample ( n = [number] ) [EOL] df_train , df_test = random_split ( df , split_ratios = [ [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] [EOL] [docstring] [EOL] [comment] [EOL] imputer = SimpleImputer ( input_columns = [ [string] , [string] ] , output_column = [string] , output_path = [string] ) [EOL] [EOL] [comment] [EOL] imputer . fit ( train_df = df_train , num_epochs = [number] ) [EOL] [EOL] [comment] [EOL] predictions = imputer . predict ( df_test ) [EOL] [EOL] [comment] [EOL] f1 = f1_score ( predictions [ [string] ] , predictions [ [string] ] , average = [string] ) [EOL] [EOL] [comment] [EOL] print ( classification_report ( predictions [ [string] ] , predictions [ [string] ] ) ) [EOL] [EOL] [comment] [EOL] [EOL] [docstring] [EOL] [comment] [EOL] imputer = SimpleImputer ( input_columns = [ [string] , [string] ] , output_column = [string] , output_path = [string] ) [EOL] [EOL] [comment] [EOL] imputer . fit_hpo ( train_df = df_train ) [EOL] [EOL] [comment] [EOL] imputer . fit_hpo ( train_df = df_train , num_epochs = [number] , patience = [number] , learning_rate_candidates = [ [number] , [number] ] , num_hash_bucket_candidates = [ [number] ** [number] ] , tokens_candidates = [ [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] [EOL] [docstring] [EOL] [comment] [EOL] imputer = SimpleImputer . load ( [string] ) [EOL] [EOL] [comment] [EOL] metrics = imputer . load_metrics ( ) [EOL] weighted_f1 = metrics [ [string] ] [EOL] avg_precision = metrics [ [string] ] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $datawig.simple_imputer.SimpleImputer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List , Union [EOL] import typing [EOL] import builtins [EOL] import datawig [EOL] [docstring] [EOL] import numpy as np [EOL] [EOL] from datawig import calibration [EOL] from datawig . column_encoders import ( BowEncoder , CategoricalEncoder , SequentialEncoder ) [EOL] from datawig . imputer import Imputer [EOL] from datawig . mxnet_input_symbols import ( BowFeaturizer , EmbeddingFeaturizer , LSTMFeaturizer ) [EOL] from datawig . utils import random_split [EOL] [EOL] [EOL] def generate_synthetic_data ( K = [number] , N = [number] , p_correct = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] train_labels = np . array ( [ np . random . randint ( [number] ) for n in range ( N ) ] ) [EOL] [EOL] [comment] [EOL] train_data = np . empty ( [ N , K ] ) [EOL] [EOL] for n in range ( N ) : [EOL] [comment] [EOL] if np . random . rand ( ) < p_correct : [EOL] label = train_labels [ n ] [EOL] else : [EOL] label = np . random . choice ( [ val for val in range ( K ) if val != train_labels [ n ] ] ) [EOL] [EOL] [comment] [EOL] for k in range ( K ) : [EOL] if label == k : [EOL] train_data [ n , k ] = np . random . uniform ( [number] , [number] ) [EOL] else : [EOL] train_data [ n , k ] = np . random . uniform ( [number] , [number] ) [EOL] [EOL] train_data [ n , : ] = calibration . softmax ( train_data [ n , : ] ) [EOL] [EOL] [comment] [EOL] assert np . all ( ( np . sum ( train_data , [number] ) - [number] ) < [number] ) [EOL] [EOL] return train_data , train_labels [EOL] [EOL] [EOL] def test_calibration_synthetic ( ) : [EOL] [docstring] [EOL] train_data , train_labels = generate_synthetic_data ( p_correct = [number] , N = [number] , K = [number] ) [EOL] [EOL] temperature = calibration . fit_temperature ( train_data , train_labels ) [EOL] [EOL] assert calibration . compute_ece ( train_data , train_labels , lbda = temperature ) < calibration . compute_ece ( train_data , train_labels ) [EOL] [EOL] [EOL] def test_automatic_calibration ( data_frame ) : [EOL] [docstring] [EOL] [EOL] feature_col = [string] [EOL] categorical_col = [string] [EOL] label_col = [string] [EOL] [EOL] n_samples = [number] [EOL] num_labels = [number] [EOL] seq_len = [number] [EOL] vocab_size = int ( [number] ** [number] ) [EOL] [EOL] latent_dim = [number] [EOL] embed_dim = [number] [EOL] [EOL] [comment] [EOL] random_data = data_frame ( feature_col = feature_col , label_col = label_col , vocab_size = vocab_size , num_labels = num_labels , num_words = seq_len , n_samples = n_samples ) [EOL] [EOL] [comment] [EOL] random_data [ categorical_col ] = random_data [ label_col ] . apply ( lambda x : x [ : [number] ] ) [EOL] [EOL] df_train , df_test , df_val = random_split ( random_data , [ [number] , [number] , [number] ] ) [EOL] [EOL] data_encoder_cols = [ BowEncoder ( feature_col , feature_col + [string] , max_tokens = vocab_size ) , SequentialEncoder ( feature_col , feature_col + [string] , max_tokens = vocab_size , seq_len = seq_len ) , CategoricalEncoder ( categorical_col , max_tokens = num_labels ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col , max_tokens = num_labels ) ] [EOL] [EOL] data_cols = [ BowFeaturizer ( feature_col + [string] , max_tokens = vocab_size ) , LSTMFeaturizer ( field_name = feature_col + [string] , seq_len = seq_len , latent_dim = latent_dim , num_hidden = [number] , embed_dim = embed_dim , num_layers = [number] , max_tokens = num_labels ) , EmbeddingFeaturizer ( field_name = categorical_col , embed_dim = embed_dim , max_tokens = num_labels ) ] [EOL] [EOL] num_epochs = [number] [EOL] batch_size = [number] [EOL] learning_rate = [number] [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols ) . fit ( train_df = df_train , test_df = df_val , learning_rate = learning_rate , num_epochs = num_epochs , batch_size = batch_size ) [EOL] [EOL] assert imputer . calibration_info [ [string] ] > imputer . calibration_info [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List [EOL] import typing [EOL] import pandas [EOL] import os [EOL] import random [EOL] [EOL] import shutil [EOL] [EOL] import mxnet as mx [EOL] import numpy as np [EOL] import pandas as pd [EOL] import pytest [EOL] [EOL] from datawig . utils import rand_string [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def reset_random_seed ( ) : [EOL] [comment] [EOL] random . seed ( [number] ) [EOL] np . random . seed ( [number] ) [EOL] mx . random . seed ( [number] ) [EOL] [EOL] yield [EOL] [EOL] [EOL] @ pytest . fixture def test_dir ( ) : [EOL] file_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) [EOL] joined = os . path . join ( file_dir , [string] ) [EOL] yield joined [EOL] shutil . rmtree ( joined ) [EOL] [EOL] @ pytest . fixture def data_frame ( ) : [EOL] [EOL] def _inner_impl ( feature_col = [string] , label_col = [string] , n_samples = [number] , word_length = [number] , num_words = [number] , vocab_size = [number] , num_labels = [number] ) : [EOL] [EOL] [docstring] [EOL] [EOL] vocab = [ rand_string ( word_length ) for i in range ( vocab_size ) ] [EOL] labels = vocab [ : num_labels ] [EOL] words = vocab [ num_labels : ] [EOL] [EOL] def _sentence_with_label ( labels = labels , words = words ) : [EOL] [docstring] [EOL] label = random . choice ( labels ) [EOL] tokens = [ random . choice ( words ) for _ in range ( num_words ) ] + [ label ] [EOL] sentence = [string] . join ( np . random . permutation ( tokens ) ) [EOL] [EOL] return sentence , label [EOL] [EOL] sentences , labels = zip ( * [ _sentence_with_label ( labels , words ) for _ in range ( n_samples ) ] ) [EOL] df = pd . DataFrame ( { feature_col : sentences , label_col : labels } ) [EOL] [EOL] return df [EOL] [EOL] return _inner_impl [EOL] [EOL] [EOL] def synthetic_label_shift_simple ( N , label_proportions , error_proba , covariates = None ) : [EOL] [docstring] [EOL] [EOL] if covariates is None : [EOL] covariates = [ ] [EOL] for i in range ( len ( label_proportions ) ) : [EOL] covariates . append ( rand_string ( [number] ) ) [EOL] [EOL] out = [ ] [EOL] for n in range ( N ) : [EOL] label = np . random . choice ( range ( len ( label_proportions ) ) , p = label_proportions ) [EOL] if np . random . rand ( ) > error_proba : [EOL] covariate = covariates [ label ] [EOL] else : [EOL] [comment] [EOL] covariate = covariates [ np . random . choice ( [ i for i in range ( len ( label_proportions ) ) if i != label ] ) ] [EOL] out . append ( ( covariate , [string] + str ( label ) ) ) [EOL] [EOL] return pd . DataFrame ( out , columns = [ [string] , [string] ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List [EOL] import typing [EOL] import datawig [EOL] [docstring] [EOL] [EOL] import os [EOL] [EOL] import numpy as np [EOL] import pandas as pd [EOL] import pytest [EOL] [EOL] from datawig import column_encoders [EOL] [EOL] df = pd . DataFrame ( { [string] : [ [string] , [string] , [string] ] , [string] : [ [string] , [string] , [string] ] } ) [EOL] [EOL] categorical_encoder = column_encoders . CategoricalEncoder ( [ [string] ] , max_tokens = [number] ) . fit ( df ) [EOL] sequential_encoder = column_encoders . SequentialEncoder ( [ [string] ] , max_tokens = [number] , seq_len = [number] ) . fit ( df ) [EOL] [EOL] [comment] [EOL] def test_categorical_encoder_unfitted_fail ( ) : [EOL] unfitted_categorical_encoder = column_encoders . CategoricalEncoder ( [ [string] ] ) [EOL] assert not unfitted_categorical_encoder . is_fitted ( ) [EOL] with pytest . raises ( column_encoders . NotFittedError ) : [EOL] unfitted_categorical_encoder . transform ( pd . DataFrame ( { [string] : [ [string] , [string] ] } ) ) [EOL] [EOL] [EOL] def test_fit_categorical_encoder ( ) : [EOL] assert categorical_encoder . is_fitted ( ) [EOL] assert categorical_encoder . token_to_idx == { [string] : [number] , [string] : [number] } [EOL] assert categorical_encoder . idx_to_token == { [number] : [string] , [number] : [string] } [EOL] [EOL] [EOL] def test_categorical_encoder_transform ( ) : [EOL] assert categorical_encoder . transform ( df ) . flatten ( ) [ [number] ] == [number] [EOL] [EOL] [EOL] def test_categorical_encoder_transform_missing_token ( ) : [EOL] assert categorical_encoder . transform ( pd . DataFrame ( { [string] : [ [string] ] } ) ) . flatten ( ) [ [number] ] == [number] [EOL] [EOL] [EOL] def test_categorical_encoder_max_token ( ) : [EOL] categorical_encoder = column_encoders . CategoricalEncoder ( [ [string] ] , max_tokens = [number] ) . fit ( df ) [EOL] assert categorical_encoder . max_tokens == [number] [EOL] [EOL] [EOL] def test_categorical_encoder_decode_token ( ) : [EOL] assert categorical_encoder . decode_token ( [number] ) == [string] [EOL] [EOL] [EOL] def test_categorical_encoder_decode_missing_token ( ) : [EOL] assert categorical_encoder . decode_token ( [number] ) == [string] [EOL] [EOL] [EOL] def test_categorical_encoder_decode ( ) : [EOL] assert categorical_encoder . decode ( pd . Series ( [ [number] ] ) ) . values [ [number] ] == [string] [EOL] [EOL] [EOL] def test_categorical_encoder_decode_missing ( ) : [EOL] assert categorical_encoder . decode ( pd . Series ( [ [number] ] ) ) . values [ [number] ] == [string] [EOL] [EOL] [EOL] def test_categorical_encoder_non_negative_embedding_indices ( ) : [EOL] assert all ( categorical_encoder . transform ( df ) . flatten ( ) >= [number] ) [EOL] [EOL] [EOL] [comment] [EOL] def test_sequential_encoder_unfitted_fail ( ) : [EOL] unfitted_sequential_encoder = column_encoders . SequentialEncoder ( [ [string] ] ) [EOL] assert not unfitted_sequential_encoder . is_fitted ( ) [EOL] with pytest . raises ( column_encoders . NotFittedError ) : [EOL] unfitted_sequential_encoder . transform ( pd . DataFrame ( { [string] : [ [string] ] } ) ) [EOL] [EOL] [EOL] def test_fit_sequential_encoder ( ) : [EOL] sequential_encoder_fewer_tokens = column_encoders . SequentialEncoder ( [ [string] ] , max_tokens = [number] , seq_len = [number] ) . fit ( df ) [EOL] assert ( set ( sequential_encoder_fewer_tokens . token_to_idx . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] } ) [EOL] [EOL] [EOL] def test_sequential_encoder_transform ( ) : [EOL] encoded = pd . Series ( [ vec . tolist ( ) for vec in sequential_encoder . transform ( df ) ] ) [EOL] true_decoded = df [ [string] ] . apply ( lambda x : x [ : sequential_encoder . output_dim ] ) [EOL] assert all ( sequential_encoder . decode ( encoded ) == true_decoded ) [EOL] [EOL] [EOL] def test_sequential_encoder_transform_missing_token ( ) : [EOL] assert ( sequential_encoder . transform ( pd . DataFrame ( { [string] : [ [string] ] } ) ) [ [number] ] . tolist ( ) == [ [number] , [number] , [number] ] ) [EOL] [EOL] [EOL] def test_sequential_encoder_max_token ( ) : [EOL] sequential_encoder_short = column_encoders . SequentialEncoder ( [string] , max_tokens = [number] , seq_len = [number] ) [EOL] sequential_encoder_short . fit ( df ) [EOL] assert sequential_encoder . is_fitted ( ) [EOL] assert sequential_encoder_short . max_tokens == [number] [EOL] [EOL] [EOL] def test_sequential_encoder_non_negative_embedding_indices ( ) : [EOL] assert all ( sequential_encoder . transform ( df ) . flatten ( ) >= [number] ) [EOL] [EOL] [EOL] def test_bow_encoder ( ) : [EOL] bow_encoder = column_encoders . BowEncoder ( [string] , max_tokens = [number] ) [EOL] assert bow_encoder . is_fitted ( ) [EOL] bow = bow_encoder . transform ( df ) [ [number] ] . toarray ( ) [ [number] ] [EOL] true = np . array ( [ [number] , - [number] , - [number] , - [number] , [number] ] ) [EOL] assert true == pytest . approx ( bow , [number] ) [EOL] [EOL] [EOL] def test_bow_encoder_multicol ( ) : [EOL] bow_encoder = column_encoders . BowEncoder ( [ [string] , [string] ] , max_tokens = [number] ) [EOL] data = pd . DataFrame ( { [string] : [ [string] ] , [string] : [ [string] ] } ) [EOL] bow = bow_encoder . transform ( data ) [ [number] ] . toarray ( ) [ [number] ] [EOL] true = np . array ( [ [number] , - [number] , - [number] , [number] , - [number] ] ) [EOL] assert true == pytest . approx ( bow , [number] ) [EOL] data_strings = [ [string] ] [EOL] assert true == pytest . approx ( bow_encoder . vectorizer . transform ( data_strings ) . toarray ( ) [ [number] ] ) [EOL] [EOL] [EOL] def test_categorical_encoder_numeric ( ) : [EOL] df = pd . DataFrame ( { [string] : [ [number] , [number] , [number] ] } ) [EOL] try : [EOL] column_encoders . CategoricalEncoder ( [string] ) . fit ( df ) [EOL] except TypeError : [EOL] pytest . fail ( [string] ) [EOL] [EOL] [EOL] def test_categorical_encoder_numeric_transform ( ) : [EOL] df = pd . DataFrame ( { [string] : [ [number] , [number] , [number] , [number] , [number] , [number] , np . nan , None ] } ) [EOL] col_enc = column_encoders . CategoricalEncoder ( [string] ) . fit ( df ) [EOL] assert np . array_equal ( col_enc . transform ( df ) , np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] ] ) ) [EOL] [EOL] [EOL] def test_categorical_encoder_numeric_nan ( ) : [EOL] df = pd . DataFrame ( { [string] : [ [number] , [number] , [number] , None ] } ) [EOL] try : [EOL] column_encoders . CategoricalEncoder ( [string] ) . fit ( df ) [EOL] except TypeError : [EOL] pytest . fail ( [string] ) [EOL] [EOL] def test_column_encoder_no_list_input_column ( ) : [EOL] column_encoder = column_encoders . ColumnEncoder ( [string] ) [EOL] assert column_encoder . input_columns == [ [string] ] [EOL] assert column_encoder . output_column == [string] [EOL] with pytest . raises ( ValueError ) : [EOL] column_encoders . ColumnEncoder ( [number] ) [EOL] with pytest . raises ( ValueError ) : [EOL] column_encoders . ColumnEncoder ( [ [number] ] ) [EOL] [EOL] [EOL] def test_numeric_encoder ( ) : [EOL] df = pd . DataFrame ( { [string] : [ [number] , [number] , [number] , np . nan , None ] , [string] : [ [number] , - [number] , np . nan , None , [number] ] } ) [EOL] unfitted_numerical_encoder = column_encoders . NumericalEncoder ( [ [string] , [string] ] , normalize = False ) [EOL] assert unfitted_numerical_encoder . is_fitted ( ) [EOL] fitted_unnormalized_numerical_encoder = unfitted_numerical_encoder . fit ( df ) [EOL] df_unnormalized = fitted_unnormalized_numerical_encoder . transform ( df . copy ( ) ) [EOL] [EOL] assert np . array_equal ( df_unnormalized , np . array ( [ [ [number] , [number] ] , [ [number] , - [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , dtype = np . float32 ) ) [EOL] df_nans = pd . DataFrame ( { [string] : [ None ] , [string] : [ np . nan ] } ) [EOL] df_unnormalized_nans = fitted_unnormalized_numerical_encoder . transform ( df_nans . copy ( ) ) [EOL] assert np . array_equal ( df_unnormalized_nans , np . array ( [ [ [number] , [number] ] ] , dtype = np . float32 ) ) [EOL] [EOL] normalized_numerical_encoder = column_encoders . NumericalEncoder ( [ [string] , [string] ] , normalize = True ) [EOL] assert not normalized_numerical_encoder . is_fitted ( ) [EOL] normalized_numerical_encoder_fitted = normalized_numerical_encoder . fit ( df ) [EOL] df_normalized = normalized_numerical_encoder_fitted . transform ( df ) [EOL] [EOL] assert normalized_numerical_encoder . is_fitted ( ) [EOL] assert np . array_equal ( df_normalized , np . array ( [ [ - [number] , - [number] ] , [ [number] , - [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , dtype = np . float32 ) ) [EOL] [EOL] def test_tfidf_encoder ( ) : [EOL] tfidf_encoder = column_encoders . TfIdfEncoder ( [string] , max_tokens = [number] ) [EOL] assert tfidf_encoder . is_fitted ( ) is False [EOL] tfidf_encoder . fit ( df ) [EOL] bow = tfidf_encoder . transform ( df ) [ [number] ] . toarray ( ) [ [number] ] [EOL] true = np . array ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert tfidf_encoder . is_fitted ( ) is True [EOL] assert true == pytest . approx ( bow , [number] ) [EOL] [EOL] decoded_indices = tfidf_encoder . decode ( pd . Series ( [ [number] , [number] , [number] ] ) ) [EOL] assert np . array_equal ( decoded_indices . values , np . array ( [ [string] , [string] , [string] ] ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , List , Union [EOL] import typing [EOL] import datawig [EOL] [docstring] [EOL] [EOL] import numpy as np [EOL] import os [EOL] import pandas as pd [EOL] import pytest [EOL] import warnings [EOL] from sklearn . metrics import precision_score [EOL] from stat import * [EOL] import string [EOL] [EOL] import datawig [EOL] from datawig . column_encoders import ( BowEncoder , CategoricalEncoder , NumericalEncoder , SequentialEncoder , TfIdfEncoder ) [EOL] from datawig . imputer import ( Imputer , INSTANCE_WEIGHT_COLUMN ) [EOL] from datawig . mxnet_input_symbols import ( BowFeaturizer , EmbeddingFeaturizer , LSTMFeaturizer , NumericalFeaturizer ) [EOL] from datawig . utils import random_split [EOL] [EOL] [EOL] warnings . filterwarnings ( [string] ) [EOL] [EOL] [EOL] def test_drop_missing ( test_dir ) : [EOL] [docstring] [EOL] df_train = pd . DataFrame ( { [string] : [ [number] , None , np . nan , [number] ] * [number] , [string] : [ [string] , [string] , [string] , [string] ] * [number] } ) [EOL] df_test = df_train . copy ( ) [EOL] [EOL] max_tokens = int ( [number] ** [number] ) [EOL] [EOL] batch_size = [number] [EOL] [EOL] data_encoder_cols = [ BowEncoder ( [string] , max_tokens = max_tokens ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( [string] , max_tokens = [number] ) ] [EOL] data_cols = [ BowFeaturizer ( [string] , max_tokens = max_tokens ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df_train , test_df = df_test , batch_size = batch_size ) [EOL] [EOL] df_dropped = imputer . _Imputer__drop_missing_labels ( df_train , how = [string] ) [EOL] [EOL] df_dropped_true = pd . DataFrame ( { [string] : { [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] } , [string] : { [number] : [number] , [number] : [number] , [number] : [number] , [number] : [number] } } ) [EOL] [EOL] assert df_dropped [ [ [string] , [string] ] ] . equals ( df_dropped_true [ [ [string] , [string] ] ] ) [EOL] [EOL] [EOL] def test_imputer_init ( ) : [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = [string] , label_encoders = [ [string] ] , data_encoders = [string] ) [EOL] [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( [string] ) ] , label_encoders = [string] , data_encoders = [string] ) [EOL] [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( [string] ) ] , label_encoders = [ CategoricalEncoder ( [string] ) ] , data_encoders = [string] ) [EOL] [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( [string] ) ] , label_encoders = [ CategoricalEncoder ( [string] ) ] , data_encoders = [ BowEncoder ( [string] ) ] ) [EOL] [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( [string] ) ] , label_encoders = [ CategoricalEncoder ( [string] ) ] , data_encoders = [ BowEncoder ( [string] ) ] ) [EOL] [EOL] label_encoders = [ CategoricalEncoder ( [string] , max_tokens = [number] ) ] [EOL] data_featurizers = [ LSTMFeaturizer ( [string] ) , EmbeddingFeaturizer ( [string] ) ] [EOL] [EOL] data_encoders = [ SequentialEncoder ( [string] ) , CategoricalEncoder ( [string] ) ] [EOL] [EOL] imputer = Imputer ( data_featurizers = data_featurizers , label_encoders = label_encoders , data_encoders = data_encoders ) [EOL] [EOL] assert imputer . output_path == [string] [EOL] assert imputer . module_path == [string] [EOL] assert imputer . metrics_path == [string] [EOL] [EOL] assert imputer . output_path == [string] [EOL] assert imputer . module_path == [string] [EOL] assert imputer . metrics_path == [string] [EOL] [EOL] imputer = Imputer ( data_featurizers = data_featurizers , label_encoders = [ CategoricalEncoder ( [string] , max_tokens = [number] ) ] , data_encoders = data_encoders ) [EOL] assert imputer . output_path == [string] [EOL] [EOL] [EOL] def test_imputer_duplicate_encoder_output_columns ( test_dir , data_frame ) : [EOL] [docstring] [EOL] [EOL] feature_col = [string] [EOL] categorical_col = [string] [EOL] label_col = [string] [EOL] [EOL] n_samples = [number] [EOL] num_labels = [number] [EOL] seq_len = [number] [EOL] max_tokens = int ( [number] ** [number] ) [EOL] [EOL] latent_dim = [number] [EOL] embed_dim = [number] [EOL] [EOL] [comment] [EOL] random_data = data_frame ( feature_col = feature_col , label_col = label_col , vocab_size = max_tokens , num_labels = num_labels , num_words = seq_len , n_samples = n_samples ) [EOL] [EOL] [comment] [EOL] random_data [ categorical_col ] = random_data [ label_col ] . apply ( lambda x : x [ : [number] ] ) [EOL] [EOL] df_train , df_test , df_val = random_split ( random_data , [ [number] , [number] , [number] ] ) [EOL] [EOL] data_encoder_cols = [ BowEncoder ( feature_col , feature_col , max_tokens = max_tokens ) , SequentialEncoder ( feature_col , feature_col , max_tokens = max_tokens , seq_len = seq_len ) , CategoricalEncoder ( categorical_col , max_tokens = num_labels ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col , max_tokens = num_labels ) ] [EOL] [EOL] data_cols = [ BowFeaturizer ( feature_col , max_tokens = max_tokens ) , LSTMFeaturizer ( field_name = feature_col , seq_len = seq_len , latent_dim = latent_dim , num_hidden = [number] , embed_dim = embed_dim , num_layers = [number] , max_tokens = num_labels ) , EmbeddingFeaturizer ( field_name = categorical_col , embed_dim = embed_dim , max_tokens = num_labels ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] num_epochs = [number] [EOL] batch_size = [number] [EOL] learning_rate = [number] [EOL] [EOL] with pytest . raises ( ValueError ) as e : [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) [EOL] imputer . fit ( train_df = df_train , test_df = df_val , learning_rate = learning_rate , num_epochs = num_epochs , batch_size = batch_size ) [EOL] [EOL] [EOL] def test_imputer_real_data_all_featurizers ( test_dir , data_frame ) : [EOL] [docstring] [EOL] [EOL] feature_col = [string] [EOL] categorical_col = [string] [EOL] label_col = [string] [EOL] [EOL] n_samples = [number] [EOL] num_labels = [number] [EOL] seq_len = [number] [EOL] max_tokens = int ( [number] ** [number] ) [EOL] [EOL] latent_dim = [number] [EOL] embed_dim = [number] [EOL] [EOL] [comment] [EOL] random_data = data_frame ( feature_col = feature_col , label_col = label_col , vocab_size = max_tokens , num_labels = num_labels , num_words = seq_len , n_samples = n_samples ) [EOL] [EOL] [comment] [EOL] random_data [ categorical_col ] = random_data [ label_col ] . apply ( lambda x : x [ : [number] ] ) [EOL] [EOL] df_train , df_test , df_val = random_split ( random_data , [ [number] , [number] , [number] ] ) [EOL] [EOL] data_encoder_cols = [ BowEncoder ( feature_col , feature_col + [string] , max_tokens = max_tokens ) , SequentialEncoder ( feature_col , feature_col + [string] , max_tokens = max_tokens , seq_len = seq_len ) , CategoricalEncoder ( categorical_col , max_tokens = num_labels ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col , max_tokens = num_labels ) ] [EOL] [EOL] data_cols = [ BowFeaturizer ( feature_col + [string] , max_tokens = max_tokens ) , LSTMFeaturizer ( field_name = feature_col + [string] , seq_len = seq_len , latent_dim = latent_dim , num_hidden = [number] , embed_dim = embed_dim , num_layers = [number] , max_tokens = num_labels ) , EmbeddingFeaturizer ( field_name = categorical_col , embed_dim = embed_dim , max_tokens = num_labels ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] num_epochs = [number] [EOL] batch_size = [number] [EOL] learning_rate = [number] [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df_train , test_df = df_val , learning_rate = learning_rate , num_epochs = num_epochs , batch_size = batch_size , calibrate = False ) [EOL] [EOL] len_df_before_predict = len ( df_test ) [EOL] pred = imputer . transform ( df_test ) [EOL] [EOL] assert len ( pred [ label_col ] ) == len_df_before_predict [EOL] [EOL] assert sum ( df_test [ label_col ] . values == pred [ label_col ] ) == len ( df_test ) [EOL] [EOL] _ = imputer . predict_proba_top_k ( df_test , top_k = [number] ) [EOL] [EOL] _ , metrics = imputer . transform_and_compute_metrics ( df_test ) [EOL] [EOL] assert metrics [ label_col ] [ [string] ] > [number] [EOL] [EOL] deserialized = Imputer . load ( imputer . output_path ) [EOL] [EOL] _ , metrics_deserialized = deserialized . transform_and_compute_metrics ( df_test ) [EOL] [EOL] assert metrics_deserialized [ label_col ] [ [string] ] > [number] [EOL] [EOL] [comment] [EOL] not_so_precise_imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df_train [ : [number] ] , test_df = df_test , learning_rate = learning_rate , num_epochs = num_epochs , batch_size = batch_size , calibrate = False ) [EOL] [EOL] df_test = df_test . reset_index ( ) [EOL] predictions_df = not_so_precise_imputer . predict ( df_test , precision_threshold = [number] , imputation_suffix = [string] ) [EOL] [EOL] assert predictions_df . columns . contains ( label_col + [string] ) [EOL] assert predictions_df . columns . contains ( label_col + [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def test_imputer_without_train_df ( test_dir ) : [EOL] [docstring] [EOL] df_train = [ [string] ] [EOL] [EOL] data_encoder_cols = [ BowEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( [string] ) ] [EOL] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path , ) [EOL] [EOL] with pytest . raises ( ValueError , match = [string] ) : [EOL] imputer . fit ( train_df = df_train ) [EOL] [EOL] with pytest . raises ( ValueError , match = [string] ) : [EOL] imputer . fit ( train_df = None ) [EOL] [EOL] [EOL] def test_imputer_without_test_set_random_split ( test_dir , data_frame ) : [EOL] [docstring] [EOL] [EOL] feature_col = [string] [EOL] label_col = [string] [EOL] [EOL] n_samples = [number] [EOL] num_labels = [number] [EOL] seq_len = [number] [EOL] max_tokens = int ( [number] ** [number] ) [EOL] [EOL] [comment] [EOL] df_train = data_frame ( feature_col = feature_col , label_col = label_col , vocab_size = max_tokens , num_labels = num_labels , num_words = seq_len , n_samples = n_samples ) [EOL] [EOL] [EOL] num_epochs = [number] [EOL] batch_size = [number] [EOL] learning_rate = [number] [EOL] [EOL] data_encoder_cols = [ BowEncoder ( feature_col , max_tokens = max_tokens ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col , max_tokens = num_labels ) ] [EOL] [EOL] data_cols = [ BowFeaturizer ( feature_col , max_tokens = max_tokens ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) [EOL] [EOL] try : [EOL] imputer . fit ( train_df = df_train , learning_rate = learning_rate , num_epochs = num_epochs , batch_size = batch_size ) [EOL] except TypeError : [EOL] pytest . fail ( [string] ) [EOL] [EOL] [EOL] def test_imputer_load_read_exec_only_dir ( tmpdir , data_frame ) : [EOL] import stat [EOL] [EOL] [comment] [EOL] tmpdir = str ( tmpdir ) [EOL] feature = [string] [EOL] label = [string] [EOL] [EOL] df = data_frame ( feature , label , n_samples = [number] ) [EOL] [comment] [EOL] [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( feature ) ] , label_encoders = [ CategoricalEncoder ( label ) ] , data_encoders = [ BowEncoder ( feature ) ] , output_path = tmpdir ) [EOL] imputer . fit ( train_df = df , num_epochs = [number] ) [EOL] [EOL] [comment] [EOL] os . chmod ( tmpdir , stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH | stat . S_IREAD | stat . S_IRGRP | stat . S_IROTH ) [EOL] [EOL] try : [EOL] Imputer . load ( tmpdir ) [EOL] except AssertionError as e : [EOL] print ( e ) [EOL] pytest . fail ( [string] ) [EOL] [EOL] def test_imputer_load_with_invalid_context ( tmpdir , data_frame ) : [EOL] [EOL] [comment] [EOL] tmpdir = str ( tmpdir ) [EOL] feature = [string] [EOL] label = [string] [EOL] [EOL] df = data_frame ( feature , label , n_samples = [number] ) [EOL] [comment] [EOL] [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( feature ) ] , label_encoders = [ CategoricalEncoder ( label ) ] , data_encoders = [ BowEncoder ( feature ) ] , output_path = tmpdir ) [EOL] imputer . fit ( train_df = df , num_epochs = [number] ) [EOL] imputer . ctx = None [EOL] imputer . save ( ) [EOL] [EOL] imputer_deser = Imputer . load ( tmpdir ) [EOL] _ = imputer_deser . predict ( df ) [EOL] [EOL] [EOL] def test_imputer_fit_fail_non_writable_output_dir ( tmpdir , data_frame ) : [EOL] import stat [EOL] [EOL] [comment] [EOL] tmpdir = str ( tmpdir ) [EOL] feature = [string] [EOL] label = [string] [EOL] df = data_frame ( feature , label , n_samples = [number] ) [EOL] [comment] [EOL] imputer = Imputer ( data_featurizers = [ BowFeaturizer ( feature ) ] , label_encoders = [ CategoricalEncoder ( label ) ] , data_encoders = [ BowEncoder ( feature ) ] , output_path = tmpdir ) [EOL] [EOL] [comment] [EOL] os . chmod ( tmpdir , stat . S_IEXEC | stat . S_IXGRP | stat . S_IXOTH | stat . S_IREAD | stat . S_IRGRP | stat . S_IROTH ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( AssertionError ) as e : [EOL] imputer . fit ( df , num_epochs = [number] ) [EOL] [EOL] [EOL] def test_imputer_numeric_data ( test_dir ) : [EOL] [docstring] [EOL] [comment] [EOL] N = [number] [EOL] x = np . random . uniform ( - np . pi , np . pi , ( N , ) ) [EOL] df = pd . DataFrame ( { [string] : x , [string] : np . cos ( x ) , [string] : x * [number] , [string] : x ** [number] } ) [EOL] [EOL] df_train , df_test = random_split ( df , [ [number] , [number] ] ) [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] data_encoder_cols = [ NumericalEncoder ( [ [string] ] ) ] [EOL] data_cols = [ NumericalFeaturizer ( [string] , numeric_latent_dim = [number] ) ] [EOL] [EOL] for target in [ [string] , [string] , [string] ] : [EOL] label_encoder_cols = [ NumericalEncoder ( [ target ] , normalize = False ) ] [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) [EOL] imputer . fit ( train_df = df_train , learning_rate = [number] , num_epochs = [number] , patience = [number] , test_split = [number] , weight_decay = [number] , batch_size = [number] ) [EOL] [EOL] pred , metrics = imputer . transform_and_compute_metrics ( df_test ) [EOL] df_test [ [string] + target ] = pred [ target ] . flatten ( ) [EOL] print ( [string] . format ( metrics [ target ] ) ) [EOL] assert metrics [ target ] < [number] [EOL] [EOL] def test_imputer_unrepresentative_test_df ( test_dir , data_frame ) : [EOL] [docstring] [EOL] [comment] [EOL] random_data = data_frame ( n_samples = [number] ) [EOL] [EOL] df_train , df_test , _ = random_split ( random_data , [ [number] , [number] , [number] ] ) [EOL] [EOL] excluded = df_train [ [string] ] . values [ [number] ] [EOL] df_test = df_test [ df_test [ [string] ] != excluded ] [EOL] [EOL] data_encoder_cols = [ BowEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( [string] ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df_train , test_df = df_test , num_epochs = [number] ) [EOL] [EOL] only_excluded_df = df_train [ df_train [ [string] ] == excluded ] [EOL] imputations = imputer . predict_above_precision ( only_excluded_df , precision_threshold = [number] ) [ [string] ] [EOL] assert all ( [ x == ( ) for x in imputations ] ) [EOL] [EOL] [EOL] def test_imputer_tfidf ( test_dir , data_frame ) : [EOL] label_col = [string] [EOL] df = data_frame ( n_samples = [number] , label_col = label_col ) [EOL] [EOL] data_encoder_cols = [ TfIdfEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df , num_epochs = [number] ) [EOL] [EOL] _ , metrics = imputer . transform_and_compute_metrics ( df ) [EOL] assert metrics [ [string] ] [ [string] ] > [number] [EOL] [EOL] [EOL] def test_mxnet_module_wrapper ( data_frame ) : [EOL] from datawig . imputer import _MXNetModule [EOL] import mxnet as mx [EOL] from datawig . iterators import ImputerIterDf [EOL] [EOL] feature_col , label_col = [string] , [string] [EOL] df = data_frame ( n_samples = [number] , feature_col = feature_col , label_col = label_col ) [EOL] label_encoders = [ CategoricalEncoder ( label_col ) ] [EOL] data_encoders = [ BowEncoder ( feature_col ) ] [EOL] data_featurizers = [ BowFeaturizer ( feature_col , max_tokens = [number] ) ] [EOL] iter_train = ImputerIterDf ( df , data_encoders , label_encoders ) [EOL] [EOL] mod = _MXNetModule ( mx . current_context ( ) , label_encoders , data_featurizers , final_fc_hidden_units = [ ] ) ( iter_train ) [EOL] [EOL] assert mod . _label_names == [ label_col ] [EOL] assert sorted ( mod . data_names ) == sorted ( [ feature_col ] + [ INSTANCE_WEIGHT_COLUMN ] ) [EOL] [comment] [EOL] assert len ( mod . _arg_params ) == [number] [EOL] [EOL] [EOL] def test_inplace_prediction ( test_dir , data_frame ) : [EOL] label_col = [string] [EOL] df = data_frame ( n_samples = [number] , label_col = label_col ) [EOL] [EOL] data_encoder_cols = [ TfIdfEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df , num_epochs = [number] ) [EOL] [EOL] predicted = imputer . predict ( df , inplace = True ) [EOL] [EOL] assert predicted is df [EOL] [EOL] [EOL] def test_not_explainable ( test_dir , data_frame ) : [EOL] label_col = [string] [EOL] df = data_frame ( n_samples = [number] , label_col = label_col ) [EOL] [EOL] data_encoder_cols = [ BowEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df , num_epochs = [number] ) [EOL] [EOL] assert not imputer . is_explainable [EOL] [EOL] try : [EOL] imputer . explain ( [string] ) [EOL] raise pytest . fail ( [string] ) [EOL] except ValueError as exception : [EOL] assert exception . args [ [number] ] == [string] [EOL] [EOL] instance = pd . Series ( { [string] : [string] } ) [EOL] try : [EOL] imputer . explain_instance ( instance ) [EOL] raise pytest . fail ( [string] ) [EOL] except ValueError as exception : [EOL] assert exception . args [ [number] ] == [string] [EOL] [EOL] [EOL] def test_explain_instance_without_label ( test_dir , data_frame ) : [EOL] label_col = [string] [EOL] df = data_frame ( n_samples = [number] , label_col = label_col ) [EOL] [EOL] data_encoder_cols = [ TfIdfEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] , [string] ) [EOL] [EOL] imputer = Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df , num_epochs = [number] ) [EOL] [EOL] assert imputer . is_explainable [EOL] [EOL] instance = pd . Series ( { [string] : [string] } ) [EOL] [comment] [EOL] _ = imputer . explain_instance ( instance ) [EOL] assert True [EOL] [EOL] [EOL] def test_explain_method_synthetic ( test_dir ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] N = [number] [EOL] cat_in_col = [ [string] if r > ( [number] / [number] ) else [string] for r in np . random . rand ( N ) ] [EOL] text_in_col = [ [string] if r > ( [number] / [number] ) else [string] for r in np . random . rand ( N ) ] [EOL] hash_in_col = [ [string] for r in range ( N ) ] [EOL] cat_out_col = [ [string] if [string] in input [ [number] ] + input [ [number] ] else [string] for input in zip ( cat_in_col , text_in_col ) ] [EOL] [EOL] df = pd . DataFrame ( ) [EOL] df [ [string] ] = cat_in_col [EOL] df [ [string] ] = text_in_col [EOL] df [ [string] ] = hash_in_col [EOL] df [ [string] ] = cat_out_col [EOL] [EOL] [comment] [EOL] data_encoder_cols = [ datawig . column_encoders . TfIdfEncoder ( [string] , tokens = [string] ) , datawig . column_encoders . CategoricalEncoder ( [string] , max_tokens = [number] ) , datawig . column_encoders . BowEncoder ( [string] , tokens = [string] ) ] [EOL] data_featurizer_cols = [ datawig . mxnet_input_symbols . BowFeaturizer ( [string] ) , datawig . mxnet_input_symbols . EmbeddingFeaturizer ( [string] ) , datawig . mxnet_input_symbols . BowFeaturizer ( [string] ) ] [EOL] [EOL] label_encoder_cols = [ datawig . column_encoders . CategoricalEncoder ( [string] ) ] [EOL] [EOL] [comment] [EOL] imputer = datawig . Imputer ( data_featurizers = data_featurizer_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = os . path . join ( test_dir , [string] , [string] ) ) [EOL] [EOL] [comment] [EOL] tr , te = random_split ( df . sample ( [number] ) , [ [number] , [number] ] ) [EOL] imputer . fit ( train_df = tr , test_df = te , num_epochs = [number] , learning_rate = [number] ) [EOL] predictions = imputer . predict ( te ) [EOL] [EOL] [comment] [EOL] assert precision_score ( predictions . out_cat , predictions . out_cat_imputed , average = [string] ) > [number] [EOL] [EOL] [comment] [EOL] for i in np . random . choice ( N , [number] ) : [EOL] explanation = imputer . explain_instance ( df . iloc [ i ] ) [EOL] top_label = explanation [ [string] ] [EOL] [EOL] if top_label == [string] : [EOL] assert ( explanation [ [string] ] [ [number] ] [ [number] ] == [string] and explanation [ [string] ] [ [number] ] [ [number] ] == [string] ) [EOL] elif top_label == [string] : [EOL] assert ( explanation [ [string] ] [ [number] ] [ [number] ] == [string] or explanation [ [string] ] [ [number] ] [ [number] ] == [string] ) [EOL] [EOL] [comment] [EOL] assert np . all ( [ [string] in token for token , weight in imputer . explain ( [string] ) [ [string] ] ] [ : [number] ] ) [EOL] assert [ [string] in token for token , weight in imputer . explain ( [string] ) [ [string] ] ] [ [number] ] [EOL] [EOL] [comment] [EOL] imputer . save ( ) [EOL] imputer_from_disk = Imputer . load ( imputer . output_path ) [EOL] assert np . all ( [ [string] in token for token , weight in imputer_from_disk . explain ( [string] ) [ [string] ] ] [ : [number] ] ) [EOL] [EOL] [EOL] def test_non_writable_output_path ( test_dir , data_frame ) : [EOL] label_col = [string] [EOL] df = data_frame ( n_samples = [number] , label_col = label_col ) [EOL] [EOL] data_encoder_cols = [ TfIdfEncoder ( [string] ) ] [EOL] label_encoder_cols = [ CategoricalEncoder ( label_col ) ] [EOL] data_cols = [ BowFeaturizer ( [string] ) ] [EOL] [EOL] output_path = os . path . join ( test_dir , [string] ) [EOL] [EOL] Imputer ( data_featurizers = data_cols , label_encoders = label_encoder_cols , data_encoders = data_encoder_cols , output_path = output_path ) . fit ( train_df = df , num_epochs = [number] ) . save ( ) [EOL] [EOL] from datawig . utils import logger [EOL] [EOL] try : [EOL] [comment] [EOL] os . chmod ( output_path , S_IREAD | S_IXUSR ) [EOL] [EOL] [comment] [EOL] os . chmod ( os . path . join ( output_path , [string] ) , S_IREAD ) [EOL] imputer = Imputer . load ( output_path ) [EOL] _ = imputer . predict ( df ) [EOL] logger . warning ( [string] ) [EOL] [EOL] [comment] [EOL] os . chmod ( os . path . join ( output_path , [string] ) , S_IREAD | S_IXUSR | S_IWUSR ) [EOL] os . chmod ( output_path , S_IREAD | S_IXUSR | S_IWUSR ) [EOL] os . remove ( os . path . join ( output_path , [string] ) ) [EOL] [EOL] [comment] [EOL] os . chmod ( output_path , S_IREAD | S_IXUSR ) [EOL] [EOL] imputer = Imputer . load ( output_path ) [EOL] _ = imputer . predict ( df ) [EOL] logger . warning ( [string] ) [EOL] os . chmod ( output_path , S_IREAD | S_IXUSR | S_IWUSR ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] pytest . fail ( [string] ) [EOL] [EOL] [EOL] def test_fit_resumes ( test_dir , data_frame ) : [EOL] feature_col , label_col = [string] , [string] [EOL] [EOL] df = data_frame ( feature_col = feature_col , label_col = label_col ) [EOL] [EOL] imputer = Imputer ( data_encoders = [ TfIdfEncoder ( [ feature_col ] ) ] , data_featurizers = [ datawig . mxnet_input_symbols . BowFeaturizer ( feature_col ) ] , label_encoders = [ CategoricalEncoder ( label_col ) ] , output_path = test_dir ) [EOL] [EOL] assert imputer . module is None [EOL] [EOL] imputer . fit ( df , num_epochs = [number] ) [EOL] first_fit_module = imputer . module [EOL] [EOL] imputer . fit ( df , num_epochs = [number] ) [EOL] second_fit_module = imputer . module [EOL] [EOL] assert first_fit_module == second_fit_module [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , List [EOL] import typing [EOL] import datawig [EOL] [docstring] [EOL] import itertools [EOL] [EOL] import numpy as np [EOL] import pandas as pd [EOL] [EOL] from datawig . column_encoders import ( BowEncoder , CategoricalEncoder , SequentialEncoder ) [EOL] from datawig . iterators import ImputerIterDf [EOL] from datawig . imputer import INSTANCE_WEIGHT_COLUMN [EOL] [EOL] feature_col = [string] [EOL] label_col = [string] [EOL] max_tokens = [number] [EOL] num_labels = [number] [EOL] [EOL] [EOL] def test_iter_next_df ( data_frame ) : [EOL] it = get_new_iterator_df ( data_frame ( ) ) [EOL] _ , next_batch = next ( it ) , next ( it ) [EOL] print ( [string] + str ( next_batch . label [ [number] ] . asnumpy ( ) ) ) [EOL] assert ( ( next_batch . label [ [number] ] . asnumpy ( ) == np . array ( [ [ [number] ] , [ [number] ] ] ) ) . all ( ) ) [EOL] [EOL] def test_iter_df_bow ( data_frame ) : [EOL] df = data_frame ( ) [EOL] it = get_new_iterator_df_bow ( df ) [EOL] tt = next ( it ) [EOL] bow = tt . data [ [number] ] . asnumpy ( ) [ [number] , : ] [EOL] true = it . data_columns [ [number] ] . vectorizer . transform ( [ df . loc [ [number] , [string] ] ] ) . toarray ( ) [ [number] ] [EOL] assert ( true - bow ) . sum ( ) < [number] [EOL] [EOL] [EOL] def test_iter_provide_label_or_data_df ( data_frame ) : [EOL] it = get_new_iterator_df ( data_frame ( ) ) [EOL] [comment] [EOL] assert it . provide_data [ [number] ] [ [number] ] == INSTANCE_WEIGHT_COLUMN [EOL] assert it . provide_data [ [number] ] [ [number] ] == label_col [EOL] assert it . provide_data [ [number] ] [ [number] ] == ( [number] , [number] ) [EOL] assert it . provide_label [ [number] ] [ [number] ] == label_col [EOL] assert it . provide_label [ [number] ] [ [number] ] == ( [number] , [number] ) [EOL] [EOL] [EOL] def test_iter_index_df ( data_frame ) : [EOL] it = get_new_iterator_df ( data_frame ( ) ) [EOL] idx_it = list ( itertools . chain ( * [ b . index for b in it ] ) ) [EOL] idx_true = data_frame ( ) . index . tolist ( ) [EOL] assert idx_it == idx_true [EOL] [EOL] [EOL] def test_iter_decoder_df ( ) : [EOL] [comment] [EOL] brands = [ { feature_col : brand } for brand in list ( map ( lambda e : str ( int ( e ) ) , np . random . exponential ( scale = [number] , size = [number] ) ) ) ] [EOL] [EOL] brand_df = pd . DataFrame ( brands ) [EOL] it = ImputerIterDf ( brand_df , data_columns = [ SequentialEncoder ( feature_col , max_tokens = [number] , seq_len = [number] ) ] , label_columns = [ CategoricalEncoder ( feature_col , max_tokens = [number] ) ] , batch_size = [number] ) [EOL] decoded = it . decode ( next ( it ) . label ) [EOL] np . testing . assert_array_equal ( decoded [ [number] ] , brand_df [ feature_col ] . head ( it . batch_size ) . values ) [EOL] [EOL] [EOL] def test_iter_padding_offset ( ) : [EOL] col = [string] [EOL] df = pd . DataFrame ( [ { col : brand } for brand in list ( map ( lambda e : str ( int ( e ) ) , np . random . exponential ( scale = [number] , size = [number] ) ) ) ] ) [EOL] df_train = df . sample ( frac = [number] ) [EOL] it = ImputerIterDf ( df_train , data_columns = [ BowEncoder ( col ) ] , label_columns = [ CategoricalEncoder ( col , max_tokens = [number] ) ] , batch_size = [number] ) [EOL] assert it . start_padding_idx == df_train . shape [ [number] ] [EOL] [EOL] def get_new_iterator_df_bow ( df ) : [EOL] return ImputerIterDf ( df , data_columns = [ BowEncoder ( feature_col , max_tokens = max_tokens ) ] , label_columns = [ CategoricalEncoder ( label_col , max_tokens = num_labels ) ] , batch_size = [number] ) [EOL] [EOL] [EOL] def get_new_iterator_df ( df ) : [EOL] return ImputerIterDf ( df , data_columns = [ SequentialEncoder ( label_col , max_tokens = max_tokens , seq_len = [number] ) ] , label_columns = [ CategoricalEncoder ( label_col , max_tokens = max_tokens ) ] , batch_size = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Tuple , Dict , Any , List , Union [EOL] import typing [EOL] [docstring] [EOL] [EOL] import warnings [EOL] [EOL] import pandas as pd [EOL] [EOL] from datawig . evaluation import ( evaluate_model_outputs , evaluate_model_outputs_single_attribute ) [EOL] [EOL] warnings . filterwarnings ( [string] ) [EOL] [EOL] def test_evaluation_single ( ) : [EOL] str_values = pd . Series ( [ [string] , [string] ] ) [EOL] int_values = pd . Series ( [ [number] , [number] ] ) [EOL] float_values = pd . Series ( [ [number] , [number] ] ) [EOL] metrics_str = evaluate_model_outputs_single_attribute ( str_values , str_values ) [EOL] metrics_int = evaluate_model_outputs_single_attribute ( int_values , int_values ) [EOL] metrics_float = evaluate_model_outputs_single_attribute ( float_values , float_values ) [EOL] [EOL] assert metrics_str [ [string] ] == [number] [EOL] assert metrics_int [ [string] ] == [number] [EOL] assert metrics_float [ [string] ] == [number] [EOL] [EOL] [EOL] def test_evaluation ( ) : [EOL] df = pd . DataFrame ( [ ( [string] , [string] , [string] ) , ( [string] , [string] , [string] ) , ( [string] , [string] , [string] ) ] , columns = [ [string] , [string] , [string] ] ) [EOL] [EOL] correct_df = { [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [ ( [string] , [number] ) , ( [string] , [number] ) ] , [string] : [ [number] , [number] ] , [string] : [ [number] , [number] ] , [string] : [ [number] , [number] ] , [string] : [ ( [string] , [ ( [string] , [number] ) ] ) , ( [string] , [ ( [string] , [number] ) ] ) ] , [string] : [number] , [string] : [number] } } [EOL] [EOL] evaluation_df = evaluate_model_outputs ( df ) [EOL] [EOL] assert ( evaluation_df == correct_df ) [EOL] [EOL] df = pd . DataFrame ( [ ( [string] , [string] , [string] ) , ( [string] , [string] , [string] ) , ( [string] , [string] , [string] ) ] , columns = [ [string] , [string] , [string] ] ) [EOL] [EOL] wrong_df = { [string] : { [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [number] / [number] , [string] : [ ( [string] , [number] ) , ( [string] , [number] ) ] , [string] : [ [number] , [number] ] , [string] : [ [number] , [number] / [number] ] , [string] : [ [number] , [number] ] , [string] : [ ( [string] , [ ( [string] , [number] ) ] ) , ( [string] , [ ( [string] , [number] ) ] ) ] , [string] : [number] , [string] : [number] } } [EOL] [EOL] evaluation_df = evaluate_model_outputs ( df ) [EOL] [EOL] assert evaluation_df == wrong_df [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , List [EOL] import typing [EOL] [docstring] [EOL] [EOL] import numpy as np [EOL] import pandas as pd [EOL] [EOL] from datawig . utils import merge_dicts , normalize_dataframe , random_split , random_cartesian_product [EOL] [EOL] [EOL] def test_random_split ( ) : [EOL] df = pd . DataFrame ( [ { [string] : [number] } , { [string] : [number] } ] ) [EOL] train_df , test_df = random_split ( df , split_ratios = [ [number] , [number] ] , seed = [number] ) [EOL] assert all ( train_df . values . flatten ( ) == np . array ( [ [number] ] ) ) [EOL] assert all ( test_df . values . flatten ( ) == np . array ( [ [number] ] ) ) [EOL] [EOL] [EOL] def test_normalize_dataframe ( ) : [EOL] assert ( normalize_dataframe ( pd . DataFrame ( { [string] : [ [string] , [number] ] } ) ) [ [string] ] . values . tolist ( ) == [ [string] , [string] ] ) [EOL] [EOL] [EOL] def test_merge_dicts ( ) : [EOL] d1 = { [string] : [number] } [EOL] d2 = { [string] : [number] } [EOL] merged = merge_dicts ( d1 , d2 ) [EOL] assert merged == { [string] : [number] , [string] : [number] } [EOL] [EOL] [EOL] def test_random_cartesian_product ( ) : [EOL] A = [ [number] , [number] , [number] ] [EOL] B = [ [string] , [string] , [string] ] [EOL] C = [ [string] , [string] ] [EOL] [EOL] out = random_cartesian_product ( ( A , B , C ) , [number] ) [EOL] [EOL] print ( out ) [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Tuple , ItemsView , Type , Dict , Set , Iterator , Any , List [EOL] import column_encoders [EOL] import typing [EOL] import builtins [EOL] import mxnet_input_symbols [EOL] import logging [EOL] import pandas [EOL] import datawig [EOL] import mxnet [EOL] import iterators [EOL] [docstring] [EOL] [EOL] import glob [EOL] import inspect [EOL] import itertools [EOL] import os [EOL] import pickle [EOL] import time [EOL] from typing import Any , List , Tuple [EOL] [EOL] import mxnet as mx [EOL] import numpy as np [EOL] [EOL] import pandas as pd [EOL] from mxnet . callback import save_checkpoint [EOL] from sklearn . preprocessing import StandardScaler [EOL] [EOL] from . import calibration [EOL] from . column_encoders import ( CategoricalEncoder , ColumnEncoder , NumericalEncoder , TfIdfEncoder ) [EOL] from . evaluation import evaluate_and_persist_metrics [EOL] from . iterators import ImputerIterDf , INSTANCE_WEIGHT_COLUMN [EOL] from . mxnet_input_symbols import Featurizer [EOL] from . utils import ( AccuracyMetric , ColumnOverwriteException , LogMetricCallBack , MeanSymbol , get_context , logger , merge_dicts , random_split , timing , log_formatter ) [EOL] from logging import FileHandler [EOL] [EOL] [EOL] class Imputer : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_encoders , data_featurizers , label_encoders , output_path = [string] ) : [EOL] [EOL] self . ctx = None [EOL] self . module = None [EOL] self . data_encoders = data_encoders [EOL] [EOL] self . batch_size = [number] [EOL] [EOL] self . data_featurizers = data_featurizers [EOL] self . label_encoders = label_encoders [EOL] self . final_fc_hidden_units = [ ] [EOL] [EOL] self . train_losses = None [EOL] self . test_losses = None [EOL] [EOL] self . training_time = [number] [EOL] self . calibration_temperature = None [EOL] [EOL] self . precision_recall_curves = { } [EOL] self . calibration_info = { } [EOL] [EOL] self . __class_patterns = None [EOL] [comment] [EOL] self . is_explainable = np . any ( [ isinstance ( encoder , CategoricalEncoder ) or isinstance ( encoder , TfIdfEncoder ) for encoder in self . data_encoders ] ) and ( len ( self . label_encoders ) == [number] ) and ( isinstance ( self . label_encoders [ [number] ] , CategoricalEncoder ) ) [EOL] [EOL] if len ( self . data_featurizers ) != len ( self . data_encoders ) : [EOL] raise ValueError ( [string] . format ( len ( self . data_encoders ) , len ( self . data_featurizers ) ) ) [EOL] [EOL] for encoder in self . data_encoders : [EOL] encoder_type = type ( encoder ) [EOL] if not issubclass ( encoder_type , ColumnEncoder ) : [EOL] raise ValueError ( [string] + [string] . format ( encoder_type ) ) [EOL] [EOL] for encoder in self . label_encoders : [EOL] encoder_type = type ( encoder ) [EOL] if encoder_type not in [ CategoricalEncoder , NumericalEncoder ] : [EOL] raise ValueError ( [string] . format ( encoder_type ) ) [EOL] [EOL] encoder_outputs = [ encoder . output_column for encoder in self . data_encoders ] [EOL] [EOL] for featurizer in self . data_featurizers : [EOL] featurizer_type = type ( featurizer ) [EOL] if not issubclass ( featurizer_type , Featurizer ) : [EOL] raise ValueError ( [string] . format ( featurizer_type ) ) [EOL] [EOL] if featurizer . field_name not in encoder_outputs : [EOL] raise ValueError ( [string] . format ( [string] . join ( encoder_outputs ) , featurizer_type ) ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] input_col_names = [ c . field_name for c in self . data_featurizers ] [EOL] label_col_names = list ( itertools . chain ( * [ c . input_columns for c in self . label_encoders ] ) ) [EOL] [EOL] if len ( set ( input_col_names ) . intersection ( set ( label_col_names ) ) ) != [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] [comment] [EOL] if ( output_path == [string] ) or ( not output_path ) : [EOL] output_path = [string] [EOL] [EOL] self . output_path = output_path [EOL] [EOL] [comment] [EOL] if self . output_path == [string] : [EOL] label_names = [ c . output_column . lower ( ) . replace ( [string] , [string] ) for c in self . label_encoders ] [EOL] self . output_path = [string] . join ( label_names ) [EOL] [EOL] if not os . path . exists ( self . output_path ) : [EOL] os . makedirs ( self . output_path ) [EOL] [EOL] self . __attach_log_filehandler ( filename = os . path . join ( self . output_path , [string] ) ) [EOL] [EOL] self . module_path = os . path . join ( self . output_path , [string] ) [EOL] [EOL] self . metrics_path = os . path . join ( self . output_path , [string] ) [EOL] [EOL] def __attach_log_filehandler ( self , filename , level = [string] ) : [EOL] [docstring] [EOL] [EOL] if os . access ( os . path . dirname ( filename ) , os . W_OK ) : [EOL] file_handler = FileHandler ( filename , mode = [string] ) [EOL] file_handler . setLevel ( level ) [EOL] file_handler . setFormatter ( log_formatter ) [EOL] logger . addHandler ( file_handler ) [EOL] else : [EOL] logger . warning ( [string] . format ( filename ) ) [EOL] [EOL] def __close_filehandlers ( self ) : [EOL] [docstring] [EOL] [EOL] handlers = logger . handlers [ : ] [EOL] for handler in handlers : [EOL] handler . close ( ) [EOL] logger . removeHandler ( handler ) [EOL] [EOL] def __check_data ( self , data_frame ) : [EOL] [docstring] [EOL] for col_enc in self . label_encoders : [EOL] [EOL] if not col_enc . is_fitted ( ) : [EOL] logger . warning ( [string] . format ( type ( col_enc ) , [string] . join ( col_enc . input_columns ) ) ) [EOL] elif isinstance ( col_enc , CategoricalEncoder ) : [EOL] values_not_in_test_set = set ( col_enc . token_to_idx . keys ( ) ) - set ( data_frame [ col_enc . input_columns [ [number] ] ] . unique ( ) ) [EOL] if len ( values_not_in_test_set ) > [number] : [EOL] logger . warning ( [string] [string] . format ( [string] . join ( values_not_in_test_set ) , col_enc . input_columns [ [number] ] ) ) [EOL] [EOL] def fit ( self , train_df , test_df = None , ctx = get_context ( ) , learning_rate = [number] , num_epochs = [number] , patience = [number] , test_split = [number] , weight_decay = [number] , batch_size = [number] , final_fc_hidden_units = None , calibrate = True ) : [EOL] [docstring] [EOL] if final_fc_hidden_units is None : [EOL] final_fc_hidden_units = [ ] [EOL] [EOL] [comment] [EOL] assert os . access ( self . output_path , os . W_OK ) , [string] . format ( self . output_path ) [EOL] [EOL] self . batch_size = batch_size [EOL] self . final_fc_hidden_units = final_fc_hidden_units [EOL] [EOL] self . ctx = ctx [EOL] logger . debug ( [string] . format ( ctx ) ) [EOL] [EOL] if ( train_df is None ) or ( not isinstance ( train_df , pd . core . frame . DataFrame ) ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if test_df is None : [EOL] train_df , test_df = random_split ( train_df , [ [number] - test_split , test_split ] ) [EOL] [EOL] iter_train , iter_test = self . __build_iterators ( train_df , test_df , test_split ) [EOL] [EOL] self . __check_data ( test_df ) [EOL] [EOL] [comment] [EOL] if self . module is None : [EOL] self . module = self . __build_module ( iter_train ) [EOL] [EOL] self . __fit_module ( iter_train , iter_test , learning_rate , num_epochs , patience , weight_decay ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if calibrate is True : [EOL] self . calibrate ( iter_test ) [EOL] [EOL] _ , metrics = self . __transform_and_compute_metrics_mxnet_iter ( iter_test , metrics_path = self . metrics_path ) [EOL] [EOL] for att , att_metric in metrics . items ( ) : [EOL] if isinstance ( att_metric , dict ) and ( [string] in att_metric ) : [EOL] self . precision_recall_curves [ att ] = att_metric [ [string] ] [EOL] [EOL] self . __prune_models ( ) [EOL] self . save ( ) [EOL] [EOL] if self . is_explainable : [EOL] self . __persist_class_prototypes ( iter_train , train_df ) [EOL] [EOL] self . __close_filehandlers ( ) [EOL] [EOL] return self [EOL] [EOL] def __persist_class_prototypes ( self , iter_train , train_df ) : [EOL] [docstring] [EOL] [EOL] if len ( self . label_encoders ) > [number] : [EOL] logger . warning ( [string] [string] + str ( self . label_encoders [ [number] ] . output_column ) + [string] ) [EOL] label_name = self . label_encoders [ [number] ] . output_column [EOL] [EOL] iter_train . reset ( ) [EOL] p = self . __predict_mxnet_iter ( iter_train ) [ label_name ] [comment] [EOL] [EOL] [comment] [EOL] p_normalized = StandardScaler ( ) . fit_transform ( p ) [EOL] [EOL] [comment] [EOL] explainable_data_encoders = [ ] [EOL] explainable_data_encoders_idx = [ ] [EOL] for encoder_idx , encoder in enumerate ( self . data_encoders ) : [EOL] if not ( isinstance ( encoder , TfIdfEncoder ) or isinstance ( encoder , CategoricalEncoder ) ) : [EOL] logger . warning ( [string] . format ( type ( encoder ) ) ) [EOL] else : [EOL] explainable_data_encoders . append ( encoder ) [EOL] explainable_data_encoders_idx . append ( encoder_idx ) [EOL] [EOL] [comment] [EOL] X = [ enc . transform ( train_df ) . transpose ( ) for enc in explainable_data_encoders ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] X_scaled = [ StandardScaler ( with_mean = False ) . fit_transform ( feature_matrix ) for feature_matrix in X ] [EOL] [EOL] [comment] [EOL] class_patterns = [ ] [EOL] for feature_matrix_scaled , encoder in zip ( X_scaled , explainable_data_encoders ) : [EOL] if isinstance ( encoder , TfIdfEncoder ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] class_patterns . append ( ( encoder , feature_matrix_scaled [ : , : p_normalized . shape [ [number] ] ] . dot ( p_normalized ) ) ) [EOL] elif isinstance ( encoder , CategoricalEncoder ) : [EOL] [comment] [EOL] class_patterns . append ( ( encoder , np . array ( [ np . sum ( p_normalized [ np . where ( feature_matrix_scaled [ [number] , : ] == category ) [ [number] ] , : ] , axis = [number] ) for category in encoder . idx_to_token . keys ( ) ] ) ) ) [EOL] else : [EOL] logger . warning ( [string] ) [EOL] [EOL] self . __class_patterns = class_patterns [EOL] [EOL] def __get_label_encoder ( self , label_column = None ) : [EOL] [docstring] [EOL] [EOL] if label_column is not None : [EOL] label_encoders = [ enc for enc in self . label_encoders if enc . output_column == label_column ] [EOL] if len ( label_encoders ) == [number] : [EOL] raise ValueError ( [string] ) [EOL] else : [EOL] label_encoder = label_encoders [ [number] ] [EOL] else : [EOL] label_encoder = self . label_encoders [ [number] ] [EOL] [EOL] return label_encoder [EOL] [EOL] def explain ( self , label , k = [number] , label_column = None ) : [EOL] [docstring] [EOL] [EOL] if not self . is_explainable : [EOL] raise ValueError ( [string] ) [EOL] [EOL] label_encoder = self . __get_label_encoder ( label_column ) [EOL] [EOL] [comment] [EOL] if label not in label_encoder . token_to_idx . keys ( ) : [EOL] raise ValueError ( [string] . format ( label ) ) [EOL] [EOL] [comment] [EOL] label_idx = label_encoder . token_to_idx [ label ] [EOL] [EOL] [comment] [EOL] feature_dict = dict ( explained_label = label ) [EOL] for encoder , pattern in self . __class_patterns : [EOL] [comment] [EOL] if isinstance ( encoder , CategoricalEncoder ) : [EOL] idx_tuples = zip ( pattern [ : , label_idx ] . argsort ( ) [ : : - [number] ] [ : k ] , sorted ( pattern [ : , label_idx ] ) [ : : - [number] ] [ : k ] ) [EOL] keymap = { i + [number] : i for i in range ( len ( encoder . idx_to_token ) ) } [EOL] idx2token_temp = dict ( ( keymap [ key ] , val ) for key , val in encoder . idx_to_token . items ( ) ) [EOL] if isinstance ( encoder , TfIdfEncoder ) : [EOL] idx_tuples = zip ( pattern [ : , label_idx ] . argsort ( ) [ : : - [number] ] [ : k ] , sorted ( pattern [ : , label_idx ] ) [ : : - [number] ] [ : k ] ) [EOL] idx2token_temp = encoder . idx_to_token [EOL] feature_dict [ encoder . output_column ] = [ ( idx2token_temp [ token ] , weight ) for token , weight in idx_tuples ] [EOL] [EOL] return feature_dict [EOL] [EOL] def explain_instance ( self , instance , k = [number] , label_column = None , label = None ) : [EOL] [docstring] [EOL] [EOL] if not self . is_explainable : [EOL] raise ValueError ( [string] ) [EOL] [EOL] label_encoder = self . __get_label_encoder ( label_column ) [EOL] [EOL] [comment] [EOL] if label is None : [EOL] df_temp = pd . DataFrame ( [ list ( instance . values ) ] , columns = list ( instance . index ) ) [EOL] label = self . predict ( df_temp ) [ label_encoder . output_column + [string] ] . values [ [number] ] [EOL] else : [EOL] assert label in label_encoder . token_to_idx . keys ( ) [EOL] [EOL] top_label_idx = label_encoder . token_to_idx [ label ] [EOL] [EOL] [comment] [EOL] feature_dict = dict ( explained_label = label ) [EOL] for encoder , pattern in self . __class_patterns : [EOL] [EOL] output_col = encoder . output_column [EOL] feature_dict [ output_col ] = { } [EOL] [EOL] for input_col in encoder . input_columns : [EOL] [EOL] token = instance [ input_col ] [EOL] [comment] [EOL] [EOL] if isinstance ( encoder , TfIdfEncoder ) : [EOL] input_encoded = encoder . vectorizer . transform ( [ token ] ) . todense ( ) [comment] [EOL] projection = input_encoded . dot ( pattern ) [comment] [EOL] feature_weights = np . multiply ( pattern [ : , top_label_idx ] , input_encoded ) [comment] [EOL] ordered_feature_idx = np . argsort ( np . multiply ( pattern [ : , top_label_idx ] , input_encoded ) ) [EOL] ordered_feature_idx = ordered_feature_idx . tolist ( ) [ [number] ] [ : : - [number] ] [EOL] [EOL] feature_dict [ output_col ] = [ ( encoder . idx_to_token [ idx ] , feature_weights [ [number] , idx ] ) for idx in ordered_feature_idx [ : k ] ] [EOL] [EOL] elif isinstance ( encoder , CategoricalEncoder ) : [EOL] input_encoded = encoder . token_to_idx [ token ] - [number] [comment] [EOL] class_weights = pattern [ input_encoded ] [comment] [EOL] [comment] [EOL] top_class = label_encoder . idx_to_token [ top_label_idx ] [EOL] top_class_weight = pattern [ input_encoded , top_label_idx ] [EOL] [EOL] feature_dict [ output_col ] = [ ( token , top_class_weight ) ] [EOL] [EOL] return feature_dict [EOL] [EOL] def __fit_module ( self , iter_train , iter_test , learning_rate , num_epochs , patience , weight_decay ) : [EOL] [docstring] [EOL] metric_name = [string] [EOL] [EOL] train_cb = LogMetricCallBack ( [ metric_name ] ) [EOL] test_cb = LogMetricCallBack ( [ metric_name ] , patience = patience ) [EOL] [EOL] def checkpoint ( epoch , sym , arg , aux ) : [EOL] save_checkpoint ( self . module_path , epoch , sym , arg , aux ) [EOL] [EOL] start = time . time ( ) [EOL] cross_entropy_metric = MeanSymbol ( metric_name ) [EOL] accuracy_metrics = [ AccuracyMetric ( name = label_col . output_column , label_index = i ) for i , label_col in enumerate ( self . label_encoders ) ] [EOL] combined_metric = mx . metric . CompositeEvalMetric ( metrics = [ cross_entropy_metric ] + accuracy_metrics ) [EOL] [EOL] with timing ( [string] ) : [EOL] try : [EOL] self . module . fit ( train_data = iter_train , eval_data = iter_test , eval_metric = combined_metric , num_epoch = num_epochs , initializer = mx . init . Xavier ( factor_type = [string] , magnitude = [number] ) , optimizer = [string] , optimizer_params = ( ( [string] , learning_rate ) , ( [string] , weight_decay ) ) , batch_end_callback = [ mx . callback . Speedometer ( iter_train . batch_size , int ( np . ceil ( iter_train . df_iterator . data [ [number] ] [ [number] ] . shape [ [number] ] / iter_train . batch_size / [number] ) ) , auto_reset = True ) ] , eval_end_callback = [ test_cb , train_cb ] , epoch_end_callback = checkpoint ) [EOL] except StopIteration : [EOL] [comment] [EOL] [comment] [EOL] logger . debug ( [string] ) [EOL] pass [EOL] [EOL] self . training_time = time . time ( ) - start [EOL] self . train_losses , self . test_losses = train_cb . metrics [ metric_name ] , test_cb . metrics [ metric_name ] [EOL] [EOL] def __build_module ( self , iter_train ) : [EOL] mod = _MXNetModule ( self . ctx , self . label_encoders , self . data_featurizers , self . final_fc_hidden_units ) [EOL] return mod ( iter_train ) [EOL] [EOL] def __build_iterators ( self , train_df , test_df , test_split ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if not isinstance ( train_df , pd . core . frame . DataFrame ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if ( test_df is not None ) & ( not isinstance ( test_df , pd . core . frame . DataFrame ) ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] input_col_names = [ c . field_name for c in self . data_featurizers ] [EOL] label_col_names = list ( itertools . chain ( * [ c . input_columns for c in self . label_encoders ] ) ) [EOL] [EOL] missing_columns = set ( input_col_names + label_col_names ) - set ( train_df . columns ) [EOL] [EOL] if len ( missing_columns ) > [number] : [EOL] ValueError ( [string] . format ( missing_columns ) ) [EOL] [EOL] for encoder in self . label_encoders : [EOL] if not encoder . is_fitted ( ) : [EOL] encoder_type = type ( encoder ) [EOL] logger . debug ( [string] . format ( encoder_type , len ( train_df ) ) ) [EOL] encoder . fit ( train_df ) [EOL] [EOL] [comment] [EOL] train_df = self . __drop_missing_labels ( train_df , how = [string] ) [EOL] [EOL] [comment] [EOL] if test_df is None : [EOL] train_df , test_df = random_split ( train_df , [ [number] - test_split , test_split ] ) [EOL] [EOL] test_df = self . __drop_missing_labels ( test_df , how = [string] ) [EOL] [EOL] logger . debug ( [string] . format ( len ( train_df ) , len ( test_df ) ) ) [EOL] [EOL] for encoder in self . data_encoders : [EOL] if not encoder . is_fitted ( ) : [EOL] encoder_type = type ( encoder ) [EOL] logger . debug ( [string] . format ( encoder_type , [string] . join ( encoder . input_columns ) , len ( train_df ) , encoder . __dict__ ) ) [EOL] [EOL] encoder . fit ( train_df ) [EOL] [EOL] logger . debug ( [string] . format ( len ( train_df ) ) ) [EOL] iter_train = ImputerIterDf ( data_frame = train_df , data_columns = self . data_encoders , label_columns = self . label_encoders , batch_size = self . batch_size ) [EOL] [EOL] logger . debug ( [string] . format ( len ( test_df ) ) ) [EOL] iter_test = ImputerIterDf ( data_frame = test_df , data_columns = iter_train . data_columns , label_columns = iter_train . label_columns , batch_size = self . batch_size ) [EOL] [EOL] return iter_train , iter_test [EOL] [EOL] def __transform_mxnet_iter ( self , mxnet_iter ) : [EOL] [docstring] [EOL] labels , model_outputs = zip ( * self . __predict_mxnet_iter ( mxnet_iter ) . items ( ) ) [EOL] predictions = { } [EOL] for col_enc , label , model_output in zip ( mxnet_iter . label_columns , labels , model_outputs ) : [EOL] if isinstance ( col_enc , CategoricalEncoder ) : [EOL] predictions [ label ] = col_enc . decode ( pd . Series ( model_output . argmax ( axis = [number] ) ) ) [EOL] elif isinstance ( col_enc , NumericalEncoder ) : [EOL] predictions [ label ] = model_output [EOL] return predictions [EOL] [EOL] @ staticmethod def __filter_predictions ( predictions , precision_threshold ) : [EOL] [docstring] [EOL] [EOL] filtered_predictions = [ ] [EOL] for prediction in predictions : [EOL] if prediction [ [number] ] [ [number] ] > precision_threshold : [EOL] filtered_predictions . append ( prediction [ [number] ] ) [EOL] else : [EOL] filtered_predictions . append ( ( ) ) [EOL] [EOL] return filtered_predictions [EOL] [EOL] def __predict_above_precision_mxnet_iter ( self , mxnet_iter , precision_threshold = [number] ) : [EOL] [docstring] [EOL] predictions = self . __predict_top_k_mxnet_iter ( mxnet_iter , top_k = [number] ) [EOL] for col_enc , att in zip ( self . label_encoders , predictions . keys ( ) ) : [EOL] if isinstance ( col_enc , CategoricalEncoder ) : [EOL] predictions [ att ] = self . __filter_predictions ( predictions [ att ] , precision_threshold ) [EOL] else : [EOL] logger . debug ( [string] . format ( att ) ) [EOL] predictions [ att ] = predictions [ att ] [EOL] [EOL] return predictions [EOL] [EOL] def __predict_mxnet_iter ( self , mxnet_iter ) : [EOL] [docstring] [EOL] if not self . module . for_training : [EOL] self . module . bind ( data_shapes = mxnet_iter . provide_data , label_shapes = mxnet_iter . provide_label ) [EOL] [comment] [EOL] [comment] [EOL] mod_output = [ o . asnumpy ( ) [ : mxnet_iter . start_padding_idx , : ] for o in self . module . predict ( mxnet_iter ) [ [number] : ] ] [EOL] output = { } [EOL] for label_encoder , pred in zip ( mxnet_iter . label_columns , mod_output ) : [EOL] if isinstance ( label_encoder , NumericalEncoder ) : [EOL] output [ label_encoder . output_column ] = label_encoder . decode ( pred ) [EOL] else : [EOL] [comment] [EOL] if self . calibration_temperature is not None : [EOL] pred = calibration . calibrate ( pred , self . calibration_temperature ) [EOL] output [ label_encoder . output_column ] = pred [EOL] return output [EOL] [EOL] def __predict_top_k_mxnet_iter ( self , mxnet_iter , top_k = [number] ) : [EOL] [docstring] [EOL] col_enc_att_probas = zip ( mxnet_iter . label_columns , * zip ( * self . __predict_mxnet_iter ( mxnet_iter ) . items ( ) ) ) [EOL] top_k_predictions = { } [EOL] for col_enc , att , probas in col_enc_att_probas : [EOL] if isinstance ( col_enc , CategoricalEncoder ) : [EOL] top_k_predictions [ att ] = [ ] [EOL] for pred in probas : [EOL] top_k_pred_idx = pred . argsort ( ) [ - top_k : ] [ : : - [number] ] [EOL] label_proba_tuples = [ ( col_enc . decode_token ( idx ) , pred [ idx ] ) for idx in top_k_pred_idx ] [EOL] top_k_predictions [ att ] . append ( label_proba_tuples ) [EOL] else : [EOL] logger . debug ( [string] . format ( att , type ( col_enc ) ) ) [EOL] top_k_predictions [ att ] = probas [EOL] [EOL] return top_k_predictions [EOL] [EOL] def __transform_and_compute_metrics_mxnet_iter ( self , mxnet_iter , metrics_path = None ) : [EOL] [docstring] [EOL] [comment] [EOL] mxnet_iter . reset ( ) [EOL] all_predictions = self . __transform_mxnet_iter ( mxnet_iter ) [EOL] [comment] [EOL] mxnet_iter . reset ( ) [EOL] all_predictions_proba = self . __predict_mxnet_iter ( mxnet_iter ) [EOL] mxnet_iter . reset ( ) [EOL] true_labels_idx_array = mx . nd . concat ( * [ mx . nd . concat ( * [ l for l in b . label ] , dim = [number] ) for b in mxnet_iter ] , dim = [number] ) [EOL] true_labels_string = { } [EOL] true_labels_idx = { } [EOL] predictions_categorical = { } [EOL] predictions_categorical_proba = { } [EOL] [EOL] predictions_numerical = { } [EOL] true_labels_numerical = { } [EOL] [EOL] for l_idx , col_enc in enumerate ( mxnet_iter . label_columns ) : [EOL] [comment] [EOL] n_predictions = len ( all_predictions [ col_enc . output_column ] ) [EOL] if isinstance ( col_enc , CategoricalEncoder ) : [EOL] predictions_categorical [ col_enc . output_column ] = all_predictions [ col_enc . output_column ] [EOL] predictions_categorical_proba [ col_enc . output_column ] = all_predictions_proba [ col_enc . output_column ] [EOL] attribute = col_enc . output_column [EOL] true_labels_idx [ attribute ] = true_labels_idx_array [ : n_predictions , l_idx ] . asnumpy ( ) [EOL] true_labels_string [ attribute ] = [ col_enc . decode_token ( idx ) for idx in true_labels_idx [ attribute ] ] [EOL] elif isinstance ( col_enc , NumericalEncoder ) : [EOL] true_labels_numerical [ col_enc . output_column ] = true_labels_idx_array [ : n_predictions , l_idx ] . asnumpy ( ) [EOL] predictions_numerical [ col_enc . output_column ] = all_predictions [ col_enc . output_column ] [EOL] [EOL] metrics = evaluate_and_persist_metrics ( true_labels_string , true_labels_idx , predictions_categorical , predictions_categorical_proba , metrics_path , None , true_labels_numerical , predictions_numerical ) [EOL] [EOL] return merge_dicts ( predictions_categorical , predictions_numerical ) , metrics [EOL] [EOL] def transform ( self , data_frame ) : [EOL] [docstring] [EOL] mxnet_iter = self . __mxnet_iter_from_df ( data_frame ) [EOL] return self . __transform_mxnet_iter ( mxnet_iter ) [EOL] [EOL] def predict ( self , data_frame , precision_threshold = [number] , imputation_suffix = [string] , score_suffix = [string] , inplace = False ) : [EOL] [docstring] [EOL] [EOL] if not inplace : [EOL] data_frame = data_frame . copy ( ) [EOL] [EOL] numerical_outputs = list ( itertools . chain ( * [ c . input_columns for c in self . label_encoders if isinstance ( c , NumericalEncoder ) ] ) ) [EOL] [EOL] predictions = self . predict_above_precision ( data_frame , precision_threshold ) . items ( ) [EOL] for label , imputations in predictions : [EOL] imputation_col = label + imputation_suffix [EOL] if imputation_col in data_frame . columns : [EOL] raise ColumnOverwriteException ( [string] . format ( imputation_col ) ) [EOL] [EOL] if label not in numerical_outputs : [EOL] imputation_proba_col = label + score_suffix [EOL] if imputation_proba_col in data_frame . columns : [EOL] raise ColumnOverwriteException ( [string] . format ( imputation_proba_col ) ) [EOL] [EOL] imputed_values , imputed_value_scores = [ ] , [ ] [EOL] for imputation_above_precision_threshold in imputations : [EOL] if not imputation_above_precision_threshold : [EOL] imputed_value , imputed_value_score = [string] , np . nan [EOL] else : [EOL] imputed_value , imputed_value_score = imputation_above_precision_threshold [EOL] imputed_values . append ( imputed_value ) [EOL] imputed_value_scores . append ( imputed_value_score ) [EOL] [EOL] data_frame [ imputation_col ] = imputed_values [EOL] data_frame [ imputation_proba_col ] = imputed_value_scores [EOL] [EOL] elif label in numerical_outputs : [EOL] data_frame [ imputation_col ] = imputations [EOL] [EOL] return data_frame [EOL] [EOL] def predict_proba ( self , data_frame ) : [EOL] [docstring] [EOL] mxnet_iter = self . __mxnet_iter_from_df ( data_frame ) [EOL] return self . __predict_mxnet_iter ( mxnet_iter ) [EOL] [EOL] def predict_above_precision ( self , data_frame , precision_threshold = [number] ) : [EOL] [docstring] [EOL] mxnet_iter = self . __mxnet_iter_from_df ( data_frame ) [EOL] return self . __predict_above_precision_mxnet_iter ( mxnet_iter , precision_threshold = precision_threshold ) [EOL] [EOL] def predict_proba_top_k ( self , data_frame , top_k = [number] ) : [EOL] [docstring] [EOL] mxnet_iter = self . __mxnet_iter_from_df ( data_frame ) [EOL] return self . __predict_top_k_mxnet_iter ( mxnet_iter , top_k ) [EOL] [EOL] def transform_and_compute_metrics ( self , data_frame , metrics_path = None ) : [EOL] [docstring] [EOL] for col_enc in self . label_encoders : [EOL] if col_enc . input_columns [ [number] ] not in data_frame . columns : [EOL] raise ValueError ( [string] . format ( col_enc . output_column , [string] . join ( data_frame . columns ) ) ) [EOL] [EOL] mxnet_iter = self . __mxnet_iter_from_df ( data_frame ) [EOL] return self . __transform_and_compute_metrics_mxnet_iter ( mxnet_iter , metrics_path = metrics_path ) [EOL] [EOL] def __drop_missing_labels ( self , data_frame , how = [string] ) : [EOL] [docstring] [EOL] n_samples = len ( data_frame ) [EOL] missing_idx = - [number] [EOL] for col_enc in self . label_encoders : [EOL] if isinstance ( col_enc , CategoricalEncoder ) : [EOL] [comment] [EOL] [comment] [EOL] col_missing_idx = data_frame [ col_enc . input_columns [ [number] ] ] . isna ( ) | ~ data_frame [ col_enc . input_columns [ [number] ] ] . isin ( col_enc . token_to_idx . keys ( ) ) [EOL] elif isinstance ( col_enc , NumericalEncoder ) : [EOL] [comment] [EOL] col_missing_idx = data_frame [ col_enc . input_columns [ [number] ] ] . isna ( ) [EOL] [EOL] logger . debug ( [string] . format ( col_missing_idx . sum ( ) , col_enc . input_columns [ [number] ] ) ) [EOL] [EOL] if missing_idx == - [number] : [EOL] missing_idx = col_missing_idx [EOL] elif how == [string] : [EOL] missing_idx = missing_idx & col_missing_idx [EOL] elif how == [string] : [EOL] missing_idx = missing_idx | col_missing_idx [EOL] [EOL] logger . debug ( [string] . format ( missing_idx . sum ( ) , n_samples ) ) [EOL] [EOL] return data_frame . loc [ ~ missing_idx , : ] [EOL] [EOL] def __prune_models ( self ) : [EOL] [docstring] [EOL] best_model = glob . glob ( self . module_path + [string] . format ( self . __get_best_epoch ( ) ) ) [EOL] logger . debug ( [string] . format ( best_model [ [number] ] ) ) [EOL] worse_models = set ( glob . glob ( self . module_path + [string] ) ) - set ( best_model ) [EOL] [comment] [EOL] for worse_epoch in worse_models : [EOL] logger . debug ( [string] . format ( worse_epoch ) ) [EOL] os . remove ( worse_epoch ) [EOL] [EOL] def __get_best_epoch ( self ) : [EOL] [docstring] [EOL] return sorted ( enumerate ( self . test_losses ) , key = lambda x : x [ [number] ] ) [ [number] ] [ [number] ] [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] [comment] [EOL] params = { k : v for k , v in self . __dict__ . items ( ) if k != [string] } [EOL] pickle . dump ( params , open ( os . path . join ( self . output_path , [string] ) , [string] ) ) [EOL] [EOL] @ staticmethod def load ( output_path ) : [EOL] [docstring] [EOL] [EOL] logger . debug ( [string] . format ( output_path ) ) [EOL] params = pickle . load ( open ( os . path . join ( output_path , [string] ) , [string] ) ) [EOL] imputer_signature = inspect . getfullargspec ( Imputer . __init__ ) [ [number] ] [EOL] [comment] [EOL] constructor_args = { p : params [ p ] for p in imputer_signature if p != [string] } [EOL] non_constructor_args = { p : params [ p ] for p in params . keys ( ) if p not in [ [string] ] + list ( constructor_args . keys ( ) ) } [EOL] [EOL] [comment] [EOL] imputer = Imputer ( ** constructor_args ) [EOL] [comment] [EOL] for arg , value in non_constructor_args . items ( ) : [EOL] setattr ( imputer , arg , value ) [EOL] [EOL] [comment] [EOL] imputer . module_path = os . path . join ( output_path , [string] ) [EOL] imputer . output_path = output_path [EOL] [comment] [EOL] ctx = get_context ( ) [EOL] [EOL] logger . debug ( [string] . format ( imputer . module_path ) ) [EOL] [EOL] [comment] [EOL] if isinstance ( imputer . label_encoders [ [number] ] , NumericalEncoder ) : [EOL] data_names = [ s . field_name for s in imputer . data_featurizers ] [EOL] else : [EOL] data_names = [ s . field_name for s in imputer . data_featurizers ] + [ INSTANCE_WEIGHT_COLUMN ] [EOL] [EOL] [comment] [EOL] imputer . module = mx . module . Module . load ( imputer . module_path , imputer . __get_best_epoch ( ) , context = ctx , data_names = data_names , label_names = [ s . output_column for s in imputer . label_encoders ] ) [EOL] return imputer [EOL] [EOL] def __mxnet_iter_from_df ( self , data_frame ) : [EOL] [docstring] [EOL] return ImputerIterDf ( data_frame = data_frame , data_columns = self . data_encoders , label_columns = self . label_encoders , batch_size = self . batch_size ) [EOL] [EOL] def calibrate ( self , test_iter ) : [EOL] [docstring] [EOL] [EOL] test_iter . reset ( ) [EOL] proba = self . __predict_mxnet_iter ( test_iter ) [EOL] [EOL] test_iter . reset ( ) [EOL] labels = mx . nd . concat ( * [ mx . nd . concat ( * [ l for l in b . label ] , dim = [number] ) for b in test_iter ] , dim = [number] ) [EOL] [EOL] if len ( test_iter . label_columns ) != [number] : [EOL] logger . warning ( [string] ) [EOL] return [EOL] [EOL] output_label = test_iter . label_columns [ [number] ] . output_column [EOL] n_labels = proba [ output_label ] . shape [ [number] ] [EOL] [EOL] scores = proba [ output_label ] [EOL] labels = labels . asnumpy ( ) . squeeze ( ) [ : n_labels ] [EOL] [EOL] ece_pre = calibration . compute_ece ( scores , labels ) [EOL] self . calibration_info [ [string] ] = ece_pre [EOL] self . calibration_info [ [string] ] = calibration . reliability ( scores , labels ) [EOL] logger . debug ( [string] . format ( [number] * ece_pre ) ) [EOL] [EOL] temperature = calibration . fit_temperature ( scores , labels ) [EOL] ece_post = calibration . compute_ece ( scores , labels , temperature ) [EOL] self . calibration_info [ [string] ] = ece_post [EOL] logger . debug ( [string] . format ( [number] * ece_post ) ) [EOL] [EOL] [comment] [EOL] if ece_pre - ece_post > [number] : [EOL] self . calibration_info [ [string] ] = calibration . reliability ( calibration . calibrate ( scores , temperature ) , labels ) [EOL] self . calibration_info [ [string] ] = calibration . compute_ece ( scores , labels , temperature ) [EOL] self . calibration_temperature = temperature [EOL] [EOL] [EOL] class _MXNetModule : [EOL] def __init__ ( self , ctx , label_encoders , data_featurizers , final_fc_hidden_units ) : [EOL] [docstring] [EOL] self . ctx = ctx [EOL] self . data_featurizers = data_featurizers [EOL] self . label_encoders = label_encoders [EOL] self . final_fc_hidden_units = final_fc_hidden_units [EOL] [EOL] def __call__ ( self , iter_train ) : [EOL] [docstring] [EOL] [EOL] predictions , loss = self . __make_loss ( ) [EOL] [EOL] logger . debug ( [string] ) [EOL] output_symbols = [ ] [EOL] for col_enc , output in zip ( self . label_encoders , predictions ) : [EOL] output_symbols . append ( mx . sym . BlockGrad ( output , name = [string] . format ( col_enc . output_column ) ) ) [EOL] [EOL] mod = mx . mod . Module ( mx . sym . Group ( [ loss ] + output_symbols ) , context = self . ctx , data_names = [ name for name , dim in iter_train . provide_data if name in loss . list_arguments ( ) ] , label_names = [ name for name , dim in iter_train . provide_label ] ) [EOL] [EOL] if mod . binded is False : [EOL] mod . bind ( data_shapes = [ d for d in iter_train . provide_data if d . name in loss . list_arguments ( ) ] , label_shapes = iter_train . provide_label ) [EOL] [EOL] return mod [EOL] [EOL] @ staticmethod def __make_categorical_loss ( latents , label_field_name , num_labels , final_fc_hidden_units = None ) : [EOL] [docstring] [EOL] [EOL] fully_connected = None [EOL] if len ( final_fc_hidden_units ) == [number] : [EOL] [comment] [EOL] fully_connected = mx . sym . FullyConnected ( data = latents , num_hidden = num_labels , name = [string] . format ( label_field_name ) ) [EOL] else : [EOL] layer_size = final_fc_hidden_units [EOL] with mx . name . Prefix ( [string] . format ( label_field_name ) ) : [EOL] for i , layer in enumerate ( layer_size ) : [EOL] if i == len ( layer_size ) - [number] : [EOL] fully_connected = mx . sym . FullyConnected ( data = latents , num_hidden = layer ) [EOL] else : [EOL] latents = mx . sym . FullyConnected ( data = latents , num_hidden = layer ) [EOL] [EOL] instance_weight = mx . sym . Variable ( INSTANCE_WEIGHT_COLUMN ) [EOL] pred = mx . sym . softmax ( fully_connected ) [EOL] label = mx . sym . Variable ( label_field_name ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] logger . debug ( [string] . format ( label , num_labels ) ) [EOL] [EOL] num_labels_vec = label * [number] + num_labels [EOL] indices = mx . sym . broadcast_lesser ( label , num_labels_vec ) [EOL] label = label * indices [EOL] [EOL] [comment] [EOL] label = mx . sym . split ( label , axis = [number] , num_outputs = [number] , squeeze_axis = [number] ) [EOL] [EOL] [comment] [EOL] missing_labels = mx . sym . zeros_like ( label ) [EOL] positive_mask = mx . sym . broadcast_greater ( label , missing_labels ) [EOL] [EOL] [comment] [EOL] cross_entropy = mx . sym . pick ( mx . sym . log_softmax ( fully_connected ) , label ) * - [number] * positive_mask [EOL] [comment] [EOL] cross_entropy = cross_entropy * mx . sym . pick ( instance_weight , label ) [EOL] [EOL] [comment] [EOL] num_positive_indices = mx . sym . sum ( positive_mask ) [EOL] cross_entropy = mx . sym . broadcast_div ( cross_entropy , num_positive_indices + [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] batch_size = mx . sym . sum ( mx . sym . ones_like ( label ) ) [EOL] cross_entropy = mx . sym . broadcast_mul ( cross_entropy , batch_size ) [EOL] [EOL] return pred , cross_entropy [EOL] [EOL] @ staticmethod def __make_numerical_loss ( latents , label_field_name ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] pred = mx . sym . FullyConnected ( data = latents , num_hidden = [number] , name = [string] . format ( label_field_name ) ) [EOL] [EOL] target = mx . sym . Variable ( label_field_name ) [EOL] [EOL] [comment] [EOL] loss = mx . sym . sum ( ( pred - target ) ** [number] ) [EOL] [EOL] return pred , loss [EOL] [EOL] def __make_loss ( self , eps = [number] ) : [EOL] [EOL] logger . debug ( [string] . format ( len ( self . data_featurizers ) ) ) [EOL] [EOL] unique_input_field_names = set ( [ feat . field_name for feat in self . data_featurizers ] ) [EOL] if len ( unique_input_field_names ) < len ( self . data_featurizers ) : [EOL] raise ValueError ( [string] . format ( [string] . join ( unique_input_field_names ) ) ) [EOL] [EOL] [comment] [EOL] latents = mx . sym . concat ( * [ f . latent_symbol ( ) for f in self . data_featurizers ] , dim = [number] ) [EOL] [EOL] [comment] [EOL] outputs = [ ] [EOL] for output_col in self . label_encoders : [EOL] if isinstance ( output_col , CategoricalEncoder ) : [EOL] logger . debug ( [string] . format ( output_col . output_column , output_col . max_tokens ) ) [EOL] outputs . append ( self . __make_categorical_loss ( latents , output_col . output_column , output_col . max_tokens + [number] , self . final_fc_hidden_units ) ) [EOL] elif isinstance ( output_col , NumericalEncoder ) : [EOL] logger . debug ( [string] . format ( output_col . output_column ) ) [EOL] outputs . append ( self . __make_numerical_loss ( latents , output_col . output_column ) ) [EOL] [EOL] predictions , losses = zip ( * outputs ) [EOL] [EOL] [comment] [EOL] mean_batch_losses = [ mx . sym . mean ( l ) + eps for l in losses ] [EOL] [EOL] [comment] [EOL] normalized_losses = [ mx . sym . broadcast_div ( l , mean_loss ) for l , mean_loss in zip ( losses , mean_batch_losses ) ] [EOL] [EOL] [comment] [EOL] mean_label_batch_loss = mx . sym . ElementWiseSum ( * mean_batch_losses ) / float ( len ( mean_batch_losses ) ) [EOL] [EOL] [comment] [EOL] loss = mx . sym . broadcast_mul ( mx . sym . ElementWiseSum ( * normalized_losses ) / float ( len ( mean_batch_losses ) ) , mean_label_batch_loss ) [EOL] loss = mx . sym . MakeLoss ( loss , normalization = [string] , valid_thresh = [number] ) [EOL] [EOL] return predictions , loss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 $mxnet.symbol$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $mxnet.symbol$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
[comment] [EOL] from . column_encoders import CategoricalEncoder , BowEncoder , NumericalEncoder , SequentialEncoder [EOL] from . mxnet_input_symbols import BowFeaturizer , LSTMFeaturizer , NumericalFeaturizer , EmbeddingFeaturizer [EOL] from . simple_imputer import SimpleImputer [EOL] from . imputer import Imputer [EOL] [EOL] name = [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Tuple , Any , List [EOL] import typing [EOL] [docstring] [EOL] [EOL] import json [EOL] import pandas as pd [EOL] from sklearn . metrics import precision_score , recall_score , f1_score , accuracy_score , precision_recall_curve , mean_squared_error [EOL] from . utils import logger [EOL] [EOL] [EOL] def evaluate_and_persist_metrics ( true_labels_string , true_labels_int , predictions , predictions_proba , metrics_file = None , missing_symbol = None , numerical_labels = { } , numerical_predictions = { } ) : [EOL] [docstring] [EOL] [EOL] if missing_symbol is None : [EOL] missing_symbol = { k : [string] for k in true_labels_string . keys ( ) } [EOL] [EOL] if len ( true_labels_string ) > [number] : [EOL] [comment] [EOL] dfs = [ ] [EOL] for att in true_labels_string . keys ( ) : [EOL] true_and_predicted = [ ( true , pred ) for true , pred in zip ( true_labels_string [ att ] , predictions [ att ] ) if true != missing_symbol [ att ] ] [EOL] assert len ( true_and_predicted ) > [number] , [string] . format ( att ) [EOL] truth , predicted = zip ( * true_and_predicted ) [EOL] logger . debug ( [string] . format ( len ( truth ) , len ( true_labels_string [ att ] ) , att ) ) [EOL] dfs . append ( pd . DataFrame ( { [string] : att , [string] : truth , [string] : predicted } ) ) [EOL] [EOL] df = pd . concat ( dfs ) [EOL] [EOL] metrics = evaluate_model_outputs ( df ) [EOL] [EOL] logger . debug ( [string] ) [EOL] for label , att_metrics in metrics . items ( ) : [EOL] logger . debug ( [string] . format ( label , list ( filter ( lambda x : x [ [number] ] . startswith ( [string] ) , att_metrics . items ( ) ) ) ) ) [EOL] [EOL] logger . debug ( [string] ) [EOL] for label , att_metrics in metrics . items ( ) : [EOL] logger . debug ( [string] . format ( label , list ( filter ( lambda x : x [ [number] ] . startswith ( [string] ) , att_metrics . items ( ) ) ) ) ) [EOL] [EOL] for att in true_labels_string . keys ( ) : [EOL] metrics [ att ] [ [string] ] = { } [EOL] for label in range ( [number] , predictions_proba [ att ] . shape [ - [number] ] ) : [EOL] true_labels = ( true_labels_int [ att ] == label ) . nonzero ( ) [ [number] ] [EOL] if len ( true_labels ) > [number] : [EOL] true_label = true_labels_string [ att ] [ true_labels [ [number] ] ] [EOL] y_true = ( true_labels_int [ att ] == label ) * [number] [EOL] y_score = predictions_proba [ att ] [ : , label ] [EOL] prec , rec , thresholds = precision_recall_curve ( y_true , y_score ) [EOL] metrics [ att ] [ [string] ] [ true_label ] = { [string] : prec , [string] : rec , [string] : thresholds } [EOL] threshold_idx = ( prec > [number] ) . nonzero ( ) [ [number] ] [ [number] ] - [number] [EOL] logger . debug ( [string] . format ( att , true_label , prec [ threshold_idx ] , rec [ threshold_idx ] , thresholds [ threshold_idx ] ) ) [EOL] else : [EOL] metrics = { } [EOL] [EOL] for col_name in numerical_labels . keys ( ) : [EOL] metrics [ col_name ] = [number] * mean_squared_error ( numerical_labels [ col_name ] , numerical_predictions [ col_name ] ) [EOL] [EOL] if metrics_file : [EOL] import copy [EOL] serialize_metrics = copy . deepcopy ( metrics ) [EOL] [comment] [EOL] for att in true_labels_string . keys ( ) : [EOL] for label in metrics [ att ] [ [string] ] . keys ( ) : [EOL] serialize_metrics [ att ] [ [string] ] [ label ] = { [string] : metrics [ att ] [ [string] ] [ label ] [ [string] ] . tolist ( ) , [string] : metrics [ att ] [ [string] ] [ label ] [ [string] ] . tolist ( ) , [string] : metrics [ att ] [ [string] ] [ label ] [ [string] ] . tolist ( ) } [EOL] logger . debug ( [string] . format ( metrics_file ) ) [EOL] with open ( metrics_file , [string] ) as fp : [EOL] json . dump ( serialize_metrics , fp ) [EOL] [EOL] return metrics [EOL] [EOL] [EOL] def evaluate_model_outputs ( df , attribute_column_name = [string] , true_label_column_name = [string] , predicted_label_column_name = [string] ) : [EOL] [docstring] [EOL] [EOL] groups = df . groupby ( attribute_column_name ) [EOL] [EOL] model_metrics = { } [EOL] [EOL] for group , group_df in groups : [EOL] true = group_df [ true_label_column_name ] [EOL] predicted = group_df [ predicted_label_column_name ] [EOL] [EOL] model_metrics [ group ] = evaluate_model_outputs_single_attribute ( true , predicted ) [EOL] [EOL] return model_metrics [EOL] [EOL] [EOL] def evaluate_model_outputs_single_attribute ( true , predicted , topMisclassifications = [number] ) : [EOL] true = true . astype ( str ) [EOL] predicted = predicted . astype ( str ) [EOL] [EOL] labels = true . value_counts ( ) [EOL] [EOL] model_metrics = dict ( ) [EOL] [EOL] model_metrics [ [string] ] = [ ( a , int ( b ) ) for a , b in labels . iteritems ( ) ] [EOL] [EOL] [comment] [EOL] model_metrics [ [string] ] = precision_score ( true , predicted , average = [string] ) [EOL] model_metrics [ [string] ] = recall_score ( true , predicted , average = [string] ) [EOL] model_metrics [ [string] ] = f1_score ( true , predicted , average = [string] ) [EOL] model_metrics [ [string] ] = accuracy_score ( true , predicted ) [EOL] [EOL] [comment] [EOL] model_metrics [ [string] ] = precision_score ( true , predicted , average = [string] ) [EOL] model_metrics [ [string] ] = recall_score ( true , predicted , average = [string] ) [EOL] model_metrics [ [string] ] = f1_score ( true , predicted , average = [string] ) [EOL] [comment] [EOL] model_metrics [ [string] ] = accuracy_score ( true , predicted ) [EOL] [EOL] [comment] [EOL] model_metrics [ [string] ] = precision_score ( true , predicted , average = None , labels = labels . index . tolist ( ) ) . tolist ( ) [EOL] model_metrics [ [string] ] = recall_score ( true , predicted , average = None , labels = labels . index . tolist ( ) ) . tolist ( ) [EOL] model_metrics [ [string] ] = f1_score ( true , predicted , average = None , labels = labels . index . tolist ( ) ) . tolist ( ) [EOL] model_metrics [ [string] ] = accuracy_score ( true , predicted ) . tolist ( ) [EOL] model_metrics [ [string] ] = len ( model_metrics [ [string] ] ) [EOL] model_metrics [ [string] ] = int ( sum ( count for ( class_name , count ) in model_metrics [ [string] ] ) ) [EOL] [EOL] true_name = [string] [EOL] pred_name = [string] [EOL] groups = pd . DataFrame ( list ( zip ( true , predicted ) ) , columns = [ true_name , pred_name ] ) . groupby ( true_name ) [EOL] model_metrics [ [string] ] = [ ] [EOL] for label in labels . index . tolist ( ) : [EOL] confusion_matrix_series = groups . get_group ( label ) [ pred_name ] . value_counts ( ) [ : topMisclassifications ] [EOL] confusion_matrix = list ( zip ( confusion_matrix_series . index . tolist ( ) , map ( int , confusion_matrix_series . tolist ( ) ) ) ) [EOL] model_metrics [ [string] ] . append ( ( label , confusion_matrix ) ) [EOL] [EOL] return model_metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import builtins [EOL] import numpy [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import numpy as np [EOL] from scipy . optimize import minimize [EOL] from . utils import softmax [EOL] [EOL] [EOL] def compute_ece ( scores , labels , lbda = [number] , step = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] probas = calibrate ( scores , lbda ) [EOL] [EOL] [comment] [EOL] top_probas = np . max ( probas , axis = [number] ) [EOL] [EOL] [comment] [EOL] predictions = np . argmax ( probas , [number] ) [EOL] [EOL] bin_means = np . arange ( step / [number] , [number] , step ) [EOL] [EOL] ece = [number] [EOL] [EOL] [comment] [EOL] for bin_lower , bin_upper in [ ( mean - step / [number] , mean + step / [number] ) for mean in bin_means ] : [EOL] [EOL] [comment] [EOL] bin_mask = ( top_probas >= bin_lower ) & ( top_probas < bin_upper ) [EOL] if np . any ( bin_mask ) : [EOL] in_bin_confidence = np . mean ( top_probas [ bin_mask ] ) [EOL] in_bin_precision = np . mean ( labels [ bin_mask ] == predictions [ bin_mask ] ) [EOL] ece += np . abs ( in_bin_confidence - in_bin_precision ) * np . mean ( bin_mask ) [EOL] [EOL] return ece [EOL] [EOL] [EOL] def ece_loss ( lbda , * args ) : [EOL] [docstring] [EOL] [EOL] scores , labels = args [EOL] [EOL] return compute_ece ( scores , labels , lbda = lbda ) [EOL] [EOL] [EOL] def logits_from_probas ( probas , force = False ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if ( force is True ) or ( np . all ( np . sum ( probas , [number] ) - [number] < [number] ) and np . all ( probas <= [number] ) ) : [EOL] return np . log ( probas ) [EOL] else : [EOL] return probas [EOL] [EOL] [EOL] def probas_from_logits ( scores , lbda = [number] , force = False ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if np . all ( np . sum ( scores , [number] ) - [number] < [number] ) and np . all ( scores <= [number] ) and force is False : [EOL] return scores [EOL] else : [EOL] return np . array ( [ softmax ( lbda * row ) for row in scores ] ) [EOL] [EOL] [EOL] def calibrate ( scores , lbda ) : [EOL] [docstring] [EOL] [EOL] logits = logits_from_probas ( scores , force = True ) [EOL] [EOL] return np . array ( [ softmax ( lbda * row ) for row in logits ] ) [EOL] [EOL] [EOL] def reliability ( scores , labels , step = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] probas = probas_from_logits ( scores ) [EOL] [EOL] [comment] [EOL] top_probas = np . max ( probas , axis = [number] ) [EOL] [EOL] predictions = np . argmax ( probas , [number] ) [EOL] [EOL] bin_means = np . arange ( step / [number] , [number] , step ) [EOL] [EOL] in_bin_precisions = np . zeros ( len ( bin_means ) ) [EOL] [EOL] for i , ( bin_lower , bin_upper ) in enumerate ( [ ( mean - step / [number] , mean + step / [number] ) for mean in bin_means ] ) : [EOL] [EOL] bin_mask = ( top_probas >= bin_lower ) & ( top_probas < bin_upper ) [EOL] if np . any ( bin_mask ) : [EOL] in_bin_precisions [ i ] = np . mean ( labels [ bin_mask ] == predictions [ bin_mask ] ) [EOL] [EOL] return bin_means , in_bin_precisions [EOL] [EOL] [EOL] def fit_temperature ( scores , labels ) : [EOL] [docstring] [EOL] [EOL] probas = probas_from_logits ( scores ) [EOL] [EOL] res = minimize ( ece_loss , [number] , method = [string] , tol = [number] , args = ( probas , labels ) , options = { [string] : [number] } , bounds = ( ( [number] , [number] ) , ) ) [EOL] [EOL] assert res [ [string] ] == True [EOL] [EOL] return res [ [string] ] [ [number] ] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.tuple$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , List [EOL] import column_encoders [EOL] import typing [EOL] import builtins [EOL] import pandas [EOL] import datawig [EOL] import mxnet [EOL] [docstring] [EOL] from typing import List , Any [EOL] import mxnet as mx [EOL] import numpy as np [EOL] import pandas as pd [EOL] from pandas . api . types import is_numeric_dtype [EOL] [EOL] from . column_encoders import ColumnEncoder , NumericalEncoder [EOL] from . utils import logger [EOL] INSTANCE_WEIGHT_COLUMN = [string] [EOL] [EOL] [EOL] class ImputerIter ( mx . io . DataIter ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_columns , label_columns , batch_size = [number] ) : [EOL] [EOL] mx . io . DataIter . __init__ ( self , batch_size ) [EOL] [EOL] self . data_columns = data_columns [EOL] self . label_columns = label_columns [EOL] self . cur_batch = [number] [EOL] [EOL] self . _provide_data = None [EOL] self . _provide_label = None [EOL] self . df_iterator = None [EOL] self . indices = [ ] [EOL] self . start_padding_idx = [number] [EOL] [EOL] def __iter__ ( self ) : [EOL] return self [EOL] [EOL] def __next__ ( self ) : [EOL] return self . next ( ) [EOL] [EOL] @ property def provide_data ( self ) : [EOL] [docstring] [EOL] return self . _provide_data [EOL] [EOL] @ property def provide_label ( self ) : [EOL] [docstring] [EOL] return self . _provide_label [EOL] [EOL] def decode ( self , mxnet_label_predictions ) : [EOL] [docstring] [EOL] return [ col . decode ( pd . Series ( pred . asnumpy ( ) . flatten ( ) ) ) . tolist ( ) for col , pred in zip ( self . label_columns , mxnet_label_predictions ) ] [EOL] [EOL] def mxnet_iterator_from_df ( self , data_frame ) : [EOL] [docstring] [EOL] n_samples = len ( data_frame ) [EOL] [comment] [EOL] data = { } [EOL] for col_enc in self . data_columns : [EOL] data_array_numpy = col_enc . transform ( data_frame ) [EOL] data [ col_enc . output_column ] = mx . nd . array ( data_array_numpy [ : n_samples , : ] ) [EOL] logger . debug ( [string] . format ( len ( data_frame ) , [string] . join ( col_enc . input_columns ) , col_enc . __class__ , type ( data_array_numpy ) , data_array_numpy . shape , data [ col_enc . output_column ] . shape ) ) [EOL] [EOL] [comment] [EOL] labels = { } [EOL] for col_enc in self . label_columns : [EOL] assert len ( col_enc . input_columns ) == [number] , [string] . format ( len ( col_enc . input_columns ) ) [EOL] [EOL] if col_enc . input_columns [ [number] ] in data_frame . columns : [EOL] labels_array_numpy = col_enc . transform ( data_frame ) . astype ( np . float64 ) [EOL] labels [ col_enc . output_column ] = mx . nd . array ( labels_array_numpy [ : n_samples , : ] ) [EOL] logger . debug ( [string] . format ( len ( data_frame ) , col_enc . input_columns [ [number] ] , col_enc . __class__ , type ( labels_array_numpy ) , labels_array_numpy . shape , labels [ col_enc . output_column ] . shape ) ) [EOL] else : [EOL] labels [ col_enc . input_columns [ [number] ] ] = mx . nd . zeros ( ( n_samples , [number] ) ) [EOL] logger . debug ( [string] . format ( col_enc . input_columns [ [number] ] , n_samples ) ) [EOL] [EOL] [comment] [EOL] assert len ( labels . keys ( ) ) == [number] [comment] [EOL] [EOL] [comment] [EOL] if not isinstance ( self . label_columns [ [number] ] , NumericalEncoder ) : [EOL] [EOL] [comment] [EOL] if INSTANCE_WEIGHT_COLUMN in data_frame . columns : [EOL] data [ INSTANCE_WEIGHT_COLUMN ] = mx . nd . array ( np . expand_dims ( data_frame [ INSTANCE_WEIGHT_COLUMN ] , [number] ) ) [EOL] else : [EOL] data [ INSTANCE_WEIGHT_COLUMN ] = mx . nd . array ( np . ones ( [ n_samples , [number] ] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] return mx . io . NDArrayIter ( data , labels , batch_size = self . batch_size , last_batch_handle = [string] ) [EOL] [EOL] def _n_rows_padding ( self , data_frame ) : [EOL] [docstring] [EOL] n_test_samples = data_frame . shape [ [number] ] [EOL] n_rows = int ( self . batch_size - n_test_samples % self . batch_size ) [EOL] [EOL] pad = [number] [EOL] if n_rows != self . batch_size : [EOL] pad = n_rows [EOL] [EOL] return pad [EOL] [EOL] def reset ( self ) : [EOL] [docstring] [EOL] self . cur_batch = [number] [EOL] self . df_iterator . reset ( ) [EOL] [EOL] def next ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] next_batch = next ( self . df_iterator ) [EOL] [EOL] [comment] [EOL] start_batch = self . cur_batch * self . batch_size [EOL] next_batch . index = self . indices [ start_batch : start_batch + self . batch_size ] [EOL] [EOL] self . cur_batch += [number] [EOL] [EOL] return next_batch [EOL] [EOL] [EOL] class ImputerIterDf ( ImputerIter ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_frame , data_columns , label_columns , batch_size = [number] ) : [EOL] super ( ImputerIterDf , self ) . __init__ ( data_columns , label_columns , batch_size ) [EOL] [EOL] if not isinstance ( data_frame , pd . core . frame . DataFrame ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] [comment] [EOL] numerical_columns = [ c for c in data_frame . columns if is_numeric_dtype ( data_frame [ c ] ) ] [EOL] string_columns = list ( set ( data_frame . columns ) - set ( numerical_columns ) ) [EOL] data_frame = data_frame . fillna ( value = { x : [string] for x in string_columns } ) [EOL] data_frame = data_frame . fillna ( value = { x : np . nan for x in numerical_columns } ) [EOL] [EOL] self . indices = data_frame . index . tolist ( ) [EOL] data_frame = data_frame . reset_index ( drop = True ) [EOL] [EOL] [comment] [EOL] padding_n_rows = self . _n_rows_padding ( data_frame ) [EOL] self . start_padding_idx = int ( data_frame . index . max ( ) + [number] ) [EOL] for idx in range ( self . start_padding_idx , self . start_padding_idx + padding_n_rows ) : [EOL] data_frame . loc [ idx , : ] = data_frame . loc [ self . start_padding_idx - [number] , : ] [EOL] [EOL] for column_encoder in self . data_columns + self . label_columns : [EOL] [comment] [EOL] if not column_encoder . is_fitted ( ) : [EOL] column_encoder . fit ( data_frame ) [EOL] [EOL] self . df_iterator = self . mxnet_iterator_from_df ( data_frame ) [EOL] self . df_iterator . reset ( ) [EOL] self . _provide_data = self . df_iterator . provide_data [EOL] self . _provide_label = self . df_iterator . provide_label [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $mxnet.io.NDArrayIter$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 $builtins.int$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 $typing.Any$ 0 $pandas.DataFrame$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.io.DataBatch$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $typing.List[column_encoders.ColumnEncoder]$ 0 $typing.List[column_encoders.ColumnEncoder]$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[column_encoders.ColumnEncoder]$ 0 $typing.List[column_encoders.ColumnEncoder]$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[column_encoders.ColumnEncoder]$ 0 0 0 $typing.List[column_encoders.ColumnEncoder]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , List [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import pandas [EOL] import mxnet [EOL] [docstring] [EOL] [EOL] import contextlib [EOL] import itertools [EOL] import logging [EOL] import math [EOL] import random [EOL] import sys [EOL] import time [EOL] import string [EOL] import collections [EOL] from typing import Any , List , Tuple , Dict [EOL] [EOL] import mxnet as mx [EOL] import numpy as np [EOL] import pandas as pd [EOL] [EOL] mx . random . seed ( [number] ) [EOL] random . seed ( [number] ) [EOL] np . random . seed ( [number] ) [EOL] [EOL] [comment] [EOL] log_formatter = logging . Formatter ( [string] ) [EOL] logger = logging . getLogger ( ) [EOL] consoleHandler = logging . StreamHandler ( ) [EOL] consoleHandler . setFormatter ( log_formatter ) [EOL] consoleHandler . setLevel ( [string] ) [EOL] logger . addHandler ( consoleHandler ) [EOL] logger . setLevel ( [string] ) [EOL] [EOL] [EOL] def set_stream_log_level ( level ) : [EOL] for handler in logger . handlers : [EOL] if type ( handler ) is logging . StreamHandler : [EOL] handler . setLevel ( level ) [EOL] [EOL] [EOL] def flatten_dict ( d , parent_key = [string] , sep = [string] ) : [EOL] [docstring] [EOL] [EOL] items = [ ] [EOL] for k , v in d . items ( ) : [EOL] new_key = parent_key + sep + k if parent_key else k [EOL] if isinstance ( v , collections . MutableMapping ) : [EOL] items . extend ( flatten_dict ( v , new_key , sep = sep ) . items ( ) ) [EOL] else : [EOL] items . append ( ( new_key , v ) ) [EOL] return dict ( items ) [EOL] [EOL] [EOL] class ColumnOverwriteException ( Exception ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] def stringify_list ( cols ) : [EOL] [docstring] [EOL] return [ str ( c ) for c in cols ] [EOL] [EOL] [EOL] def merge_dicts ( d1 , d2 ) : [EOL] [docstring] [EOL] return dict ( itertools . chain ( d1 . items ( ) , d2 . items ( ) ) ) [EOL] [EOL] [EOL] def get_context ( ) : [EOL] [docstring] [EOL] context_list = [ ] [EOL] for gpu_number in range ( [number] ) : [EOL] try : [EOL] _ = mx . nd . array ( [ [number] , [number] , [number] ] , ctx = mx . gpu ( gpu_number ) ) [EOL] context_list . append ( mx . gpu ( gpu_number ) ) [EOL] except mx . MXNetError : [EOL] pass [EOL] [EOL] if len ( context_list ) == [number] : [EOL] context_list . append ( mx . cpu ( ) ) [EOL] [EOL] return context_list [EOL] [EOL] [EOL] def random_split ( data_frame , split_ratios = None , seed = [number] ) : [EOL] [docstring] [EOL] if split_ratios is None : [EOL] split_ratios = [ [number] , [number] ] [EOL] sections = np . array ( [ int ( r * len ( data_frame ) ) for r in split_ratios ] ) . cumsum ( ) [EOL] return np . split ( data_frame . sample ( frac = [number] , random_state = seed ) , sections ) [ : len ( split_ratios ) ] [EOL] [EOL] [EOL] def rand_string ( length = [number] ) : [EOL] [docstring] [EOL] import random , string [EOL] return [string] . join ( [ random . choice ( string . ascii_letters + string . digits ) for _ in range ( length ) ] ) [EOL] [EOL] [EOL] @ contextlib . contextmanager def timing ( marker ) : [EOL] start_time = time . time ( ) [EOL] sys . stdout . flush ( ) [EOL] logger . info ( [string] % marker ) [EOL] try : [EOL] yield [EOL] finally : [EOL] logger . info ( [string] % ( ( time . time ( ) - start_time ) , marker ) ) [EOL] sys . stdout . flush ( ) [EOL] [EOL] [EOL] class MeanSymbol ( mx . metric . EvalMetric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , name , symbol_index = [number] , output_names = None , label_names = None ) : [EOL] super ( MeanSymbol , self ) . __init__ ( name , output_names = output_names , label_names = label_names ) [EOL] self . symbol_index = symbol_index [EOL] [EOL] def update ( self , _ , preds ) : [EOL] sym = preds [ self . symbol_index ] [EOL] [comment] [EOL] self . sum_metric += mx . ndarray . sum ( sym ) . asscalar ( ) [EOL] self . num_inst += sym . size [EOL] [EOL] [EOL] class AccuracyMetric ( mx . metric . EvalMetric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , name , label_index = [number] ) : [EOL] super ( AccuracyMetric , self ) . __init__ ( [string] . format ( name ) ) [EOL] self . label_index = label_index [EOL] [EOL] def update ( self , labels , preds ) : [EOL] chosen = preds [ [number] + self . label_index ] . asnumpy ( ) . argmax ( axis = [number] ) [EOL] labels_values = labels [ self . label_index ] . asnumpy ( ) . squeeze ( axis = [number] ) [EOL] self . sum_metric += sum ( ( chosen == labels_values ) | ( labels_values == [number] ) ) [EOL] self . num_inst += preds [ [number] ] . size [EOL] [EOL] [EOL] class LogMetricCallBack ( object ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , tracked_metrics , patience = None ) : [EOL] [docstring] [EOL] self . tracked_metrics = tracked_metrics [EOL] self . metrics = { metric : [ ] for metric in tracked_metrics } [EOL] self . patience = patience [EOL] [EOL] def __call__ ( self , param ) : [EOL] if param . eval_metric is not None : [EOL] name_value = param . eval_metric . get_name_value ( ) [EOL] for metric in self . tracked_metrics : [EOL] for name , value in name_value : [EOL] if metric in name and not math . isnan ( value ) : [EOL] self . metrics [ metric ] . append ( value ) [EOL] if self . patience is not None : [EOL] self . check_regression ( ) [EOL] [EOL] def check_regression ( self ) : [EOL] [docstring] [EOL] _ , errors = next ( iter ( self . metrics . items ( ) ) ) [EOL] [EOL] def convert_nans ( e ) : [EOL] if math . isnan ( e ) : [EOL] logger . warning ( [string] ) [EOL] return [number] [EOL] else : [EOL] return e [EOL] [EOL] errors = [ convert_nans ( e ) for e in errors ] [EOL] [EOL] if self . patience < len ( errors ) : [EOL] [comment] [EOL] metric_before_patience = errors [ ( - [number] * self . patience ) - [number] ] [EOL] [EOL] no_improvement = all ( [ errors [ - i ] >= metric_before_patience for i in range ( [number] , self . patience ) ] ) [EOL] if no_improvement : [EOL] logger . info ( [string] [string] . format ( self . patience , metric_before_patience , errors [ - [number] ] ) ) [EOL] raise StopIteration [EOL] [EOL] [EOL] def normalize_dataframe ( data_frame ) : [EOL] [docstring] [EOL] return data_frame . apply ( lambda x : x . astype ( str ) . str . lower ( ) . str . replace ( [string] , [string] ) . str . strip ( ) . str . replace ( [string] , [string] ) ) [EOL] [EOL] [EOL] def softmax ( x ) : [EOL] [docstring] [EOL] e_x = np . exp ( x - np . max ( x ) ) [EOL] return e_x / e_x . sum ( ) [EOL] [EOL] [EOL] def generate_df_string ( word_length = [number] , vocab_size = [number] , num_labels = [number] , num_words = [number] , num_samples = [number] , label_column_name = [string] , data_column_name = [string] ) : [EOL] [docstring] [EOL] vocab = [ rand_string ( word_length ) for _ in range ( vocab_size ) ] [EOL] labels , words = vocab [ : num_labels ] , vocab [ num_labels : ] [EOL] [EOL] def sentence_with_label ( labels = labels , words = words ) : [EOL] label = random . choice ( labels ) [EOL] return [string] . join ( np . random . permutation ( [ random . choice ( words ) for _ in range ( num_words ) ] + [ label ] ) ) , label [EOL] [EOL] sentences , labels = zip ( * [ sentence_with_label ( labels , words ) for _ in range ( num_samples ) ] ) [EOL] [EOL] return pd . DataFrame ( { data_column_name : sentences , label_column_name : labels } ) [EOL] [EOL] [EOL] def generate_df_numeric ( num_samples = [number] , label_column_name = [string] , data_column_name = [string] ) : [EOL] [docstring] [EOL] numeric_data = np . random . uniform ( - np . pi , np . pi , ( num_samples , ) ) [EOL] return pd . DataFrame ( { data_column_name : numeric_data , label_column_name : numeric_data ** [number] + np . random . normal ( [number] , [number] , ( num_samples , ) ) , } ) [EOL] [EOL] [EOL] def random_cartesian_product ( sets , num = [number] ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] N = np . prod ( [ len ( y ) for y in sets ] ) [EOL] [EOL] [comment] [EOL] if N > [number] : [comment] [EOL] idxs = [ ] [EOL] while len ( idxs ) < min ( num , N ) : [EOL] idx_candidate = np . random . randint ( N ) [EOL] if idx_candidate not in idxs : [EOL] idxs . append ( idx_candidate ) [EOL] else : [EOL] idxs = np . random . choice ( range ( N ) , size = min ( num , N ) , replace = False ) [EOL] [EOL] out = [ ] [EOL] for idx in idxs : [EOL] out . append ( sample_cartesian ( sets , idx , N ) ) [EOL] [EOL] return out [EOL] [EOL] [EOL] def sample_cartesian ( sets , idx , n = None ) : [EOL] [docstring] [EOL] [EOL] if n is None : [EOL] n = np . prod ( [ len ( y ) for y in sets ] ) [EOL] [EOL] out = [ ] [comment] [EOL] width = n [comment] [EOL] for item_set in sets : [EOL] width = width / len ( item_set ) [comment] [EOL] bucket = int ( np . floor ( idx / width ) ) [comment] [EOL] out . append ( item_set [ bucket ] ) [EOL] idx = idx - bucket * width [comment] [EOL] [EOL] assert width == [number] [comment] [EOL] [EOL] return out [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import mxnet [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] [EOL] from typing import Any , List [EOL] [EOL] import mxnet as mx [EOL] [EOL] from . utils import get_context [EOL] [EOL] [EOL] class Featurizer ( object ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , field_name , latent_dim ) : [EOL] self . field_name = field_name [EOL] self . latent_dim = int ( latent_dim ) [EOL] self . input_symbol = mx . sym . Variable ( [string] . format ( field_name ) ) [EOL] self . prefix = self . field_name + [string] [EOL] self . symbol = None [EOL] [EOL] def latent_symbol ( self ) : [EOL] [docstring] [EOL] return self . symbol [EOL] [EOL] [EOL] class NumericalFeaturizer ( Featurizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , field_name , numeric_latent_dim = [number] , numeric_hidden_layers = [number] ) : [EOL] super ( NumericalFeaturizer , self ) . __init__ ( field_name , numeric_latent_dim ) [EOL] [EOL] self . numeric_hidden_layers = int ( numeric_hidden_layers ) [EOL] self . numeric_latent_dim = int ( numeric_latent_dim ) [EOL] [EOL] with mx . name . Prefix ( self . prefix ) : [EOL] self . symbol = self . input_symbol [EOL] for _ in range ( self . numeric_hidden_layers ) : [EOL] symbol = mx . sym . FullyConnected ( data = self . symbol , num_hidden = self . numeric_latent_dim ) [EOL] self . symbol = mx . symbol . Activation ( data = symbol , act_type = [string] ) [EOL] [EOL] [EOL] class LSTMFeaturizer ( Featurizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , field_name , seq_len = [number] , max_tokens = [number] , embed_dim = [number] , num_hidden = [number] , num_layers = [number] , latent_dim = [number] , use_gpu = False if mx . cpu ( ) in get_context ( ) else True ) : [EOL] super ( LSTMFeaturizer , self ) . __init__ ( field_name , latent_dim ) [EOL] [EOL] self . vocab_size = int ( max_tokens ) [EOL] self . embed_dim = int ( embed_dim ) [EOL] self . seq_len = int ( seq_len ) [EOL] self . num_hidden = int ( num_hidden ) [EOL] self . num_layers = int ( num_layers ) [EOL] [EOL] with mx . name . Prefix ( field_name + [string] ) : [EOL] embed_symbol = mx . sym . Embedding ( data = self . input_symbol , input_dim = self . vocab_size , output_dim = self . embed_dim ) [EOL] [EOL] def make_cell ( layer_index ) : [EOL] prefix = [string] . format ( layer_index , self . field_name ) [EOL] cell_type = mx . rnn . FusedRNNCell if use_gpu else mx . rnn . LSTMCell [EOL] cell = cell_type ( num_hidden = self . num_hidden , prefix = prefix ) [EOL] [comment] [EOL] return cell if layer_index == [number] else mx . rnn . ResidualCell ( cell ) [EOL] [EOL] stack = mx . rnn . SequentialRNNCell ( ) [EOL] for i in range ( self . num_layers ) : [EOL] stack . add ( make_cell ( layer_index = i ) ) [EOL] output , _ = stack . unroll ( self . seq_len , inputs = embed_symbol , merge_outputs = True ) [EOL] [EOL] self . symbol = mx . sym . FullyConnected ( data = output , num_hidden = self . latent_dim ) [EOL] [EOL] [EOL] class EmbeddingFeaturizer ( Featurizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , field_name , max_tokens = [number] , embed_dim = [number] ) : [EOL] super ( EmbeddingFeaturizer , self ) . __init__ ( field_name , embed_dim ) [EOL] [EOL] self . vocab_size = int ( max_tokens ) [EOL] self . embed_dim = int ( embed_dim ) [EOL] [EOL] with mx . name . Prefix ( field_name + [string] ) : [EOL] symbol = mx . sym . Embedding ( data = self . input_symbol , input_dim = self . vocab_size , output_dim = self . embed_dim ) [EOL] self . symbol = mx . sym . FullyConnected ( data = symbol , num_hidden = self . latent_dim ) [EOL] [EOL] [EOL] class BowFeaturizer ( Featurizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , field_name , max_tokens = [number] ** [number] ) : [EOL] super ( BowFeaturizer , self ) . __init__ ( field_name , max_tokens ) [EOL] [EOL] with mx . name . Prefix ( field_name + [string] ) : [EOL] self . symbol = mx . sym . Variable ( [string] . format ( field_name ) , stype = [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $mxnet.symbol$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0
from typing import Union [EOL] import flask [EOL] import typing [EOL] import os [EOL] [EOL] from blueprints import datawig [EOL] from flask import Flask [EOL] [EOL] app = Flask ( __name__ ) [EOL] [EOL] app . register_blueprint ( datawig . datawig ) [EOL] [comment] [EOL] app . secret_key = os . urandom ( [number] ) [EOL] [EOL] if __name__ == [string] : [EOL] app . run ( host = [string] , port = [number] , debug = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import flask [EOL] from flask import render_template , Blueprint [EOL] [EOL] datawig = Blueprint ( [string] , __name__ , url_prefix = [string] ) [EOL] [EOL] [EOL] @ datawig . route ( [string] , methods = [ [string] ] ) def index ( ) : [EOL] return render_template ( [string] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 $flask.blueprints.Blueprint$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $flask.blueprints.Blueprint$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0