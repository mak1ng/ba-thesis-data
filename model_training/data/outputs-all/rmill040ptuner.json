from typing import Any , List , Dict [EOL] import typing [EOL] import ptuner [EOL] from keras . layers import Activation , BatchNormalization , Dense , Dropout [EOL] from keras . models import Sequential [EOL] from keras . optimizers import Adam , SGD [EOL] from keras . regularizers import l1 , l2 [EOL] [EOL] import os [EOL] import numpy as np [EOL] from sklearn . metrics import roc_auc_score [EOL] from sklearn . model_selection import train_test_split [EOL] from sklearn . preprocessing import StandardScaler [EOL] [EOL] import tensorflow as tf [EOL] os . environ [ [string] ] = [string] [EOL] [EOL] [comment] [EOL] from ptuner import ParallelPipelineTuner , STATUS_FAIL , STATUS_OK [EOL] from ptuner . spaces import MLPClassifierSampler , NaiveFeatureSampler , SpaceSampler [EOL] [EOL] DATA_DIR = os . path . dirname ( os . path . abspath ( __file__ ) ) [EOL] SEED = [number] [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] df = np . loadtxt ( os . path . join ( DATA_DIR , [string] ) , delimiter = [string] ) [EOL] X , y = df [ : , : - [number] ] , df [ : , - [number] ] [EOL] [EOL] X_train , X_test , y_train , y_test = train_test_split ( X , y , stratify = y , test_size = [number] , random_state = SEED ) [EOL] [EOL] [comment] [EOL] scaler = StandardScaler ( ) . fit ( X_train ) [EOL] X_train , X_test = scaler . transform ( X_train ) , scaler . transform ( X_test ) [EOL] [EOL] [comment] [EOL] n_class0 = ( y_train == [number] ) . sum ( ) [EOL] n_class1 = ( y_test == [number] ) . sum ( ) [EOL] class_weight = { [number] : [number] , [number] : n_class0 / n_class1 } [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] def objective ( params , current_round ) : [EOL] [docstring] [EOL] [EOL] epochs = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] from keras . layers import Activation , BatchNormalization , Dense , Dropout [EOL] from keras . models import Sequential [EOL] from keras . optimizers import Adam , SGD [EOL] from keras . regularizers import l1 , l2 [EOL] import tensorflow as tf [EOL] os . environ [ [string] ] = [string] [EOL] [EOL] try : [EOL] [comment] [EOL] features = params [ [string] ] [EOL] hps = params [ [string] ] [EOL] [EOL] [comment] [EOL] model = Sequential ( ) [EOL] for i in range ( [number] , hps [ [string] ] + [number] ) : [EOL] [comment] [EOL] model . add ( Dense ( units = hps [ [string] % i ] , kernel_regularizer = l2 ( hps [ [string] ] ) , activity_regularizer = l1 ( hps [ [string] ] ) , input_dim = p if i == [number] else None ) ) [EOL] [comment] [EOL] model . add ( Activation ( [string] ) ) [EOL] [EOL] [comment] [EOL] if hps [ [string] ] == [string] : [EOL] model . add ( BatchNormalization ( ) ) [EOL] [EOL] [comment] [EOL] model . add ( Dropout ( hps [ [string] ] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Dense ( units = [number] , activation = [string] ) ) [EOL] [EOL] [comment] [EOL] if hps [ [string] ] == [string] : [EOL] optimizer = Adam ( lr = hps [ [string] ] ) [EOL] else : [EOL] optimizer = SGD ( lr = hps [ [string] ] , nesterov = True ) [EOL] [EOL] [comment] [EOL] model . compile ( loss = [string] , optimizer = optimizer , metrics = [ [string] ] ) [EOL] model . fit ( X_train , y_train , class_weight = class_weight , epochs = epochs [ current_round ] , batch_size = hps [ [string] ] , verbose = [number] ) [EOL] [EOL] [comment] [EOL] y_pred = model . predict_proba ( X_test ) [EOL] try : [EOL] metric = roc_auc_score ( y_test , y_pred ) [EOL] except : [EOL] metric = [number] [EOL] [EOL] [comment] [EOL] return { [string] : STATUS_OK , [string] : None , [string] : metric } [EOL] except Exception as e : [EOL] return { [string] : STATUS_FAIL , [string] : e , [string] : [number] } [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] tuner = ParallelPipelineTuner ( db_host = [string] , db_port = [number] , lower_is_better = False , experiment_name = [string] , role = [string] , n_jobs = [number] , backend = [string] ) [EOL] [EOL] tuner . search ( objective = objective , max_configs_per_round = [ [number] , [number] , [number] , [number] , [number] ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.Any]$ 0 $typing.Dict[builtins.int,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.ParallelPipelineTuner$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.ParallelPipelineTuner$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Union , Dict [EOL] import typing [EOL] import ptuner [EOL] import numpy as np [EOL] from sklearn . model_selection import train_test_split [EOL] from xgboost import XGBClassifier [EOL] [EOL] [comment] [EOL] from ptuner import ParallelPipelineTuner , STATUS_FAIL , STATUS_OK [EOL] from ptuner . spaces import NaiveFeatureSampler , SpaceSampler , XGBClassifierSampler [EOL] [EOL] SEED = [number] [EOL] [EOL] def make_data ( N , p ) : [EOL] [docstring] [EOL] X = np . random . normal ( [number] , [number] , ( N , p ) ) [EOL] y = [number] * X [ : , [number] ] + [number] * X [ : , [number] ] + [number] * X [ : , [number] ] [EOL] y = [number] / ( [number] + np . exp ( - y ) ) [EOL] y = np . random . binomial ( [number] , y , N ) [EOL] [EOL] return X , y [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] np . random . seed ( SEED ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] N , p = [number] , [number] [EOL] X , y = make_data ( N , p ) [EOL] [EOL] X_train , X_test , y_train , y_test = train_test_split ( X , y , stratify = y , test_size = [number] , random_state = SEED ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def objective ( params , current_round ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] fit_params = { [string] : None , [string] : [string] , [string] : None , [string] : False } [EOL] [EOL] early_stopping_rounds = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] try : [EOL] [comment] [EOL] features = params [ [string] ] [EOL] hps = params [ [string] ] [EOL] [EOL] [comment] [EOL] fit_params [ [string] ] = [ ( X_test [ : , features ] , y_test ) ] [EOL] fit_params [ [string] ] = early_stopping_rounds [ current_round ] [EOL] [EOL] [comment] [EOL] clf = XGBClassifier ( ** hps ) . fit ( X_train [ : , features ] , y_train , ** fit_params ) [EOL] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] = clf . best_iteration + [number] [EOL] [EOL] [comment] [EOL] return { [string] : STATUS_OK , [string] : None , [string] : clf . best_score } [EOL] except Exception as e : [EOL] return { [string] : STATUS_FAIL , [string] : e , [string] : [number] } [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] tuner = ParallelPipelineTuner ( db_host = [string] , db_port = [number] , lower_is_better = False , experiment_name = [string] , role = [string] , n_jobs = - [number] , backend = [string] ) [EOL] [EOL] tuner . search ( objective = objective , max_configs_per_round = [ [number] , [number] , [number] , [number] , [number] ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.ParallelPipelineTuner$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.ParallelPipelineTuner$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Union , Dict [EOL] import typing [EOL] import ptuner [EOL] import numpy as np [EOL] import os [EOL] from sklearn . model_selection import train_test_split [EOL] from xgboost import XGBClassifier [EOL] [EOL] [comment] [EOL] from ptuner import LocalPipelineTuner , STATUS_FAIL , STATUS_OK [EOL] from ptuner . spaces import NaiveFeatureSampler , SpaceSampler , XGBClassifierSampler [EOL] [EOL] DATA_DIR = os . path . dirname ( os . path . abspath ( __file__ ) ) [EOL] SEED = [number] [EOL] [EOL] def make_data ( N , p ) : [EOL] [docstring] [EOL] X = np . random . normal ( [number] , [number] , ( N , p ) ) [EOL] y = [number] + [number] * X [ : , [number] ] + [number] * X [ : , [number] ] + [number] * X [ : , [number] ] [EOL] y = [number] / ( [number] + np . exp ( - y ) ) [EOL] y = np . random . binomial ( [number] , y , N ) [EOL] np . savetxt ( os . path . join ( DATA_DIR , [string] ) , np . column_stack ( [ X , y ] ) , delimiter = [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] np . random . seed ( SEED ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] N , p = [number] , [number] [EOL] make_data ( N , p ) [EOL] [EOL] [comment] [EOL] df = np . loadtxt ( os . path . join ( DATA_DIR , [string] ) , delimiter = [string] ) [EOL] X , y = df [ : , : - [number] ] , df [ : , - [number] ] [EOL] [EOL] X_train , X_test , y_train , y_test = train_test_split ( X , y , stratify = y , test_size = [number] , random_state = SEED ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] sampler = SpaceSampler ( ) [EOL] sampler . add_feature_sampler ( name = [string] , sampler = NaiveFeatureSampler ( p = p ) ) [EOL] sampler . add_hyperparameter_sampler ( name = [string] , sampler = XGBClassifierSampler ( early_stopping = True ) ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def objective ( params , current_round ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] fit_params = { [string] : None , [string] : [string] , [string] : None , [string] : False } [EOL] [EOL] early_stopping_rounds = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] try : [EOL] [comment] [EOL] features = params [ [string] ] [EOL] hps = params [ [string] ] [EOL] [EOL] [comment] [EOL] fit_params [ [string] ] = [ ( X_test [ : , features ] , y_test ) ] [EOL] fit_params [ [string] ] = early_stopping_rounds [ current_round ] [EOL] [EOL] [comment] [EOL] clf = XGBClassifier ( ** hps ) . fit ( X_train [ : , features ] , y_train , ** fit_params ) [EOL] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] = clf . best_iteration + [number] [EOL] [EOL] [comment] [EOL] return { [string] : STATUS_OK , [string] : None , [string] : clf . best_score } [EOL] except Exception as e : [EOL] return { [string] : STATUS_FAIL , [string] : e , [string] : [number] } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] tuner = LocalPipelineTuner ( lower_is_better = False , experiment_name = [string] , n_jobs = - [number] , backend = [string] , save_name = None ) [EOL] [EOL] tuner . search ( objective = objective , sampler = sampler , max_configs_per_round = [ [number] , [number] , [number] , [number] , [number] ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.LocalPipelineTuner$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.pipeline.LocalPipelineTuner$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import builtins [EOL] import typing [EOL] import ptuner [EOL] import os [EOL] import numpy as np [EOL] from sklearn . metrics import roc_auc_score [EOL] from sklearn . model_selection import train_test_split [EOL] from sklearn . preprocessing import StandardScaler [EOL] [EOL] [comment] [EOL] from ptuner import PipelineTuner , STATUS_FAIL , STATUS_OK [EOL] from ptuner . spaces import MLPClassifierSampler , NaiveFeatureSampler , SpaceSampler [EOL] [EOL] DATA_DIR = os . path . dirname ( os . path . abspath ( __file__ ) ) [EOL] SEED = [number] [EOL] [EOL] [EOL] def make_data ( N , p ) : [EOL] [docstring] [EOL] X = np . random . normal ( [number] , [number] , ( N , p ) ) [EOL] y = - [number] + [number] * X [ : , [number] ] + [number] * X [ : , [number] ] + [number] * X [ : , [number] ] [EOL] y = [number] / ( [number] + np . exp ( - y ) ) [EOL] y = np . random . binomial ( [number] , y , N ) [EOL] np . savetxt ( os . path . join ( DATA_DIR , [string] ) , np . column_stack ( [ X , y ] ) , delimiter = [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] N , p = [number] , [number] [EOL] make_data ( N , p ) [EOL] [EOL] [comment] [EOL] df = np . loadtxt ( os . path . join ( DATA_DIR , [string] ) , delimiter = [string] ) [EOL] X , y = df [ : , : - [number] ] , df [ : , - [number] ] [EOL] [EOL] X_train , X_test , y_train , y_test = train_test_split ( X , y , stratify = y , test_size = [number] , random_state = SEED ) [EOL] [EOL] [comment] [EOL] n_class0 = ( y_train == [number] ) . sum ( ) [EOL] n_class1 = ( y_test == [number] ) . sum ( ) [EOL] class_weight = { [number] : [number] , [number] : n_class0 / n_class1 } [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] sampler = SpaceSampler ( ) [EOL] sampler . add_feature_sampler ( name = [string] , sampler = NaiveFeatureSampler ( p = p ) ) [EOL] sampler . add_hyperparameter_sampler ( name = [string] , sampler = MLPClassifierSampler ( early_stopping = False , n_hidden_layers = [number] , max_neurons = [number] , max_epochs = [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] def objective ( params , current_round ) : [EOL] [docstring] [EOL] [EOL] epochs = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] from keras . layers import Activation , BatchNormalization , Dense , Dropout [EOL] from keras . models import Sequential [EOL] from keras . optimizers import Adam , SGD [EOL] from keras . regularizers import l1 , l2 [EOL] import tensorflow as tf [EOL] os . environ [ [string] ] = [string] [EOL] [EOL] try : [EOL] [comment] [EOL] features = params [ [string] ] [EOL] hps = params [ [string] ] [EOL] [EOL] [comment] [EOL] model = Sequential ( ) [EOL] for i in range ( [number] , hps [ [string] ] + [number] ) : [EOL] [comment] [EOL] model . add ( Dense ( hps [ [string] % i ] , kernel_regularizer = l2 ( hps [ [string] ] ) , activity_regularizer = l1 ( hps [ [string] ] ) , input_dim = p if i == [number] else None ) ) [EOL] [comment] [EOL] model . add ( Activation ( [string] ) ) [EOL] [EOL] [comment] [EOL] if hps [ [string] ] == [string] : [EOL] model . add ( BatchNormalization ( ) ) [EOL] [EOL] [comment] [EOL] model . add ( Dropout ( hps [ [string] ] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Dense ( [number] , activation = [string] ) ) [EOL] [EOL] [comment] [EOL] if hps [ [string] ] == [string] : [EOL] optimizer = Adam ( lr = hps [ [string] ] ) [EOL] else : [EOL] optimizer = SGD ( lr = hps [ [string] ] , nesterov = True ) [EOL] [EOL] [comment] [EOL] model . compile ( loss = [string] , optimizer = optimizer , metrics = [ [string] ] ) [EOL] model . fit ( X_train , y_train , class_weight = class_weight , epochs = epochs [ current_round ] , batch_size = hps [ [string] ] , verbose = [number] ) [EOL] [EOL] [comment] [EOL] y_pred = model . predict_proba ( X_test ) [EOL] try : [EOL] metric = roc_auc_score ( y_test , y_pred ) [EOL] except : [EOL] metric = [number] [EOL] [EOL] [comment] [EOL] return { [string] : STATUS_OK , [string] : None , [string] : metric } [EOL] except Exception as e : [EOL] return { [string] : STATUS_FAIL , [string] : e , [string] : [number] } [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] tuner = PipelineTuner ( db_host = [string] , db_port = [number] , lower_is_better = False , experiment_name = [string] , role = [string] , n_jobs = - [number] , backend = [string] ) [EOL] [EOL] tuner . search ( objective = objective , sampler = sampler , max_configs_per_round = [ [number] , [number] , [number] , [number] , [number] ] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.Any]$ 0 $typing.Dict[builtins.int,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Union , Tuple , Any , Optional , Literal , Dict [EOL] import typing [EOL] import spaces [EOL] import typing_extensions [EOL] import ptuner [EOL] import builtins [EOL] import logging [EOL] import pandas [EOL] from joblib import delayed , Parallel [EOL] import json [EOL] import logging [EOL] from multiprocessing import cpu_count , Manager [EOL] import numpy as np [EOL] import pandas as pd [EOL] import time [EOL] from tqdm import trange [EOL] from typing import Any , cast , Dict , List , Optional , Tuple [EOL] [EOL] [comment] [EOL] STATUS_FAIL = [string] [EOL] STATUS_OK = [string] [EOL] [EOL] [comment] [EOL] from . base . _pipeline import BasePipelineTuner [EOL] from . db import init_collection , is_running , MongoError , MongoWorker [EOL] from . spaces import SpaceSampler [EOL] from . utils . helper import countdown , get_hostname [EOL] [EOL] __all__ = [ [string] , [string] , [string] , [string] ] [EOL] [EOL] _LOGGER = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class LocalPipelineTuner ( BasePipelineTuner ) : [EOL] [docstring] [EOL] def __init__ ( self , lower_is_better , n_jobs = - [number] , backend = [string] , experiment_name = [string] , save_name = None , verbose = True ) : [EOL] [EOL] super ( ) . __init__ ( lower_is_better = lower_is_better , experiment_name = experiment_name , n_jobs = n_jobs , backend = backend , save_name = save_name , verbose = verbose ) [EOL] [EOL] [comment] [EOL] if not self . verbose : logging . getLogger ( ) . setLevel ( logging . WARN ) [EOL] [EOL] self . history = [ ] [EOL] self . best_results = Manager ( ) . dict ( ) [EOL] self . best_results [ [string] ] = np . inf if self . lower_is_better else - np . inf [EOL] self . best_results [ [string] ] = None [EOL] [EOL] [EOL] def _export_all_results ( self ) : [EOL] [docstring] [EOL] if self . save_name : [EOL] try : [EOL] pd . concat ( [ pd . DataFrame ( h ) for h in self . history ] , axis = [number] ) . sort_values ( [string] , ascending = self . lower_is_better ) . to_csv ( self . save_name , index = False ) [EOL] _LOGGER . info ( [string] % self . save_name ) [EOL] except Exception as e : [EOL] _LOGGER . error ( [string] % e ) [EOL] [EOL] [EOL] def _update_space ( self , sampler , hof , n_round ) : [EOL] [docstring] [EOL] [comment] [EOL] df_all = pd . concat ( [ pd . DataFrame ( h ) for h in self . history ] , axis = [number] ) [EOL] df_all = df_all [ df_all [ [string] ] == n_round ] . sort_values ( by = [string] , ascending = self . lower_is_better ) [ : hof ] [EOL] [EOL] [comment] [EOL] if sampler . feature_sampler : [EOL] df_features = pd . DataFrame . from_records ( df_all [ [string] ] . apply ( lambda x : x [ sampler . feature_sampler . name ] ) . values . tolist ( ) ) [EOL] sampler . feature_sampler . sampler . update_space ( df_features ) [EOL] [EOL] [comment] [EOL] if sampler . hyperparameter_samplers : [EOL] for s in sampler . hyperparameter_samplers : [EOL] df_hyperparameters = pd . DataFrame . from_records ( df_all [ [string] ] . apply ( lambda x : x [ s . name ] ) . values . tolist ( ) ) [EOL] s . sampler . update_space ( df_hyperparameters ) [EOL] [EOL] return sampler [EOL] [EOL] [EOL] def _evaluate_candidate ( self , objective , candidate , i , n_candidates ) : [EOL] [docstring] [EOL] if i % self . n_jobs == [number] : [EOL] _LOGGER . info ( [string] % ( n_candidates - i ) ) [EOL] [EOL] results = objective ( params = candidate [ [string] ] , current_round = candidate [ [string] ] - [number] ) [EOL] [EOL] [comment] [EOL] if STATUS_FAIL in results [ [string] ] . upper ( ) or STATUS_OK not in results [ [string] ] . upper ( ) : [EOL] msg = [string] [EOL] if [string] in results . keys ( ) : [EOL] if results [ [string] ] : msg += [string] % results [ [string] ] [EOL] _LOGGER . warn ( msg ) [EOL] return { [string] : results [ [string] ] , [string] : np . inf if self . lower_is_better else - np . inf , [string] : candidate [ [string] ] , [string] : candidate [ [string] ] , [string] : candidate [ [string] ] } [EOL] [EOL] [comment] [EOL] if self . lower_is_better : [EOL] if results [ [string] ] < self . best_results [ [string] ] : [EOL] _LOGGER . info ( [string] % results [ [string] ] ) [EOL] self . best_results [ [string] ] = results [ [string] ] [EOL] self . best_results [ [string] ] = candidate [ [string] ] [EOL] else : [EOL] if results [ [string] ] > self . best_results [ [string] ] : [EOL] _LOGGER . info ( [string] % results [ [string] ] ) [EOL] self . best_results [ [string] ] = results [ [string] ] [EOL] self . best_results [ [string] ] = candidate [ [string] ] [EOL] [EOL] return { [string] : results [ [string] ] , [string] : results [ [string] ] , [string] : candidate [ [string] ] , [string] : candidate [ [string] ] , [string] : candidate [ [string] ] } [EOL] [EOL] [EOL] def search ( self , objective , sampler , max_configs_per_round , subsample_factor = [number] ) : [EOL] [docstring] [EOL] _LOGGER . info ( [string] % ( self . n_jobs , self . backend ) ) [EOL] start = time . time ( ) [EOL] [EOL] [comment] [EOL] n_rounds = len ( max_configs_per_round ) [EOL] for n , n_configs in enumerate ( max_configs_per_round ) : [EOL] [EOL] [comment] [EOL] hof = int ( np . ceil ( n_configs / subsample_factor ) ) [EOL] [EOL] [comment] [EOL] n += [number] [EOL] _LOGGER . info ( [string] % ( n , n_rounds ) ) [EOL] [EOL] [comment] [EOL] candidates = [ ] [EOL] with trange ( n_configs ) as t : [EOL] for i in t : [EOL] t . set_description ( [string] % ( i + [number] , n_configs ) ) [EOL] candidates . append ( { [string] : sampler . sample_space ( ) , [string] : n , [string] : i + [number] } ) [EOL] n_candidates = len ( candidates ) [EOL] _LOGGER . info ( [string] % len ( candidates ) ) [EOL] [EOL] [comment] [EOL] if n_candidates : [EOL] _LOGGER . info ( [string] % n_candidates ) [EOL] output = Parallel ( n_jobs = self . n_jobs , verbose = False , backend = self . backend ) ( delayed ( self . _evaluate_candidate ) ( objective , candidate , i + [number] , n_candidates ) for i , candidate in enumerate ( candidates ) ) [EOL] self . history . append ( output ) [EOL] [EOL] [comment] [EOL] if n < n_rounds : [EOL] _LOGGER . info ( [string] ) [EOL] sampler = self . _update_space ( sampler , hof , n_round = n ) [EOL] [EOL] [comment] [EOL] self . _export_all_results ( ) [EOL] [EOL] total_time = ( time . time ( ) - start ) / [number] [EOL] _LOGGER . info ( [string] % total_time ) [EOL] [EOL] [EOL] class ParallelPipelineTuner ( BasePipelineTuner ) : [EOL] [docstring] [EOL] def __init__ ( self , db_host , db_port , lower_is_better , role = [string] , n_jobs = - [number] , backend = [string] , experiment_name = [string] , save_name = None , verbose = True ) : [EOL] [EOL] super ( ) . __init__ ( lower_is_better = lower_is_better , experiment_name = experiment_name , n_jobs = n_jobs , backend = backend , save_name = save_name , verbose = verbose ) [EOL] [EOL] [comment] [EOL] if not self . verbose : logging . getLogger ( ) . setLevel ( logging . WARN ) [EOL] [EOL] [comment] [EOL] self . best_results = { } [EOL] [EOL] [comment] [EOL] self . db_host = db_host [EOL] self . db_port = int ( db_port ) [EOL] [EOL] [comment] [EOL] self . _mongo_params = { [string] : self . db_host , [string] : self . db_port } [EOL] [EOL] [comment] [EOL] self . role = role [EOL] self . candidates_name = self . experiment_name + [string] + [string] [EOL] [EOL] [comment] [EOL] self . computer_name = get_hostname ( ) [EOL] _LOGGER . info ( [string] % ( role , self . computer_name ) ) [EOL] [EOL] [comment] [EOL] status , info = is_running ( ** self . _mongo_params ) [EOL] msg = [string] [EOL] if not status : [EOL] msg = [string] . format ( ** self . _mongo_params , reason = info ) [EOL] _LOGGER . error ( msg ) [EOL] raise MongoError ( msg ) [EOL] _LOGGER . info ( info ) [EOL] [EOL] overwrite = True if self . role . lower ( ) . strip ( ) == [string] else False [EOL] [EOL] [comment] [EOL] status , info = init_collection ( collection = self . experiment_name , overwrite = overwrite , computer_name = self . computer_name , ** self . _mongo_params ) [EOL] if not status : [EOL] msg = [string] . format ( col = experiment_name , reason = info ) [EOL] _LOGGER . error ( msg ) [EOL] raise MongoError ( msg ) [EOL] _LOGGER . info ( info ) [EOL] [EOL] [comment] [EOL] status , info = init_collection ( ** self . _mongo_params , collection = self . candidates_name , overwrite = overwrite , computer_name = self . computer_name ) [EOL] if not status : [EOL] msg = [string] . format ( col = experiment_name , reason = info ) [EOL] _LOGGER . error ( msg ) [EOL] raise MongoError ( msg ) [EOL] _LOGGER . info ( info ) [EOL] [EOL] [EOL] def _export_all_results ( self ) : [EOL] [docstring] [EOL] if self . save_name : [EOL] with MongoWorker ( collection = self . experiment_name , ** self . _mongo_params ) as db : [EOL] pd . DataFrame . from_records ( db . find ( ) ) . drop ( [string] , axis = [number] ) . to_csv ( self . experiment_name + [string] , index = False ) [EOL] _LOGGER . info ( [string] % self . experiment_name ) [EOL] [EOL] [EOL] def _select_best ( self ) : [EOL] [docstring] [EOL] with MongoWorker ( collection = self . experiment_name , ** self . _mongo_params ) as db : [EOL] query = db . find_one ( sort = [ ( [string] , [number] if self . lower_is_better else - [number] ) ] ) [EOL] if [string] not in query . keys ( ) : [EOL] return None , None [EOL] else : [EOL] return query [ [string] ] , query [ [string] ] [EOL] [EOL] [EOL] def _update_space ( self , sampler , hof , n_round ) : [EOL] [docstring] [EOL] with MongoWorker ( collection = self . experiment_name , ** self . _mongo_params ) as db : [EOL] query = list ( db . aggregate ( [ { [string] : { [string] : - [number] } } , { [string] : hof } , { [string] : { [string] : { [string] : n_round } } } ] ) ) [EOL] [EOL] [comment] [EOL] df_all = pd . DataFrame ( query ) [EOL] [EOL] [comment] [EOL] if sampler . feature_sampler : [EOL] df_features = pd . DataFrame . from_records ( df_all [ [string] ] . apply ( lambda x : x [ sampler . feature_sampler . name ] ) . values . tolist ( ) ) [EOL] sampler . feature_sampler . sampler . update_space ( df_features ) [EOL] [EOL] [comment] [EOL] if sampler . hyperparameter_samplers : [EOL] for s in sampler . hyperparameter_samplers : [EOL] df_hyperparameters = pd . DataFrame . from_records ( df_all [ [string] ] . apply ( lambda x : x [ s . name ] ) . values . tolist ( ) ) [EOL] s . sampler . update_space ( df_hyperparameters ) [EOL] [EOL] return sampler [EOL] [EOL] [EOL] def _current_round ( self ) : [EOL] [docstring] [EOL] with MongoWorker ( collection = self . experiment_name , ** self . _mongo_params ) as db : [EOL] try : [EOL] return cast ( int , db . find_one ( sort = [ ( [string] , - [number] ) ] ) [ [string] ] ) [EOL] except : [EOL] return - [number] [EOL] [EOL] [EOL] def _n_candidates_remaining ( self ) : [EOL] [docstring] [EOL] with MongoWorker ( collection = self . candidates_name , ** self . _mongo_params ) as db : [EOL] n_count = db . count ( ) [EOL] if n_count == [number] : [EOL] if [string] in db . find_one ( ) . keys ( ) : n_count -= [number] [EOL] return n_count [EOL] [EOL] [EOL] def _evaluate_candidate ( self , objective ) : [EOL] [docstring] [EOL] with MongoWorker ( collection = None , ** self . _mongo_params ) as db : [EOL] [comment] [EOL] db_candidates = db [ self . candidates_name ] [EOL] query = db_candidates . find_and_modify ( query = { [string] : { [string] : True } } , remove = True ) [EOL] [EOL] [comment] [EOL] if not query : [EOL] _LOGGER . warn ( [string] ) [EOL] return [string] [EOL] [EOL] [comment] [EOL] db_experiment = db [ self . experiment_name ] [EOL] results = objective ( params = query [ [string] ] , current_round = query [ [string] ] - [number] ) [EOL] [EOL] [comment] [EOL] if [string] in results [ [string] ] . upper ( ) or [string] not in results [ [string] ] . upper ( ) : [EOL] msg = [string] [EOL] if [string] in results . keys ( ) : [EOL] if results [ [string] ] : msg += [string] % results [ [string] ] [EOL] _LOGGER . warn ( msg ) [EOL] return [string] [EOL] [EOL] [comment] [EOL] best_metric , best_params = self . _select_best ( ) [EOL] if best_metric : [EOL] if self . lower_is_better : [EOL] if results [ [string] ] < best_metric : [EOL] _LOGGER . info ( [string] % results [ [string] ] ) [EOL] _LOGGER . info ( [string] % best_params ) [EOL] else : [EOL] if results [ [string] ] > best_metric : [EOL] _LOGGER . info ( [string] % results [ [string] ] ) [EOL] _LOGGER . info ( [string] % best_params ) [EOL] else : [EOL] [comment] [EOL] _LOGGER . info ( [string] % results [ [string] ] ) [EOL] [EOL] db_experiment . insert_one ( { [string] : results [ [string] ] , [string] : results [ [string] ] , [string] : query [ [string] ] , [string] : query [ [string] ] , [string] : query [ [string] ] } ) [EOL] return [string] [EOL] [EOL] [EOL] def _master_search ( self , objective , sampler , max_configs_per_round , subsample_factor ) : [EOL] [docstring] [EOL] [comment] [EOL] n_rounds = len ( max_configs_per_round ) [EOL] for n , n_configs in enumerate ( max_configs_per_round ) : [EOL] [EOL] [comment] [EOL] hof = int ( np . ceil ( n_configs / subsample_factor ) ) [EOL] [EOL] [comment] [EOL] n += [number] [EOL] _LOGGER . info ( [string] % ( n , n_rounds ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if n == [number] : [EOL] with MongoWorker ( collection = self . candidates_name , ** self . _mongo_params ) as db : [EOL] db . find_and_modify ( query = { [string] : { [string] : True } } , remove = True ) [EOL] with MongoWorker ( collection = self . experiment_name , ** self . _mongo_params ) as db : [EOL] db . find_and_modify ( query = { [string] : { [string] : True } } , remove = True ) [EOL] [EOL] [comment] [EOL] candidates = [ ] [EOL] with trange ( n_configs ) as t : [EOL] for i in t : [EOL] t . set_description ( [string] % ( i + [number] , n_configs ) ) [EOL] candidates . append ( { [string] : sampler . sample_space ( ) , [string] : n , [string] : i + [number] } ) [EOL] with MongoWorker ( collection = self . candidates_name , ** self . _mongo_params ) as db : [EOL] db . insert_many ( candidates ) [EOL] _LOGGER . info ( [string] % len ( candidates ) ) [EOL] [EOL] [comment] [EOL] n_candidates = self . _n_candidates_remaining ( ) [EOL] if n_candidates : [EOL] while n_candidates : [EOL] _LOGGER . info ( [string] % n_candidates ) [EOL] output = Parallel ( n_jobs = self . n_jobs , verbose = False , backend = self . backend ) ( delayed ( self . _evaluate_candidate ) ( objective ) for _ in range ( self . n_jobs ) ) [EOL] if [string] in output : [EOL] _LOGGER . info ( [string] ) [EOL] break [EOL] n_candidates = self . _n_candidates_remaining ( ) [EOL] [EOL] [comment] [EOL] if n < n_rounds : [EOL] _LOGGER . info ( [string] ) [EOL] sampler = self . _update_space ( sampler , hof , n_round = n ) [EOL] [EOL] [EOL] def _worker_search ( self , objective , n_rounds , max_attempts = [number] , backoff_period = [number] ) : [EOL] [docstring] [EOL] n_candidates = self . _n_candidates_remaining ( ) [EOL] current_round = self . _current_round ( ) [EOL] if current_round == - [number] : [EOL] _LOGGER . error ( [string] ) [EOL] return [EOL] if not n_candidates and current_round == n_rounds : [EOL] _LOGGER . info ( [string] ) [EOL] return [EOL] [EOL] finished = False [EOL] while not finished : [EOL] if n_candidates : [EOL] while n_candidates : [EOL] _LOGGER . info ( [string] % n_candidates ) [EOL] output = Parallel ( n_jobs = self . n_jobs , verbose = False , backend = self . backend ) ( delayed ( self . _evaluate_candidate ) ( objective ) for _ in range ( self . n_jobs ) ) [EOL] if [string] in output : [EOL] _LOGGER . info ( [string] ) [EOL] break [EOL] n_candidates = self . _n_candidates_remaining ( ) [EOL] else : [EOL] current_round = self . _current_round ( ) [EOL] [EOL] [comment] [EOL] if current_round < n_rounds : [EOL] _LOGGER . info ( [string] % current_round ) [EOL] attempts = [number] [EOL] while not n_candidates : [EOL] _LOGGER . info ( [string] % attempts ) [EOL] countdown ( message = [string] , t = backoff_period ) [EOL] n_candidates = self . _n_candidates_remaining ( ) [EOL] [EOL] attempts += [number] [EOL] if attempts > max_attempts : [EOL] msg = [string] % max_attempts [EOL] msg += [string] [EOL] _LOGGER . warning ( msg ) [EOL] finished = True [EOL] break [EOL] [EOL] [comment] [EOL] else : [EOL] finished = True [EOL] break [EOL] [EOL] [EOL] def search ( self , objective , max_configs_per_round , sampler , subsample_factor = [number] , max_attempts = [number] , backoff_period = [number] ) : [EOL] [docstring] [EOL] _LOGGER . info ( [string] % ( self . role , self . n_jobs , self . backend ) ) [EOL] [EOL] start = time . time ( ) [EOL] [EOL] if self . role == [string] : [EOL] self . _master_search ( objective = objective , sampler = sampler , max_configs_per_round = max_configs_per_round , subsample_factor = subsample_factor ) [EOL] else : [EOL] self . _worker_search ( objective = objective , n_rounds = len ( max_configs_per_round ) , max_attempts = [number] , backoff_period = backoff_period ) [EOL] [EOL] self . _export_all_results ( ) [EOL] [EOL] [comment] [EOL] self . best_results [ [string] ] , self . best_results [ [string] ] = self . _select_best ( ) [EOL] [EOL] total_time = ( time . time ( ) - start ) / [number] [EOL] _LOGGER . info ( [string] % total_time )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $spaces.SpaceSampler$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $spaces.SpaceSampler$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $typing.Any$ 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $builtins.str$ 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $spaces.SpaceSampler$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $spaces.SpaceSampler$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 $spaces.SpaceSampler$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 $builtins.str$ 0 0 0 $logging.Logger$ 0 0 0 $builtins.str$ 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $typing.List[builtins.int]$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.float$ 0
from typing import Any [EOL] import typing [EOL] import logging [EOL] from os . path import join [EOL] import sys [EOL] from typing import Any [EOL] [EOL] [comment] [EOL] from ptuner . utils import constants as c [EOL] from ptuner . pipeline import ( LocalPipelineTuner , ParallelPipelineTuner , STATUS_FAIL , STATUS_OK ) [EOL] [EOL] logging . basicConfig ( level = logging . INFO , format = c . FORMAT ) [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] with open ( join ( c . PACKAGE_ROOT , [string] ) ) as version_file : [EOL] __version__ = version_file . read ( ) . strip ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict , Optional [EOL] import builtins [EOL] import typing [EOL] import pandas [EOL] from abc import ABC , abstractmethod [EOL] import pandas as pd [EOL] from typing import Any , Dict , Optional [EOL] [EOL] __all__ = [ [string] ] [EOL] [EOL] [EOL] class BaseSampler ( ABC ) : [EOL] [docstring] [EOL] @ abstractmethod def __init__ ( self , dynamic_update = True , seed = None , ** kwargs , ) : [EOL] self . space = self . _starting_space ( ) [EOL] self . dynamic_update = dynamic_update [EOL] self . seed = seed [EOL] [EOL] [EOL] @ abstractmethod def __str__ ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def _starting_space ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def sample_space ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def update_space ( self , data ) : [EOL] [docstring] [EOL] pass	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0
from typing import Any , List , Optional [EOL] import builtins [EOL] import typing [EOL] import spaces [EOL] from abc import ABC , abstractmethod [EOL] from multiprocessing import cpu_count [EOL] from typing import Any , List , Optional , Dict [EOL] [EOL] [comment] [EOL] from . . spaces . sampler import SpaceSampler [EOL] [EOL] __all__ = [ [string] ] [EOL] [EOL] [EOL] class BasePipelineTuner ( ABC ) : [EOL] [docstring] [EOL] @ abstractmethod def __init__ ( self , lower_is_better , n_jobs , backend = [string] , experiment_name = [string] , save_name = None , verbose = True ) : [EOL] [comment] [EOL] max_cpus = cpu_count ( ) [EOL] if n_jobs == [number] : [EOL] n_jobs = [number] [EOL] elif abs ( n_jobs ) > max_cpus : [EOL] n_jobs = max_cpus [EOL] else : [EOL] if n_jobs < [number] : n_jobs = list ( range ( [number] , cpu_count ( ) + [number] ) ) [ n_jobs ] [EOL] [EOL] self . n_jobs = n_jobs [EOL] [EOL] [comment] [EOL] self . lower_is_better = lower_is_better [EOL] self . experiment_name = experiment_name [EOL] if backend not in [ [string] , [string] , [string] ] : [EOL] raise ValueError ( [string] % backend , [string] ) [EOL] self . backend = backend [EOL] self . save_name = save_name [EOL] self . verbose = verbose [EOL] [EOL] [EOL] @ abstractmethod def _export_all_results ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def _evaluate_candidate ( self , objective ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def _update_space ( self , sampler , hof , n_round ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] @ abstractmethod def search ( self , objective , sampler , max_configs_per_round , subsample_factor ) : [EOL] [docstring] [EOL] pass	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 $builtins.int$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $spaces.sampler.SpaceSampler$ 0 0 0 $spaces.sampler.SpaceSampler$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $spaces.sampler.SpaceSampler$ 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0
	0
from . . base . _sampler import BaseSampler	0 0 0 0 0 0 0 0
from typing import List , Tuple , Any , Optional , Dict [EOL] import builtins [EOL] import typing [EOL] import pandas [EOL] from hyperopt import hp [EOL] from hyperopt . pyll . stochastic import sample [EOL] from math import log [EOL] import numpy as np [EOL] import pandas as pd [EOL] from typing import Any , Dict , List , Optional , Tuple [EOL] [EOL] [comment] [EOL] from . . base . _sampler import BaseSampler [EOL] [EOL] __all__ = [ [string] , [string] , [string] ] [EOL] [EOL] class MLPClassifierSampler ( BaseSampler ) : [EOL] [docstring] [EOL] def __init__ ( self , dynamic_update = True , early_stopping = True , n_hidden_layers = [number] , max_neurons = [number] , max_epochs = [number] , seed = None ) : [EOL] self . n_hidden_layers = n_hidden_layers [EOL] self . max_neurons = max_neurons [EOL] self . max_epochs = max_epochs [EOL] self . early_stopping = early_stopping [EOL] super ( ) . __init__ ( dynamic_update = dynamic_update , seed = seed ) [EOL] [EOL] [EOL] def __str__ ( self ) : [EOL] [docstring] [EOL] return [string] [EOL] [EOL] [EOL] def _starting_space ( self ) : [EOL] [docstring] [EOL] params = { } [EOL] params [ [string] ] = self . n_hidden_layers [EOL] [EOL] [comment] [EOL] for i in range ( [number] , self . n_hidden_layers + [number] ) : [EOL] params [ [string] % i ] = hp . quniform ( [string] % i , [number] , self . max_neurons , [number] ) [EOL] [EOL] params . update ( { [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , self . max_epochs , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . pchoice ( [string] , [ ( [number] , [string] ) , ( [number] , [string] ) ] ) , [string] : hp . pchoice ( [string] , [ ( [number] , [string] ) , ( [number] , [string] ) ] ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) } ) [EOL] [EOL] return params [EOL] [EOL] [EOL] def sample_space ( self ) : [EOL] [docstring] [EOL] hypers = { } [EOL] for param , dist in self . space . items ( ) : [EOL] value = sample ( dist ) [EOL] if [string] in param or param in [ [string] , [string] ] : [EOL] hypers [ param ] = int ( value ) [EOL] else : [EOL] hypers [ param ] = value [EOL] [EOL] hypers [ [string] ] = self . n_hidden_layers [EOL] [EOL] return hypers [EOL] [EOL] [EOL] def update_space ( self , data ) : [EOL] [docstring] [EOL] if not self . dynamic_update : return [EOL] [EOL] [comment] [EOL] for param in self . space . keys ( ) : [EOL] [EOL] if param in [ [string] , [string] ] : [EOL] pchoice = data [ param ] . value_counts ( True ) . sort_index ( ) . to_dict ( ) [EOL] pchoice_hp = [ ( value , key ) for key , value in pchoice . items ( ) ] [EOL] [EOL] [comment] [EOL] label = [string] if param == [string] else [string] [EOL] label = [string] + label [EOL] self . space [ param ] = hp . pchoice ( label , pchoice_hp ) [EOL] else : [EOL] [comment] [EOL] min_value , max_value = data [ param ] . min ( ) , data [ param ] . max ( ) [EOL] [EOL] if [string] in param : [EOL] label = [string] + param [ - [number] ] [EOL] self . space [ param ] = hp . quniform ( label , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] [comment] [EOL] elif param == [string] : [EOL] if not self . early_stopping : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] else : [EOL] raise ValueError ( [string] % param ) [EOL] [EOL] [EOL] class XGBClassifierSampler ( BaseSampler ) : [EOL] [docstring] [EOL] def __init__ ( self , dynamic_update = True , early_stopping = True , seed = [number] ) : [EOL] self . early_stopping = early_stopping [EOL] super ( ) . __init__ ( dynamic_update = dynamic_update , seed = seed ) [EOL] [EOL] [EOL] def __str__ ( self ) : [EOL] [docstring] [EOL] return [string] [EOL] [EOL] [EOL] def _starting_space ( self ) : [EOL] [docstring] [EOL] return { [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) } [EOL] [EOL] [EOL] def sample_space ( self ) : [EOL] [docstring] [EOL] hypers = { } [EOL] for param , dist in self . space . items ( ) : [EOL] hypers [ param ] = int ( sample ( dist ) ) \ [EOL] if param in [ [string] , [string] , [string] , [string] ] \ [EOL] else sample ( dist ) [EOL] [EOL] [comment] [EOL] hypers [ [string] ] = self . seed [EOL] [EOL] return hypers [EOL] [EOL] [EOL] def update_space ( self , data ) : [EOL] [docstring] [EOL] if not self . dynamic_update : return [EOL] [EOL] [comment] [EOL] for param in self . space . keys ( ) : [EOL] [EOL] [comment] [EOL] min_value , max_value = data [ param ] . min ( ) , data [ param ] . max ( ) [EOL] [EOL] [comment] [EOL] if param == [string] : [EOL] if not self . early_stopping : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] else : [EOL] raise ValueError ( [string] % param ) [EOL] [EOL] [EOL] class LightGBMClassifierSampler ( BaseSampler ) : [EOL] [docstring] [EOL] def __init__ ( self , dynamic_update = True , early_stopping = True , seed = [number] ) : [EOL] self . early_stopping = early_stopping [EOL] super ( ) . __init__ ( dynamic_update = dynamic_update , seed = seed ) [EOL] [EOL] [EOL] def __str__ ( self ) : [EOL] [docstring] [EOL] return [string] [EOL] [EOL] [EOL] def _starting_space ( self ) : [EOL] [docstring] [EOL] return { [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . uniform ( [string] , [number] , [number] ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , [string] : hp . quniform ( [string] , [number] , [number] , [number] ) , [string] : hp . pchoice ( [string] , [ ( [number] , None ) , ( [number] , [string] ) ] ) } [EOL] [EOL] [EOL] def sample_space ( self ) : [EOL] [docstring] [EOL] hypers = { } [EOL] for param , dist in self . space . items ( ) : [EOL] hypers [ param ] = int ( sample ( dist ) ) \ [EOL] if param in [ [string] , [string] , [string] , [string] , [string] , [string] ] \ [EOL] else sample ( dist ) [EOL] [EOL] [comment] [EOL] hypers [ [string] ] = self . seed [EOL] [EOL] return hypers [EOL] [EOL] [EOL] def update_space ( self , data ) : [EOL] [docstring] [EOL] if not self . dynamic_update : return [EOL] [EOL] [comment] [EOL] for param in self . space . keys ( ) : [EOL] [EOL] if param == [string] : [EOL] [comment] [EOL] [comment] [EOL] pchoice = data [ param ] . astype ( str ) . value_counts ( True ) . sort_index ( ) . to_dict ( ) [EOL] pchoice_hp = [ ( value , key if key != [string] else None ) for key , value in pchoice . items ( ) ] [EOL] [EOL] [comment] [EOL] self . space [ param ] = hp . pchoice ( [string] , pchoice_hp ) [EOL] [EOL] else : [EOL] min_value , max_value = data [ param ] . min ( ) , data [ param ] . max ( ) [EOL] [EOL] [comment] [EOL] if param == [string] : [EOL] if not self . early_stopping : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . uniform ( [string] , min_value , max_value ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . loguniform ( [string] , log ( min_value ) , log ( max_value ) ) [EOL] [EOL] elif param == [string] : [EOL] self . space [ param ] = hp . quniform ( [string] , min_value , max_value , [number] ) [EOL] [EOL] else : [EOL] raise ValueError ( [string] % param ) [EOL] [EOL] [EOL] class SkGBMClassifierSampler ( BaseSampler ) : [EOL] [comment] [EOL] pass	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $None$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $None$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $None$ 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 $pandas.DataFrame$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict , Optional [EOL] from collections import namedtuple [EOL] import builtins [EOL] import typing [EOL] import collections [EOL] from collections import namedtuple [EOL] import numpy as np [EOL] import re [EOL] from scipy . optimize import minimize [EOL] from scipy . stats import norm [EOL] from sklearn . gaussian_process import GaussianProcessRegressor [EOL] from sklearn . gaussian_process . kernels import ConstantKernel , Matern [EOL] from typing import Any , Dict , List , Optional [EOL] [EOL] [EOL] __all__ = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] NAMED_SAMPLER = namedtuple ( [string] , [string] ) [EOL] [EOL] class SpaceSampler : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] self . feature_sampler = None [EOL] self . hyperparameter_samplers = [ ] [EOL] self . _initialized = False [EOL] [EOL] [EOL] def add_feature_sampler ( self , sampler , name ) : [EOL] [docstring] [EOL] self . feature_sampler = NAMED_SAMPLER ( name = name , sampler = sampler ) [EOL] self . _initialized = True [EOL] return self [EOL] [EOL] [EOL] def add_hyperparameter_sampler ( self , sampler , name ) : [EOL] [docstring] [EOL] self . hyperparameter_samplers . append ( NAMED_SAMPLER ( name = name , sampler = sampler ) ) [EOL] self . _initialized = True [EOL] return self [EOL] [EOL] [EOL] def sample_space ( self ) : [EOL] [docstring] [EOL] assert ( self . _initialized ) , [string] [EOL] [EOL] [comment] [EOL] params = { } [EOL] if self . feature_sampler : [EOL] params [ self . feature_sampler . name ] = self . feature_sampler . sampler . sample_space ( ) [EOL] if self . hyperparameter_samplers : [EOL] for s in self . hyperparameter_samplers : [EOL] params [ s . name ] = s . sampler . sample_space ( ) [EOL] return params [EOL] [EOL] [EOL] def update_space ( self , data_features , data_hyperparameters ) : [EOL] [docstring] [EOL] assert ( self . _initialized ) , [string] [EOL] [EOL] [comment] [EOL] if self . feature_sampler : [EOL] self . feature_sampler . sampler . update_space ( data_features ) [EOL] if self . hyperparameter_samplers : [EOL] for s in self . hyperparameter_samplers : [EOL] s . sampler . update_space ( data_hyperparameters )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $collections.namedtuple$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Optional[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 $collections.namedtuple$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $collections.namedtuple$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $None$ 0 0 0 $typing.Optional[typing.Any]$ 0 $typing.Optional[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Any]$ 0
from typing import Any , List [EOL] import numpy [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import pandas [EOL] import logging [EOL] import numpy as np [EOL] import pandas as pd [EOL] from typing import Any , Dict , List [EOL] [EOL] [comment] [EOL] from . . base . _sampler import BaseSampler [EOL] [EOL] _LOGGER = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class NaiveFeatureSampler ( BaseSampler ) : [EOL] [docstring] [EOL] def __init__ ( self , p , dynamic_update = True , muting_threshold = [number] ) : [EOL] [EOL] assert ( muting_threshold > [number] and muting_threshold <= [number] ) , [string] % muting_threshold [EOL] [EOL] self . p = p [EOL] self . muting_threshold = muting_threshold [EOL] self . selection_probs = np . array ( [ [number] ] * self . p ) [EOL] self . space = self . _starting_space ( ) [EOL] super ( ) . __init__ ( dynamic_update = dynamic_update ) [EOL] [EOL] [EOL] def __str__ ( self ) : [EOL] [docstring] [EOL] return [string] [EOL] [EOL] [EOL] def _starting_space ( self ) : [EOL] [docstring] [EOL] return [ True ] * self . p [EOL] [EOL] [EOL] def sample_space ( self ) : [EOL] [docstring] [EOL] selected = np . random . binomial ( [number] , self . selection_probs , self . p ) . astype ( bool ) [EOL] selected &= np . array ( self . space ) [EOL] if not selected . sum ( ) : [EOL] _LOGGER . warn ( [string] ) [EOL] selected = self . space [EOL] return [ bool ( s ) for s in selected ] [EOL] [EOL] [EOL] def update_space ( self , data ) : [EOL] [docstring] [EOL] [comment] [EOL] self . selection_probs = np . mean ( data . values , axis = [number] ) [EOL] status = self . selection_probs > self . muting_threshold [EOL] [EOL] [comment] [EOL] updated = np . array ( self . space ) & status [EOL] if not updated . sum ( ) : [EOL] _LOGGER . warn ( [string] % self . muting_threshold + [string] % self . space . sum ( ) ) [EOL] return [EOL] else : [EOL] selected = [ bool ( u ) for u in updated ] [EOL] _LOGGER . info ( [string] % ( sum ( selected ) , len ( selected ) ) ) [EOL] self . space = selected [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.bool]$ 0 0 0 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . classifiers import * [EOL] from . features import * [EOL] from . regressors import * [EOL] from . sampler import *	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List , Iterator [EOL] import typing [EOL] import ptuner [EOL] from math import log [EOL] import pandas as pd [EOL] import pytest [EOL] import re [EOL] [EOL] [comment] [EOL] from ptuner . spaces import LightGBMClassifierSampler , MLPClassifierSampler , XGBClassifierSampler [EOL] from ptuner . utils import helper [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ True , False ] ) @ pytest . mark . parametrize ( [string] , [ True , False ] ) def test_lightgbm_classifier_sampler ( dynamic_update , early_stopping ) : [EOL] [docstring] [EOL] [comment] [EOL] sampler = LightGBMClassifierSampler ( dynamic_update = dynamic_update , early_stopping = early_stopping , seed = [number] ) [EOL] [EOL] [comment] [EOL] df_configs = pd . DataFrame ( [ sampler . sample_space ( ) for _ in range ( [number] ) ] ) [EOL] for column in df_configs : [EOL] [comment] [EOL] if column == [string] : [EOL] assert ( df_configs [ column ] . unique ( ) [ [number] ] == [number] ) , [string] [EOL] continue [EOL] [EOL] if column == [string] : [EOL] assert ( set ( df_configs [ column ] . unique ( ) ) == { [string] , None } ) , [string] % column [EOL] [EOL] [comment] [EOL] assert ( df_configs [ column ] . unique ( ) . shape [ [number] ] > [number] ) , [string] % column [EOL] [EOL] [comment] [EOL] df_configs = df_configs . sort_values ( by = [string] , ascending = True ) [EOL] df_hof = df_configs . copy ( deep = True ) . iloc [ : [number] ] [EOL] [EOL] [comment] [EOL] df_hof . loc [ df_hof [ [string] ] == [number] , [string] ] = [number] [EOL] [EOL] [comment] [EOL] sampler . update_space ( df_hof ) [EOL] [EOL] for column in df_hof : [EOL] [comment] [EOL] if column in [ [string] , [string] ] : continue [EOL] [EOL] actual_min = df_hof [ column ] . min ( ) [EOL] actual_max = df_hof [ column ] . max ( ) [EOL] param_type , hp_bounds = helper . parse_hyperopt_param ( str ( sampler . space [ column ] ) ) [EOL] [EOL] [comment] [EOL] if param_type == [string] : [EOL] actual_min = log ( actual_min ) [EOL] actual_max = log ( actual_max ) [EOL] [EOL] [comment] [EOL] if column == [string] : [EOL] if dynamic_update : [EOL] [EOL] [comment] [EOL] [comment] [EOL] if early_stopping : [EOL] assert ( actual_min > hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] [comment] [EOL] [comment] [EOL] else : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] if dynamic_update : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max == hp_bounds [ [number] ] ) , [string] % column [EOL] else : [EOL] assert ( actual_min >= hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max <= hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ True , False ] ) @ pytest . mark . parametrize ( [string] , [ True , False ] ) @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_mlp_classifier_sampler ( dynamic_update , early_stopping , n_hidden_layers , max_neurons , max_epochs ) : [EOL] [docstring] [EOL] [comment] [EOL] sampler = MLPClassifierSampler ( dynamic_update = dynamic_update , early_stopping = early_stopping , n_hidden_layers = n_hidden_layers , max_neurons = max_neurons , max_epochs = max_epochs , seed = None ) [EOL] [EOL] [comment] [EOL] assert ( sampler . space [ [string] ] == n_hidden_layers ) , [string] [EOL] [EOL] [comment] [EOL] names = filter ( lambda x : [string] in x and [string] not in x , sampler . space . keys ( ) ) [EOL] cols2check = [ [string] % i for i in range ( [number] , n_hidden_layers + [number] ) ] [EOL] assert ( set ( names ) == set ( cols2check ) ) , [string] [EOL] [EOL] [comment] [EOL] df_configs = pd . DataFrame ( [ sampler . sample_space ( ) for _ in range ( [number] ) ] ) [EOL] for column in df_configs : [EOL] [comment] [EOL] if column == [string] : [EOL] assert ( df_configs [ column ] . unique ( ) [ [number] ] == n_hidden_layers ) , [string] [EOL] continue [EOL] [EOL] [comment] [EOL] assert ( df_configs [ column ] . unique ( ) . shape [ [number] ] > [number] ) , [string] % column [EOL] [EOL] [comment] [EOL] [comment] [EOL] for column in cols2check + [ [string] ] : [EOL] [EOL] max_sampled = max ( [ sampler . sample_space ( ) [ column ] for _ in range ( [number] ) ] ) [EOL] if column == [string] : [EOL] assert ( max_sampled <= max_epochs ) , [string] [EOL] else : [EOL] assert ( max_sampled <= max_neurons ) , [string] % column [EOL] [EOL] [comment] [EOL] df_configs = df_configs . sort_values ( by = [string] , ascending = True ) [EOL] df_hof = df_configs . copy ( deep = True ) . iloc [ : [number] ] [EOL] [EOL] [comment] [EOL] df_hof . loc [ df_hof [ [string] ] == [number] , [string] ] = [number] [EOL] [EOL] [comment] [EOL] sampler . update_space ( df_hof ) [EOL] [EOL] for column in df_hof : [EOL] [comment] [EOL] if column in [ [string] , [string] , [string] ] : continue [EOL] [EOL] actual_min = df_hof [ column ] . min ( ) [EOL] actual_max = df_hof [ column ] . max ( ) [EOL] param_type , hp_bounds = helper . parse_hyperopt_param ( str ( sampler . space [ column ] ) ) [EOL] [EOL] [comment] [EOL] if param_type == [string] : [EOL] actual_min = log ( actual_min ) [EOL] actual_max = log ( actual_max ) [EOL] [EOL] [comment] [EOL] if column == [string] : [EOL] if dynamic_update : [EOL] [EOL] [comment] [EOL] [comment] [EOL] if early_stopping : [EOL] assert ( actual_min > hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] [comment] [EOL] [comment] [EOL] else : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] if dynamic_update : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max == hp_bounds [ [number] ] ) , [string] % column [EOL] else : [EOL] assert ( actual_min >= hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max <= hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ True , False ] ) @ pytest . mark . parametrize ( [string] , [ True , False ] ) def test_xgb_classifier_sampler ( dynamic_update , early_stopping ) : [EOL] [docstring] [EOL] [comment] [EOL] sampler = XGBClassifierSampler ( dynamic_update = dynamic_update , early_stopping = early_stopping , seed = [number] ) [EOL] [EOL] [comment] [EOL] df_configs = pd . DataFrame ( [ sampler . sample_space ( ) for _ in range ( [number] ) ] ) [EOL] for column in df_configs : [EOL] [comment] [EOL] if column == [string] : [EOL] assert ( df_configs [ column ] . unique ( ) [ [number] ] == [number] ) , [string] [EOL] continue [EOL] [EOL] [comment] [EOL] assert ( df_configs [ column ] . unique ( ) . shape [ [number] ] > [number] ) , [string] % column [EOL] [EOL] [comment] [EOL] df_configs = df_configs . sort_values ( by = [string] , ascending = True ) [EOL] df_hof = df_configs . copy ( deep = True ) . iloc [ : [number] ] [EOL] [EOL] [comment] [EOL] df_hof . loc [ df_hof [ [string] ] == [number] , [string] ] = [number] [EOL] [EOL] [comment] [EOL] sampler . update_space ( df_hof ) [EOL] [EOL] for column in df_hof : [EOL] [comment] [EOL] if column == [string] : continue [EOL] [EOL] actual_min = df_hof [ column ] . min ( ) [EOL] actual_max = df_hof [ column ] . max ( ) [EOL] param_type , hp_bounds = helper . parse_hyperopt_param ( str ( sampler . space [ column ] ) ) [EOL] [EOL] [comment] [EOL] if param_type == [string] : [EOL] actual_min = log ( actual_min ) [EOL] actual_max = log ( actual_max ) [EOL] [EOL] [comment] [EOL] if column == [string] : [EOL] if dynamic_update : [EOL] [EOL] [comment] [EOL] [comment] [EOL] if early_stopping : [EOL] assert ( actual_min > hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] [comment] [EOL] [comment] [EOL] else : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] + [string] [EOL] continue [EOL] [EOL] if dynamic_update : [EOL] assert ( actual_min == hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max == hp_bounds [ [number] ] ) , [string] % column [EOL] else : [EOL] assert ( actual_min >= hp_bounds [ [number] ] ) , [string] % column [EOL] [EOL] assert ( actual_max <= hp_bounds [ [number] ] ) , [string] % column	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
from typing import Any , List , Union , Dict [EOL] import typing [EOL] import ptuner [EOL] import numpy as np [EOL] import pytest [EOL] from sklearn . datasets import make_classification [EOL] from sklearn . model_selection import train_test_split [EOL] from xgboost import XGBClassifier [EOL] [EOL] [comment] [EOL] from ptuner import LocalPipelineTuner , ParallelPipelineTuner , STATUS_FAIL , STATUS_OK [EOL] from ptuner . spaces import NaiveFeatureSampler , SpaceSampler , XGBClassifierSampler [EOL] [EOL] [comment] [EOL] N = [number] [EOL] p = [number] [EOL] FEATURE_SAMPLER = [string] [EOL] XGB_SAMPLER = [string] [EOL] [EOL] [comment] [EOL] sampler = SpaceSampler ( ) [EOL] sampler . add_feature_sampler ( name = FEATURE_SAMPLER , sampler = NaiveFeatureSampler ( p = p ) ) [EOL] sampler . add_hyperparameter_sampler ( name = XGB_SAMPLER , sampler = XGBClassifierSampler ( early_stopping = True ) ) [EOL] [EOL] @ pytest . fixture def clf_binary ( ) : [EOL] [docstring] [EOL] X , y = make_classification ( n_samples = N , n_features = p , n_informative = [number] , n_redundant = [number] , n_classes = [number] , class_sep = [number] , random_state = [number] ) [EOL] [EOL] [comment] [EOL] X_train , X_test , y_train , y_test = train_test_split ( X , y , stratify = y , test_size = [number] , random_state = [number] ) [EOL] return X_train , X_test , y_train , y_test [EOL] [EOL] [EOL] def test_local_pipeline_tuner ( clf_binary ) : [EOL] [docstring] [EOL] def objective ( params , current_round ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] fit_params = { [string] : None , [string] : [string] , [string] : None , [string] : False } [EOL] [EOL] early_stopping_rounds = [ [number] , [number] , [number] ] [EOL] [EOL] try : [EOL] [comment] [EOL] features = params [ [string] ] [EOL] hps = params [ [string] ] [EOL] [EOL] [comment] [EOL] fit_params [ [string] ] = [ ( X_test [ : , features ] , y_test ) ] [EOL] fit_params [ [string] ] = early_stopping_rounds [ current_round ] [EOL] [EOL] [comment] [EOL] clf = XGBClassifier ( ** hps ) . fit ( X_train [ : , features ] , y_train , ** fit_params ) [EOL] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] = clf . best_iteration + [number] [EOL] [EOL] [comment] [EOL] return { [string] : STATUS_OK , [string] : None , [string] : clf . best_score } [EOL] except Exception as e : [EOL] return { [string] : STATUS_FAIL , [string] : e , [string] : [number] } [EOL] [EOL] X_train , X_test , y_train , y_test = clf_binary [EOL] [EOL] tuner = LocalPipelineTuner ( lower_is_better = False , n_jobs = - [number] , backend = [string] , experiment_name = [string] , save_name = None , verbose = False ) [EOL] tuner . search ( objective = objective , sampler = sampler , max_configs_per_round = [ [number] , [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] param_keys = list ( tuner . best_results [ [string] ] . keys ( ) ) [EOL] assert ( set ( param_keys ) == set ( [ FEATURE_SAMPLER , XGB_SAMPLER ] ) ) , [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert ( tuner . best_results [ [string] ] > [number] ) , [string] [EOL] [EOL] [comment] [EOL] assert ( sum ( tuner . best_results [ [string] ] [ [string] ] ) < p ) , [string] + [string] [EOL] [EOL] [EOL] def test_parallel_pipeline_tuner ( clf_binary ) : [EOL] [docstring] [EOL] [comment] [EOL] pass	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $builtins.str$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 $builtins.str$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ptuner.spaces.sampler.SpaceSampler$ 0 $ptuner.spaces.sampler.SpaceSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] import pathlib [EOL] from typing import Any [EOL] [EOL] [comment] [EOL] import ptuner [EOL] [EOL] DB_NAME = [string] [EOL] [EOL] [comment] [EOL] FORMAT = [string] [EOL] [EOL] [comment] [EOL] PACKAGE_ROOT = pathlib . Path ( ptuner . __file__ ) . resolve ( ) . parent	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import logging [EOL] import socket [EOL] import logging [EOL] import random [EOL] import re [EOL] import socket [EOL] import time [EOL] from typing import Any [EOL] [EOL] __all__ = [ [string] , [string] , [string] ] [EOL] [EOL] _LOGGER = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def get_hostname ( ) : [EOL] [docstring] [EOL] try : [EOL] return socket . gethostname ( ) [EOL] except Exception as e : [EOL] name = [string] + str ( random . randint ( [number] , [number] ) ) [EOL] msg = [string] % ( e , name ) [EOL] _LOGGER . warn ( msg ) [EOL] return name [EOL] [EOL] [EOL] def get_ip_address ( ) : [EOL] [docstring] [EOL] try : [EOL] s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) [EOL] s . connect ( ( [string] , [number] ) ) [EOL] ip_address = str ( s . getsockname ( ) [ [number] ] ) [EOL] s . close ( ) [EOL] return ip_address [EOL] except : [EOL] return [string] [EOL] [EOL] [EOL] def countdown ( message , t ) : [EOL] [docstring] [EOL] while t : [EOL] mins , secs = divmod ( t , [number] ) [EOL] timeformat = [string] . format ( message , mins , secs ) [EOL] print ( timeformat , end = [string] ) [EOL] time . sleep ( [number] ) [EOL] t -= [number] [EOL] [EOL] [EOL] def parse_hyperopt_param ( string ) : [EOL] [docstring] [EOL] param_type = re . findall ( [string] , string ) [ [number] ] [EOL] if param_type == [string] : [EOL] raise ValueError ( [string] ) [EOL] else : [EOL] [comment] [EOL] string = [string] . join ( re . findall ( [string] , string ) ) [EOL] [EOL] [comment] [EOL] parsed = list ( map ( float , re . findall ( [string] , string ) ) ) [EOL] return param_type , parsed [ : [number] ]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] from hyperopt import hp [EOL] from math import log [EOL] import pytest [EOL] [EOL] [comment] [EOL] from ptuner . utils . helper import parse_hyperopt_param [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( hp . loguniform ( [string] , log ( [number] ) , log ( [number] ) ) , ( [string] , [ log ( [number] ) , log ( [number] ) ] ) ) , ( hp . quniform ( [string] , [number] , [number] , [number] ) , ( [string] , [ [number] , [number] ] ) ) , ( hp . uniform ( [string] , [number] , [number] ) , ( [string] , [ [number] , [number] ] ) ) ] ) def test_parse_hyperopt_param ( parameter , expected ) : [EOL] [docstring] [EOL] string = str ( parameter ) [EOL] param_type , hp_bounds = parse_hyperopt_param ( string ) [EOL] assert ( param_type == expected [ [number] ] and hp_bounds == expected [ [number] ] ) , [string] [EOL] [EOL] def test_parse_hyperopt_param_error ( ) : [EOL] [docstring] [EOL] [comment] [EOL] parameter = hp . pchoice ( [string] , [ ( [number] , [string] ) , ( [number] , [string] ) ] ) [EOL] with pytest . raises ( ValueError ) : [EOL] parse_hyperopt_param ( str ( parameter ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . mongodb import *	0 0 0 0 0
from typing import Tuple , Any , List , Optional [EOL] import builtins [EOL] import typing [EOL] import pymongo [EOL] from pymongo import MongoClient [EOL] from typing import Any , Optional , Tuple [EOL] [EOL] [comment] [EOL] from . . utils . constants import DB_NAME [EOL] [EOL] __all__ = [ [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] class MongoError ( Exception ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] [EOL] class MongoWorker : [EOL] [docstring] [EOL] def __init__ ( self , host , port , collection = None ) : [EOL] self . host = host [EOL] self . port = port [EOL] self . collection = collection [EOL] [EOL] [EOL] def __enter__ ( self ) : [EOL] [docstring] [EOL] [comment] [EOL] self . client = MongoClient ( host = self . host , port = self . port ) [EOL] db = self . client [ DB_NAME ] [EOL] return db [ self . collection ] if self . collection else db [EOL] [EOL] [EOL] def __exit__ ( self , * args ) : [EOL] self . client . close ( ) [EOL] [EOL] [EOL] def is_running ( host , port ) : [EOL] [docstring] [EOL] try : [EOL] client = MongoClient ( host = host , port = port , serverSelectionTimeoutMS = [number] ) [EOL] info = client . server_info ( ) [EOL] return ( True , [string] % ( host , port ) ) [EOL] except Exception as e : [EOL] return ( False , e ) [EOL] [EOL] [EOL] def insert_init_record ( collection , computer_name ) : [EOL] [docstring] [EOL] collection . insert ( { [string] : computer_name , [string] : [string] , } ) [EOL] [EOL] [EOL] def init_collection ( host , port , collection , overwrite , computer_name ) : [EOL] [docstring] [EOL] try : [EOL] msg = [string] [EOL] with MongoWorker ( host , port , collection = None ) as db : [EOL] if collection not in db . collection_names ( ) : [EOL] db_collection = db [ collection ] [EOL] insert_init_record ( db_collection , computer_name ) [EOL] else : [EOL] if overwrite : [EOL] db [ collection ] . drop ( ) [EOL] db_collection = db [ collection ] [EOL] insert_init_record ( db_collection , computer_name ) [EOL] else : [EOL] msg = [string] % collection [EOL] [EOL] [comment] [EOL] if not msg : [EOL] msg = [string] % collection [EOL] return ( True , msg ) [EOL] [EOL] except Exception as e : [EOL] return ( False , e )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $pymongo.MongoClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $pymongo.MongoClient$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.bool,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.bool,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0