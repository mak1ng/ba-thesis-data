[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] from contrib . rpc . translate_service . Translate import Client [EOL] [EOL] from tornado . ioloop import IOLoop [EOL] from tornado import gen [EOL] from thrift import Thrift [EOL] from torthrift . pool import TStreamPool [EOL] from thrift . protocol . TBinaryProtocol import TBinaryProtocolFactory [EOL] from torthrift . client import PoolClient [EOL] [EOL] ioloop = IOLoop . instance ( ) [EOL] [EOL] [EOL] @ gen . coroutine def test ( ) : [EOL] try : [EOL] transport = TStreamPool ( [string] , [number] , max_stream = [number] ) [EOL] client = PoolClient ( Client , transport , TBinaryProtocolFactory ( ) ) [EOL] for i in range ( [number] , [number] ) : [EOL] res = yield client . translate ( [string] ) [EOL] res = res . replace ( [string] , [string] ) [EOL] print ( res ) [EOL] res = yield client . translate ( [string] ) [EOL] res = res . replace ( [string] , [string] ) [EOL] print ( res ) [EOL] except Thrift . TException as ex : [EOL] print ( [string] % ex . message ) [EOL] ioloop . stop ( ) [EOL] [EOL] [EOL] def main ( ) : [EOL] ioloop . add_callback ( test ) [EOL] ioloop . start ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from contrib . sacrebleu . sacrebleu import raw_corpus_bleu , compute_bleu , corpus_chrf , CHRF_ORDER , CHRF_BETA [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import pytest [EOL] [EOL] import numpy as np [EOL] import mxnet as mx [EOL] [EOL] import sockeye . init_embedding as init_embedding [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , { [string] : [number] , [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } , mx . nd . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) ) ] ) def test_init_weight ( embed , vocab_in , vocab_out , expected_embed_init ) : [EOL] embed_init = init_embedding . init_weight ( embed , vocab_in , vocab_out ) [EOL] [EOL] assert ( embed_init == expected_embed_init ) . asnumpy ( ) . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import builtins [EOL] import typing [EOL] import unittest [EOL] import json [EOL] from unittest . mock import Mock [EOL] [EOL] import mxnet as mx [EOL] import numpy as np [EOL] import pytest [EOL] from math import ceil [EOL] [EOL] import sockeye . constants as C [EOL] import sockeye . data_io [EOL] import sockeye . inference [EOL] from sockeye . utils import SockeyeError [EOL] [EOL] _BOS = [number] [EOL] _EOS = - [number] [EOL] [EOL] [EOL] def mock_translator ( num_source_factors ) : [EOL] t_mock = Mock ( sockeye . inference . Translator ) [EOL] t_mock . num_source_factors = num_source_factors [EOL] return t_mock [EOL] [EOL] [EOL] def test_concat_translations ( ) : [EOL] expected_target_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , - [number] ] [EOL] num_src = [number] [EOL] [EOL] def length_penalty ( length ) : [EOL] return [number] / length [EOL] [EOL] expected_score = ( [number] + [number] + [number] ) / length_penalty ( len ( expected_target_ids ) ) [EOL] [EOL] translations = [ sockeye . inference . Translation ( [ [number] , [number] , [number] , - [number] ] , np . zeros ( ( [number] , num_src ) ) , [number] / length_penalty ( [number] ) ) , sockeye . inference . Translation ( [ [number] , [number] , [number] ] , np . zeros ( ( [number] , num_src ) ) , [number] / length_penalty ( [number] ) ) , sockeye . inference . Translation ( [ [number] , [number] , [number] , [number] , - [number] ] , np . zeros ( ( [number] , num_src ) ) , [number] / length_penalty ( [number] ) ) ] [EOL] combined = sockeye . inference . _concat_translations ( translations , start_id = _BOS , stop_ids = { _EOS } , length_penalty = length_penalty ) [EOL] [EOL] assert combined . target_ids == expected_target_ids [EOL] assert combined . attention_matrix . shape == ( len ( expected_target_ids ) , len ( translations ) * num_src ) [EOL] assert np . isclose ( combined . score , expected_score ) [EOL] [EOL] [EOL] def test_length_penalty_default ( ) : [EOL] lengths = mx . nd . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] length_penalty = sockeye . inference . LengthPenalty ( [number] , [number] ) [EOL] expected_lp = np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] [EOL] assert np . isclose ( length_penalty ( lengths ) . asnumpy ( ) , expected_lp ) . all ( ) [EOL] [EOL] [EOL] def test_length_penalty ( ) : [EOL] lengths = mx . nd . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] length_penalty = sockeye . inference . LengthPenalty ( [number] , [number] ) [EOL] expected_lp = np . array ( [ [ [number] ** [number] / [number] ** [number] ] , [ [number] ** [number] / [number] ** [number] ] , [ [number] ** [number] / [number] ** [number] ] ] ) [EOL] [EOL] assert np . isclose ( length_penalty ( lengths ) . asnumpy ( ) , expected_lp ) . all ( ) [EOL] [EOL] [EOL] def test_length_penalty_int_input ( ) : [EOL] length = [number] [EOL] length_penalty = sockeye . inference . LengthPenalty ( [number] , [number] ) [EOL] expected_lp = [ [number] ** [number] / [number] ** [number] ] [EOL] [EOL] assert np . isclose ( np . asarray ( [ length_penalty ( length ) ] ) , np . asarray ( expected_lp ) ) . all ( ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [number] , [string] , None , [number] ) , ( [number] , [string] , None , [number] ) , ( [number] , [string] , None , [number] ) , ( [number] , [string] , None , [number] ) , ( [number] , [string] , [ [ [string] , [string] ] ] , [number] ) , ( [number] , [string] , [ [ [string] , [string] ] , [ [string] , [string] ] ] , [number] ) ] ) def test_translator_input ( sentence_id , sentence , factors , chunk_size ) : [EOL] tokens = sentence . split ( ) [EOL] trans_input = sockeye . inference . TranslatorInput ( sentence_id = sentence_id , tokens = tokens , factors = factors ) [EOL] [EOL] assert trans_input . sentence_id == sentence_id [EOL] assert trans_input . tokens == tokens [EOL] assert len ( trans_input ) == len ( tokens ) [EOL] assert trans_input . factors == factors [EOL] if factors is not None : [EOL] for factor in trans_input . factors : [EOL] assert len ( factor ) == len ( tokens ) [EOL] assert trans_input . chunk_id == - [number] [EOL] [EOL] chunked_inputs = list ( trans_input . chunks ( chunk_size ) ) [EOL] assert len ( chunked_inputs ) == ceil ( len ( tokens ) / chunk_size ) [EOL] for chunk_id , chunk_input in enumerate ( chunked_inputs ) : [EOL] assert chunk_input . sentence_id == sentence_id [EOL] assert chunk_input . chunk_id == chunk_id [EOL] assert chunk_input . tokens == trans_input . tokens [ chunk_id * chunk_size : ( chunk_id + [number] ) * chunk_size ] [EOL] if factors : [EOL] assert len ( chunk_input . factors ) == len ( factors ) [EOL] for factor , expected_factor in zip ( chunk_input . factors , factors ) : [EOL] assert len ( factor ) == len ( chunk_input . tokens ) [EOL] assert factor == expected_factor [ chunk_id * chunk_size : ( chunk_id + [number] ) * chunk_size ] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] [string] [string] , [ ( [number] , [number] , [number] , None , [number] , [number] , [number] , [number] ) , ( [number] , [number] , [number] , None , [number] , [number] , [number] , [number] ) , ( None , [number] , [number] , None , [number] , [number] , [number] , [number] ) , ( [number] , None , [number] , None , [number] , [number] , [number] , [number] ) , ( None , None , [number] , None , [number] , [number] , [number] , [number] ) , ( [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ) , ] ) def test_get_max_input_output_length ( supported_max_seq_len_source , supported_max_seq_len_target , training_max_seq_len_source , forced_max_input_len , length_ratio_mean , length_ratio_std , expected_max_input_len , expected_max_output_len ) : [EOL] max_input_len , get_max_output_len = sockeye . inference . get_max_input_output_length ( supported_max_seq_len_source = supported_max_seq_len_source , supported_max_seq_len_target = supported_max_seq_len_target , training_max_seq_len_source = training_max_seq_len_source , forced_max_input_len = forced_max_input_len , length_ratio_mean = length_ratio_mean , length_ratio_std = length_ratio_std , num_stds = [number] ) [EOL] max_output_len = get_max_output_len ( max_input_len ) [EOL] [EOL] if supported_max_seq_len_source is not None : [EOL] assert max_input_len <= supported_max_seq_len_source [EOL] if supported_max_seq_len_target is not None : [EOL] assert max_output_len <= supported_max_seq_len_target [EOL] if expected_max_input_len is not None : [EOL] assert max_input_len == expected_max_input_len [EOL] if expected_max_output_len is not None : [EOL] assert max_output_len == expected_max_output_len [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [number] , [string] , [ [string] , [string] , [string] , [string] ] , None ) , ( [string] , [number] , [string] , [ [string] , [string] , [string] , [string] ] , None ) , ( [string] , [number] , [string] , [ [string] , [string] ] , None ) , ( [string] , [number] , [string] , [ ] , None ) , ( [string] , [number] , [string] , [ ] , [ [ ] ] ) , ( [string] , [number] , [string] , [ [string] , [string] , [string] ] , [ [ [string] , [string] , [string] ] ] ) , ( [string] , [number] , [string] , [ [string] , [string] ] , [ [ [string] , [string] ] , [ [string] , [string] ] ] ) , ( [string] , [number] , [string] , [ [string] ] , [ [ [string] ] , [ [string] ] ] ) ] ) def test_make_input_from_factored_string ( sentence , num_expected_factors , delimiter , expected_tokens , expected_factors ) : [EOL] sentence_id = [number] [EOL] translator = mock_translator ( num_expected_factors ) [EOL] [EOL] inp = sockeye . inference . make_input_from_factored_string ( sentence_id = sentence_id , factored_string = sentence , translator = translator , delimiter = delimiter ) [EOL] assert isinstance ( inp , sockeye . inference . TranslatorInput ) [EOL] assert inp . sentence_id == sentence_id [EOL] assert inp . chunk_id == - [number] [EOL] assert inp . tokens == expected_tokens [EOL] assert inp . factors == expected_factors [EOL] if num_expected_factors > [number] : [EOL] assert len ( inp . factors ) == num_expected_factors - [number] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) , ( [string] , [number] , [string] ) ] ) def test_factor_parsing ( sentence , num_expected_factors , delimiter ) : [EOL] [docstring] [EOL] sentence_id = [number] [EOL] translator = mock_translator ( num_expected_factors ) [EOL] inp = sockeye . inference . make_input_from_factored_string ( sentence_id = sentence_id , factored_string = sentence , translator = translator , delimiter = delimiter ) [EOL] assert isinstance ( inp , sockeye . inference . BadTranslatorInput ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , None , [string] , [string] , [string] , [string] ] ) def test_make_input_whitespace_delimiter ( delimiter ) : [EOL] [docstring] [EOL] sentence_id = [number] [EOL] translator = mock_translator ( [number] ) [EOL] sentence = [string] [EOL] with pytest . raises ( SockeyeError ) as e : [EOL] sockeye . inference . make_input_from_factored_string ( sentence_id = sentence_id , factored_string = sentence , translator = translator , delimiter = delimiter ) [EOL] assert str ( e . value ) == [string] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , None ) , ( [string] , None ) , ( [string] , [ [string] , [string] ] ) , ( [string] , [ [string] ] ) , ( [string] , [ ] ) ] ) def test_make_input_from_valid_json_string ( text , factors ) : [EOL] sentence_id = [number] [EOL] expected_tokens = list ( sockeye . data_io . get_tokens ( text ) ) [EOL] inp = sockeye . inference . make_input_from_json_string ( sentence_id , json . dumps ( { C . JSON_TEXT_KEY : text , C . JSON_FACTORS_KEY : factors } ) ) [EOL] assert len ( inp ) == len ( expected_tokens ) [EOL] assert inp . tokens == expected_tokens [EOL] if factors is not None : [EOL] assert len ( inp . factors ) == len ( factors ) [EOL] else : [EOL] assert inp . factors is None [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] , None , [string] ) ] ) def test_failed_make_input_from_valid_json_string ( text , text_key , factors , factors_key ) : [EOL] sentence_id = [number] [EOL] inp = sockeye . inference . make_input_from_json_string ( sentence_id , json . dumps ( { text_key : text , factors_key : factors } ) ) [EOL] assert isinstance ( inp , sockeye . inference . BadTranslatorInput ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [ [string] ] , [ [string] , [string] , [string] ] ] ) def test_make_input_from_multiple_strings ( strings ) : [EOL] inp = sockeye . inference . make_input_from_multiple_strings ( [number] , strings ) [EOL] [EOL] expected_tokens = list ( sockeye . data_io . get_tokens ( strings [ [number] ] ) ) [EOL] expected_factors = [ list ( sockeye . data_io . get_tokens ( f ) ) for f in strings [ [number] : ] ] [EOL] assert len ( inp ) == len ( expected_tokens ) [EOL] assert inp . tokens == expected_tokens [EOL] assert inp . factors == expected_factors [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import sockeye [EOL] import pytest [EOL] import mxnet as mx [EOL] import numpy as np [EOL] [EOL] import sockeye . constants as C [EOL] import sockeye . encoder [EOL] [EOL] [EOL] _BATCH_SIZE = [number] [EOL] _SEQ_LEN = [number] [EOL] _NUM_EMBED = [number] [EOL] _DATA_LENGTH_ND = mx . nd . array ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] [EOL] def test_get_recurrent_encoder_no_conv_config ( ) : [EOL] rnn_config = sockeye . rnn . RNNConfig ( cell_type = C . LSTM_TYPE , num_hidden = [number] , num_layers = [number] , dropout_inputs = [number] , dropout_states = [number] ) [EOL] config = sockeye . encoder . RecurrentEncoderConfig ( rnn_config , conv_config = None , reverse_input = True , dtype = [string] ) [EOL] encoder = sockeye . encoder . get_recurrent_encoder ( config , prefix = [string] ) [EOL] [EOL] assert type ( encoder ) == sockeye . encoder . EncoderSequence [EOL] assert len ( encoder . encoders ) == [number] [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvertLayout [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , target_layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ReverseSequence [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . BiDirectionalRNNEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( layout = [string] , prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . RecurrentEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvertLayout [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , target_layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] [EOL] def test_get_recurrent_encoder ( ) : [EOL] rnn_config = sockeye . rnn . RNNConfig ( cell_type = C . LSTM_TYPE , num_hidden = [number] , num_layers = [number] , dropout_inputs = [number] , dropout_states = [number] ) [EOL] conv_config = sockeye . encoder . ConvolutionalEmbeddingConfig ( num_embed = [number] , add_positional_encoding = True ) [EOL] config = sockeye . encoder . RecurrentEncoderConfig ( rnn_config , conv_config , reverse_input = True , dtype = [string] ) [EOL] encoder = sockeye . encoder . get_recurrent_encoder ( config , prefix = [string] ) [EOL] [EOL] assert type ( encoder ) == sockeye . encoder . EncoderSequence [EOL] assert len ( encoder . encoders ) == [number] [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvolutionalEmbeddingEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_embed = [number] , prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . AddSinCosPositionalEmbeddings [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_embed = [number] , prefix = [string] , scale_up_input = False , scale_down_positions = False , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvertLayout [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , target_layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ReverseSequence [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . BiDirectionalRNNEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( layout = [string] , prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . RecurrentEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvertLayout [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_hidden = [number] , target_layout = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] [EOL] def test_get_transformer_encoder ( ) : [EOL] conv_config = sockeye . encoder . ConvolutionalEmbeddingConfig ( num_embed = [number] , add_positional_encoding = True ) [EOL] config = sockeye . transformer . TransformerConfig ( model_size = [number] , attention_heads = [number] , feed_forward_num_hidden = [number] , act_type = [string] , num_layers = [number] , dropout_attention = [number] , dropout_act = [number] , dropout_prepost = [number] , positional_embedding_type = C . LEARNED_POSITIONAL_EMBEDDING , preprocess_sequence = [string] , postprocess_sequence = [string] , max_seq_len_source = [number] , max_seq_len_target = [number] , conv_config = conv_config , dtype = [string] ) [EOL] encoder = sockeye . encoder . get_transformer_encoder ( config , prefix = [string] ) [EOL] [EOL] assert type ( encoder ) == sockeye . encoder . EncoderSequence [EOL] assert len ( encoder . encoders ) == [number] [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . AddLearnedPositionalEmbeddings [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_embed = [number] , max_seq_len = [number] , prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvolutionalEmbeddingEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_embed = [number] , prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . TransformerEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( prefix = [string] , dtype = [string] ) . items ( ) [EOL] [EOL] [EOL] def test_get_convolutional_encoder ( ) : [EOL] cnn_config = sockeye . convolution . ConvolutionConfig ( kernel_width = [number] , num_hidden = [number] ) [EOL] config = sockeye . encoder . ConvolutionalEncoderConfig ( num_embed = [number] , max_seq_len_source = [number] , cnn_config = cnn_config , num_layers = [number] , positional_embedding_type = C . NO_POSITIONAL_EMBEDDING , dtype = [string] ) [EOL] encoder = sockeye . encoder . get_convolutional_encoder ( config , prefix = [string] ) [EOL] [EOL] assert type ( encoder ) == sockeye . encoder . EncoderSequence [EOL] assert len ( encoder . encoders ) == [number] [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . NoOpPositionalEmbeddings [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( num_embed = [number] , dtype = [string] ) . items ( ) [EOL] [EOL] assert type ( encoder . encoders [ [number] ] ) == sockeye . encoder . ConvolutionalEncoder [EOL] assert encoder . encoders [ [number] ] . __dict__ . items ( ) >= dict ( dtype = [string] ) . items ( ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( sockeye . encoder . ConvolutionalEmbeddingConfig ( num_embed = _NUM_EMBED , output_dim = None , max_filter_width = [number] , num_filters = [ [number] , [number] , [number] ] , pool_stride = [number] , num_highway_layers = [number] , dropout = [number] , add_positional_encoding = False ) , ( [number] , [number] , [number] ) , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [number] ) , ( sockeye . encoder . ConvolutionalEmbeddingConfig ( num_embed = _NUM_EMBED , output_dim = [number] , max_filter_width = [number] , num_filters = [ [number] , [number] ] , pool_stride = [number] , num_highway_layers = [number] , dropout = [number] , add_positional_encoding = True ) , ( [number] , [number] , [number] ) , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [number] ) , ] ) def test_convolutional_embedding_encoder ( config , out_data_shape , out_data_length , out_seq_len ) : [EOL] conv_embed = sockeye . encoder . ConvolutionalEmbeddingEncoder ( config ) [EOL] [EOL] data_nd = mx . nd . random_normal ( shape = ( _BATCH_SIZE , _SEQ_LEN , _NUM_EMBED ) ) [EOL] [EOL] data = mx . sym . Variable ( [string] , shape = data_nd . shape ) [EOL] data_length = mx . sym . Variable ( [string] , shape = _DATA_LENGTH_ND . shape ) [EOL] [EOL] ( encoded_data , encoded_data_length , encoded_seq_len ) = conv_embed . encode ( data = data , data_length = data_length , seq_len = _SEQ_LEN ) [EOL] [EOL] exe = encoded_data . simple_bind ( mx . cpu ( ) , data = data_nd . shape ) [EOL] exe . forward ( data = data_nd ) [EOL] assert exe . outputs [ [number] ] . shape == out_data_shape [EOL] [EOL] exe = encoded_data_length . simple_bind ( mx . cpu ( ) , data_length = _DATA_LENGTH_ND . shape ) [EOL] exe . forward ( data_length = _DATA_LENGTH_ND ) [EOL] assert np . equal ( exe . outputs [ [number] ] . asnumpy ( ) , np . asarray ( out_data_length ) ) . all ( ) [EOL] [EOL] assert encoded_seq_len == out_seq_len [EOL] [EOL] [EOL] def test_sincos_positional_embeddings ( ) : [EOL] [comment] [EOL] data = mx . sym . Variable ( [string] ) [EOL] positions = mx . sym . Variable ( [string] ) [EOL] pos_encoder = sockeye . encoder . AddSinCosPositionalEmbeddings ( num_embed = _NUM_EMBED , scale_up_input = False , scale_down_positions = False , prefix = [string] ) [EOL] encoded , _ , __ = pos_encoder . encode ( data , None , _SEQ_LEN ) [EOL] nd_encoded = encoded . eval ( data = mx . nd . zeros ( ( _BATCH_SIZE , _SEQ_LEN , _NUM_EMBED ) ) ) [ [number] ] [EOL] [comment] [EOL] nd_encoded = nd_encoded [ [number] ] [EOL] [EOL] encoded_positions = pos_encoder . encode_positions ( positions , data ) [EOL] [comment] [EOL] nd_encoded_positions = encoded_positions . eval ( positions = mx . nd . arange ( [number] , _SEQ_LEN ) , data = mx . nd . zeros ( ( _SEQ_LEN , _NUM_EMBED ) ) ) [ [number] ] [EOL] assert np . isclose ( nd_encoded . asnumpy ( ) , nd_encoded_positions . asnumpy ( ) ) . all ( ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] import logging [EOL] import logging [EOL] import random [EOL] [EOL] import pytest [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] from test . common import tmp_digits_dataset , run_train_translate [EOL] [EOL] _TRAIN_LINE_COUNT = [number] [EOL] _DEV_LINE_COUNT = [number] [EOL] _LINE_MAX_LENGTH = [number] [EOL] _TEST_LINE_COUNT = [number] [EOL] _TEST_LINE_COUNT_EMPTY = [number] [EOL] _TEST_MAX_LENGTH = [number] [EOL] _SEED_TRAIN_DATA = [number] [EOL] _SEED_DEV_DATA = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] seed = random . randint ( [number] , [number] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] [string] [string] [string] [string] , [string] , True , [number] , [number] ) , ( [string] , [string] [string] [string] [string] , [string] , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] , [string] , True , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] [string] , [string] , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] [string] , [string] , True , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] , [string] , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] , [string] , True , [number] , [number] ) ] ) def test_seq_copy ( name , train_params , translate_params , use_prepared_data , perplexity_thresh , bleu_thresh ) : [EOL] [docstring] [EOL] with tmp_digits_dataset ( [string] , _TRAIN_LINE_COUNT , _LINE_MAX_LENGTH , _DEV_LINE_COUNT , _LINE_MAX_LENGTH , _TEST_LINE_COUNT , _TEST_LINE_COUNT_EMPTY , _TEST_MAX_LENGTH , seed_train = _SEED_TRAIN_DATA , seed_dev = _SEED_DEV_DATA ) as data : [EOL] [comment] [EOL] perplexity , bleu , bleu_restrict , chrf = run_train_translate ( train_params = train_params , translate_params = translate_params , translate_params_equiv = None , train_source_path = data [ [string] ] , train_target_path = data [ [string] ] , dev_source_path = data [ [string] ] , dev_target_path = data [ [string] ] , test_source_path = data [ [string] ] , test_target_path = data [ [string] ] , use_prepared_data = use_prepared_data , max_seq_len = _LINE_MAX_LENGTH + [number] , restrict_lexicon = True , work_dir = data [ [string] ] , seed = seed ) [EOL] logger . info ( [string] , name ) [EOL] logger . info ( [string] , perplexity , bleu , bleu_restrict , chrf ) [EOL] assert perplexity <= perplexity_thresh [EOL] assert bleu >= bleu_thresh [EOL] assert bleu_restrict >= bleu_thresh [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] [string] [string] , [string] , True , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] , [string] , False , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] [string] , [string] , True , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] [string] , [string] , False , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] , [string] , True , False , [number] , [number] ) , ( [string] , [string] [string] [string] [string] [string] , [string] , True , True , [number] , [number] ) , ( [string] , [string] [string] [string] [string] , [string] , False , False , [number] , [number] ) ] ) def test_seq_sort ( name , train_params , translate_params , use_prepared_data , use_source_factor , perplexity_thresh , bleu_thresh ) : [EOL] [docstring] [EOL] with tmp_digits_dataset ( [string] , _TRAIN_LINE_COUNT , _LINE_MAX_LENGTH , _DEV_LINE_COUNT , _LINE_MAX_LENGTH , _TEST_LINE_COUNT , _TEST_LINE_COUNT_EMPTY , _TEST_MAX_LENGTH , sort_target = True , seed_train = _SEED_TRAIN_DATA , seed_dev = _SEED_DEV_DATA , with_source_factors = use_source_factor ) as data : [EOL] [comment] [EOL] perplexity , bleu , bleu_restrict , chrf = run_train_translate ( train_params = train_params , translate_params = translate_params , translate_params_equiv = None , train_source_path = data [ [string] ] , train_target_path = data [ [string] ] , dev_source_path = data [ [string] ] , dev_target_path = data [ [string] ] , test_source_path = data [ [string] ] , test_target_path = data [ [string] ] , train_source_factor_paths = data . get ( [string] ) , dev_source_factor_paths = data . get ( [string] ) , test_source_factor_paths = data . get ( [string] ) , use_prepared_data = use_prepared_data , max_seq_len = _LINE_MAX_LENGTH + [number] , restrict_lexicon = True , work_dir = data [ [string] ] , seed = seed ) [EOL] logger . info ( [string] , name ) [EOL] logger . info ( [string] , perplexity , bleu , bleu_restrict , chrf ) [EOL] assert perplexity <= perplexity_thresh [EOL] assert bleu >= bleu_thresh [EOL] assert bleu_restrict >= bleu_thresh [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] __version__ = [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0