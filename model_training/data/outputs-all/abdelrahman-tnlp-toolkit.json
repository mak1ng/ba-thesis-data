	0
	0
from typing import List , Tuple , Any [EOL] import logging [EOL] import typing [EOL] import nlp_toolkit [EOL] import builtins [EOL] [docstring] [EOL] import logging [EOL] from typing import List , Tuple [EOL] [EOL] from gensim . summarization import keywords as _extract_keywords [EOL] from gensim . summarization . summarizer import summarize as _summarize [EOL] [EOL] from . farasa import Farasa [EOL] from . utils import _preprocess_arabic_text , setup_logger [EOL] [EOL] LOGGER = setup_logger ( [string] , logging . DEBUG ) [EOL] [EOL] farasa = Farasa ( ) [EOL] [EOL] [EOL] def summarize ( text , ratio ) : [EOL] [docstring] [EOL] try : [EOL] summary = _summarize ( _preprocess_arabic_text ( text , remove_emails_urls_html = True ) , ratio ) [EOL] [EOL] except ValueError : [EOL] summary = text [EOL] [EOL] return summary or text [EOL] [EOL] [EOL] def extract_keywords ( text , pos_filter , top_n = None ) : [EOL] [docstring] [EOL] text = _preprocess_arabic_text ( text , remove_emails_urls_html = True ) [EOL] [EOL] try : [EOL] keywords = _extract_keywords ( farasa . filter_pos ( text , keep = pos_filter ) , split = True ) [EOL] [EOL] return keywords [ : top_n ] [EOL] [EOL] except Exception as e : [EOL] LOGGER . exception ( e ) [EOL] return [ ] [EOL] [EOL] [EOL] def extract_entities ( text ) : [EOL] [docstring] [EOL] try : [EOL] text = _preprocess_arabic_text ( text , remove_emails_urls_html = True ) [EOL] return farasa . get_named_entities ( text ) [EOL] [EOL] except Exception as e : [EOL] LOGGER . exception ( e ) [EOL] return [ ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp_toolkit.tools.farasa.Farasa$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $nlp_toolkit.tools.farasa.Farasa$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp_toolkit.tools.farasa.Farasa$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $nlp_toolkit.tools.farasa.Farasa$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0
from typing import List , Pattern , Dict , Any , Tuple [EOL] import logging [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] import logging [EOL] import os . path [EOL] import re [EOL] import string [EOL] from typing import List [EOL] [EOL] import wrapt [EOL] [EOL] FILE_PATH = os . path . dirname ( __file__ ) [EOL] [EOL] RE_WHITESPACE = [string] [EOL] [EOL] RE_NUM = [string] [EOL] RE_HASHTAG = [string] [EOL] RE_MENTION = [string] [EOL] [EOL] RE_URL = [string] [EOL] RE_EMAIL = [string] [EOL] RE_HTML = [string] [EOL] RE_NON_ARABIC = [string] [EOL] [EOL] TWITTER = [ RE_HASHTAG , RE_MENTION ] [EOL] [EOL] WEB = [ RE_URL , RE_EMAIL , RE_HTML , ] [EOL] [EOL] IGNORED = [ RE_WHITESPACE ] [EOL] [EOL] LONGATION = re . compile ( [string] ) [EOL] TASHKEEL = re . compile ( [string] ) [EOL] [EOL] TRANSLATE_TABLE = dict ( ( ord ( char ) , None ) for char in string . punctuation ) [EOL] [EOL] ASSETS_DIR = f'{ FILE_PATH } [string] ' [EOL] [EOL] [EOL] def remove_extra_spaces ( text ) : [EOL] [docstring] [EOL] template = [ ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) ] [EOL] [EOL] for token , replacement in template : [EOL] text . replace ( token , replacement ) [EOL] [EOL] return [string] . join ( text . split ( ) ) [EOL] [EOL] [EOL] def _preprocess_arabic_text ( text , remove_non_arabic = False , remove_punctuation = False , remove_numbers = False , remove_emails_urls_html = False , remove_hashtags_mentions = False ) : [EOL] [docstring] [EOL] template = [ ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] [EOL] [EOL] expressions = [ ] [EOL] [EOL] if remove_non_arabic : [EOL] expressions . append ( RE_NON_ARABIC ) [EOL] [EOL] if remove_numbers : [EOL] expressions . append ( RE_NUM ) [EOL] [EOL] if remove_emails_urls_html : [EOL] expressions . extend ( WEB ) [EOL] [EOL] if remove_hashtags_mentions : [EOL] expressions . extend ( TWITTER ) [EOL] [EOL] if expressions : [EOL] re_pattern = [string] . join ( IGNORED ) + [string] + [string] . join ( expressions ) + [string] [comment] [EOL] text = re . sub ( re_pattern , [string] , text ) [EOL] [EOL] text = re . sub ( TASHKEEL , [string] , text ) [EOL] text = re . sub ( LONGATION , [string] , text ) [EOL] text = text . replace ( [string] , [string] ) [EOL] text = text . replace ( [string] , [string] ) [EOL] text = text . replace ( [string] , [string] ) [EOL] [EOL] for token , replacement in template : [EOL] text = text . replace ( token , replacement ) [EOL] [EOL] if remove_punctuation : [EOL] text = text . translate ( TRANSLATE_TABLE ) [EOL] [EOL] return remove_extra_spaces ( text ) [EOL] [EOL] [EOL] def preprorcess_arabic_text ( ** kwargs ) : [EOL] [docstring] [EOL] @ wrapt . decorator def wrapper ( wrapped , instance , args , kwargs ) : [comment] [EOL] try : [EOL] if not isinstance ( args [ [number] ] , str ) : [EOL] raise TypeError [EOL] [EOL] text = _preprocess_arabic_text ( args [ [number] ] , ** kwargs ) [EOL] [EOL] except ( IndexError , TypeError ) : [EOL] raise TypeError ( [string] ) [EOL] [EOL] return wrapped ( text ) [EOL] return wrapper [EOL] [EOL] [EOL] def setup_logger ( name , level ) : [EOL] [docstring] [EOL] logger = logging . getLogger ( name ) [EOL] logger . setLevel ( level ) [EOL] [EOL] file_handler = logging . FileHandler ( f'{ name } [string] ' ) [EOL] stream_handler = logging . StreamHandler ( ) [EOL] [EOL] formatter = logging . Formatter ( f' [string] { name } [string] ' ) [EOL] [EOL] [comment] [EOL] file_handler . setLevel ( level ) [EOL] stream_handler . setLevel ( level ) [EOL] [EOL] [comment] [EOL] stream_handler . setFormatter ( formatter ) [EOL] file_handler . setFormatter ( formatter ) [EOL] [EOL] [comment] [EOL] logger . addHandler ( file_handler ) [EOL] logger . addHandler ( stream_handler ) [EOL] [EOL] return logger [EOL] [EOL] [EOL] def load_stop_words ( filename ) : [EOL] [docstring] [EOL] path = os . path . join ( ASSETS_DIR , f'{ filename } [string] ' ) [EOL] [EOL] return open ( path ) . read ( ) . split ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.str$ 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , List , Any , Tuple , Generator , Set [EOL] import numpy [EOL] import typing [EOL] import gensim [EOL] import builtins [EOL] [docstring] [EOL] from typing import Generator , List , Optional , Set , Tuple [EOL] [EOL] import numpy [EOL] from gensim . models import Word2Vec [EOL] from nltk import ngrams as _create_ngrams [EOL] from nltk import word_tokenize [EOL] [EOL] [EOL] class WordEmbedding : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , path ) : [EOL] [docstring] [EOL] self . _w2v = Word2Vec . load ( path ) [EOL] [EOL] def get_word_vector ( self , word ) : [EOL] [docstring] [EOL] return self . _w2v . wv . get ( word , None ) [EOL] [EOL] def create_ngrams ( self , tokens , nrange = ( [number] , [number] ) ) : [EOL] [docstring] [EOL] for n in range ( nrange [ [number] ] , nrange [ [number] ] - [number] , - [number] ) : [EOL] for ngram in _create_ngrams ( tokens , n ) : [EOL] yield [string] . join ( ngram ) [EOL] [EOL] def create_valid_trigrams ( self , text ) : [EOL] [docstring] [EOL] tokens = word_tokenize ( text ) [EOL] coverage = set ( ) [EOL] [EOL] for ngram in filter ( lambda token : token in self . _w2v . wv , self . create_ngrams ( tokens , nrange = ( [number] , [number] ) ) ) : [EOL] [EOL] parts = set ( ngram . split ( [string] ) ) [EOL] [EOL] if len ( parts & coverage ) != len ( parts ) : [EOL] coverage |= parts [EOL] yield ngram [EOL] [EOL] if len ( coverage ) == len ( tokens ) : [EOL] break [EOL] [EOL] def encode_document ( self , text ) : [EOL] [docstring] [EOL] tokens = self . create_valid_trigrams ( text ) [EOL] [EOL] yield from map ( self . get_word_vector , tokens ) [EOL] [EOL] def get_distance ( self , document1 , document2 ) : [EOL] [docstring] [EOL] return self . _w2v . wmdistance ( document1 , document2 ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Generator[builtins.str,None,None]$ 0 0 0 $typing.List[builtins.str]$ 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Generator[builtins.str,None,None]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Set$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 $typing.Set$ 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 $typing.Set$ 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Set$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Generator[numpy.ndarray,None,None]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Generator[builtins.str,None,None]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Generator[builtins.str,None,None]$ 0 0 0 0 $builtins.float$ 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0
from typing import List , Any , Sequence , Iterable , Generator , Set [EOL] import word_embedding [EOL] import builtins [EOL] import typing [EOL] import nlp_toolkit [EOL] import logging [EOL] [docstring] [EOL] import logging [EOL] from typing import Generator , Iterable , List , Sequence , Set [EOL] [EOL] import nltk [EOL] from nltk import word_tokenize [EOL] from tqdm import tqdm [EOL] [EOL] from . utils import _preprocess_arabic_text , setup_logger [EOL] from . word_embedding import WordEmbedding [EOL] [EOL] LOGGER = setup_logger ( [string] , logging . DEBUG ) [EOL] [EOL] nltk . download ( [string] ) [EOL] [EOL] [EOL] class WMDSimilarityClustering : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , stop_words , word_embeddings ) : [EOL] [docstring] [EOL] self . stop_words = stop_words [EOL] self . word_embeddings = word_embeddings [EOL] [EOL] def preprocess_document ( self , document ) : [EOL] [docstring] [EOL] return _preprocess_arabic_text ( document , remove_non_arabic = True , remove_punctuation = True , remove_numbers = True , remove_emails_urls_html = True , remove_hashtags_mentions = True ) [EOL] [EOL] def preprocess_documents ( self , documents ) : [EOL] [docstring] [EOL] progress = tqdm ( total = len ( documents ) ) [EOL] [EOL] LOGGER . info ( [string] ) [EOL] for document in documents : [EOL] [EOL] if document != [string] : [EOL] yield document [EOL] [EOL] progress . update ( ) [EOL] [EOL] LOGGER . info ( [string] ) [EOL] [EOL] def tokenize ( self , document ) : [EOL] [docstring] [EOL] tokens = word_tokenize ( document ) [EOL] return [ token for token in tokens if token not in self . stop_words ] [EOL] [EOL] def fit ( self , documents , preprocess ) : [EOL] [docstring] [EOL] documents_iter = documents if not preprocess else self . preprocess_documents ( documents ) [EOL] [EOL] documents_tokens = [ ] [EOL] [EOL] for document in documents_iter : [EOL] documents_tokens . append ( self . tokenize ( document ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Set[builtins.str]$ 0 $nlp_toolkit.tools.word_embedding.WordEmbedding$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 $typing.Set[builtins.str]$ 0 0 0 $nlp_toolkit.tools.word_embedding.WordEmbedding$ 0 $nlp_toolkit.tools.word_embedding.WordEmbedding$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Generator[builtins.str,None,None]$ 0 0 0 $typing.Sequence[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Sequence[builtins.str]$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $typing.Sequence[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Sequence[builtins.str]$ 0 $builtins.bool$ 0 0 0 0 0 $typing.Iterable$ 0 $typing.Sequence[builtins.str]$ 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Sequence[builtins.str]$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.Iterable$ 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0