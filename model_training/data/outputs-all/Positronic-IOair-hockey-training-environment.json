from typing import Tuple , Any [EOL] import builtins [EOL] import environment [EOL] import typing [EOL] [docstring] [EOL] import json [EOL] from typing import Any , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment . table import Table [EOL] from environment import config [EOL] from environment . goal import Goal [EOL] from environment . mallet import Mallet [EOL] [EOL] [EOL] class Puck : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , x , y , dx = - [number] , dy = [number] ) : [EOL] [docstring] [EOL] [EOL] self . name = [string] [EOL] self . radius = config . puck [ [string] ] [EOL] [EOL] [comment] [EOL] self . x = x [EOL] self . y = y [EOL] [EOL] [comment] [EOL] self . last_x = x [EOL] self . last_y = y [EOL] [EOL] [comment] [EOL] self . dx = dx [comment] [EOL] self . dy = dy [EOL] [EOL] [comment] [EOL] self . mass = config . puck [ [string] ] [EOL] self . imass = [number] / self . mass [comment] [EOL] [EOL] [comment] [EOL] self . puck_start_x = self . x [EOL] self . puck_start_y = self . y [EOL] [EOL] [comment] [EOL] self . default_speed = config . puck [ [string] ] [EOL] [EOL] def update ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if self . x <= self . radius : [EOL] self . x = self . radius [EOL] self . dx *= - [number] [EOL] elif self . x >= config . table [ [string] ] [ [number] ] - self . radius : [EOL] self . x = config . table [ [string] ] [ [number] ] - self . radius [EOL] self . dx *= - [number] [EOL] [EOL] if self . y <= self . radius : [EOL] self . y = self . radius [EOL] self . dy *= - [number] [EOL] elif self . y >= config . table [ [string] ] [ [number] ] - self . radius : [EOL] self . y = config . table [ [string] ] [ [number] ] - self . radius [EOL] self . dy *= - [number] [EOL] [EOL] [comment] [EOL] self . last_x = self . x [EOL] self . last_y = self . y [EOL] [EOL] [comment] [EOL] self . x += self . dx [EOL] self . y += self . dy [EOL] [EOL] return None [EOL] [EOL] def friction_on_puck ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if self . dx > [number] : [EOL] self . dx -= [number] [EOL] if self . dx < - [number] : [EOL] self . dx += [number] [EOL] [EOL] [comment] [EOL] if self . dy > [number] : [EOL] self . dy -= [number] [EOL] elif self . dy < - [number] : [EOL] self . dy += [number] [EOL] [EOL] return None [EOL] [EOL] def limit_puck_speed ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if self . dx > self . default_speed : [EOL] self . dx = self . default_speed [EOL] if self . dx < - self . default_speed : [EOL] self . dx = - self . default_speed [EOL] [EOL] [comment] [EOL] if self . dy > self . default_speed : [EOL] self . dy = self . default_speed [EOL] if self . dy < - self . default_speed : [EOL] self . dy = self . default_speed [EOL] [EOL] [comment] [EOL] self . last_x = self . x [EOL] self . last_y = self . y [EOL] [EOL] return None [EOL] [EOL] def reset ( self ) : [EOL] [docstring] [EOL] self . x = self . puck_start_x [EOL] self . y = self . puck_start_y [EOL] self . dx = np . random . uniform ( - [number] , [number] ) [EOL] self . dy = np . random . uniform ( - [number] , [number] ) [EOL] [EOL] return None [EOL] [EOL] def location ( self ) : [EOL] [docstring] [EOL] [EOL] return int ( self . x ) , int ( self . y ) [EOL] [EOL] def prev_location ( self ) : [EOL] [docstring] [EOL] [EOL] return int ( self . last_x ) , int ( self . last_y ) [EOL] [EOL] def velocity ( self ) : [EOL] [docstring] [EOL] [EOL] return self . dx , self . dy [EOL] [EOL] def __and__ ( self , mallet ) : [EOL] [docstring] [EOL] [EOL] distance = np . sqrt ( ( self . x - mallet . x ) ** [number] + ( self . y - mallet . y ) ** [number] ) [EOL] radius = self . radius + mallet . radius [EOL] [EOL] [comment] [EOL] if distance < radius : [EOL] return True [EOL] [EOL] [comment] [EOL] return False [EOL] [EOL] def __lshift__ ( self , table ) : [EOL] [docstring] [EOL] [EOL] if abs ( self . x - table . left_wall ) < self . radius : [EOL] return True [EOL] [EOL] [comment] [EOL] return False [EOL] [EOL] def __rshift__ ( self , table ) : [EOL] [docstring] [EOL] [EOL] if abs ( self . x - table . right_wall ) < self . radius : [EOL] return True [EOL] [EOL] [comment] [EOL] return False [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $"Mallet"$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $"Mallet"$ 0 0 0 0 0 0 0 0 0 0 0 $"Mallet"$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $"Mallet"$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] from typing import Any [EOL] [EOL] [EOL] class Goal : [EOL] def __init__ ( self , x , y ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . x = x [EOL] self . y = y [EOL] [EOL] def __contains__ ( self , puck ) : [EOL] [EOL] if abs ( self . x - puck . x ) < [number] * puck . radius and abs ( self . y - puck . y ) < [number] : [EOL] return True [EOL] [EOL] return False [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Dict , Union [EOL] import typing [EOL] [docstring] [EOL] [EOL] [comment] [EOL] table = { [string] : ( [number] , [number] ) , [string] : ( [number] , [number] ) , [string] : [number] } [EOL] [EOL] [comment] [EOL] mallet = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } [EOL] [EOL] [comment] [EOL] puck = { [string] : [number] , [string] : [number] , [string] : [number] } [EOL] [EOL] [comment] [EOL] physics = { [string] : [number] , [string] : [number] , [string] : [number] } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . airhockey import AirHockey [EOL] from . goal import Goal [EOL] from . mallet import Mallet [EOL] from . puck import Puck [EOL] from . table import Table [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple [EOL] import typing [EOL] import environment [EOL] import builtins [EOL] [docstring] [EOL] import json [EOL] import random [EOL] from typing import Tuple [EOL] [EOL] from environment import config [EOL] from environment . table import Table [EOL] [EOL] [EOL] class Mallet : [EOL] def __init__ ( self , name , x , y , dx = [number] , dy = [number] , ** kwargs ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . name = name [EOL] [EOL] [comment] [EOL] self . x , self . y = x , y [EOL] [EOL] [comment] [EOL] self . last_x = self . x [EOL] self . last_y = self . y [EOL] [EOL] [comment] [EOL] self . mass = config . mallet [ [string] ] [EOL] self . imass = [number] / self . mass [comment] [EOL] self . radius = config . mallet [ [string] ] [EOL] [EOL] [comment] [EOL] self . table = Table ( ) [EOL] [EOL] [comment] [EOL] self . u_lim = self . radius [EOL] self . b_lim = self . table . size [ [number] ] - self . radius [EOL] [EOL] [comment] [EOL] if self . name == [string] : [EOL] self . left_lim = self . radius [EOL] self . right_lim = self . table . midpoints [ [number] ] - self . radius [EOL] elif self . name == [string] : [EOL] self . left_lim = self . table . midpoints [ [number] ] + self . radius [EOL] self . right_lim = self . table . size [ [number] ] - self . radius [EOL] else : [EOL] self . left_lim , self . right_lim = ( self . radius , self . table . size [ [number] ] - self . radius ) [EOL] [EOL] [comment] [EOL] self . dx = dx [EOL] self . dy = dx [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def update ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . last_x = self . x [EOL] self . last_y = self . y [EOL] [EOL] [comment] [EOL] self . x += self . dx [EOL] self . y += self . dy [EOL] [EOL] [comment] [EOL] if self . x < self . left_lim : [EOL] self . x = self . left_lim [EOL] elif self . x > self . right_lim : [EOL] self . x = self . right_lim [EOL] [EOL] if self . y < self . u_lim : [EOL] self . y = self . u_lim [EOL] elif self . y > self . b_lim : [EOL] self . y = self . b_lim [EOL] [EOL] return None [EOL] [EOL] def reset ( self ) : [EOL] [docstring] [EOL] [EOL] self . dx = [number] [EOL] self . dy = [number] [EOL] [EOL] return None [EOL] [EOL] def location ( self ) : [EOL] [docstring] [EOL] [EOL] return int ( self . x ) , int ( self . y ) [EOL] [EOL] def prev_location ( self ) : [EOL] [docstring] [EOL] [EOL] return int ( self . last_x ) , int ( self . last_y ) [EOL] [EOL] def velocity ( self ) : [EOL] [docstring] [EOL] [EOL] return self . dx , self . dy [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.table.Table$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] from typing import Tuple [EOL] [EOL] from environment import config [EOL] [EOL] [EOL] class Table : [EOL] def __init__ ( self ) : [EOL] [docstring] [EOL] [EOL] self . size = config . table [ [string] ] [EOL] self . midpoints = tuple ( map ( lambda x : int ( x / [number] ) , self . size ) ) [EOL] [EOL] self . left_wall = int ( config . table [ [string] ] [ [number] ] / [number] ) [EOL] self . right_wall = int ( self . size [ [number] ] - config . table [ [string] ] + config . table [ [string] ] [ [number] ] / [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import environment [EOL] [docstring] [EOL] from environment import AirHockey [EOL] [EOL] [EOL] class TestEnvironment : [EOL] def setup ( self ) : [EOL] pass [EOL] [EOL] def test_puck_update_location ( self ) : [EOL] env = AirHockey ( ) [EOL] [comment] [EOL] env . update_state ( ( [number] , [number] ) , [string] ) [EOL] env . update_state ( ( [number] , [number] ) , [string] ) [EOL] env . update_state ( ( [number] , [number] ) , [string] ) [EOL] env . update_state ( ( [number] , [number] ) , [string] ) [EOL] assert env . puck . location ( ) == ( [number] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.airhockey.AirHockey$ 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , List [EOL] import lib [EOL] import typing [EOL] [docstring] [EOL] import random [EOL] [EOL] from lib . buffer import MemoryBuffer [EOL] [EOL] [EOL] class TestMemoryBuffer : [EOL] def setup ( self ) : [EOL] random . seed ( [number] ) [EOL] [EOL] def test_memory_buffer_default ( self ) : [EOL] [docstring] [EOL] [EOL] defaults = ( [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ) [EOL] buffer = MemoryBuffer ( [number] , [ [number] , [number] ] ) [EOL] assert buffer . retreive ( ) == defaults [EOL] [EOL] def test_memory_buffer_sample ( self ) : [EOL] [docstring] [EOL] [EOL] output = [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] [EOL] [EOL] capacity = [number] [EOL] data = [ [ random . randint ( [number] , [number] ) , random . randint ( [number] , [number] ) ] for _ in range ( capacity ) ] [EOL] buffer = MemoryBuffer ( capacity ) [EOL] [EOL] for item in data : [EOL] buffer . append ( item ) [EOL] [EOL] assert buffer . sample ( [number] ) == output [EOL] [EOL] def test_memory_buffer_purge ( self ) : [EOL] [docstring] [EOL] [EOL] capacity = [number] [EOL] data = [ [ random . randint ( [number] , [number] ) , random . randint ( [number] , [number] ) ] for _ in range ( capacity ) ] [EOL] buffer = MemoryBuffer ( capacity ) [EOL] [EOL] for item in data : [EOL] buffer . append ( item ) [EOL] [EOL] buffer . purge ( ) [EOL] assert len ( buffer ) == [number] [EOL] [EOL] def test_memory_buffer_retrieve_average ( self ) : [EOL] [docstring] [EOL] [EOL] buffer = MemoryBuffer ( [number] ) [EOL] data = [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] for item in data : [EOL] buffer . append ( item ) [EOL] [EOL] assert buffer . retreive ( average = True ) == ( ( [number] , [number] ) , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.List[builtins.int],typing.List[builtins.int],typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 $typing.Tuple[typing.List[builtins.int],typing.List[builtins.int],typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 $lib.buffer.MemoryBuffer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import environment [EOL] [docstring] [EOL] from environment import Goal , Puck , Table [EOL] [EOL] [EOL] class TestGoal : [EOL] def setup ( self ) : [EOL] [comment] [EOL] self . table = Table ( ) [EOL] [EOL] [comment] [EOL] self . left_goal = Goal ( x = [number] , y = self . table . midpoints [ [number] ] ) [EOL] self . right_goal = Goal ( x = self . table . size [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] def test_puck_intersect_left_goal ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] puck = Puck ( x = self . table . midpoints [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] puck . x , puck . y = [number] , self . left_goal . y [EOL] assert puck in self . left_goal [EOL] [EOL] [comment] [EOL] puck . x , puck . y = [number] , self . left_goal . y - [number] [EOL] assert not ( puck in self . left_goal ) [EOL] [EOL] def test_puck_intersect_right_goal ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] puck = Puck ( x = self . table . midpoints [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . right_goal . x , self . right_goal . y [EOL] assert puck in self . right_goal [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . right_goal . x , self . right_goal . y - [number] [EOL] assert not ( puck in self . right_goal ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0
import environment [EOL] [docstring] [EOL] from environment import Goal , Mallet , Puck , Table [EOL] [EOL] [EOL] class TestPuck : [EOL] def setup ( self ) : [EOL] [comment] [EOL] self . table = Table ( ) [EOL] [EOL] [comment] [EOL] self . left_goal = Goal ( x = [number] , y = self . table . midpoints [ [number] ] ) [EOL] self . right_goal = Goal ( x = self . table . size [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] self . mallet_l = self . table . midpoints [ [number] ] - [number] , self . table . midpoints [ [number] ] [EOL] self . mallet_r = self . table . midpoints [ [number] ] + [number] , self . table . midpoints [ [number] ] [EOL] [EOL] def test_puck_intersect_mallet ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] puck = Puck ( x = self . table . midpoints [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] mallet = Mallet ( [string] , self . mallet_l [ [number] ] , self . mallet_l [ [number] ] , right_lim = self . table . midpoints [ [number] ] , table_size = self . table . size ) [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . mallet_l [ [number] ] - [number] , self . mallet_l [ [number] ] - [number] [EOL] assert puck & mallet [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . mallet_l [ [number] ] - [number] , self . mallet_l [ [number] ] - [number] [EOL] assert not puck & mallet [EOL] [EOL] def test_puck_intersect_left_wall ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] puck = Puck ( x = self . table . midpoints [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] puck . x , puck . y = [number] , [number] [EOL] assert puck << self . table [EOL] [EOL] [comment] [EOL] puck . x , puck . y = [number] , [number] [EOL] assert not puck << self . table [EOL] [EOL] def test_puck_intersect_right_wall ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] puck = Puck ( x = self . table . midpoints [ [number] ] , y = self . table . midpoints [ [number] ] ) [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . table . size [ [number] ] - [number] , [number] [EOL] assert puck >> self . table [EOL] [EOL] [comment] [EOL] puck . x , puck . y = self . table . size [ [number] ] - [number] , [number] [EOL] assert not puck >> self . table [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.mallet.Mallet$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 $environment.mallet.Mallet$ 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 $environment.mallet.Mallet$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 $environment.puck.Puck$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $environment.puck.Puck$ 0 0 0 0 0
	0
from typing import Tuple , List , Any , Union [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] import random [EOL] from collections import deque [EOL] from typing import Any , Deque , List , Tuple , Union [EOL] [EOL] from . types import Observation , State [EOL] [EOL] random . seed ( [number] ) [EOL] [EOL] [EOL] class MemoryBuffer : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , capacity , default = None ) : [EOL] [EOL] self . capacity = int ( capacity ) [EOL] self . buffer = deque ( maxlen = self . capacity ) [EOL] [EOL] [comment] [EOL] if default : [EOL] for _ in range ( self . capacity ) : [EOL] self . append ( default ) [EOL] [EOL] def append ( self , state ) : [EOL] [docstring] [EOL] [EOL] self . buffer . appendleft ( state ) [EOL] [EOL] if len ( self . buffer ) > self . capacity : [EOL] self . buffer . pop ( ) [EOL] [EOL] assert len ( self . buffer ) < self . capacity + [number] , [string] [EOL] [EOL] def retreive ( self , average = False ) : [EOL] [docstring] [EOL] [EOL] def buffer_average ( data ) : [EOL] return tuple ( int ( sum ( col ) / len ( data ) ) for col in zip ( * data ) ) [EOL] [EOL] [comment] [EOL] if average : [EOL] retval = buffer_average ( tuple ( self . buffer ) ) [EOL] return ( retval , ) [EOL] [EOL] return tuple ( self . buffer ) [EOL] [EOL] def sample ( self , batch_size ) : [EOL] [docstring] [EOL] [EOL] return random . sample ( self . buffer , batch_size ) [EOL] [EOL] def purge ( self , default = None ) : [EOL] [docstring] [EOL] [EOL] del self . buffer [EOL] self . buffer = deque ( maxlen = self . capacity ) [EOL] [EOL] [comment] [EOL] if default : [EOL] for _ in range ( self . capacity ) : [EOL] self . append ( default ) [EOL] [EOL] def __len__ ( self ) : [EOL] [docstring] [EOL] [EOL] return len ( self . buffer ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union["State","Observation"]]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Type [EOL] import typing [EOL] import lib [EOL] [docstring] [EOL] from collections import namedtuple [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] Action = Union [ int , str , Tuple [ int , int ] ] [EOL] State = namedtuple ( [string] , [ [string] , [string] , [string] , [string] ] ) [EOL] Observation = namedtuple ( [string] , [ [string] , [string] , [string] , [string] , [string] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[lib.types.State]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[lib.types.Observation]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Type [EOL] import builtins [EOL] import lib [EOL] import typing [EOL] [docstring] [EOL] from environment import AirHockey [EOL] from lib . agents import A2C , A2C_1 , DDQN , PPO , Dueling , QLearner , c51 [EOL] [EOL] [EOL] class Strategy : [EOL] strategies = { [string] : QLearner , [string] : DDQN , [string] : Dueling , [string] : c51 , [string] : A2C , [string] : A2C_1 , [string] : PPO } [EOL] [EOL] def __init__ ( self ) : [EOL] pass [EOL] [EOL] @ classmethod def make ( self , env , strategy , train = True ) : [EOL] return self . strategies . get ( strategy ) ( env , train ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] class ProjectNotFoundError ( Exception ) : [EOL] pass [EOL] [EOL] [EOL] class InvalidAgentError ( Exception ) : [EOL] pass [EOL] [EOL] [EOL] class StrategyNotFoundError ( Exception ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Dict , List , Any , Optional [EOL] import builtins [EOL] import csv [EOL] import logging [EOL] import typing [EOL] [docstring] [EOL] [EOL] import csv [EOL] import inspect [EOL] import glob [EOL] import json [EOL] import logging [EOL] import os [EOL] import shutil [EOL] import sys [EOL] from collections import namedtuple [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] from lib . utils . exceptions import ProjectNotFoundError , StrategyNotFoundError [EOL] [EOL] [comment] [EOL] logging . basicConfig ( level = logging . INFO ) [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] def get_runid ( path ) : [EOL] [docstring] [EOL] listing = glob . glob ( f"{ path } [string] " ) [EOL] runid = max ( [ [number] ] + [ int ( x . split ( [string] ) [ - [number] ] . split ( [string] ) [ [number] ] ) for x in listing ] ) + [number] [EOL] path = os . path . join ( path , str ( runid ) ) [EOL] os . makedirs ( path ) [EOL] return runid , path [EOL] [EOL] [EOL] def record_reward ( path ) : [EOL] [docstring] [EOL] [EOL] with open ( os . path . join ( path , [string] ) , [string] ) as file : [EOL] from lib import rewards [EOL] [EOL] file . write ( inspect . getsource ( rewards ) ) [EOL] [EOL] [EOL] def record_model ( strategy , path ) : [EOL] [docstring] [EOL] [EOL] from lib . agents . a2c import model as a2c_model [EOL] from lib . agents . a2c_1 import model as a2c_1_model [EOL] from lib . agents . c51 import model as c51_model [EOL] from lib . agents . ddqn import model as ddqn_model [EOL] from lib . agents . dueling import model as dueling_model [EOL] from lib . agents . ppo import model as ppo_model [EOL] [EOL] strategies = { [string] : a2c_1_model , [string] : a2c_1_model , [string] : c51_model , [string] : ddqn_model , [string] : dueling_model , [string] : ppo_model , } [EOL] [EOL] try : [EOL] with open ( os . path . join ( path , [string] ) , [string] ) as file : [EOL] file . write ( inspect . getsource ( strategies [ strategy ] ) ) [comment] [EOL] except KeyError : [EOL] logger . error ( f" [string] { strategy } [string] " ) [EOL] raise StrategyNotFoundError ( f" [string] { strategy } [string] " ) [EOL] [EOL] [EOL] def record_data ( strategy , path = [string] ) : [EOL] [docstring] [EOL] [comment] [EOL] path = path or os . getenv ( [string] ) [EOL] [EOL] record_model ( strategy , path ) [EOL] record_reward ( path ) [EOL] return None [EOL] [EOL] [EOL] def record_data_csv ( name , payload ) : [EOL] [docstring] [EOL] path = os . getenv ( [string] ) [EOL] if not path : [EOL] raise ProjectNotFoundError [EOL] [EOL] with open ( os . path . join ( path , f"{ name } [string] " ) , [string] ) as file : [EOL] fieldnames = payload . keys ( ) [EOL] writer = csv . DictWriter ( file , fieldnames = fieldnames ) [EOL] writer . writerow ( payload ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import numpy as np [EOL] from keras import activations [EOL] from keras import backend as K [EOL] from keras import constraints , initializers , regularizers [EOL] from keras . layers import Layer [EOL] [EOL] [EOL] class NoisyDense ( Layer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , units , sigma_init = [number] , activation = None , use_bias = True , kernel_initializer = [string] , bias_initializer = [string] , kernel_regularizer = None , bias_regularizer = None , activity_regularizer = None , kernel_constraint = None , bias_constraint = None , ** kwargs ) : [EOL] if [string] not in kwargs and [string] in kwargs : [EOL] kwargs [ [string] ] = ( kwargs . pop ( [string] ) , ) [EOL] super ( NoisyDense , self ) . __init__ ( ** kwargs ) [EOL] self . units = units [EOL] self . sigma_init = sigma_init [EOL] self . activation = activations . get ( activation ) [EOL] self . use_bias = use_bias [EOL] self . kernel_initializer = initializers . get ( kernel_initializer ) [EOL] self . bias_initializer = initializers . get ( bias_initializer ) [EOL] self . kernel_regularizer = regularizers . get ( kernel_regularizer ) [EOL] self . bias_regularizer = regularizers . get ( bias_regularizer ) [EOL] self . activity_regularizer = regularizers . get ( activity_regularizer ) [EOL] self . kernel_constraint = constraints . get ( kernel_constraint ) [EOL] self . bias_constraint = constraints . get ( bias_constraint ) [EOL] [EOL] def build ( self , input_shape ) : [EOL] assert len ( input_shape ) >= [number] [EOL] self . input_dim = input_shape [ - [number] ] [EOL] [EOL] self . kernel = self . add_weight ( shape = ( self . input_dim , self . units ) , initializer = self . kernel_initializer , name = [string] , regularizer = self . kernel_regularizer , constraint = self . kernel_constraint , ) [EOL] [EOL] self . sigma_kernel = self . add_weight ( shape = ( self . input_dim , self . units ) , initializer = initializers . Constant ( value = self . sigma_init ) , name = [string] , ) [EOL] [EOL] if self . use_bias : [EOL] self . bias = self . add_weight ( shape = ( self . units , ) , initializer = self . bias_initializer , name = [string] , regularizer = self . bias_regularizer , constraint = self . bias_constraint , ) [EOL] self . sigma_bias = self . add_weight ( shape = ( self . units , ) , initializer = initializers . Constant ( value = self . sigma_init ) , name = [string] ) [EOL] else : [EOL] self . bias = None [EOL] self . epsilon_bias = None [EOL] [EOL] self . epsilon_kernel = K . zeros ( shape = ( self . input_dim , self . units ) ) [EOL] self . epsilon_bias = K . zeros ( shape = ( self . units , ) ) [EOL] [EOL] self . sample_noise ( ) [EOL] super ( NoisyDense , self ) . build ( input_shape ) [EOL] [EOL] def call ( self , X ) : [EOL] perturbation = self . sigma_kernel * self . epsilon_kernel [EOL] perturbed_kernel = self . kernel + perturbation [EOL] output = K . dot ( X , perturbed_kernel ) [EOL] if self . use_bias : [EOL] bias_perturbation = self . sigma_bias * self . epsilon_bias [EOL] perturbed_bias = self . bias + bias_perturbation [EOL] output = K . bias_add ( output , perturbed_bias ) [EOL] if self . activation is not None : [EOL] output = self . activation ( output ) [EOL] return output [EOL] [EOL] def compute_output_shape ( self , input_shape ) : [EOL] assert input_shape and len ( input_shape ) >= [number] [EOL] assert input_shape [ - [number] ] [EOL] output_shape = list ( input_shape ) [EOL] output_shape [ - [number] ] = self . units [EOL] return tuple ( output_shape ) [EOL] [EOL] def sample_noise ( self ) : [EOL] K . set_value ( self . epsilon_kernel , np . random . normal ( [number] , [number] , ( self . input_dim , self . units ) ) ) [EOL] K . set_value ( self . epsilon_bias , np . random . normal ( [number] , [number] , ( self . units , ) ) ) [EOL] [EOL] def remove_noise ( self ) : [EOL] K . set_value ( self . epsilon_kernel , np . zeros ( shape = ( self . input_dim , self . units ) ) ) [EOL] K . set_value ( self . epsilon_bias , np . zeros ( shape = self . units ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Any [EOL] [EOL] import numpy as np [EOL] import tensorflow as tf [EOL] [EOL] from lib . types import State [EOL] [EOL] [EOL] def huber_loss ( y_true , y_pred , clip_delta = [number] ) : [EOL] [docstring] [EOL] error = y_true - y_pred [EOL] cond = tf . keras . backend . abs ( error ) < clip_delta [EOL] [EOL] squared_loss = [number] * tf . keras . backend . square ( error ) [EOL] linear_loss = clip_delta * ( tf . keras . backend . abs ( error ) - [number] * clip_delta ) [EOL] return tf . where ( cond , squared_loss , linear_loss ) [EOL] [EOL] [EOL] def gaussian ( x , mu , sigma ) : [EOL] [docstring] [EOL] return tf . keras . backend . exp ( - tf . keras . backend . power ( x - mu , [number] ) / ( [number] * tf . keras . backend . power ( sigma , [number] ) ) ) [EOL] [EOL] [EOL] def proximal_policy_optimization_loss ( advantage , old_prediction ) : [EOL] def loss ( y_true , y_pred ) : [EOL] LOSS_CLIPPING = [number] [comment] [EOL] ENTROPY_LOSS = [number] [EOL] [EOL] prob = y_true * y_pred [EOL] old_prob = y_true * old_prediction [EOL] r = prob / ( old_prob + [number] ) [EOL] return - tf . keras . backend . mean ( tf . keras . backend . minimum ( r * advantage , tf . keras . backend . clip ( r , min_value = [number] - LOSS_CLIPPING , max_value = [number] + LOSS_CLIPPING ) * advantage , ) + ENTROPY_LOSS * - ( prob * tf . keras . backend . log ( prob + [number] ) ) ) [EOL] [EOL] return loss [EOL] [EOL] [EOL] def proximal_policy_optimization_loss_continuous ( advantage , old_prediction ) : [EOL] def loss ( y_true , y_pred ) : [EOL] LOSS_CLIPPING = [number] [comment] [EOL] NOISE = [number] [comment] [EOL] [EOL] var = tf . keras . backend . square ( NOISE ) [EOL] denom = tf . keras . backend . sqrt ( [number] * np . pi * var ) [EOL] prob_num = tf . keras . backend . exp ( - tf . keras . backend . square ( y_true - y_pred ) / ( [number] * var ) ) [EOL] old_prob_num = tf . keras . backend . exp ( - tf . keras . backend . square ( y_true - old_prediction ) / ( [number] * var ) ) [EOL] [EOL] prob = prob_num / denom [EOL] old_prob = old_prob_num / denom [EOL] r = prob / ( old_prob + [number] ) [EOL] [EOL] return - tf . keras . backend . mean ( tf . keras . backend . minimum ( r * advantage , tf . keras . backend . clip ( r , min_value = [number] - LOSS_CLIPPING , max_value = [number] + LOSS_CLIPPING ) * advantage , ) ) [EOL] [EOL] return loss [EOL] [EOL] [EOL] def serialize_state ( state , dim = [number] ) : [EOL] [docstring] [EOL] [EOL] s = np . asarray ( state ) . flatten ( ) [EOL] if dim == [number] : [EOL] expanded = np . expand_dims ( np . expand_dims ( s , axis = [number] ) , axis = [number] ) [EOL] else : [EOL] expanded = np . expand_dims ( s , axis = [number] ) [EOL] [EOL] return expanded [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . connect import RedisConnection [EOL] from . noisy_dense import NoisyDense	0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] [EOL] import json [EOL] from abc import ABC , abstractmethod , abstractstaticmethod [EOL] from typing import Any , Dict , List , Union [EOL] [EOL] import requests [EOL] from redis import ConnectionError , StrictRedis [EOL] [EOL] [EOL] class RedisConnection : [EOL] def __init__ ( self ) : [EOL] self . redis = StrictRedis ( ) [EOL] self . p = self . redis . pubsub ( ignore_subscribe_messages = True ) [EOL] [EOL] def get ( self , key = [string] ) : [EOL] [docstring] [EOL] try : [EOL] obj = self . redis . get ( key ) [EOL] except ConnectionError : [EOL] print ( f" [string] { key } [string] " ) [EOL] return [string] [EOL] return { key : json . loads ( obj ) } [EOL] [EOL] def post ( self , payload ) : [EOL] [docstring] [EOL] [EOL] for key , value in payload . items ( ) : [EOL] self . redis . set ( key , json . dumps ( value ) ) [EOL] [EOL] def publish ( self , channel , payload = True ) : [EOL] [docstring] [EOL] self . redis . publish ( channel , json . dumps ( payload ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0
from typing import Tuple , Any [EOL] import builtins [EOL] import numpy [EOL] import typing [EOL] [docstring] [EOL] import numpy as np [EOL] from typing import Tuple [EOL] [EOL] [EOL] [EOL] class SoftmaxPolicy : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , action_size ) : [EOL] self . action_size = action_size [EOL] [EOL] def step ( self , probs , train = True ) : [EOL] [docstring] [EOL] return np . random . choice ( self . action_size , p = np . nan_to_num ( probs ) ) [EOL] [EOL] [EOL] class EpsilonGreedy : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , action_size , ** kwargs ) : [EOL] self . actions = action_size [EOL] self . epsilon = kwargs . get ( [string] , [number] ) [EOL] self . initial_epsilon = kwargs . get ( [string] , [number] ) [EOL] self . final_epsilon = kwargs . get ( [string] , [number] ) [EOL] self . observe = kwargs . get ( [string] , [number] ) [EOL] self . explore = kwargs . get ( [string] , [number] ) [EOL] self . t = [number] [EOL] [EOL] def decrease ( self ) : [EOL] [docstring] [EOL] [EOL] if self . epsilon > self . final_epsilon and self . t % self . observe == [number] : [EOL] self . epsilon -= ( self . initial_epsilon - self . final_epsilon ) / self . explore [EOL] [EOL] def step ( self , q_values ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . t += [number] [EOL] [EOL] [comment] [EOL] self . decrease ( ) [EOL] [EOL] if np . random . uniform ( [number] , [number] ) < self . epsilon : [EOL] return np . random . randint ( [number] , self . actions ) [EOL] return np . argmax ( q_values ) [EOL] [EOL] [EOL] class BoltzmannQPolicy : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , action_size , tau = [number] , clip = ( - [number] , [number] ) ) : [EOL] self . action_size = action_size [EOL] self . tau = tau [EOL] self . clip = clip [EOL] [EOL] def step ( self , q_values ) : [EOL] [docstring] [EOL] [EOL] exp_values = np . exp ( np . clip ( q_values / self . tau , self . clip [ [number] ] , self . clip [ [number] ] ) ) [EOL] probs = exp_values / np . sum ( exp_values ) [EOL] return np . random . choice ( self . action_size , p = np . nan_to_num ( probs ) ) [EOL] [EOL] [EOL] class MaxBoltzmannQPolicy : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , action_size , ** kwargs ) : [EOL] self . action_size = action_size [EOL] self . tau = kwargs . get ( [string] , [number] ) [EOL] self . clip = kwargs . get ( [string] , ( - [number] , [number] ) ) [EOL] self . epsilon = kwargs . get ( [string] , [number] ) [EOL] self . initial_epsilon = kwargs . get ( [string] , [number] ) [EOL] self . final_epsilon = kwargs . get ( [string] , [number] ) [EOL] self . observe = kwargs . get ( [string] , [number] ) [EOL] self . explore = kwargs . get ( [string] , [number] ) [EOL] self . t = [number] [EOL] [EOL] def decrease ( self ) : [EOL] [docstring] [EOL] [EOL] if self . epsilon > self . final_epsilon and self . t % self . observe == [number] : [EOL] self . epsilon -= ( self . initial_epsilon - self . final_epsilon ) / self . explore [EOL] [EOL] def step ( self , q_values ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . t += [number] [EOL] [EOL] [comment] [EOL] self . decrease ( ) [EOL] [EOL] if np . random . uniform ( [number] , [number] ) < self . epsilon : [EOL] exp_values = np . exp ( np . clip ( q_values / self . tau , self . clip [ [number] ] , self . clip [ [number] ] ) ) [EOL] probs = exp_values / np . sum ( exp_values ) [EOL] return np . random . choice ( self . action_size , p = np . nan_to_num ( probs ) ) [EOL] [EOL] return np . argmax ( q_values ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . strategies import * [EOL] from . random import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] [docstring] [EOL] import numpy as np [EOL] [EOL] [EOL] class RandomProcess ( object ) : [EOL] def reset_states ( self ) : [EOL] pass [EOL] [EOL] [EOL] class AnnealedGaussianProcess ( RandomProcess ) : [EOL] def __init__ ( self , mu , sigma , sigma_min , n_steps_annealing ) : [EOL] self . mu = mu [EOL] self . sigma = sigma [EOL] self . n_steps = [number] [EOL] [EOL] if sigma_min is not None : [EOL] self . m = - float ( sigma - sigma_min ) / float ( n_steps_annealing ) [EOL] self . c = sigma [EOL] self . sigma_min = sigma_min [EOL] else : [EOL] self . m = [number] [EOL] self . c = sigma [EOL] self . sigma_min = sigma [EOL] [EOL] @ property def current_sigma ( self ) : [EOL] sigma = max ( self . sigma_min , self . m * float ( self . n_steps ) + self . c ) [EOL] return sigma [EOL] [EOL] [EOL] class GaussianWhiteNoiseProcess ( AnnealedGaussianProcess ) : [EOL] def __init__ ( self , mu = [number] , sigma = [number] , sigma_min = None , n_steps_annealing = [number] , size = [number] ) : [EOL] super ( GaussianWhiteNoiseProcess , self ) . __init__ ( mu = mu , sigma = sigma , sigma_min = sigma_min , n_steps_annealing = n_steps_annealing ) [EOL] self . size = size [EOL] [EOL] def sample ( self ) : [EOL] sample = np . random . normal ( self . mu , self . current_sigma , self . size ) [EOL] self . n_steps += [number] [EOL] return sample [EOL] [EOL] [EOL] class OrnsteinUhlenbeckProcess ( AnnealedGaussianProcess ) : [EOL] [comment] [EOL] def __init__ ( self , theta , mu = [number] , sigma = [number] , dt = [number] , size = [number] , sigma_min = None , n_steps_annealing = [number] ) : [EOL] super ( OrnsteinUhlenbeckProcess , self ) . __init__ ( mu = mu , sigma = sigma , sigma_min = sigma_min , n_steps_annealing = n_steps_annealing ) [EOL] self . theta = theta [EOL] self . mu = mu [EOL] self . dt = dt [EOL] self . size = size [EOL] self . reset_states ( ) [EOL] [EOL] def sample ( self ) : [EOL] x = ( self . x_prev + self . theta * ( self . mu - self . x_prev ) * self . dt + self . current_sigma * np . sqrt ( self . dt ) * np . random . normal ( size = self . size ) ) [EOL] self . x_prev = x [EOL] self . n_steps += [number] [EOL] return x [EOL] [EOL] def reset_states ( self ) : [EOL] self . x_prev = np . random . normal ( self . mu , self . current_sigma , self . size ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . agent import Agent [EOL] from . a2c import A2C [EOL] from . a2c_1 import A2C_1 [EOL] from . c51 import c51 [EOL] from . ddqn import DDQN [EOL] from . dueling import Dueling [EOL] from . ppo import PPO [EOL] from . qlearner import QLearner [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Union [EOL] import typing [EOL] import builtins [EOL] import numpy [EOL] import lib [EOL] import environment [EOL] [docstring] [EOL] import os [EOL] from typing import Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . rewards import Rewards [EOL] from lib . types import Action , Observation , State [EOL] from lib . utils . exceptions import ProjectNotFoundError [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [EOL] class Agent : [EOL] def __init__ ( self , env ) : [EOL] self . env = env [EOL] self . reward = [number] [EOL] self . done = False [EOL] self . name = [string] [EOL] self . reward_tracker = Rewards ( self . name , self . env . left_goal , self . env . right_goal , self . env . table ) [EOL] self . path = os . getenv ( [string] ) [EOL] [EOL] if not self . path : [EOL] raise ProjectNotFoundError [EOL] [EOL] def model_path ( self , strategy = [string] ) : [EOL] [docstring] [EOL] if os . getenv ( [string] ) : [comment] [EOL] return os . path . join ( [string] , [string] , [string] , [string] , str ( os . getenv ( [string] ) ) ) , True [EOL] return os . path . abspath ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , strategy ) ) , False [EOL] [EOL] def move ( self , action ) : [EOL] [docstring] [EOL] [EOL] action = int ( action ) if isinstance ( action , np . int64 ) else action [EOL] return self . env . update_state ( action = action , agent_name = self . name ) [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def get_action ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] state = self . env . get_state ( agent_name = self . name ) [EOL] action = self . _get_action ( serialize_state ( state ) ) [EOL] return action [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def update ( self , data ) : [EOL] pass [EOL] [EOL] def step ( self , action ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] state = self . env . get_state ( agent_name = self . name ) [EOL] [EOL] [comment] [EOL] self . move ( action ) [EOL] [EOL] [comment] [EOL] new_state = self . env . get_state ( agent_name = self . name ) [EOL] [EOL] [comment] [EOL] mallet = self . env . robot if self . name == [string] else self . env . opponent [EOL] reward , score , done = self . reward_tracker ( self . env . puck , mallet ) [EOL] [EOL] [comment] [EOL] observation = Observation ( state = state , action = action , reward = reward , done = done , new_state = new_state ) [EOL] self . update ( observation ) [EOL] [EOL] [comment] [EOL] return score , observation [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , List , Any , Union [EOL] import builtins [EOL] import lib [EOL] import logging [EOL] import typing [EOL] [docstring] [EOL] import imp [EOL] import logging [EOL] import math [EOL] import os [EOL] from typing import Any , Dict , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . agents import Agent [EOL] from lib . buffer import MemoryBuffer [EOL] from lib . exploration import EpsilonGreedy [EOL] from lib . types import Observation , State [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [comment] [EOL] np . random . seed ( [number] ) [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] class c51 ( Agent ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , train ) : [EOL] super ( ) . __init__ ( env ) [EOL] [EOL] logger . info ( f" [string] { self . name } [string] { self . __repr__ ( ) }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . state_size = ( [number] , [number] ) [EOL] self . action_size = [number] [EOL] [EOL] [comment] [EOL] path , to_load = self . model_path ( [string] ) [EOL] model = imp . load_source ( [string] , os . path . join ( path , [string] ) ) [EOL] [EOL] [comment] [EOL] self . config = model . config ( ) [EOL] [EOL] [comment] [EOL] self . gamma = self . config [ [string] ] [ [string] ] [EOL] self . learning_rate = self . config [ [string] ] [ [string] ] [EOL] self . batch_size = self . config [ [string] ] [ [string] ] [EOL] self . frame_per_action = self . config [ [string] ] [ [string] ] [EOL] self . update_target_freq = self . config [ [string] ] [ [string] ] [EOL] self . timestep_per_train = self . config [ [string] ] [ [string] ] [EOL] self . timestep_per_train = self . config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . num_atoms = self . config [ [string] ] [ [string] ] [comment] [EOL] self . v_max = self . config [ [string] ] [ [string] ] [comment] [EOL] self . v_min = self . config [ [string] ] [ [string] ] [EOL] self . delta_z = ( self . v_max - self . v_min ) / float ( self . num_atoms - [number] ) [EOL] self . z = [ self . v_min + i * self . delta_z for i in range ( self . num_atoms ) ] [EOL] [EOL] [comment] [EOL] self . model = model . create ( self . state_size , self . action_size , self . num_atoms , self . learning_rate ) [EOL] self . target_model = model . create ( self . state_size , self . action_size , self . num_atoms , self . learning_rate ) [EOL] [EOL] [comment] [EOL] self . param_noise = True [EOL] [EOL] if to_load : [EOL] try : [EOL] logger . info ( f" [string] { path } [string] " ) [EOL] self . model . load_weights ( os . path . join ( path , [string] ) ) [EOL] except OSError : [EOL] logger . info ( [string] ) [EOL] pass [comment] [EOL] [EOL] [comment] [EOL] self . transfer_weights ( ) [EOL] logger . info ( self . model . summary ( ) ) [EOL] [EOL] [comment] [EOL] self . max_memory = self . config [ [string] ] [ [string] ] [EOL] self . memory = MemoryBuffer ( self . max_memory ) [EOL] [EOL] [comment] [EOL] self . train = train [EOL] [EOL] [comment] [EOL] self . epochs = self . config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . t = [number] [EOL] [EOL] [comment] [EOL] self . exploration_strategy = EpsilonGreedy ( action_size = self . action_size ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return [string] [EOL] [EOL] def transfer_weights ( self ) : [EOL] [docstring] [EOL] [EOL] if self . param_noise : [EOL] tau = np . random . uniform ( [number] , [number] ) [EOL] W , target_W = self . model . get_weights ( ) , self . target_model . get_weights ( ) [EOL] for i in range ( len ( W ) ) : [EOL] target_W [ i ] = tau * W [ i ] + ( [number] - tau ) * target_W [ i ] [EOL] self . target_model . set_weights ( target_W ) [EOL] return None [EOL] [EOL] self . target_model . set_weights ( self . model . get_weights ( ) ) [EOL] return None [EOL] [EOL] def update_target_model ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . update_target_freq == [number] : [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] self . transfer_weights ( ) [EOL] return None [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] z = self . model . predict ( serialize_state ( state ) ) [EOL] z_concat = np . vstack ( z ) [EOL] q_values = np . sum ( np . multiply ( z_concat , np . array ( self . z ) ) , axis = [number] ) [EOL] assert q_values . shape == ( self . action_size , ) , f" [string] { q_values . shape } [string] " [EOL] [EOL] return self . exploration_strategy . step ( q_values ) if self . train else np . argmax ( q_values ) [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . memory . append ( data ) [EOL] [EOL] [comment] [EOL] self . update_target_model ( ) [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . timestep_per_train == [number] : [EOL] [EOL] logger . info ( f" [string] " ) [EOL] [EOL] [comment] [EOL] num_samples = min ( self . batch_size , len ( self . memory ) ) [EOL] replay_samples = self . memory . sample ( num_samples ) [EOL] [EOL] [comment] [EOL] action = np . array ( [ sample [ [number] ] for sample in replay_samples ] , dtype = np . int32 ) [EOL] reward = np . array ( [ sample [ [number] ] for sample in replay_samples ] , dtype = np . float64 ) [EOL] done = np . array ( [ [number] if sample [ [number] ] else [number] for sample in replay_samples ] , dtype = np . int8 ) [EOL] [EOL] state_inputs = np . array ( [ serialize_state ( sample [ [number] ] , dim = [number] ) for sample in replay_samples ] ) [EOL] next_states = np . array ( [ serialize_state ( sample [ [number] ] , dim = [number] ) for sample in replay_samples ] ) [EOL] [EOL] assert state_inputs . shape == ( ( num_samples , ) + self . state_size ) , f" [string] { state_inputs . shape } [string] { ( ( num_samples , ) + self . state_size ) }" [EOL] assert next_states . shape == ( ( num_samples , ) + self . state_size ) , f" [string] { next_states . shape } [string] { ( ( num_samples , ) + self . state_size ) }" [EOL] [EOL] [comment] [EOL] m_prob = [ np . zeros ( ( num_samples , self . num_atoms ) ) for i in range ( self . action_size ) ] [EOL] [EOL] z = self . model . predict ( next_states ) [EOL] z_ = self . target_model . predict ( next_states ) [EOL] [EOL] [comment] [EOL] z_concat = np . vstack ( z ) [EOL] q = np . sum ( np . multiply ( z_concat , np . array ( self . z ) ) , axis = [number] ) [comment] [EOL] q = q . reshape ( ( num_samples , self . action_size ) , order = [string] ) [EOL] optimal_action_idxs = np . argmax ( q , axis = [number] ) [EOL] [EOL] [comment] [EOL] for i in range ( num_samples ) : [EOL] if done [ i ] : [comment] [EOL] [comment] [EOL] Tz = min ( self . v_max , max ( self . v_min , reward [ i ] ) ) [EOL] bj = ( Tz - self . v_min ) / self . delta_z [EOL] m_l , m_u = math . floor ( bj ) , math . ceil ( bj ) [EOL] m_prob [ action [ i ] ] [ i ] [ int ( m_l ) ] += m_u - bj [EOL] m_prob [ action [ i ] ] [ i ] [ int ( m_u ) ] += bj - m_l [EOL] else : [EOL] for j in range ( self . num_atoms ) : [EOL] Tz = min ( self . v_max , max ( self . v_min , reward [ i ] + self . gamma * self . z [ j ] ) ) [EOL] bj = ( Tz - self . v_min ) / self . delta_z [EOL] m_l , m_u = math . floor ( bj ) , math . ceil ( bj ) [EOL] m_prob [ action [ i ] ] [ i ] [ int ( m_l ) ] += z_ [ optimal_action_idxs [ i ] ] [ i ] [ j ] * ( m_u - bj ) [EOL] m_prob [ action [ i ] ] [ i ] [ int ( m_u ) ] += z_ [ optimal_action_idxs [ i ] ] [ i ] [ j ] * ( bj - m_l ) [EOL] [EOL] self . model . fit ( state_inputs , m_prob , batch_size = self . batch_size , epochs = self . epochs , verbose = [number] ) [EOL] [EOL] [comment] [EOL] if self . train and self . t % self . timestep_per_train == [number] : [EOL] self . save ( ) [EOL] [EOL] self . t += [number] [EOL] return None [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] logger . info ( f" [string] { self . path }" ) [EOL] path = os . path . join ( self . path , [string] ) [EOL] self . model . save_weights ( path , overwrite = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from . c51 import c51 [EOL]	0 0 0 0 0 0
from typing import Tuple , Dict , List , Any , Union [EOL] import typing [EOL] import keras [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple , Union , Dict [EOL] [EOL] from keras import backend as K [EOL] from keras . layers import ( BatchNormalization , Dense , Flatten , GaussianNoise , Input , Lambda , add , Dropout , LSTM , TimeDistributed , Activation , ) [EOL] from keras . models import Model , Sequential , load_model [EOL] from keras . optimizers import Adam , RMSprop [EOL] from lib . utils . helpers import huber_loss [EOL] from lib . utils . noisy_dense import NoisyDense [EOL] [EOL] [EOL] def config ( ) : [EOL] return { [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : - [number] , [string] : [number] , [string] : [number] , } } [EOL] [EOL] [EOL] def create ( state_size , action_size , num_atoms , learning_rate ) : [EOL] [docstring] [EOL] [EOL] state_input = Input ( shape = state_size ) [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( state_input ) [EOL] x = BatchNormalization ( ) ( x ) [EOL] [EOL] x = Flatten ( ) ( x ) [EOL] [EOL] distribution_list = list ( ) [EOL] for _ in range ( action_size ) : [EOL] x = Dense ( num_atoms , activation = [string] ) ( x ) [EOL] distribution_list . append ( x ) [EOL] [EOL] model = Model ( state_input , distribution_list ) [EOL] [EOL] model . compile ( loss = huber_loss , optimizer = Adam ( lr = learning_rate ) ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . a2c_1 import A2C_1 [EOL]	0 0 0 0 0 0
from typing import Tuple , List , Any , Union [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import _importlib_modulespec [EOL] import lib [EOL] [docstring] [EOL] import imp [EOL] import logging [EOL] import math [EOL] import os [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . agents import Agent [EOL] [EOL] from lib . buffer import MemoryBuffer [EOL] from lib . types import Observation , State [EOL] from lib . exploration import SoftmaxPolicy [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [comment] [EOL] np . random . seed ( [number] ) [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] class A2C ( Agent ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , train ) : [EOL] super ( ) . __init__ ( env ) [EOL] [EOL] logger . info ( f" [string] { self . name } [string] { self . __repr__ ( ) }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . state_size = ( [number] , [number] ) [EOL] self . action_size = [number] [EOL] self . value_size = [number] [EOL] [EOL] [comment] [EOL] path , to_load = self . model_path ( [string] ) [EOL] model = imp . load_source ( [string] , os . path . join ( path , [string] ) ) [EOL] [EOL] [comment] [EOL] config = model . config ( ) [EOL] self . batch_size = config [ [string] ] [ [string] ] [EOL] [EOL] self . frame_per_action = config [ [string] ] [ [string] ] [EOL] self . timestep_per_train = config [ [string] ] [ [string] ] [EOL] self . iterations_on_save = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . gamma = config [ [string] ] [ [string] ] [EOL] self . actor_lr = config [ [string] ] [ [string] ] [EOL] self . critic_lr = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . max_memory = config [ [string] ] [ [string] ] [EOL] self . memory = MemoryBuffer ( self . max_memory ) [EOL] [EOL] [comment] [EOL] self . epochs = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . actor_model , self . critic_model = model . create ( state_size = self . state_size , action_size = self . action_size , value_size = self . value_size , actor_learning_rate = self . actor_lr , critic_learning_rate = self . critic_lr , ) [EOL] [EOL] if to_load : [EOL] try : [EOL] logger . info ( f" [string] { path } [string] " ) [EOL] self . actor_model . load_weights ( os . path . join ( path , [string] ) ) [EOL] self . critic_model . load_weights ( os . path . join ( path , [string] ) ) [EOL] except OSError : [EOL] logger . info ( [string] ) [EOL] pass [comment] [EOL] [EOL] logger . info ( [string] ) [EOL] logger . info ( self . actor_model . summary ( ) ) [EOL] logger . info ( [string] ) [EOL] logger . info ( self . critic_model . summary ( ) ) [EOL] [EOL] [comment] [EOL] self . train = train [EOL] [EOL] [comment] [EOL] self . t = [number] [EOL] [EOL] [comment] [EOL] self . exploration_strategy = SoftmaxPolicy ( action_size = self . action_size ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return [string] [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] policy = self . actor_model . predict ( serialize_state ( state ) ) [ [number] ] [EOL] assert policy . shape == ( self . action_size , ) , f" [string] { policy . shape } [string] " [EOL] return self . exploration_strategy . step ( policy ) if self . train else np . argmax ( policy ) [EOL] [EOL] def discount_rewards ( self , rewards ) : [EOL] [docstring] [EOL] [EOL] discounted_r , cumul_r = np . zeros_like ( rewards ) , [number] [EOL] for t in reversed ( range ( [number] , len ( rewards ) ) ) : [EOL] cumul_r = rewards [ t ] + cumul_r * self . gamma [EOL] discounted_r [ t ] = cumul_r [EOL] return discounted_r [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] self . memory . append ( data ) [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . timestep_per_train == [number] : [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] observations = self . memory . retreive ( ) [EOL] states = np . array ( [ serialize_state ( observation . state , dim = [number] ) for observation in observations ] ) [EOL] actions = np . array ( [ observation . action for observation in observations ] ) [EOL] rewards = np . array ( [ observation . reward for observation in observations ] ) [EOL] episode_length = len ( self . memory ) [EOL] discounted_rewards = self . discount_rewards ( rewards ) [EOL] [EOL] [comment] [EOL] values = np . array ( self . critic_model . predict ( states ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] advantages = np . zeros ( ( episode_length , self . action_size ) ) [EOL] [EOL] for i in range ( episode_length ) : [EOL] advantages [ i ] [ actions [ i ] ] = discounted_rewards [ i ] - values [ i ] [EOL] [comment] [EOL] self . actor_model . fit ( states , advantages , epochs = self . epochs , verbose = [number] ) [EOL] self . critic_model . fit ( states , discounted_rewards , epochs = self . epochs , verbose = [number] ) [EOL] [EOL] [comment] [EOL] self . memory . purge ( ) [EOL] [EOL] [comment] [EOL] if self . train and self . t % self . timestep_per_train == [number] : [EOL] self . save ( ) [EOL] [EOL] self . t += [number] [EOL] return None [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] logger . info ( f" [string] { self . path }" ) [EOL] [EOL] [comment] [EOL] actor_path = os . path . join ( self . path , [string] ) [EOL] self . actor_model . save_weights ( actor_path , overwrite = True ) [EOL] [EOL] [comment] [EOL] critic_path = os . path . join ( self . path , [string] ) [EOL] self . critic_model . save_weights ( critic_path , overwrite = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from typing import Tuple , Any , Dict , Union [EOL] import typing [EOL] import keras [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple , Union , Dict [EOL] [EOL] from keras import backend as K [EOL] from keras . layers import ( LSTM , BatchNormalization , Dense , Dropout , Flatten , GaussianNoise , Input , Lambda , TimeDistributed , add , Activation , ) [EOL] from keras . models import Model , Sequential , load_model [EOL] from keras . optimizers import Adam , RMSprop [EOL] [EOL] from lib . utils . helpers import huber_loss [EOL] from lib . utils . noisy_dense import NoisyDense [EOL] [EOL] [EOL] def config ( ) : [EOL] [docstring] [EOL] return { [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } } [EOL] [EOL] [EOL] def create ( state_size , action_size , value_size , actor_learning_rate , critic_learning_rate , ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] actor = Sequential ( ) [EOL] [EOL] actor . add ( Dense ( state_size [ [number] ] , kernel_initializer = [string] , input_shape = state_size ) ) [EOL] actor . add ( Activation ( [string] ) ) [EOL] actor . add ( BatchNormalization ( ) ) [EOL] [EOL] actor . add ( Flatten ( ) ) [EOL] [EOL] actor . add ( Dense ( action_size , kernel_initializer = [string] ) ) [EOL] actor . add ( Activation ( [string] ) ) [EOL] [EOL] actor . compile ( loss = [string] , optimizer = Adam ( lr = actor_learning_rate , epsilon = [number] ) ) [EOL] [EOL] [comment] [EOL] critic = Sequential ( ) [EOL] [EOL] critic . add ( Dense ( state_size [ [number] ] , kernel_initializer = [string] , input_shape = state_size ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] critic . add ( BatchNormalization ( ) ) [EOL] [EOL] critic . add ( Flatten ( ) ) [EOL] [EOL] critic . add ( Dense ( value_size , kernel_initializer = [string] ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] [EOL] critic . compile ( loss = huber_loss , optimizer = Adam ( lr = critic_learning_rate , epsilon = [number] ) ) [EOL] [EOL] return actor , critic [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[keras.models.Model,keras.models.Model]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . a2c import A2C [EOL]	0 0 0 0 0 0
from typing import Tuple , Any , Dict , Union [EOL] import typing [EOL] import keras [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple , Union , Dict [EOL] [EOL] from keras import backend as K [EOL] from keras . layers import ( LSTM , BatchNormalization , Dense , Dropout , Flatten , GaussianNoise , Input , Lambda , TimeDistributed , add , Activation , ) [EOL] from keras . models import Model , Sequential , load_model [EOL] from keras . optimizers import Adam , RMSprop [EOL] [EOL] from lib . utils . helpers import huber_loss [EOL] from lib . utils . noisy_dense import NoisyDense [EOL] [EOL] [EOL] def config ( ) : [EOL] return { [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } } [EOL] [EOL] [EOL] def create ( state_size , action_size , learning_rate ) : [EOL] [docstring] [EOL] [EOL] state_input = Input ( shape = state_size ) [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( state_input ) [EOL] [comment] [EOL] [EOL] x = Flatten ( ) ( x ) [EOL] [EOL] [comment] [EOL] state_value = Dense ( state_size [ [number] ] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] state_value = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( state_value ) [EOL] state_value = Lambda ( lambda s : K . expand_dims ( s [ : , [number] ] ) , output_shape = ( action_size , ) ) ( state_value ) [EOL] [EOL] [comment] [EOL] action_advantage = Dense ( state_size [ [number] ] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] action_advantage = Dense ( action_size , kernel_initializer = [string] , activation = [string] ) ( action_advantage ) [EOL] action_advantage = Lambda ( lambda a : a [ : , : ] - K . mean ( a [ : , : ] , keepdims = True ) , output_shape = ( action_size , ) ) ( action_advantage ) [EOL] [EOL] [comment] [EOL] state_action_value = add ( [ state_value , action_advantage ] ) [EOL] [EOL] model = Model ( state_input , state_action_value ) [EOL] model . compile ( loss = huber_loss , optimizer = Adam ( lr = learning_rate ) ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . dueling import Dueling [EOL]	0 0 0 0 0 0
from typing import Tuple , List , Any , Union [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import lib [EOL] [docstring] [EOL] import imp [EOL] import logging [EOL] import os [EOL] from typing import Any , Dict , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . agents import Agent [EOL] from lib . buffer import MemoryBuffer [EOL] from lib . exploration import EpsilonGreedy [EOL] from lib . types import Observation , State [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [comment] [EOL] np . random . seed ( [number] ) [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] class Dueling ( Agent ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , train ) : [EOL] super ( ) . __init__ ( env ) [EOL] [EOL] logger . info ( f" [string] { self . name } [string] { self . __repr__ ( ) }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . state_size = ( [number] , [number] ) [EOL] self . action_size = [number] [EOL] [EOL] [comment] [EOL] self . train = train [EOL] [EOL] [comment] [EOL] path , to_load = self . model_path ( [string] ) [EOL] model = imp . load_source ( [string] , os . path . join ( path , [string] ) ) [EOL] [EOL] [comment] [EOL] config = model . config ( ) [EOL] self . gamma = config [ [string] ] [ [string] ] [EOL] self . learning_rate = config [ [string] ] [ [string] ] [EOL] self . batch_size = config [ [string] ] [ [string] ] [EOL] [EOL] self . frame_per_action = config [ [string] ] [ [string] ] [EOL] self . update_target_freq = config [ [string] ] [ [string] ] [EOL] self . timestep_per_train = config [ [string] ] [ [string] ] [EOL] self . iterations_on_save = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . max_memory = config [ [string] ] [ [string] ] [EOL] self . memory = MemoryBuffer ( self . max_memory ) [EOL] [EOL] [comment] [EOL] self . param_noise = True [EOL] [EOL] [comment] [EOL] self . model = model . create ( self . state_size , self . action_size , self . learning_rate ) [EOL] self . target_model = model . create ( self . state_size , self . action_size , self . learning_rate ) [EOL] [EOL] if to_load : [EOL] try : [EOL] logger . info ( f" [string] { path } [string] " ) [EOL] self . model . load_weights ( os . path . join ( path , [string] ) ) [EOL] except OSError : [EOL] logger . info ( [string] ) [EOL] pass [comment] [EOL] [EOL] [comment] [EOL] self . transfer_weights ( ) [EOL] logger . info ( self . model . summary ( ) ) [EOL] [EOL] [comment] [EOL] self . t = [number] [EOL] [EOL] [comment] [EOL] self . exploration_strategy = EpsilonGreedy ( action_size = self . action_size ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return [string] [EOL] [EOL] def transfer_weights ( self ) : [EOL] [docstring] [EOL] [EOL] if self . param_noise : [EOL] tau = np . random . uniform ( [number] , [number] ) [EOL] W , target_W = self . model . get_weights ( ) , self . target_model . get_weights ( ) [EOL] for i in range ( len ( W ) ) : [EOL] target_W [ i ] = tau * W [ i ] + ( [number] - tau ) * target_W [ i ] [EOL] self . target_model . set_weights ( target_W ) [EOL] return None [EOL] [EOL] self . target_model . set_weights ( self . model . get_weights ( ) ) [EOL] return None [EOL] [EOL] def update_target_model ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if self . t % self . update_target_freq == [number] : [EOL] [EOL] logger . info ( [string] ) [EOL] self . transfer_weights ( ) [EOL] [EOL] return None [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] q_values = self . model . predict ( serialize_state ( state ) ) . flatten ( ) [EOL] assert q_values . shape == ( self . action_size , ) , f" [string] { q_values . shape } [string] " [EOL] return self . exploration_strategy . step ( q_values ) if self . train else np . argmax ( q_values ) [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . memory . append ( data ) [EOL] [EOL] [comment] [EOL] self . update_target_model ( ) [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . timestep_per_train == [number] : [EOL] [EOL] logger . info ( f" [string] " ) [EOL] [EOL] [comment] [EOL] num_samples = min ( self . batch_size , len ( self . memory ) ) [EOL] replay_samples = self . memory . sample ( num_samples ) [EOL] [EOL] [comment] [EOL] action = np . array ( [ sample [ [number] ] for sample in replay_samples ] , dtype = np . int32 ) [EOL] reward = np . array ( [ sample [ [number] ] for sample in replay_samples ] , dtype = np . float64 ) [EOL] done = np . array ( [ [number] if sample [ [number] ] else [number] for sample in replay_samples ] , dtype = np . int8 ) [EOL] [EOL] update_input = np . array ( [ serialize_state ( sample [ [number] ] , dim = [number] ) for sample in replay_samples ] ) [EOL] update_target = np . array ( [ serialize_state ( sample [ [number] ] , dim = [number] ) for sample in replay_samples ] ) [EOL] [EOL] assert update_input . shape == ( ( num_samples , ) + self . state_size ) , f" [string] { update_input . shape } [string] { ( ( num_samples , ) + self . state_size ) }" [EOL] assert update_target . shape == ( ( num_samples , ) + self . state_size ) , f" [string] { update_target . shape } [string] { ( ( num_samples , ) + self . state_size ) }" [EOL] [EOL] target = self . model . predict ( update_input ) [EOL] target_val = self . model . predict ( update_target ) [EOL] target_val_ = self . target_model . predict ( update_target ) [EOL] [EOL] assert target . shape == ( ( num_samples , ) + ( [number] , self . action_size ) ) , f" [string] { target . shape }" [EOL] assert target_val . shape == ( ( num_samples , ) + ( [number] , self . action_size ) ) , f" [string] { target_val . shape }" [EOL] assert target_val_ . shape == ( ( num_samples , ) + ( [number] , self . action_size ) ) , f" [string] { target_val_ . shape }" [EOL] [EOL] for i in range ( num_samples ) : [EOL] [comment] [EOL] [comment] [EOL] if done [ i ] : [EOL] target [ i ] [ [number] ] [ action [ i ] ] = reward [ i ] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] a = np . argmax ( target_val [ i ] [ [number] ] ) [EOL] target [ i ] [ [number] ] [ action [ i ] ] = reward [ i ] + self . gamma * ( target_val_ [ i ] [ [number] ] [ a ] ) [EOL] [EOL] self . model . fit ( update_input , target , batch_size = self . batch_size , epochs = [number] , verbose = [number] ) [EOL] [EOL] [comment] [EOL] if self . train and self . t % self . timestep_per_train == [number] : [EOL] self . save ( ) [EOL] [EOL] self . t += [number] [EOL] [EOL] return None [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] logger . info ( f" [string] { self . path }" ) [EOL] [EOL] [comment] [EOL] path = os . path . join ( self . path , [string] ) [EOL] self . model . save_weights ( path , overwrite = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from . ppo import PPO [EOL]	0 0 0 0 0 0
from typing import Tuple , List , Any , Union [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import _importlib_modulespec [EOL] import lib [EOL] [docstring] [EOL] import imp [EOL] import logging [EOL] import math [EOL] import os [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . agents import Agent [EOL] from lib . buffer import MemoryBuffer [EOL] from lib . types import Observation , State [EOL] from lib . exploration import SoftmaxPolicy , GaussianWhiteNoiseProcess , OrnsteinUhlenbeckProcess [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] class PPO ( Agent ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , train ) : [EOL] super ( ) . __init__ ( env ) [EOL] [EOL] [comment] [EOL] self . train = train [EOL] [EOL] [comment] [EOL] path , to_load = self . model_path ( [string] ) [EOL] model = imp . load_source ( [string] , os . path . join ( path , [string] ) ) [EOL] config = model . config ( ) [EOL] [EOL] [comment] [EOL] self . continuous = config [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . state_size = ( [number] , [number] ) [EOL] self . action_size = [number] if self . continuous else [number] [EOL] [EOL] [comment] [EOL] self . gamma = config [ [string] ] [ [string] ] [EOL] self . actor_learning_rate = config [ [string] ] [ [string] ] [EOL] self . critic_learning_rate = config [ [string] ] [ [string] ] [EOL] self . batch_size = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . timestep_per_train = config [ [string] ] [ [string] ] [EOL] self . iterations_on_save = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . max_memory = config [ [string] ] [ [string] ] [EOL] self . memory = MemoryBuffer ( self . max_memory ) [EOL] self . batch_size = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . epochs = config [ [string] ] [ [string] ] [EOL] [EOL] self . actor_model , self . critic_model = model . create ( state_size = self . state_size , action_size = self . action_size , actor_learning_rate = self . actor_learning_rate , critic_learning_rate = self . critic_learning_rate , continuous = self . continuous , ) [EOL] [EOL] if to_load : [EOL] try : [EOL] logger . info ( f" [string] { path } [string] " ) [EOL] self . actor_model . load_weights ( os . path . join ( path , [string] ) ) [EOL] self . critic_model . load_weights ( os . path . join ( path , [string] ) ) [EOL] except OSError : [EOL] logger . info ( [string] ) [EOL] pass [comment] [EOL] [EOL] logger . info ( [string] ) [EOL] logger . info ( self . actor_model . summary ( ) ) [EOL] logger . info ( [string] ) [EOL] logger . info ( self . critic_model . summary ( ) ) [EOL] [EOL] [comment] [EOL] self . noise = config [ [string] ] [ [string] ] [EOL] [EOL] [comment] [EOL] self . t = [number] [EOL] [EOL] [comment] [EOL] self . action_matrix , self . policy = None , None [EOL] [EOL] [comment] [EOL] self . exploration_strategy = SoftmaxPolicy ( action_size = self . action_size ) [EOL] self . noise_strategy = GaussianWhiteNoiseProcess ( size = self . action_size ) [EOL] [EOL] logger . info ( f" [string] { self . name } [string] { self . __repr__ ( ) }" ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return f"{ self . __class__ . __name__ } [string] " if self . continuous else self . __class__ . __name__ [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] if not self . continuous : [EOL] return self . _get_action_discrete ( state ) [EOL] [EOL] return self . _get_action_continuous ( state ) [EOL] [EOL] def _get_action_discrete ( self , state ) : [EOL] [docstring] [EOL] q_values = self . actor_model . predict ( [ serialize_state ( state ) , np . zeros ( shape = ( [number] , [number] ) ) , np . zeros ( shape = ( [number] , self . action_size ) ) ] ) . flatten ( ) [EOL] [EOL] [comment] [EOL] assert q_values . shape == ( self . action_size , ) , f" [string] { q_values . shape } [string] { ( self . action_size , ) }" [EOL] [EOL] action = self . exploration_strategy . step ( q_values ) if self . train else np . argmax ( q_values ) [EOL] action_matrix = np . zeros ( self . action_size ) [EOL] action_matrix [ action ] = [number] [EOL] self . action_matrix , self . q_values = action_matrix , q_values [EOL] return action [EOL] [EOL] def _get_action_continuous ( self , state ) : [EOL] [docstring] [EOL] policy = self . actor_model . predict ( [ serialize_state ( state ) , np . zeros ( shape = ( [number] , [number] ) ) , np . zeros ( shape = ( [number] , self . action_size ) ) ] ) . flatten ( ) [EOL] [EOL] if self . train : [EOL] action = action_matrix = policy + self . noise_strategy . sample ( ) [EOL] else : [EOL] action = action_matrix = policy [EOL] self . action_matrix , self . policy = action_matrix , policy [EOL] return action . tolist ( ) [EOL] [EOL] def discount_rewards ( self , rewards ) : [EOL] [docstring] [EOL] for j in range ( len ( rewards ) - [number] , - [number] , - [number] ) : [EOL] rewards [ j ] += rewards [ j + [number] ] * self . gamma [EOL] return rewards [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] self . memory . append ( ( data , ( self . action_matrix , self . policy ) ) ) [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . timestep_per_train == [number] : [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] observations = self . memory . retreive ( ) [EOL] states = np . array ( [ serialize_state ( observation [ [number] ] . state , dim = [number] ) for observation in observations ] ) [EOL] rewards = np . array ( [ observation [ [number] ] . reward for observation in observations ] ) [EOL] action_matrices = np . vstack ( [ observation [ [number] ] [ [number] ] for observation in observations ] ) [EOL] policies = np . vstack ( [ observation [ [number] ] [ [number] ] for observation in observations ] ) [EOL] [EOL] transformed_rewards = np . array ( self . discount_rewards ( rewards ) ) [EOL] advantages = transformed_rewards - np . array ( self . critic_model . predict ( states ) [ [number] ] ) [EOL] [EOL] [comment] [EOL] self . actor_model . fit ( [ states , advantages . T , policies ] , [ action_matrices ] , batch_size = self . batch_size , epochs = self . epochs , verbose = False , ) [EOL] self . critic_model . fit ( [ states ] , [ transformed_rewards ] , batch_size = self . batch_size , epochs = self . epochs , verbose = False ) [EOL] [EOL] [comment] [EOL] self . memory . purge ( ) [EOL] [EOL] [comment] [EOL] if self . train and self . t % self . timestep_per_train == [number] : [EOL] self . save ( ) [EOL] [EOL] self . t += [number] [EOL] [EOL] return None [EOL] [EOL] def save ( self ) : [EOL] [docstring] [EOL] logger . info ( f" [string] { self . path }" ) [EOL] [EOL] [comment] [EOL] actor_path = os . path . join ( self . path , [string] ) [EOL] self . actor_model . save_weights ( actor_path , overwrite = True ) [EOL] [EOL] [comment] [EOL] critic_path = os . path . join ( self . path , [string] ) [EOL] self . critic_model . save_weights ( critic_path , overwrite = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $lib.exploration.strategies.SoftmaxPolicy$ 0 0 0 0 0 0 0 0 0 0 0 0 $lib.exploration.random.GaussianWhiteNoiseProcess$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 $builtins.int$ 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 $None$ 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Union[lib.types.Observation,lib.types.State]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Union[lib.types.Observation,lib.types.State]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Union[lib.types.Observation,lib.types.State]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Union[lib.types.Observation,lib.types.State]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Union[lib.types.Observation,lib.types.State]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from typing import Tuple , Any , Dict , Union [EOL] import typing [EOL] import keras [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple , Union , Dict [EOL] [EOL] from keras import backend as K [EOL] from keras . layers import ( LSTM , BatchNormalization , Dense , Dropout , Flatten , GaussianNoise , Input , Lambda , TimeDistributed , add , Activation , ) [EOL] from keras . models import Model , Sequential , load_model [EOL] from keras . optimizers import Adam , RMSprop [EOL] [EOL] from lib . utils . helpers import ( huber_loss , proximal_policy_optimization_loss , proximal_policy_optimization_loss_continuous , ) [EOL] from lib . utils . noisy_dense import NoisyDense [EOL] [EOL] [EOL] def config ( ) : [EOL] return { [string] : False , [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } , } [EOL] [EOL] [EOL] def create ( state_size , action_size , actor_learning_rate , critic_learning_rate , continuous = False , ) : [EOL] [EOL] [comment] [EOL] def build_actor_discrete ( state_size , action_size , actor_learning_rate ) : [EOL] state_input = Input ( shape = state_size ) [EOL] advantage_input = Input ( shape = ( [number] , ) ) [EOL] old_prediction_input = Input ( shape = ( action_size , ) ) [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( state_input ) [EOL] x = BatchNormalization ( ) ( x ) [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] [comment] [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] [comment] [EOL] [EOL] x = Flatten ( ) ( x ) [EOL] [EOL] out_actions = Dense ( action_size , kernel_initializer = [string] , activation = [string] , name = [string] ) ( x ) [EOL] [EOL] model = Model ( inputs = [ state_input , advantage_input , old_prediction_input ] , outputs = [ out_actions ] ) [EOL] model . compile ( optimizer = Adam ( lr = actor_learning_rate ) , loss = proximal_policy_optimization_loss ( advantage = advantage_input , old_prediction = old_prediction_input ) , ) [EOL] return model [EOL] [EOL] [comment] [EOL] def build_actor_continuous ( state_size , action_size , actor_learning_rate ) : [EOL] state_input = Input ( shape = state_size ) [EOL] advantage_input = Input ( shape = ( [number] , ) ) [EOL] old_prediction_input = Input ( shape = ( action_size , ) ) [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( state_input ) [EOL] [comment] [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] [comment] [EOL] [EOL] x = Dense ( [number] , kernel_initializer = [string] , activation = [string] ) ( x ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] out_actions = Dense ( action_size , kernel_initializer = [string] , activation = [string] , name = [string] ) ( x ) [EOL] [EOL] model = Model ( [ state_input , advantage_input , old_prediction_input ] , [ out_actions ] ) [EOL] model . compile ( optimizer = Adam ( lr = actor_learning_rate ) , loss = proximal_policy_optimization_loss_continuous ( advantage = advantage_input , old_prediction = old_prediction_input ) , ) [EOL] return model [EOL] [EOL] def build_critic ( state_size , critic_learning_rate ) : [EOL] [EOL] [comment] [EOL] critic = Sequential ( ) [EOL] [EOL] critic . add ( Dense ( [number] , kernel_initializer = [string] , input_shape = state_size ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] [comment] [EOL] [EOL] critic . add ( Dense ( [number] , kernel_initializer = [string] ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] critic . add ( BatchNormalization ( ) ) [EOL] [EOL] critic . add ( Dense ( [number] , kernel_initializer = [string] ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] [comment] [EOL] [EOL] critic . add ( Flatten ( ) ) [EOL] [EOL] critic . add ( Dense ( [number] , kernel_initializer = [string] ) ) [EOL] critic . add ( Activation ( [string] ) ) [EOL] [EOL] critic . compile ( loss = huber_loss , optimizer = Adam ( lr = critic_learning_rate ) ) [EOL] [EOL] return critic [EOL] [EOL] actor = object ( ) [EOL] if continuous : [EOL] actor = build_actor_continuous ( state_size , action_size , actor_learning_rate ) [EOL] else : [EOL] actor = build_actor_discrete ( state_size , action_size , actor_learning_rate ) [EOL] [EOL] critic = build_critic ( state_size , critic_learning_rate ) [EOL] [EOL] return actor , critic [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[keras.models.Model,keras.models.Model]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Union [EOL] import builtins [EOL] import typing [EOL] import logging [EOL] import lib [EOL] import environment [EOL] [docstring] [EOL] import logging [EOL] from typing import Any , Dict , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . types import Action , Observation , State [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . DEBUG ) [EOL] [EOL] [EOL] class QLearner : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env ) : [EOL] self . env = env [EOL] self . Q = dict ( ) [EOL] self . last_state = None [EOL] self . last_action = None [EOL] self . learning_rate = [number] [EOL] self . gamma = [number] [EOL] self . epsilon = [number] [EOL] self . agent_name = [string] [EOL] [EOL] def move ( self , action ) : [EOL] [docstring] [EOL] [EOL] self . env . update_state ( action , self . agent_name ) [EOL] return None [EOL] [EOL] def location ( self ) : [EOL] [docstring] [EOL] [EOL] if self . agent_name == [string] : [EOL] return self . env . robot . location ( ) [EOL] elif self . agent_name == [string] : [EOL] return self . env . opponent . location ( ) [EOL] [EOL] logging . error ( [string] ) [EOL] raise ValueError [EOL] [EOL] def get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if state in self . Q and np . random . uniform ( [number] , [number] ) < self . epsilon : [EOL] [comment] [EOL] action = max ( self . Q [ state . agent_location ] , key = self . Q [ state . agent_location ] . get ) [EOL] else : [EOL] action = np . random . randint ( [number] , len ( self . env . actions ) ) [EOL] if state not in self . Q : [EOL] self . Q [ state . agent_location ] = { } [EOL] self . Q [ state . agent_location ] [ action ] = [number] [EOL] [EOL] self . last_state = state . agent_location [EOL] self . last_action = action [EOL] [EOL] return action [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] [EOL] old = self . Q [ self . last_state ] [ self . last_action ] [EOL] [EOL] if data . new_state . agent_location in self . Q : [EOL] [comment] [EOL] [comment] [EOL] new = ( self . gamma * self . Q [ data . new_state . agent_location ] [ max ( self . Q [ data . new_state . agent_location ] , key = self . Q [ data . new_state . agent_location ] . get ) ] ) [EOL] else : [EOL] new = [number] [EOL] [EOL] [comment] [EOL] self . Q [ self . last_state ] [ self . last_action ] = ( [number] - self . learning_rate ) * old + self . learning_rate * ( data . reward + new ) [EOL] [EOL] return None [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 $lib.types.Action$ 0 0 0 0 0 0 0 0 0 0 0 0 $lib.types.Action$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[None,typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $"State"$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $None$ 0 0 0 $"Observation"$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 $"Observation"$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $"Observation"$ 0 0 0 $builtins.int$ 0 0 0 0 0 0
from . qlearner import QLearner [EOL]	0 0 0 0 0 0
from typing import Tuple , List , Any , Union [EOL] import builtins [EOL] import typing [EOL] import logging [EOL] import lib [EOL] [docstring] [EOL] import imp [EOL] import logging [EOL] import os [EOL] from typing import Any , Dict , Tuple , Union [EOL] [EOL] import numpy as np [EOL] [EOL] from environment import AirHockey [EOL] from lib . agents import Agent [EOL] from lib . buffer import MemoryBuffer [EOL] from lib . exploration import EpsilonGreedy [EOL] from lib . types import Observation , State [EOL] from lib . utils . helpers import serialize_state [EOL] [EOL] [comment] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . INFO ) [EOL] [EOL] [EOL] class DDQN ( Agent ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , train , ** kwargs ) : [EOL] super ( ) . __init__ ( env ) [EOL] [EOL] logger . info ( f" [string] { self . name } [string] { self . __repr__ ( ) }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . state_size = ( [number] , [number] ) [EOL] self . action_size = [number] [EOL] [EOL] [comment] [EOL] self . train = train [EOL] [EOL] [comment] [EOL] path , to_load = self . model_path ( [string] ) [EOL] model = imp . load_source ( [string] , os . path . join ( path , [string] ) ) [EOL] [EOL] [comment] [EOL] config = model . config ( ) [EOL] self . gamma = config [ [string] ] [ [string] ] [comment] [EOL] self . learning_rate = config [ [string] ] [ [string] ] [EOL] self . batch_size = config [ [string] ] [ [string] ] [EOL] self . sync_target_interval = config [ [string] ] [ [string] ] [EOL] self . timestep_per_train = config [ [string] ] [ [string] ] [EOL] [EOL] self . max_memory = config [ [string] ] [ [string] ] [EOL] self . memory = MemoryBuffer ( self . max_memory ) [EOL] [EOL] [comment] [EOL] self . model = model . create ( self . state_size , self . learning_rate ) [EOL] self . target_model = model . create ( self . state_size , self . learning_rate ) [EOL] [EOL] if to_load : [EOL] try : [EOL] logger . info ( f" [string] { path } [string] " ) [EOL] self . model . load_weights ( os . path . join ( path , [string] ) ) [EOL] except OSError : [EOL] logger . info ( [string] ) [EOL] pass [comment] [EOL] [EOL] self . update_target_model ( ) [EOL] logger . info ( self . model . summary ( ) ) [EOL] [EOL] [comment] [EOL] self . t = [number] [EOL] [EOL] [comment] [EOL] self . exploration_strategy = EpsilonGreedy ( action_size = self . action_size ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return [string] [EOL] [EOL] def update_target_model ( self ) : [EOL] [docstring] [EOL] [EOL] logger . info ( [string] ) [EOL] self . target_model . set_weights ( self . model . get_weights ( ) ) [EOL] [EOL] def _get_action ( self , state ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] q_values = self . model . predict ( serialize_state ( state ) ) . flatten ( ) [EOL] assert q_values . shape == ( self . action_size , ) , f" [string] { q_values . shape } [string] " [EOL] return self . exploration_strategy . step ( q_values ) if self . train else np . argmax ( q_values ) [EOL] [EOL] def update ( self , data ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] self . memory . append ( data ) [EOL] [EOL] [comment] [EOL] if self . t > [number] and self . t % self . timestep_per_train == [number] : [EOL] [EOL] logger . info ( f" [string] " ) [EOL] [EOL] [comment] [EOL] num_samples = min ( self . batch_size , len ( self . memory ) ) [EOL] minibatch = self . memory . sample ( num_samples ) [EOL] for observation in minibatch : [EOL] flattend_state = serialize_state ( observation . state ) [EOL] flattend_new_state = serialize_state ( observation . new_state ) [EOL] target = self . model . predict ( flattend_new_state ) . flatten ( ) [EOL] assert target . shape == ( self . action_size , ) , f" [string] { target . shape } [string] " [EOL] if observation . done : [EOL] [comment] [EOL] self . update_target_model ( ) [EOL] [EOL] [comment] [EOL] target [ observation . action ] = observation . reward [EOL] else : [EOL] t = self . target_model . predict ( flattend_new_state ) . flatten ( ) [EOL] assert t . shape == ( self . action_size , ) , f" [string] { t . shape } [string] " [EOL] [comment] [EOL] target [ observation . action ] = observation . reward + self . gamma * np . argmax ( t ) [EOL] [EOL] self . model . fit ( flattend_state , np . expand_dims ( target , axis = [number] ) , batch_size = self . batch_size , epochs = [number] , verbose = [number] ) [EOL] [EOL] [comment] [EOL] if self . train and self . t % self . timestep_per_train == [number] : [EOL] self . save ( ) [EOL] [EOL] self . t += [number] [EOL] return None [EOL] [EOL] def save_model ( self ) : [EOL] [docstring] [EOL] logger . info ( f" [string] { self . path }" ) [EOL] [EOL] [comment] [EOL] path = os . path . join ( self . path , [string] ) [EOL] self . model . save_weights ( path , overwrite = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from . ddqn import DDQN [EOL]	0 0 0 0 0 0
from typing import Tuple , Any , Dict , Union [EOL] import typing [EOL] import keras [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Tuple , Union , Dict [EOL] [EOL] import tensorflow as tf [EOL] from keras import backend as K [EOL] from keras . layers import ( BatchNormalization , Dense , Flatten , GaussianNoise , Input , Lambda , add , Dropout , LSTM , TimeDistributed , Activation , ) [EOL] from keras . models import Model , Sequential , load_model [EOL] from keras . optimizers import Adam , RMSprop [EOL] from lib . utils . helpers import huber_loss [EOL] from lib . utils . noisy_dense import NoisyDense [EOL] [EOL] [EOL] def config ( ) : [EOL] return { [string] : { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } } [EOL] [EOL] [EOL] def create ( state_size , learning_rate ) : [EOL] [docstring] [EOL] [EOL] model = Sequential ( ) [EOL] [EOL] model . add ( Dense ( state_size [ [number] ] , kernel_initializer = [string] , input_shape = state_size ) ) [EOL] model . add ( Activation ( [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] [EOL] model . add ( Flatten ( ) ) [EOL] [EOL] model . add ( Dense ( [number] , kernel_initializer = [string] ) ) [EOL] model . add ( Activation ( [string] ) ) [EOL] [EOL] model . compile ( loss = huber_loss , optimizer = Adam ( lr = learning_rate ) ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $keras.models.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0