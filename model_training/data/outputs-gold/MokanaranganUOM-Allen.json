[comment] [EOL] [EOL] [comment] [EOL] import sys [EOL] if sys . version_info < ( [number] , [number] ) : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] try : [EOL] [comment] [EOL] [comment] [EOL] import spacy , torch , numpy [comment] [EOL] [EOL] except ModuleNotFoundError : [EOL] print ( [string] [string] [string] ) [EOL] raise [EOL] [EOL] from allennlp . version import VERSION as __version__ [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
_MAJOR = [string] [EOL] _MINOR = [string] [EOL] _REVISION = [string] [EOL] [EOL] VERSION_SHORT = [string] . format ( _MAJOR , _MINOR ) [EOL] VERSION = [string] . format ( _MAJOR , _MINOR , _REVISION ) [EOL]	$builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0
from allennlp . models . encoder_decoders . simple_seq2seq import SimpleSeq2Seq [EOL]	0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . models . reading_comprehension . bidaf import BidirectionalAttentionFlow [EOL] from allennlp . models . reading_comprehension . bidaf_ensemble import BidafEnsemble [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . models . coreference_resolution . coref import CoreferenceResolver [EOL]	0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . coref import CorefPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . wikitables_parser import WikiTablesParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . predictor import Predictor , DEFAULT_PREDICTORS [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . sentence_tagger import SentenceTaggerPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . nlvr_parser import NlvrParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . semantic_role_labeler import SemanticRoleLabelerPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . decomposable_attention import DecomposableAttentionPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . bidaf import BidafPredictor [EOL] [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . simple_seq2seq import SimpleSeq2SeqPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . constituency_parser import ConstituencyParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . nn . activations import Activation [EOL] from allennlp . nn . initializers import Initializer , InitializerApplicator [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class Regularizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __call__ ( self , parameter ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import torch [EOL] from typing import List [EOL] [EOL] import torch [EOL] [EOL] [EOL] class RnnState : [EOL] [docstring] [EOL] def __init__ ( self , hidden_state , memory_cell , previous_action_embedding , attended_input , encoder_outputs , encoder_output_mask ) : [EOL] self . hidden_state = hidden_state [EOL] self . memory_cell = memory_cell [EOL] self . previous_action_embedding = previous_action_embedding [EOL] self . attended_input = attended_input [EOL] self . encoder_outputs = encoder_outputs [EOL] self . encoder_output_mask = encoder_output_mask [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Tuple , Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import allennlp [EOL] from collections import defaultdict [EOL] from typing import Dict , List [EOL] [EOL] import torch [EOL] [EOL] from allennlp . nn . decoding import util [EOL] from allennlp . nn . decoding . decoder_step import DecoderStep [EOL] from allennlp . nn . decoding . decoder_state import DecoderState [EOL] [EOL] [EOL] class ConstrainedBeamSearch : [EOL] [docstring] [EOL] def __init__ ( self , beam_size , allowed_sequences , allowed_sequence_mask ) : [EOL] self . _beam_size = beam_size [EOL] self . _allowed_transitions = util . construct_prefix_tree ( allowed_sequences , allowed_sequence_mask ) [EOL] [EOL] def search ( self , initial_state , decoder_step ) : [EOL] [docstring] [EOL] finished_states = defaultdict ( list ) [EOL] states = [ initial_state ] [EOL] step_num = [number] [EOL] while states : [EOL] step_num += [number] [EOL] next_states = defaultdict ( list ) [EOL] grouped_state = states [ [number] ] . combine_states ( states ) [EOL] allowed_actions = [ ] [EOL] for batch_index , action_history in zip ( grouped_state . batch_indices , grouped_state . action_history ) : [EOL] allowed_actions . append ( self . _allowed_transitions [ batch_index ] [ tuple ( action_history ) ] ) [EOL] for next_state in decoder_step . take_step ( grouped_state , max_actions = self . _beam_size , allowed_actions = allowed_actions ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] batch_index = next_state . batch_indices [ [number] ] [EOL] if next_state . is_finished ( ) : [EOL] finished_states [ batch_index ] . append ( next_state ) [EOL] else : [EOL] next_states [ batch_index ] . append ( next_state ) [EOL] states = [ ] [EOL] for batch_index , batch_states in next_states . items ( ) : [EOL] [comment] [EOL] [comment] [EOL] states . extend ( batch_states [ : self . _beam_size ] ) [EOL] best_states = { } [EOL] for batch_index , batch_states in finished_states . items ( ) : [EOL] [comment] [EOL] [comment] [EOL] finished_to_sort = [ ( - state . score [ [number] ] . item ( ) , state ) for state in batch_states ] [EOL] finished_to_sort . sort ( key = lambda x : x [ [number] ] ) [EOL] best_states [ batch_index ] = [ state [ [number] ] for state in finished_to_sort [ : self . _beam_size ] ] [EOL] return best_states [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 $allennlp.nn.decoding.decoder_state.DecoderState$ 0 $allennlp.nn.decoding.decoder_step.DecoderStep$ 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $allennlp.nn.decoding.decoder_state.DecoderState$ 0 0 $builtins.int$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.decoding.decoder_step.DecoderStep$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0
from typing import Callable , List , Any , TypeVar , Dict [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import allennlp [EOL] from typing import Callable , Dict , List , TypeVar [EOL] from collections import defaultdict [EOL] [EOL] import torch [EOL] [EOL] from allennlp . nn . decoding . decoder_step import DecoderStep [EOL] from allennlp . nn . decoding . decoder_state import DecoderState [EOL] from allennlp . nn . decoding . decoder_trainers . decoder_trainer import DecoderTrainer [EOL] from allennlp . nn import util as nn_util [EOL] [EOL] StateType = TypeVar ( [string] , bound = DecoderState ) [comment] [EOL] [EOL] [EOL] class ExpectedRiskMinimization ( DecoderTrainer [ Callable [ [ StateType ] , torch . Tensor ] ] ) : [EOL] [docstring] [EOL] def __init__ ( self , beam_size , normalize_by_length , max_decoding_steps , max_num_decoded_sequences = [number] , max_num_finished_states = None ) : [EOL] self . _beam_size = beam_size [EOL] self . _normalize_by_length = normalize_by_length [EOL] self . _max_decoding_steps = max_decoding_steps [EOL] self . _max_num_decoded_sequences = max_num_decoded_sequences [EOL] self . _max_num_finished_states = max_num_finished_states [EOL] [EOL] def decode ( self , initial_state , decode_step , supervision ) : [EOL] cost_function = supervision [EOL] finished_states = self . _get_finished_states ( initial_state , decode_step ) [EOL] loss = initial_state . score [ [number] ] . new_zeros ( [number] ) [EOL] finished_model_scores = self . _get_model_scores_by_batch ( finished_states ) [EOL] finished_costs = self . _get_costs_by_batch ( finished_states , cost_function ) [EOL] for batch_index in finished_model_scores : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] costs = torch . cat ( [ tensor . view ( - [number] ) for tensor in finished_costs [ batch_index ] ] ) [EOL] logprobs = torch . cat ( [ tensor . view ( - [number] ) for tensor in finished_model_scores [ batch_index ] ] ) [EOL] [comment] [EOL] [comment] [EOL] renormalized_probs = nn_util . masked_softmax ( logprobs , None ) [EOL] loss += renormalized_probs . dot ( costs ) [EOL] mean_loss = loss / len ( finished_model_scores ) [EOL] return { [string] : mean_loss , [string] : self . _get_best_action_sequences ( finished_states ) } [EOL] [EOL] def _get_finished_states ( self , initial_state , decode_step ) : [EOL] finished_states = [ ] [EOL] states = [ initial_state ] [EOL] num_steps = [number] [EOL] while states and num_steps < self . _max_decoding_steps : [EOL] next_states = [ ] [EOL] grouped_state = states [ [number] ] . combine_states ( states ) [EOL] [comment] [EOL] for next_state in decode_step . take_step ( grouped_state ) : [EOL] if next_state . is_finished ( ) : [EOL] finished_states . append ( next_state ) [EOL] else : [EOL] next_states . append ( next_state ) [EOL] [EOL] states = self . _prune_beam ( states = next_states , beam_size = self . _beam_size , sort_states = False ) [EOL] num_steps += [number] [EOL] if self . _max_num_finished_states is not None : [EOL] finished_states = self . _prune_beam ( states = finished_states , beam_size = self . _max_num_finished_states , sort_states = True ) [EOL] return finished_states [EOL] [EOL] [comment] [EOL] @ staticmethod def _prune_beam ( states , beam_size , sort_states = False ) : [EOL] [docstring] [EOL] states_by_batch_index = defaultdict ( list ) [EOL] for state in states : [EOL] assert len ( state . batch_indices ) == [number] [EOL] batch_index = state . batch_indices [ [number] ] [EOL] states_by_batch_index [ batch_index ] . append ( state ) [EOL] pruned_states = [ ] [EOL] for _ , instance_states in states_by_batch_index . items ( ) : [EOL] if sort_states : [EOL] scores = torch . cat ( [ state . score [ [number] ] . view ( - [number] ) for state in instance_states ] ) [EOL] _ , sorted_indices = scores . sort ( - [number] , descending = True ) [EOL] sorted_states = [ instance_states [ i ] for i in sorted_indices . detach ( ) . cpu ( ) . numpy ( ) ] [EOL] instance_states = sorted_states [EOL] for state in instance_states [ : beam_size ] : [EOL] pruned_states . append ( state ) [EOL] return pruned_states [EOL] [EOL] def _get_model_scores_by_batch ( self , states ) : [EOL] batch_scores = defaultdict ( list ) [EOL] for state in states : [EOL] for batch_index , model_score , history in zip ( state . batch_indices , state . score , state . action_history ) : [EOL] if self . _normalize_by_length : [EOL] path_length = model_score . new_tensor ( [ len ( history ) ] ) [EOL] model_score = model_score / path_length [EOL] batch_scores [ batch_index ] . append ( model_score ) [EOL] return batch_scores [EOL] [EOL] @ staticmethod def _get_costs_by_batch ( states , cost_function ) : [EOL] batch_costs = defaultdict ( list ) [EOL] for state in states : [EOL] cost = cost_function ( state ) [EOL] [comment] [EOL] [comment] [EOL] batch_index = state . batch_indices [ [number] ] [EOL] batch_costs [ batch_index ] . append ( cost ) [EOL] return batch_costs [EOL] [EOL] def _get_best_action_sequences ( self , finished_states ) : [EOL] [docstring] [EOL] batch_action_histories = defaultdict ( list ) [EOL] for state in finished_states : [EOL] for batch_index , action_history in zip ( state . batch_indices , state . action_history ) : [EOL] batch_action_histories [ batch_index ] . append ( action_history ) [EOL] [EOL] batch_scores = self . _get_model_scores_by_batch ( finished_states ) [EOL] best_action_sequences = { } [EOL] for batch_index , scores in batch_scores . items ( ) : [EOL] _ , sorted_indices = torch . cat ( [ score . view ( - [number] ) for score in scores ] ) . sort ( - [number] , descending = True ) [EOL] cpu_indices = [ int ( index ) for index in sorted_indices . detach ( ) . cpu ( ) . numpy ( ) ] [EOL] best_action_indices = cpu_indices [ : self . _max_num_decoded_sequences ] [EOL] instance_best_sequences = [ batch_action_histories [ batch_index ] [ i ] for i in best_action_indices ] [EOL] best_action_sequences [ batch_index ] = instance_best_sequences [EOL] return best_action_sequences [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[StateType]$ 0 0 0 $allennlp.nn.decoding.decoder_state.DecoderState$ 0 $allennlp.nn.decoding.decoder_step.DecoderStep$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.decoding.decoder_state.DecoderState$ 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.decoding.decoder_step.DecoderStep$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.nn.decoding.decoder_state.DecoderState]$ 0 $typing.List[allennlp.nn.decoding.decoder_state.DecoderState]$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.nn.decoding.decoder_state.DecoderState]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Dict[builtins.int,typing.List[allennlp.nn.decoding.decoder_state.DecoderState]]$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 $typing.List[StateType]$ 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 $typing.List[StateType]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 $typing.List[StateType]$ 0 $typing.Callable[[StateType],torch.Tensor]$ 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 $typing.List[StateType]$ 0 0 $typing.Any$ 0 $typing.Callable[[StateType],torch.Tensor]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.int,typing.List[torch.Tensor]]$ 0 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 $typing.List[StateType]$ 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 $typing.List[StateType]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[StateType]$ 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 $typing.Dict[builtins.int,typing.List[typing.List[builtins.int]]]$ 0
from allennlp . nn . decoding . decoder_trainers . expected_risk_minimization import ExpectedRiskMinimization [EOL] from allennlp . nn . decoding . decoder_trainers . maximum_marginal_likelihood import MaximumMarginalLikelihood [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
import builtins [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] [EOL] [EOL] class SequenceField ( Field [ DataArray ] ) : [EOL] [docstring] [EOL] def sequence_length ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Set , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] import logging [EOL] from typing import Dict , List , Set [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] from allennlp . common import Params [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ TokenIndexer . register ( [string] ) class DepLabelIndexer ( TokenIndexer [ int ] ) : [EOL] [docstring] [EOL] [comment] [EOL] def __init__ ( self , namespace = [string] ) : [EOL] self . namespace = namespace [EOL] self . _logged_errors = set ( ) [EOL] [EOL] @ overrides def count_vocab_items ( self , token , counter ) : [EOL] dep_label = token . dep_ [EOL] if not dep_label : [EOL] if token . text not in self . _logged_errors : [EOL] logger . warning ( [string] , token . text ) [EOL] self . _logged_errors . add ( token . text ) [EOL] dep_label = [string] [EOL] counter [ self . namespace ] [ dep_label ] += [number] [EOL] [EOL] @ overrides def token_to_indices ( self , token , vocabulary ) : [EOL] dep_label = token . dep_ or [string] [EOL] return vocabulary . get_token_index ( dep_label , self . namespace ) [EOL] [EOL] @ overrides def get_padding_token ( self ) : [EOL] return [number] [EOL] [EOL] @ overrides def get_padding_lengths ( self , token ) : [comment] [EOL] return { } [EOL] [EOL] @ overrides def pad_token_sequence ( self , tokens , desired_num_tokens , padding_lengths ) : [comment] [EOL] return pad_sequence_to_length ( tokens , desired_num_tokens ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] namespace = params . pop ( [string] , [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( namespace = namespace ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 $builtins.str$ 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0 $'DepLabelIndexer'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import List , Tuple , Any , Iterable , Dict [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] import logging [EOL] import random [EOL] from typing import List , Tuple , Iterable , cast , Dict [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import lazy_groups_of , add_noise_to_dict_values [EOL] from allennlp . data . dataset import Batch [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] def sort_by_padding ( instances , sorting_keys , vocab , padding_noise = [number] ) : [EOL] [docstring] [EOL] instances_with_lengths = [ ] [EOL] for instance in instances : [EOL] [comment] [EOL] instance . index_fields ( vocab ) [EOL] padding_lengths = cast ( Dict [ str , Dict [ str , float ] ] , instance . get_padding_lengths ( ) ) [EOL] if padding_noise > [number] : [EOL] noisy_lengths = { } [EOL] for field_name , field_lengths in padding_lengths . items ( ) : [EOL] noisy_lengths [ field_name ] = add_noise_to_dict_values ( field_lengths , padding_noise ) [EOL] padding_lengths = noisy_lengths [EOL] instance_with_lengths = ( [ padding_lengths [ field_name ] [ padding_key ] for ( field_name , padding_key ) in sorting_keys ] , instance ) [EOL] instances_with_lengths . append ( instance_with_lengths ) [EOL] instances_with_lengths . sort ( key = lambda x : x [ [number] ] ) [EOL] return [ instance_with_lengths [ - [number] ] for instance_with_lengths in instances_with_lengths ] [EOL] [EOL] [EOL] @ DataIterator . register ( [string] ) class BucketIterator ( DataIterator ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , sorting_keys , padding_noise = [number] , biggest_batch_first = False , batch_size = [number] , instances_per_epoch = None , max_instances_in_memory = None , cache_instances = False , track_epoch = False , maximum_samples_per_batch = None ) : [EOL] if not sorting_keys : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] super ( ) . __init__ ( cache_instances = cache_instances , track_epoch = track_epoch , batch_size = batch_size , instances_per_epoch = instances_per_epoch , max_instances_in_memory = max_instances_in_memory , maximum_samples_per_batch = maximum_samples_per_batch ) [EOL] self . _sorting_keys = sorting_keys [EOL] self . _padding_noise = padding_noise [EOL] self . _biggest_batch_first = biggest_batch_first [EOL] [EOL] @ overrides def _create_batches ( self , instances , shuffle ) : [EOL] for instance_list in self . _memory_sized_lists ( instances ) : [EOL] [EOL] instance_list = sort_by_padding ( instance_list , self . _sorting_keys , self . vocab , self . _padding_noise ) [EOL] [EOL] batches = [ ] [EOL] for batch_instances in lazy_groups_of ( iter ( instance_list ) , self . _batch_size ) : [EOL] for possibly_smaller_batches in self . _ensure_batch_is_sufficiently_small ( batch_instances ) : [EOL] batches . append ( Batch ( possibly_smaller_batches ) ) [EOL] [EOL] move_to_front = self . _biggest_batch_first and len ( batches ) > [number] [EOL] if move_to_front : [EOL] [comment] [EOL] last_batch = batches . pop ( ) [EOL] penultimate_batch = batches . pop ( ) [EOL] if shuffle : [EOL] random . shuffle ( batches ) [EOL] else : [EOL] logger . warning ( [string] [string] ) [EOL] if move_to_front : [EOL] batches . insert ( [number] , penultimate_batch ) [EOL] batches . insert ( [number] , last_batch ) [EOL] [EOL] yield from batches [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] sorting_keys = params . pop ( [string] ) [EOL] padding_noise = params . pop_float ( [string] , [number] ) [EOL] biggest_batch_first = params . pop_bool ( [string] , False ) [EOL] batch_size = params . pop_int ( [string] , [number] ) [EOL] instances_per_epoch = params . pop_int ( [string] , None ) [EOL] max_instances_in_memory = params . pop_int ( [string] , None ) [EOL] maximum_samples_per_batch = params . pop ( [string] , None ) [EOL] cache_instances = params . pop_bool ( [string] , False ) [EOL] track_epoch = params . pop_bool ( [string] , False ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] [EOL] return cls ( sorting_keys = sorting_keys , padding_noise = padding_noise , biggest_batch_first = biggest_batch_first , batch_size = batch_size , instances_per_epoch = instances_per_epoch , max_instances_in_memory = max_instances_in_memory , cache_instances = cache_instances , track_epoch = track_epoch , maximum_samples_per_batch = maximum_samples_per_batch ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.instance.Instance]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Tuple[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $typing.Tuple[builtins.str,builtins.int]$ 0 $typing.Tuple[builtins.str,builtins.int]$ 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $typing.Iterable[allennlp.data.dataset.Batch]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $'BucketIterator'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Iterator , Iterable , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Iterable [EOL] import logging [EOL] import random [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import lazy_groups_of [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . dataset import Batch [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DataIterator . register ( [string] ) class BasicIterator ( DataIterator ) : [EOL] [docstring] [EOL] def _create_batches ( self , instances , shuffle ) : [EOL] [comment] [EOL] for instance_list in self . _memory_sized_lists ( instances ) : [EOL] if shuffle : [EOL] random . shuffle ( instance_list ) [EOL] iterator = iter ( instance_list ) [EOL] [comment] [EOL] for batch_instances in lazy_groups_of ( iterator , self . _batch_size ) : [EOL] for possibly_smaller_batches in self . _ensure_batch_is_sufficiently_small ( batch_instances ) : [EOL] batch = Batch ( possibly_smaller_batches ) [EOL] yield batch [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] batch_size = params . pop_int ( [string] , [number] ) [EOL] instances_per_epoch = params . pop_int ( [string] , None ) [EOL] max_instances_in_memory = params . pop_int ( [string] , None ) [EOL] maximum_samples_per_batch = params . pop ( [string] , None ) [EOL] cache_instances = params . pop_bool ( [string] , False ) [EOL] track_epoch = params . pop_bool ( [string] , False ) [EOL] [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( batch_size = batch_size , instances_per_epoch = instances_per_epoch , max_instances_in_memory = max_instances_in_memory , cache_instances = cache_instances , track_epoch = track_epoch , maximum_samples_per_batch = maximum_samples_per_batch ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.dataset.Batch]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $'BasicIterator'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import List , Tuple , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] import logging [EOL] from typing import List , Tuple [EOL] import warnings [EOL] [EOL] from allennlp . common . params import Params [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . iterators . bucket_iterator import BucketIterator [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DataIterator . register ( [string] ) class EpochTrackingBucketIterator ( BucketIterator ) : [EOL] [docstring] [EOL] def __init__ ( self , sorting_keys , padding_noise = [number] , biggest_batch_first = False , batch_size = [number] , instances_per_epoch = None , max_instances_in_memory = None , cache_instances = False ) : [EOL] super ( ) . __init__ ( sorting_keys = sorting_keys , padding_noise = padding_noise , biggest_batch_first = biggest_batch_first , batch_size = batch_size , instances_per_epoch = instances_per_epoch , max_instances_in_memory = max_instances_in_memory , track_epoch = True , cache_instances = cache_instances ) [EOL] warnings . warn ( [string] [string] , DeprecationWarning ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] sorting_keys = params . pop ( [string] ) [EOL] padding_noise = params . pop_float ( [string] , [number] ) [EOL] biggest_batch_first = params . pop_bool ( [string] , False ) [EOL] batch_size = params . pop_int ( [string] , [number] ) [EOL] instances_per_epoch = params . pop_int ( [string] , None ) [EOL] max_instances_in_memory = params . pop_int ( [string] , None ) [EOL] cache_instances = params . pop_bool ( [string] , False ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] [EOL] return cls ( sorting_keys = sorting_keys , padding_noise = padding_noise , biggest_batch_first = biggest_batch_first , batch_size = batch_size , instances_per_epoch = instances_per_epoch , max_instances_in_memory = max_instances_in_memory , cache_instances = cache_instances ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'EpochTrackingBucketIterator'$ 0 0 0 $allennlp.common.params.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
[docstring] [EOL] [EOL] from allennlp . data . tokenizers . tokenizer import Token , Tokenizer [EOL] from allennlp . data . tokenizers . word_tokenizer import WordTokenizer [EOL] from allennlp . data . tokenizers . character_tokenizer import CharacterTokenizer [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import allennlp . data . tokenizers . utils . sinhala [EOL]	0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List [EOL] import logging [EOL] [EOL] from overrides import overrides [EOL] from nltk . tree import Tree [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import LabelField , TextField , Field [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import TokenIndexer , SingleIdTokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class StanfordSentimentTreeBankDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , token_indexers = None , use_subtrees = False , granularity = [string] , lazy = False ) : [EOL] super ( ) . __init__ ( lazy = lazy ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . _use_subtrees = use_subtrees [EOL] allowed_granularities = [ [string] , [string] , [string] ] [EOL] if granularity not in allowed_granularities : [EOL] raise ConfigurationError ( [string] . format ( granularity , allowed_granularities ) ) [EOL] self . _granularity = granularity [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] with open ( cached_path ( file_path ) , [string] ) as data_file : [EOL] logger . info ( [string] , file_path ) [EOL] for line in data_file . readlines ( ) : [EOL] line = line . strip ( [string] ) [EOL] if not line : [EOL] continue [EOL] parsed_line = Tree . fromstring ( line ) [EOL] if self . _use_subtrees : [EOL] for subtree in parsed_line . subtrees ( ) : [EOL] instance = self . text_to_instance ( subtree . leaves ( ) , subtree . label ( ) ) [EOL] if instance is not None : [EOL] yield instance [EOL] else : [EOL] instance = self . text_to_instance ( parsed_line . leaves ( ) , parsed_line . label ( ) ) [EOL] if instance is not None : [EOL] yield instance [EOL] [EOL] @ overrides def text_to_instance ( self , tokens , sentiment = None ) : [comment] [EOL] [docstring] [EOL] [comment] [EOL] text_field = TextField ( [ Token ( x ) for x in tokens ] , token_indexers = self . _token_indexers ) [EOL] fields = { [string] : text_field } [EOL] if sentiment is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if self . _granularity == [string] : [EOL] if int ( sentiment ) < [number] : [EOL] sentiment = [string] [EOL] elif int ( sentiment ) == [number] : [EOL] sentiment = [string] [EOL] else : [EOL] sentiment = [string] [EOL] elif self . _granularity == [string] : [EOL] if int ( sentiment ) < [number] : [EOL] sentiment = [string] [EOL] elif int ( sentiment ) == [number] : [EOL] return None [EOL] else : [EOL] sentiment = [string] [EOL] fields [ [string] ] = LabelField ( sentiment ) [EOL] return Instance ( fields ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] use_subtrees = params . pop ( [string] , False ) [EOL] granularity = params . pop_choice ( [string] , [ [string] , [string] , [string] ] , True ) [EOL] lazy = params . pop ( [string] , False ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return StanfordSentimentTreeBankDatasetReader ( token_indexers = token_indexers , use_subtrees = use_subtrees , granularity = granularity , lazy = lazy ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $typing.List[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $'StanfordSentimentTreeBankDatasetReader'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Dict , List , Tuple , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , Tuple , List [EOL] import logging [EOL] [EOL] from overrides import overrides [EOL] from conllu . parser import parse_line , DEFAULT_FIELDS [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , TextField , SequenceLabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] def lazy_parse ( text , fields = DEFAULT_FIELDS ) : [EOL] for sentence in text . split ( [string] ) : [EOL] if sentence : [EOL] yield [ parse_line ( line , fields ) for line in sentence . split ( [string] ) if line and not line . strip ( ) . startswith ( [string] ) ] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class UniversalDependenciesDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , token_indexers = None , lazy = False ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] [comment] [EOL] file_path = cached_path ( file_path ) [EOL] [EOL] with open ( file_path , [string] ) as conllu_file : [EOL] logger . info ( [string] , file_path ) [EOL] [EOL] for annotation in lazy_parse ( conllu_file . read ( ) ) : [EOL] [EOL] yield self . text_to_instance ( [ x [ [string] ] for x in annotation ] , [ x [ [string] ] for x in annotation ] , [ x [ [string] ] [ [number] ] for x in annotation ] ) [EOL] [EOL] @ overrides def text_to_instance ( self , words , upos_tags , dependencies = None ) : [EOL] [comment] [EOL] [docstring] [EOL] fields = { } [EOL] tokens = TextField ( [ Token ( w ) for w in words ] , self . _token_indexers ) [EOL] fields [ [string] ] = tokens [EOL] fields [ [string] ] = SequenceLabelField ( upos_tags , tokens , label_namespace = [string] ) [EOL] fields [ [string] ] = SequenceLabelField ( [ x [ [number] ] for x in dependencies ] , tokens , label_namespace = [string] ) [EOL] if dependencies is not None : [EOL] fields [ [string] ] = SequenceLabelField ( [ int ( x [ [number] ] ) for x in dependencies ] , tokens , label_namespace = [string] ) [EOL] return Instance ( fields ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] lazy = params . pop ( [string] , False ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return UniversalDependenciesDatasetReader ( token_indexers = token_indexers , lazy = lazy ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $'UniversalDependenciesDatasetReader'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Dict , List , Union , Any [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import gzip [EOL] [docstring] [EOL] [EOL] from typing import Dict , List , Any [EOL] import gzip [EOL] import json [EOL] import logging [EOL] import os [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , IndexField , KnowledgeGraphField , ListField [EOL] from allennlp . data . fields import MetadataField , ProductionRuleField , TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import TokenIndexer , SingleIdTokenIndexer [EOL] from allennlp . data . tokenizers import Token , Tokenizer , WordTokenizer [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . semparse . contexts import TableQuestionKnowledgeGraph [EOL] from allennlp . semparse . type_declarations import wikitables_type_declaration as wt_types [EOL] from allennlp . semparse . worlds import WikiTablesWorld [EOL] from allennlp . semparse . worlds . world import ParsingError [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class WikiTablesDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , lazy = False , tables_directory = None , dpd_output_directory = None , max_dpd_logical_forms = [number] , sort_dpd_logical_forms = True , max_dpd_tries = [number] , keep_if_no_dpd = False , tokenizer = None , question_token_indexers = None , table_token_indexers = None , use_table_for_vocab = False , linking_feature_extractors = None , include_table_metadata = False , max_table_tokens = None , output_agendas = False ) : [EOL] super ( ) . __init__ ( lazy = lazy ) [EOL] self . _tables_directory = tables_directory [EOL] self . _dpd_output_directory = dpd_output_directory [EOL] self . _max_dpd_logical_forms = max_dpd_logical_forms [EOL] self . _sort_dpd_logical_forms = sort_dpd_logical_forms [EOL] self . _max_dpd_tries = max_dpd_tries [EOL] self . _keep_if_no_dpd = keep_if_no_dpd [EOL] self . _tokenizer = tokenizer or WordTokenizer ( SpacyWordSplitter ( pos_tags = True ) ) [EOL] self . _question_token_indexers = question_token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . _table_token_indexers = table_token_indexers or self . _question_token_indexers [EOL] self . _use_table_for_vocab = use_table_for_vocab [EOL] self . _linking_feature_extractors = linking_feature_extractors [EOL] self . _include_table_metadata = include_table_metadata [EOL] self . _basic_types = set ( str ( type_ ) for type_ in wt_types . BASIC_TYPES ) [EOL] self . _max_table_tokens = max_table_tokens [EOL] self . _output_agendas = output_agendas [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] if file_path . endswith ( [string] ) : [EOL] yield from self . _read_examples_file ( file_path ) [EOL] elif file_path . endswith ( [string] ) : [EOL] yield from self . _read_preprocessed_file ( file_path ) [EOL] else : [EOL] raise ConfigurationError ( f" [string] { file_path }" ) [EOL] [EOL] def _read_examples_file ( self , file_path ) : [EOL] with open ( file_path , [string] ) as data_file : [EOL] num_dpd_missing = [number] [EOL] num_lines = [number] [EOL] num_instances = [number] [EOL] for line in data_file . readlines ( ) : [EOL] line = line . strip ( [string] ) [EOL] if not line : [EOL] continue [EOL] num_lines += [number] [EOL] parsed_info = self . _parse_example_line ( line ) [EOL] question = parsed_info [ [string] ] [EOL] [comment] [EOL] table_filename = os . path . join ( self . _tables_directory , parsed_info [ [string] ] . replace ( [string] , [string] ) ) [EOL] if self . _dpd_output_directory : [EOL] dpd_output_filename = os . path . join ( self . _dpd_output_directory , parsed_info [ [string] ] + [string] ) [EOL] try : [EOL] dpd_file = gzip . open ( dpd_output_filename ) [EOL] if self . _sort_dpd_logical_forms : [EOL] sempre_forms = [ dpd_line . strip ( ) . decode ( [string] ) for dpd_line in dpd_file ] [EOL] [comment] [EOL] [comment] [EOL] sempre_forms . sort ( key = lambda x : x . count ( [string] ) ) [EOL] if self . _max_dpd_tries : [EOL] sempre_forms = sempre_forms [ : self . _max_dpd_tries ] [EOL] else : [EOL] sempre_forms = [ ] [EOL] for dpd_line in dpd_file : [EOL] sempre_forms . append ( dpd_line . strip ( ) . decode ( [string] ) ) [EOL] if self . _max_dpd_tries and len ( sempre_forms ) >= self . _max_dpd_tries : [EOL] break [EOL] except FileNotFoundError : [EOL] logger . debug ( f' [string] { parsed_info [ [string] ] } [string] ' ) [EOL] sempre_forms = None [EOL] num_dpd_missing += [number] [EOL] if not self . _keep_if_no_dpd : [EOL] continue [EOL] else : [EOL] sempre_forms = None [EOL] [EOL] table_lines = open ( table_filename ) . readlines ( ) [EOL] instance = self . text_to_instance ( question = question , table_lines = table_lines , example_lisp_string = line , dpd_output = sempre_forms ) [EOL] if instance is not None : [EOL] num_instances += [number] [EOL] yield instance [EOL] [EOL] if self . _dpd_output_directory : [EOL] logger . info ( f" [string] { num_dpd_missing } [string] { num_lines } [string] " ) [EOL] num_with_dpd = num_lines - num_dpd_missing [EOL] num_bad_lfs = num_with_dpd - num_instances [EOL] logger . info ( f" [string] { num_bad_lfs } [string] { num_with_dpd } [string] " ) [EOL] if num_bad_lfs > [number] : [EOL] logger . info ( [string] ) [EOL] logger . info ( f" [string] { num_instances } [string] " ) [EOL] [EOL] def _read_preprocessed_file ( self , file_path ) : [EOL] with open ( file_path , [string] ) as data_file : [EOL] for line in data_file . readlines ( ) : [EOL] json_obj = json . loads ( line ) [EOL] yield self . _json_blob_to_instance ( json_obj ) [EOL] [EOL] @ overrides def text_to_instance ( self , question , table_lines , example_lisp_string = None , dpd_output = None , tokenized_question = None ) : [EOL] [docstring] [EOL] [comment] [EOL] tokenized_question = tokenized_question or self . _tokenizer . tokenize ( question . lower ( ) ) [EOL] question_field = TextField ( tokenized_question , self . _question_token_indexers ) [EOL] metadata = { [string] : [ x . text for x in tokenized_question ] } [EOL] metadata [ [string] ] = [string] . join ( table_lines ) [EOL] table_knowledge_graph = TableQuestionKnowledgeGraph . read_from_lines ( table_lines , tokenized_question ) [EOL] table_metadata = MetadataField ( table_lines ) [EOL] table_field = KnowledgeGraphField ( table_knowledge_graph , tokenized_question , self . _table_token_indexers , tokenizer = self . _tokenizer , feature_extractors = self . _linking_feature_extractors , include_in_vocab = self . _use_table_for_vocab , max_table_tokens = self . _max_table_tokens ) [EOL] world = WikiTablesWorld ( table_knowledge_graph ) [EOL] world_field = MetadataField ( world ) [EOL] [EOL] production_rule_fields = [ ] [EOL] for production_rule in world . all_possible_actions ( ) : [EOL] _ , rule_right_side = production_rule . split ( [string] ) [EOL] is_global_rule = not world . is_table_entity ( rule_right_side ) [EOL] field = ProductionRuleField ( production_rule , is_global_rule ) [EOL] production_rule_fields . append ( field ) [EOL] action_field = ListField ( production_rule_fields ) [EOL] [EOL] fields = { [string] : question_field , [string] : MetadataField ( metadata ) , [string] : table_field , [string] : world_field , [string] : action_field } [EOL] if self . _include_table_metadata : [EOL] fields [ [string] ] = table_metadata [EOL] if example_lisp_string : [EOL] fields [ [string] ] = MetadataField ( example_lisp_string ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] action_map = { action . rule : i for i , action in enumerate ( action_field . field_list ) } [comment] [EOL] if dpd_output : [EOL] action_sequence_fields = [ ] [EOL] for logical_form in dpd_output : [EOL] if not self . _should_keep_logical_form ( logical_form ) : [EOL] logger . debug ( f' [string] { question }' ) [EOL] logger . debug ( f' [string] { table_lines }' ) [EOL] continue [EOL] try : [EOL] expression = world . parse_logical_form ( logical_form ) [EOL] except ParsingError as error : [EOL] logger . debug ( f' [string] { error . message } [string] ' ) [EOL] logger . debug ( f' [string] { question }' ) [EOL] logger . debug ( f' [string] { logical_form }' ) [EOL] logger . debug ( f' [string] { table_lines }' ) [EOL] continue [EOL] except : [EOL] logger . error ( logical_form ) [EOL] raise [EOL] action_sequence = world . get_action_sequence ( expression ) [EOL] try : [EOL] index_fields = [ ] [EOL] for production_rule in action_sequence : [EOL] index_fields . append ( IndexField ( action_map [ production_rule ] , action_field ) ) [EOL] action_sequence_fields . append ( ListField ( index_fields ) ) [EOL] except KeyError as error : [EOL] logger . debug ( f' [string] { error . args } [string] ' ) [EOL] logger . debug ( f' [string] { question }' ) [EOL] logger . debug ( f' [string] { table_lines }' ) [EOL] logger . debug ( f' [string] { logical_form }' ) [EOL] continue [EOL] if len ( action_sequence_fields ) >= self . _max_dpd_logical_forms : [EOL] break [EOL] [EOL] if not action_sequence_fields : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return None [EOL] fields [ [string] ] = ListField ( action_sequence_fields ) [EOL] if self . _output_agendas : [EOL] agenda_index_fields = [ ] [EOL] for agenda_string in world . get_agenda ( ) : [EOL] agenda_index_fields . append ( IndexField ( action_map [ agenda_string ] , action_field ) ) [EOL] if not agenda_index_fields : [EOL] agenda_index_fields = [ IndexField ( - [number] , action_field ) ] [EOL] fields [ [string] ] = ListField ( agenda_index_fields ) [EOL] return Instance ( fields ) [EOL] [EOL] def _json_blob_to_instance ( self , json_obj ) : [EOL] question_tokens = self . _read_tokens_from_json_list ( json_obj [ [string] ] ) [EOL] question_field = TextField ( question_tokens , self . _question_token_indexers ) [EOL] question_metadata = MetadataField ( { [string] : [ x . text for x in question_tokens ] } ) [EOL] table_knowledge_graph = TableQuestionKnowledgeGraph . read_from_lines ( json_obj [ [string] ] , question_tokens ) [EOL] entity_tokens = [ self . _read_tokens_from_json_list ( token_list ) for token_list in json_obj [ [string] ] ] [EOL] table_field = KnowledgeGraphField ( table_knowledge_graph , question_tokens , tokenizer = None , token_indexers = self . _table_token_indexers , entity_tokens = entity_tokens , linking_features = json_obj [ [string] ] , include_in_vocab = self . _use_table_for_vocab , max_table_tokens = self . _max_table_tokens ) [EOL] world = WikiTablesWorld ( table_knowledge_graph ) [EOL] world_field = MetadataField ( world ) [EOL] [EOL] production_rule_fields = [ ] [EOL] for production_rule in world . all_possible_actions ( ) : [EOL] _ , rule_right_side = production_rule . split ( [string] ) [EOL] is_global_rule = not world . is_table_entity ( rule_right_side ) [EOL] field = ProductionRuleField ( production_rule , is_global_rule ) [EOL] production_rule_fields . append ( field ) [EOL] action_field = ListField ( production_rule_fields ) [EOL] [EOL] example_string_field = MetadataField ( json_obj [ [string] ] ) [EOL] [EOL] fields = { [string] : question_field , [string] : question_metadata , [string] : table_field , [string] : world_field , [string] : action_field , [string] : example_string_field } [EOL] [EOL] if [string] in json_obj or [string] in json_obj : [EOL] action_map = { action . rule : i for i , action in enumerate ( action_field . field_list ) } [comment] [EOL] if [string] in json_obj : [EOL] action_sequence_fields = [ ] [EOL] for sequence in json_obj [ [string] ] : [EOL] index_fields = [ ] [EOL] for production_rule in sequence : [EOL] index_fields . append ( IndexField ( action_map [ production_rule ] , action_field ) ) [EOL] action_sequence_fields . append ( ListField ( index_fields ) ) [EOL] fields [ [string] ] = ListField ( action_sequence_fields ) [EOL] if [string] in json_obj : [EOL] agenda_index_fields = [ ] [EOL] for agenda_action in json_obj [ [string] ] : [EOL] agenda_index_fields . append ( IndexField ( action_map [ agenda_action ] , action_field ) ) [EOL] fields [ [string] ] = ListField ( agenda_index_fields ) [EOL] return Instance ( fields ) [EOL] [EOL] @ staticmethod def _read_tokens_from_json_list ( json_list ) : [EOL] return [ Token ( text = json_obj [ [string] ] , lemma = json_obj [ [string] ] ) for json_obj in json_list ] [EOL] [EOL] @ staticmethod def _parse_example_line ( lisp_string ) : [EOL] [docstring] [EOL] id_piece , rest = lisp_string . split ( [string] ) [EOL] example_id = id_piece . split ( [string] ) [ [number] ] [EOL] question , rest = rest . split ( [string] ) [EOL] table_filename , rest = rest . split ( [string] ) [EOL] return { [string] : example_id , [string] : question , [string] : table_filename } [EOL] [EOL] @ staticmethod def _should_keep_logical_form ( logical_form ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if logical_form . count ( [string] ) > [number] : [EOL] logger . debug ( f' [string] { logical_form }' ) [EOL] return False [EOL] return True [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] lazy = params . pop ( [string] , False ) [EOL] tables_directory = params . pop ( [string] , None ) [EOL] dpd_output_directory = params . pop ( [string] , None ) [EOL] max_dpd_logical_forms = params . pop_int ( [string] , [number] ) [EOL] sort_dpd_logical_forms = params . pop_bool ( [string] , True ) [EOL] max_dpd_tries = params . pop_int ( [string] , [number] ) [EOL] keep_if_no_dpd = params . pop_bool ( [string] , False ) [EOL] default_tokenizer_params = { [string] : { [string] : [string] , [string] : True } } [EOL] tokenizer = Tokenizer . from_params ( params . pop ( [string] , default_tokenizer_params ) ) [EOL] question_token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] table_token_indexers = TokenIndexer . dict_from_params ( params . pop ( [string] , { } ) ) [EOL] use_table_for_vocab = params . pop_bool ( [string] , False ) [EOL] linking_feature_extracters = params . pop ( [string] , None ) [EOL] include_table_metadata = params . pop_bool ( [string] , False ) [EOL] max_table_tokens = params . pop_int ( [string] , None ) [EOL] output_agendas = params . pop_bool ( [string] , False ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return WikiTablesDatasetReader ( lazy = lazy , tables_directory = tables_directory , dpd_output_directory = dpd_output_directory , max_dpd_logical_forms = max_dpd_logical_forms , sort_dpd_logical_forms = sort_dpd_logical_forms , max_dpd_tries = max_dpd_tries , keep_if_no_dpd = keep_if_no_dpd , tokenizer = tokenizer , question_token_indexers = question_token_indexers , table_token_indexers = table_token_indexers , use_table_for_vocab = use_table_for_vocab , linking_feature_extractors = linking_feature_extracters , include_table_metadata = include_table_metadata , max_table_tokens = max_table_tokens , output_agendas = output_agendas ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $builtins.bool$ 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $typing.Dict[unknown,builtins.int]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $typing.Dict[unknown,builtins.int]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.List[allennlp.data.fields.Field]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $'WikiTablesDatasetReader'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . coreference_resolution . conll import ConllCorefReader [EOL] from allennlp . data . dataset_readers . coreference_resolution . winobias import WinobiasReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . reading_comprehension . squad import SquadReader [EOL] from allennlp . data . dataset_readers . reading_comprehension . triviaqa import TriviaQaReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import argparse [EOL] import builtins [EOL] [docstring] [EOL] import argparse [EOL] [EOL] class Subcommand : [EOL] [docstring] [EOL] def add_subparser ( self , name , parser ) : [EOL] [comment] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $builtins.str$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import torch [EOL] [EOL] [EOL] class LayerNorm ( torch . nn . Module ) : [EOL] [comment] [EOL] [docstring] [EOL] def __init__ ( self , dimension , eps = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] self . gamma = torch . nn . Parameter ( torch . ones ( dimension ) ) [EOL] self . beta = torch . nn . Parameter ( torch . zeros ( dimension ) ) [EOL] self . eps = eps [EOL] [EOL] def forward ( self , tensor ) : [comment] [EOL] mean = tensor . mean ( - [number] , keepdim = True ) [EOL] std = tensor . std ( - [number] , unbiased = False , keepdim = True ) [EOL] return self . gamma * ( tensor - mean ) / ( std + self . eps ) + self . beta [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import torch [EOL] [EOL] [EOL] class TimeDistributed ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , module ) : [EOL] super ( TimeDistributed , self ) . __init__ ( ) [EOL] self . _module = module [EOL] [EOL] def forward ( self , * inputs ) : [comment] [EOL] reshaped_inputs = [ ] [EOL] for input_tensor in inputs : [EOL] input_size = input_tensor . size ( ) [EOL] if len ( input_size ) <= [number] : [EOL] raise RuntimeError ( [string] + str ( input_size ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] squashed_shape = [ - [number] ] + [ x for x in input_size [ [number] : ] ] [EOL] reshaped_inputs . append ( input_tensor . contiguous ( ) . view ( * squashed_shape ) ) [EOL] [EOL] reshaped_outputs = self . _module ( * reshaped_inputs ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] new_shape = [ input_size [ [number] ] , input_size [ [number] ] ] + [ x for x in reshaped_outputs . size ( ) [ [number] : ] ] [EOL] outputs = reshaped_outputs . contiguous ( ) . view ( * new_shape ) [EOL] [EOL] return outputs [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0
from typing import List , Optional , Tuple , Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Optional , Tuple , List [EOL] [EOL] import torch [EOL] [EOL] from allennlp . nn . util import get_dropout_mask [EOL] from allennlp . nn . initializers import block_orthogonal [EOL] [EOL] [EOL] class LstmCellWithProjection ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , input_size , hidden_size , cell_size , go_forward = True , recurrent_dropout_probability = [number] , memory_cell_clip_value = None , state_projection_clip_value = None ) : [EOL] super ( LstmCellWithProjection , self ) . __init__ ( ) [EOL] [comment] [EOL] self . input_size = input_size [EOL] self . hidden_size = hidden_size [EOL] self . cell_size = cell_size [EOL] [EOL] self . go_forward = go_forward [EOL] self . state_projection_clip_value = state_projection_clip_value [EOL] self . memory_cell_clip_value = memory_cell_clip_value [EOL] self . recurrent_dropout_probability = recurrent_dropout_probability [EOL] [EOL] [comment] [EOL] self . input_linearity = torch . nn . Linear ( input_size , [number] * cell_size , bias = False ) [EOL] self . state_linearity = torch . nn . Linear ( hidden_size , [number] * cell_size , bias = True ) [EOL] [EOL] [comment] [EOL] self . state_projection = torch . nn . Linear ( cell_size , hidden_size , bias = False ) [EOL] self . reset_parameters ( ) [EOL] [EOL] def reset_parameters ( self ) : [EOL] [comment] [EOL] block_orthogonal ( self . input_linearity . weight . data , [ self . cell_size , self . input_size ] ) [EOL] block_orthogonal ( self . state_linearity . weight . data , [ self . cell_size , self . hidden_size ] ) [EOL] [EOL] self . state_linearity . bias . data . fill_ ( [number] ) [EOL] [comment] [EOL] [comment] [EOL] self . state_linearity . bias . data [ self . cell_size : [number] * self . cell_size ] . fill_ ( [number] ) [EOL] [EOL] def forward ( self , inputs , batch_lengths , initial_state = None ) : [EOL] [docstring] [EOL] batch_size = inputs . size ( ) [ [number] ] [EOL] total_timesteps = inputs . size ( ) [ [number] ] [EOL] [EOL] output_accumulator = inputs . new_zeros ( batch_size , total_timesteps , self . hidden_size ) [EOL] [EOL] if initial_state is None : [EOL] full_batch_previous_memory = inputs . new_zeros ( batch_size , self . cell_size ) [EOL] full_batch_previous_state = inputs . new_zeros ( batch_size , self . hidden_size ) [EOL] else : [EOL] full_batch_previous_state = initial_state [ [number] ] . squeeze ( [number] ) [EOL] full_batch_previous_memory = initial_state [ [number] ] . squeeze ( [number] ) [EOL] [EOL] current_length_index = batch_size - [number] if self . go_forward else [number] [EOL] if self . recurrent_dropout_probability > [number] and self . training : [EOL] dropout_mask = get_dropout_mask ( self . recurrent_dropout_probability , full_batch_previous_state ) [EOL] else : [EOL] dropout_mask = None [EOL] [EOL] for timestep in range ( total_timesteps ) : [EOL] [comment] [EOL] index = timestep if self . go_forward else total_timesteps - timestep - [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if self . go_forward : [EOL] while batch_lengths [ current_length_index ] <= index : [EOL] current_length_index -= [number] [EOL] [comment] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] while current_length_index < ( len ( batch_lengths ) - [number] ) and batch_lengths [ current_length_index + [number] ] > index : [EOL] current_length_index += [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] previous_memory = full_batch_previous_memory [ [number] : current_length_index + [number] ] . clone ( ) [EOL] [comment] [EOL] previous_state = full_batch_previous_state [ [number] : current_length_index + [number] ] . clone ( ) [EOL] [comment] [EOL] timestep_input = inputs [ [number] : current_length_index + [number] , index ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] projected_input = self . input_linearity ( timestep_input ) [EOL] projected_state = self . state_linearity ( previous_state ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] input_gate = torch . sigmoid ( projected_input [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] + projected_state [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] ) [EOL] forget_gate = torch . sigmoid ( projected_input [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] + projected_state [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] ) [EOL] memory_init = torch . tanh ( projected_input [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] + projected_state [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] ) [EOL] output_gate = torch . sigmoid ( projected_input [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] + projected_state [ : , ( [number] * self . cell_size ) : ( [number] * self . cell_size ) ] ) [EOL] memory = input_gate * memory_init + forget_gate * previous_memory [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if self . memory_cell_clip_value : [EOL] [comment] [EOL] memory = torch . clamp ( memory , - self . memory_cell_clip_value , self . memory_cell_clip_value ) [EOL] [EOL] [comment] [EOL] pre_projection_timestep_output = output_gate * torch . tanh ( memory ) [EOL] [EOL] [comment] [EOL] timestep_output = self . state_projection ( pre_projection_timestep_output ) [EOL] if self . state_projection_clip_value : [EOL] [comment] [EOL] timestep_output = torch . clamp ( timestep_output , - self . state_projection_clip_value , self . state_projection_clip_value ) [EOL] [EOL] [comment] [EOL] if dropout_mask is not None : [EOL] timestep_output = timestep_output * dropout_mask [ [number] : current_length_index + [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] full_batch_previous_memory = full_batch_previous_memory . clone ( ) [EOL] full_batch_previous_state = full_batch_previous_state . clone ( ) [EOL] full_batch_previous_memory [ [number] : current_length_index + [number] ] = memory [EOL] full_batch_previous_state [ [number] : current_length_index + [number] ] = timestep_output [EOL] output_accumulator [ [number] : current_length_index + [number] , index ] = timestep_output [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] final_state = ( full_batch_previous_state . unsqueeze ( [number] ) , full_batch_previous_memory . unsqueeze ( [number] ) ) [EOL] [EOL] return output_accumulator , final_state [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $typing.Optional[builtins.float]$ 0 0 0 $typing.Optional[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Optional[builtins.float]$ 0 $typing.Optional[builtins.float]$ 0 0 0 $typing.Optional[builtins.float]$ 0 $typing.Optional[builtins.float]$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.FloatTensor$ 0 $typing.List[builtins.int]$ 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.FloatTensor$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $None$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Tuple[typing.Any,typing.Any]$ 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import allennlp [EOL] [docstring] [EOL] [EOL] import torch [EOL] [EOL] from overrides import overrides [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . common import Params [EOL] from allennlp . nn . util import masked_softmax [EOL] [EOL] [EOL] class Attention ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] def __init__ ( self , normalize = True ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _normalize = normalize [EOL] [EOL] @ overrides def forward ( self , vector , matrix , matrix_mask = None ) : [EOL] similarities = self . _forward_internal ( vector , matrix ) [EOL] if self . _normalize : [EOL] return masked_softmax ( similarities , matrix_mask ) [EOL] else : [EOL] return similarities [EOL] [EOL] def _forward_internal ( self , vector , matrix ) : [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] clazz = cls . by_name ( params . pop_choice ( [string] , cls . list_available ( ) ) ) [EOL] return clazz . from_params ( params ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $'Attention'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.Params$ 0 0
[EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] [EOL] from overrides import overrides [EOL] from allennlp . common import Params [EOL] from allennlp . modules . attention . attention import Attention [EOL] from allennlp . modules . similarity_functions import DotProductSimilarity , SimilarityFunction [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class LegacyAttention ( Attention ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , similarity_function = None , normalize = True ) : [EOL] super ( ) . __init__ ( normalize ) [EOL] self . _similarity_function = similarity_function or DotProductSimilarity ( ) [EOL] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] tiled_vector = vector . unsqueeze ( [number] ) . expand ( vector . size ( ) [ [number] ] , matrix . size ( ) [ [number] ] , vector . size ( ) [ [number] ] ) [EOL] return self . _similarity_function ( tiled_vector , matrix ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] similarity_function = SimilarityFunction . from_params ( params . pop ( [string] , { } ) ) [EOL] normalize = params . pop_bool ( [string] , True ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( similarity_function = similarity_function , normalize = normalize ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.modules.similarity_functions.SimilarityFunction$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $allennlp.modules.similarity_functions.SimilarityFunction$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 $'Attention'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . common import Params [EOL] from allennlp . modules . attention . legacy_attention import Attention [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class CosineAttention ( Attention ) : [EOL] [docstring] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] a_norm = vector / ( vector . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] b_norm = matrix / ( matrix . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] return torch . bmm ( a_norm . unsqueeze ( dim = [number] ) , b_norm . transpose ( - [number] , - [number] ) ) . squeeze ( [number] ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] normalize = params . pop_bool ( [string] , True ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return CosineAttention ( normalize ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . common import Params [EOL] from allennlp . modules . attention . legacy_attention import Attention [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class DotProductAttention ( Attention ) : [EOL] [docstring] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] return matrix . bmm ( vector . unsqueeze ( - [number] ) ) . squeeze ( - [number] ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] normalize = params . pop_bool ( [string] , True ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return DotProductAttention ( normalize ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . modules . attention . attention import Attention [EOL] from allennlp . nn import Activation [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class BilinearAttention ( Attention ) : [EOL] [docstring] [EOL] def __init__ ( self , vector_dim , matrix_dim , activation = Activation . by_name ( [string] ) ( ) , normalize = True ) : [EOL] super ( ) . __init__ ( normalize ) [EOL] self . _weight_matrix = Parameter ( torch . Tensor ( vector_dim , matrix_dim ) ) [EOL] self . _bias = Parameter ( torch . Tensor ( [number] ) ) [EOL] self . _activation = activation [EOL] self . reset_parameters ( ) [EOL] [EOL] def reset_parameters ( self ) : [EOL] torch . nn . init . xavier_uniform_ ( self . _weight_matrix ) [EOL] self . _bias . data . fill_ ( [number] ) [EOL] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] intermediate = vector . mm ( self . _weight_matrix ) . unsqueeze ( [number] ) [EOL] return self . _activation ( intermediate . bmm ( matrix . transpose ( [number] , [number] ) ) . squeeze ( [number] ) + self . _bias ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] vector_dim = params . pop_int ( [string] ) [EOL] matrix_dim = params . pop_int ( [string] ) [EOL] activation = Activation . by_name ( params . pop ( [string] , [string] ) ) ( ) [EOL] normalize = params . pop_bool ( [string] , True ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return BilinearAttention ( vector_dim = vector_dim , matrix_dim = matrix_dim , activation = activation , normalize = normalize ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import torch [EOL] from allennlp . common . params import Params [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] [EOL] class MatrixAttention ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] def forward ( self , matrix_1 , matrix_2 ) : [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] clazz = cls . by_name ( params . pop_choice ( [string] , cls . list_available ( ) ) ) [EOL] return clazz . from_params ( params ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $'MatrixAttention'$ 0 0 0 $allennlp.common.params.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.params.Params$ 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . modules . matrix_attention . matrix_attention import MatrixAttention [EOL] from allennlp . nn import Activation [EOL] [EOL] [EOL] @ MatrixAttention . register ( [string] ) class BilinearMatrixAttention ( MatrixAttention ) : [EOL] [docstring] [EOL] def __init__ ( self , matrix_1_dim , matrix_2_dim , activation ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _weight_matrix = Parameter ( torch . Tensor ( matrix_1_dim , matrix_2_dim ) ) [EOL] self . _bias = Parameter ( torch . Tensor ( [number] ) ) [EOL] self . _activation = activation [EOL] self . reset_parameters ( ) [EOL] [EOL] def reset_parameters ( self ) : [EOL] torch . nn . init . xavier_uniform_ ( self . _weight_matrix ) [EOL] self . _bias . data . fill_ ( [number] ) [EOL] [EOL] @ overrides def forward ( self , matrix_1 , matrix_2 ) : [EOL] intermediate = matrix_1 . bmm ( self . _weight_matrix . unsqueeze ( [number] ) ) [EOL] return self . _activation ( intermediate . bmm ( matrix_2 . transpose ( [number] , [number] ) ) + self . _bias ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] matrix_1_dim = params . pop_int ( [string] ) [EOL] matrix_2_dim = params . pop_int ( [string] ) [EOL] activation = Activation . by_name ( params . pop ( [string] , [string] ) ) ( ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return BilinearMatrixAttention ( matrix_1_dim = matrix_1_dim , matrix_2_dim = matrix_2_dim , activation = activation ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . modules . similarity_functions . dot_product import DotProductSimilarity [EOL] from allennlp . modules . similarity_functions . similarity_function import SimilarityFunction [EOL] from allennlp . common import Params [EOL] from allennlp . modules . matrix_attention . matrix_attention import MatrixAttention [EOL] [EOL] [EOL] @ MatrixAttention . register ( [string] ) class LegacyMatrixAttention ( MatrixAttention ) : [EOL] [docstring] [EOL] def __init__ ( self , similarity_function = None ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _similarity_function = similarity_function or DotProductSimilarity ( ) [EOL] [EOL] @ overrides def forward ( self , matrix_1 , matrix_2 ) : [EOL] tiled_matrix_1 = matrix_1 . unsqueeze ( [number] ) . expand ( matrix_1 . size ( ) [ [number] ] , matrix_1 . size ( ) [ [number] ] , matrix_2 . size ( ) [ [number] ] , matrix_1 . size ( ) [ [number] ] ) [EOL] tiled_matrix_2 = matrix_2 . unsqueeze ( [number] ) . expand ( matrix_2 . size ( ) [ [number] ] , matrix_1 . size ( ) [ [number] ] , matrix_2 . size ( ) [ [number] ] , matrix_2 . size ( ) [ [number] ] ) [EOL] return self . _similarity_function ( tiled_matrix_1 , tiled_matrix_2 ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] similarity_function = SimilarityFunction . from_params ( params . pop ( [string] , { } ) ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( similarity_function = similarity_function ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.modules.similarity_functions.similarity_function.SimilarityFunction$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.similarity_functions.similarity_function.SimilarityFunction$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $'MatrixAttention'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
import torch [EOL] import allennlp [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . modules . matrix_attention . matrix_attention import MatrixAttention [EOL] [EOL] [EOL] @ MatrixAttention . register ( [string] ) class DotProductMatrixAttention ( MatrixAttention ) : [EOL] [docstring] [EOL] [EOL] @ overrides def forward ( self , matrix_1 , matrix_2 ) : [EOL] return matrix_1 . bmm ( matrix_2 . transpose ( [number] , [number] ) ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] params . assert_empty ( cls . __name__ ) [EOL] return DotProductMatrixAttention ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . span_extractors . endpoint_span_extractor import EndpointSpanExtractor [EOL] from allennlp . modules . span_extractors . self_attentive_span_extractor import SelfAttentiveSpanExtractor [EOL] from allennlp . modules . span_extractors . bidirectional_endpoint_span_extractor import BidirectionalEndpointSpanExtractor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . text_field_embedders . basic_text_field_embedder import BasicTextFieldEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] import math [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . modules . similarity_functions . similarity_function import SimilarityFunction [EOL] from allennlp . nn import Activation , util [EOL] [EOL] [EOL] @ SimilarityFunction . register ( [string] ) class LinearSimilarity ( SimilarityFunction ) : [EOL] [docstring] [EOL] def __init__ ( self , tensor_1_dim , tensor_2_dim , combination = [string] , activation = Activation . by_name ( [string] ) ( ) ) : [EOL] super ( LinearSimilarity , self ) . __init__ ( ) [EOL] self . _combination = combination [EOL] combined_dim = util . get_combined_dim ( combination , [ tensor_1_dim , tensor_2_dim ] ) [EOL] self . _weight_vector = Parameter ( torch . Tensor ( combined_dim ) ) [EOL] self . _bias = Parameter ( torch . Tensor ( [number] ) ) [EOL] self . _activation = activation [EOL] self . reset_parameters ( ) [EOL] [EOL] def reset_parameters ( self ) : [EOL] std = math . sqrt ( [number] / ( self . _weight_vector . size ( [number] ) + [number] ) ) [EOL] self . _weight_vector . data . uniform_ ( - std , std ) [EOL] self . _bias . data . fill_ ( [number] ) [EOL] [EOL] @ overrides def forward ( self , tensor_1 , tensor_2 ) : [EOL] combined_tensors = util . combine_tensors ( self . _combination , [ tensor_1 , tensor_2 ] ) [EOL] dot_product = torch . matmul ( combined_tensors , self . _weight_vector ) [EOL] return self . _activation ( dot_product + self . _bias ) [EOL] [EOL] @ classmethod def from_params ( cls , params ) : [EOL] tensor_1_dim = params . pop_int ( [string] ) [EOL] tensor_2_dim = params . pop_int ( [string] ) [EOL] combination = params . pop ( [string] , [string] ) [EOL] activation = Activation . by_name ( params . pop ( [string] , [string] ) ) ( ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( tensor_1_dim = tensor_1_dim , tensor_2_dim = tensor_2_dim , combination = combination , activation = activation ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 0 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $'LinearSimilarity'$ 0 0 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
[EOL] from typing import Any [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from overrides import overrides [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class DecomposableAttentionPredictor ( Predictor ) : [EOL] [docstring] [EOL] [EOL] def predict ( self , premise , hypothesis ) : [EOL] [docstring] [EOL] return self . predict_json ( { [string] : premise , [string] : hypothesis } ) [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] premise_text = json_dict [ [string] ] [EOL] hypothesis_text = json_dict [ [string] ] [EOL] return self . _dataset_reader . text_to_instance ( premise_text , hypothesis_text ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from allennlp . training . trainer import Trainer [EOL]	0 0 0 0 0 0 0 0 0
from typing import List , Dict , Any [EOL] import logging [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] [docstring] [EOL] [EOL] import logging [EOL] import re [EOL] from typing import List , Any , Dict [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class Optimizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] @ classmethod def from_params ( cls , model_parameters , params ) : [EOL] if isinstance ( params , str ) : [EOL] optimizer = params [EOL] params = Params ( { } ) [EOL] else : [EOL] optimizer = params . pop_choice ( [string] , Optimizer . list_available ( ) ) [EOL] [EOL] [comment] [EOL] groups = params . pop ( [string] , None ) [EOL] if groups : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] parameter_groups = [ { [string] : [ ] } for _ in range ( len ( groups ) + [number] ) ] [EOL] [comment] [EOL] for k in range ( len ( groups ) ) : [comment] [EOL] parameter_groups [ k ] . update ( groups [ k ] [ [number] ] . as_dict ( ) ) [EOL] [EOL] regex_use_counts = { } [EOL] parameter_group_names = [ set ( ) for _ in range ( len ( groups ) + [number] ) ] [EOL] for name , param in model_parameters : [EOL] [comment] [EOL] group_index = None [EOL] for k , group_regexes in enumerate ( groups ) : [EOL] for regex in group_regexes [ [number] ] : [EOL] if regex not in regex_use_counts : [EOL] regex_use_counts [ regex ] = [number] [EOL] if re . search ( regex , name ) : [EOL] if group_index is not None and group_index != k : [EOL] raise ValueError ( [string] . format ( name ) ) [EOL] group_index = k [EOL] regex_use_counts [ regex ] += [number] [EOL] [EOL] if group_index is not None : [EOL] parameter_groups [ group_index ] [ [string] ] . append ( param ) [EOL] parameter_group_names [ group_index ] . add ( name ) [EOL] else : [EOL] [comment] [EOL] parameter_groups [ - [number] ] [ [string] ] . append ( param ) [EOL] parameter_group_names [ - [number] ] . add ( name ) [EOL] [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] for k in range ( len ( groups ) + [number] ) : [EOL] group_options = { key : val for key , val in parameter_groups [ k ] . items ( ) if key != [string] } [EOL] logger . info ( [string] , k , list ( parameter_group_names [ k ] ) , group_options ) [EOL] [comment] [EOL] for regex , count in regex_use_counts . items ( ) : [EOL] if count == [number] : [EOL] logger . warning ( [string] [string] , regex ) [EOL] [EOL] else : [EOL] parameter_groups = [ param for name , param in model_parameters ] [EOL] [EOL] return Optimizer . by_name ( optimizer ) ( parameter_groups , ** params . as_dict ( ) ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] Registrable . _registry [ Optimizer ] = { [string] : torch . optim . Adam , [string] : torch . optim . SparseAdam , [string] : torch . optim . Adagrad , [string] : torch . optim . Adadelta , [string] : torch . optim . SGD , [string] : torch . optim . RMSprop , [string] : torch . optim . Adamax , [string] : torch . optim . ASGD , } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 $typing.List[builtins.set]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.set]$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.set]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.set]$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import TextIO , Any [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import TextIO [EOL] import os [EOL] [EOL] def replace_cr_with_newline ( message ) : [EOL] [docstring] [EOL] if [string] in message : [EOL] message = message . replace ( [string] , [string] ) [EOL] if not message or message [ - [number] ] != [string] : [EOL] message += [string] [EOL] return message [EOL] [EOL] class TeeLogger : [EOL] [docstring] [EOL] def __init__ ( self , filename , terminal , file_friendly_terminal_output ) : [EOL] self . terminal = terminal [EOL] self . file_friendly_terminal_output = file_friendly_terminal_output [EOL] parent_directory = os . path . dirname ( filename ) [EOL] os . makedirs ( parent_directory , exist_ok = True ) [EOL] self . log = open ( filename , [string] ) [EOL] [EOL] def write ( self , message ) : [EOL] cleaned = replace_cr_with_newline ( message ) [EOL] [EOL] if self . file_friendly_terminal_output : [EOL] self . terminal . write ( cleaned ) [EOL] else : [EOL] self . terminal . write ( message ) [EOL] [EOL] self . log . write ( cleaned ) [EOL] [EOL] def flush ( self ) : [EOL] self . terminal . flush ( ) [EOL] self . log . flush ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $typing.TextIO$ 0 $builtins.bool$ 0 0 0 0 0 $typing.TextIO$ 0 $typing.TextIO$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . common . params import Params [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . common . tee_logger import TeeLogger [EOL] from allennlp . common . tqdm import Tqdm [EOL] from allennlp . common . util import JsonDict [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Tuple , Any [EOL] import typing [EOL] import builtins [EOL] import copy [EOL] [EOL] from numpy . testing import assert_allclose [EOL] import torch [EOL] [EOL] from allennlp . commands . train import train_model_from_file [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . data import DataIterator , DatasetReader , Vocabulary [EOL] from allennlp . data . dataset import Batch [EOL] from allennlp . models import Model , load_archive [EOL] [EOL] [EOL] class ModelTestCase ( AllenNlpTestCase ) : [EOL] [docstring] [EOL] def set_up_model ( self , param_file , dataset_file ) : [EOL] [comment] [EOL] self . param_file = param_file [EOL] params = Params . from_file ( self . param_file ) [EOL] [EOL] reader = DatasetReader . from_params ( params [ [string] ] ) [EOL] instances = reader . read ( dataset_file ) [EOL] [comment] [EOL] [comment] [EOL] if [string] in params : [EOL] vocab_params = params [ [string] ] [EOL] vocab = Vocabulary . from_params ( params = vocab_params , instances = instances ) [EOL] else : [EOL] vocab = Vocabulary . from_instances ( instances ) [EOL] self . vocab = vocab [EOL] self . instances = instances [EOL] self . model = Model . from_params ( self . vocab , params [ [string] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . dataset = Batch ( self . instances ) [EOL] self . dataset . index_instances ( self . vocab ) [EOL] [EOL] def ensure_model_can_train_save_and_load ( self , param_file , tolerance = [number] , cuda_device = - [number] ) : [EOL] save_dir = self . TEST_DIR / [string] [EOL] archive_file = save_dir / [string] [EOL] model = train_model_from_file ( param_file , save_dir ) [EOL] loaded_model = load_archive ( archive_file , cuda_device = cuda_device ) . model [EOL] state_keys = model . state_dict ( ) . keys ( ) [EOL] loaded_state_keys = loaded_model . state_dict ( ) . keys ( ) [EOL] assert state_keys == loaded_state_keys [EOL] [comment] [EOL] for key in state_keys : [EOL] assert_allclose ( model . state_dict ( ) [ key ] . cpu ( ) . numpy ( ) , loaded_model . state_dict ( ) [ key ] . cpu ( ) . numpy ( ) , err_msg = key ) [EOL] params = Params . from_file ( param_file ) [EOL] reader = DatasetReader . from_params ( params [ [string] ] ) [EOL] [EOL] [comment] [EOL] iterator_params = params [ [string] ] [EOL] iterator_params2 = Params ( copy . deepcopy ( iterator_params . as_dict ( ) ) ) [EOL] [EOL] iterator = DataIterator . from_params ( iterator_params ) [EOL] iterator2 = DataIterator . from_params ( iterator_params2 ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] model_dataset = reader . read ( params [ [string] ] ) [EOL] iterator . index_with ( model . vocab ) [EOL] model_batch = next ( iterator ( model_dataset , shuffle = False , cuda_device = cuda_device ) ) [EOL] [EOL] loaded_dataset = reader . read ( params [ [string] ] ) [EOL] iterator2 . index_with ( loaded_model . vocab ) [EOL] loaded_batch = next ( iterator2 ( loaded_dataset , shuffle = False , cuda_device = cuda_device ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . check_model_computes_gradients_correctly ( model , model_batch ) [EOL] [EOL] [comment] [EOL] assert model_batch . keys ( ) == loaded_batch . keys ( ) [EOL] for key in model_batch . keys ( ) : [EOL] self . assert_fields_equal ( model_batch [ key ] , loaded_batch [ key ] , key , [number] ) [EOL] [EOL] [comment] [EOL] model . eval ( ) [EOL] loaded_model . eval ( ) [EOL] [comment] [EOL] [comment] [EOL] for model_ in [ model , loaded_model ] : [EOL] for module in model_ . modules ( ) : [EOL] if hasattr ( module , [string] ) and module . stateful : [EOL] module . reset_states ( ) [EOL] model_predictions = model ( ** model_batch ) [EOL] loaded_model_predictions = loaded_model ( ** loaded_batch ) [EOL] [EOL] [comment] [EOL] loaded_model_loss = loaded_model_predictions [ [string] ] [EOL] assert loaded_model_loss is not None [EOL] loaded_model_loss . backward ( ) [EOL] [EOL] [comment] [EOL] for key in model_predictions . keys ( ) : [EOL] self . assert_fields_equal ( model_predictions [ key ] , loaded_model_predictions [ key ] , name = key , tolerance = tolerance ) [EOL] [EOL] return model , loaded_model [EOL] [EOL] def assert_fields_equal ( self , field1 , field2 , name , tolerance = [number] ) : [EOL] if isinstance ( field1 , torch . Tensor ) : [EOL] assert_allclose ( field1 . detach ( ) . cpu ( ) . numpy ( ) , field2 . detach ( ) . cpu ( ) . numpy ( ) , rtol = tolerance , err_msg = name ) [EOL] elif isinstance ( field1 , dict ) : [EOL] assert field1 . keys ( ) == field2 . keys ( ) [EOL] for key in field1 : [EOL] self . assert_fields_equal ( field1 [ key ] , field2 [ key ] , tolerance = tolerance , name = name + [string] + str ( key ) ) [EOL] elif isinstance ( field1 , ( list , tuple ) ) : [EOL] assert len ( field1 ) == len ( field2 ) [EOL] for i , ( subfield1 , subfield2 ) in enumerate ( zip ( field1 , field2 ) ) : [EOL] self . assert_fields_equal ( subfield1 , subfield2 , tolerance = tolerance , name = name + f" [string] { i } [string] " ) [EOL] elif isinstance ( field1 , ( float , int ) ) : [EOL] assert_allclose ( [ field1 ] , [ field2 ] , rtol = tolerance , err_msg = name ) [EOL] else : [EOL] assert field1 == field2 [EOL] [EOL] @ staticmethod def check_model_computes_gradients_correctly ( model , model_batch ) : [EOL] model . zero_grad ( ) [EOL] result = model ( ** model_batch ) [EOL] result [ [string] ] . backward ( ) [EOL] has_zero_or_none_grads = { } [EOL] for name , parameter in model . named_parameters ( ) : [EOL] zeros = torch . zeros ( parameter . size ( ) ) [EOL] if parameter . requires_grad : [EOL] [EOL] if parameter . grad is None : [EOL] has_zero_or_none_grads [ name ] = [string] [EOL] [EOL] elif parameter . grad . is_sparse or parameter . grad . data . is_sparse : [EOL] pass [EOL] [EOL] [comment] [EOL] [comment] [EOL] elif ( parameter . grad . cpu ( ) == zeros ) . all ( ) : [EOL] has_zero_or_none_grads [ name ] = f" [string] { tuple ( parameter . grad . size ( ) ) } [string] " [EOL] else : [EOL] assert parameter . grad is None [EOL] [EOL] if has_zero_or_none_grads : [EOL] for name , grad in has_zero_or_none_grads . items ( ) : [EOL] print ( f" [string] { name } [string] { grad }" ) [EOL] raise Exception ( [string] ) [EOL] [EOL] def ensure_batch_predictions_are_consistent ( self ) : [EOL] self . model . eval ( ) [EOL] single_predictions = [ ] [EOL] for i , instance in enumerate ( self . instances ) : [EOL] dataset = Batch ( [ instance ] ) [EOL] tensors = dataset . as_tensor_dict ( dataset . get_padding_lengths ( ) ) [EOL] result = self . model ( ** tensors ) [EOL] single_predictions . append ( result ) [EOL] full_dataset = Batch ( self . instances ) [EOL] batch_tensors = full_dataset . as_tensor_dict ( full_dataset . get_padding_lengths ( ) ) [EOL] batch_predictions = self . model ( ** batch_tensors ) [EOL] for i , instance_predictions in enumerate ( single_predictions ) : [EOL] for key , single_predicted in instance_predictions . items ( ) : [EOL] tolerance = [number] [EOL] if key == [string] : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] single_predicted = single_predicted [ [number] ] [EOL] batch_predicted = batch_predictions [ key ] [ i ] [EOL] if isinstance ( single_predicted , torch . Tensor ) : [EOL] if single_predicted . size ( ) != batch_predicted . size ( ) : [EOL] slices = tuple ( slice ( [number] , size ) for size in single_predicted . size ( ) ) [EOL] batch_predicted = batch_predicted [ slices ] [EOL] assert_allclose ( single_predicted . data . numpy ( ) , batch_predicted . data . numpy ( ) , atol = tolerance , err_msg = key ) [EOL] else : [EOL] assert single_predicted == batch_predicted , key [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Tuple[builtins.slice,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Tuple[builtins.slice,...]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0
[docstring] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
from typing import Literal , Any , List , Union , Set [EOL] import typing [EOL] import typing_extensions [EOL] import codecs [EOL] import gzip [EOL] import zipfile [EOL] from copy import deepcopy [EOL] import copy [EOL] import shutil [EOL] import pytest [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Instance , Token [EOL] from allennlp . data . dataset import Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenCharactersIndexer [EOL] from allennlp . data . tokenizers import CharacterTokenizer [EOL] from allennlp . data . vocabulary import ( Vocabulary , _NamespaceDependentDefaultDict , DEFAULT_OOV_TOKEN , _read_pretrained_tokens ) [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules . token_embedders . embedding import format_embeddings_file_uri [EOL] [EOL] [EOL] class TestVocabulary ( AllenNlpTestCase ) : [EOL] [comment] [EOL] [EOL] def setUp ( self ) : [EOL] token_indexer = SingleIdTokenIndexer ( [string] ) [EOL] text_field = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , { [string] : token_indexer } ) [EOL] self . instance = Instance ( { [string] : text_field } ) [EOL] self . dataset = Batch ( [ self . instance ] ) [EOL] super ( TestVocabulary , self ) . setUp ( ) [EOL] [EOL] def test_from_dataset_respects_max_vocab_size_single_int ( self ) : [EOL] max_vocab_size = [number] [EOL] vocab = Vocabulary . from_instances ( self . dataset , max_vocab_size = max_vocab_size ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] [comment] [EOL] assert len ( words ) == max_vocab_size + [number] [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = None ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert len ( words ) == [number] [EOL] [EOL] def test_from_dataset_respects_min_count ( self ) : [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = { [string] : [number] } ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] assert [string] not in words [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = None ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] in words [EOL] assert [string] in words [EOL] [EOL] def test_from_dataset_respects_exclusive_embedding_file ( self ) : [EOL] embeddings_filename = str ( self . TEST_DIR / [string] ) [EOL] with gzip . open ( embeddings_filename , [string] ) as embeddings_file : [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = { [string] : [number] } , pretrained_files = { [string] : embeddings_filename } , only_include_pretrained_words = True ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] assert [string] not in words [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , pretrained_files = { [string] : embeddings_filename } , only_include_pretrained_words = True ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] [EOL] def test_from_dataset_respects_inclusive_embedding_file ( self ) : [EOL] embeddings_filename = str ( self . TEST_DIR / [string] ) [EOL] with gzip . open ( embeddings_filename , [string] ) as embeddings_file : [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = { [string] : [number] } , pretrained_files = { [string] : embeddings_filename } , only_include_pretrained_words = False ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , pretrained_files = { [string] : embeddings_filename } , only_include_pretrained_words = False ) [EOL] words = vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert [string] in words [EOL] assert [string] in words [EOL] assert [string] in words [EOL] [EOL] def test_add_word_to_index_gives_consistent_results ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] initial_vocab_size = vocab . get_vocab_size ( ) [EOL] word_index = vocab . add_token_to_namespace ( [string] ) [EOL] assert [string] in vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert vocab . get_token_index ( [string] ) == word_index [EOL] assert vocab . get_token_from_index ( word_index ) == [string] [EOL] assert vocab . get_vocab_size ( ) == initial_vocab_size + [number] [EOL] [EOL] [comment] [EOL] vocab . add_token_to_namespace ( [string] ) [EOL] assert [string] in vocab . get_index_to_token_vocabulary ( ) . values ( ) [EOL] assert vocab . get_token_index ( [string] ) == word_index [EOL] assert vocab . get_token_from_index ( word_index ) == [string] [EOL] assert vocab . get_vocab_size ( ) == initial_vocab_size + [number] [EOL] [EOL] def test_namespaces ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] initial_vocab_size = vocab . get_vocab_size ( ) [EOL] word_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] assert [string] in vocab . get_index_to_token_vocabulary ( namespace = [string] ) . values ( ) [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == word_index [EOL] assert vocab . get_token_from_index ( word_index , namespace = [string] ) == [string] [EOL] assert vocab . get_vocab_size ( namespace = [string] ) == initial_vocab_size + [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] word2_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] word_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] assert [string] in vocab . get_index_to_token_vocabulary ( namespace = [string] ) . values ( ) [EOL] assert [string] in vocab . get_index_to_token_vocabulary ( namespace = [string] ) . values ( ) [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == word_index [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == word2_index [EOL] assert vocab . get_token_from_index ( word_index , namespace = [string] ) == [string] [EOL] assert vocab . get_token_from_index ( word2_index , namespace = [string] ) == [string] [EOL] assert vocab . get_vocab_size ( namespace = [string] ) == initial_vocab_size + [number] [EOL] [EOL] def test_namespace_dependent_default_dict ( self ) : [EOL] default_dict = _NamespaceDependentDefaultDict ( [ [string] , [string] ] , lambda : [number] , lambda : [number] ) [EOL] [comment] [EOL] assert default_dict [ [string] ] == [number] [EOL] [comment] [EOL] assert default_dict [ [string] ] == [number] [EOL] [comment] [EOL] assert default_dict [ [string] ] == [number] [EOL] assert default_dict [ [string] ] == [number] [EOL] [EOL] def test_unknown_token ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] vocab = Vocabulary ( ) [EOL] oov_token = vocab . _oov_token [EOL] oov_index = vocab . get_token_index ( oov_token ) [EOL] assert oov_index == [number] [EOL] assert vocab . get_token_index ( [string] ) == oov_index [EOL] [EOL] def test_set_from_file_reads_padded_files ( self ) : [EOL] [comment] [EOL] vocab_filename = self . TEST_DIR / [string] [EOL] with codecs . open ( vocab_filename , [string] , [string] ) as vocab_file : [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] vocab . set_from_file ( vocab_filename , is_padded = True , oov_token = [string] ) [EOL] [EOL] assert vocab . _oov_token == DEFAULT_OOV_TOKEN [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( DEFAULT_OOV_TOKEN ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] ) == [number] [EOL] assert vocab . get_token_from_index ( [number] ) == vocab . _padding_token [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] ) == DEFAULT_OOV_TOKEN [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] ) == [string] [EOL] [EOL] def test_set_from_file_reads_non_padded_files ( self ) : [EOL] [comment] [EOL] vocab_filename = self . TEST_DIR / [string] [EOL] with codecs . open ( vocab_filename , [string] , [string] ) as vocab_file : [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] vocab_file . write ( [string] ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] vocab . set_from_file ( vocab_filename , is_padded = False , namespace = [string] ) [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] [EOL] def test_saving_and_loading ( self ) : [EOL] [comment] [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] [EOL] vocab = Vocabulary ( non_padded_namespaces = [ [string] , [string] ] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] [EOL] vocab . save_to_files ( vocab_dir ) [EOL] vocab2 = Vocabulary . from_files ( vocab_dir ) [EOL] [EOL] assert vocab2 . _non_padded_namespaces == { [string] , [string] } [EOL] [EOL] [comment] [EOL] assert vocab2 . get_vocab_size ( namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab2 . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] [EOL] [comment] [EOL] assert vocab2 . get_vocab_size ( namespace = [string] ) == [number] [comment] [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == vocab . _padding_token [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == vocab . _oov_token [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab2 . get_token_from_index ( [number] , namespace = [string] ) == [string] [EOL] assert vocab2 . get_token_index ( vocab . _padding_token , namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_index ( vocab . _oov_token , namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] assert vocab2 . get_token_index ( [string] , namespace = [string] ) == [number] [EOL] [EOL] [comment] [EOL] assert vocab . get_index_to_token_vocabulary ( [string] ) == vocab2 . get_index_to_token_vocabulary ( [string] ) [EOL] assert vocab . get_index_to_token_vocabulary ( [string] ) == vocab2 . get_index_to_token_vocabulary ( [string] ) [EOL] [EOL] def test_saving_and_loading_works_with_byte_encoding ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] tokenizer = CharacterTokenizer ( byte_encoding = [string] ) [EOL] token_indexer = TokenCharactersIndexer ( character_tokenizer = tokenizer ) [EOL] tokens = [ Token ( t ) for t in [ [string] , [string] , [string] ] ] [EOL] text_field = TextField ( tokens , { [string] : token_indexer } ) [EOL] dataset = Batch ( [ Instance ( { [string] : text_field } ) ] ) [EOL] vocab = Vocabulary . from_instances ( dataset ) [EOL] text_field . index ( vocab ) [EOL] indexed_tokens = deepcopy ( text_field . _indexed_tokens ) [comment] [EOL] [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] vocab . save_to_files ( vocab_dir ) [EOL] vocab2 = Vocabulary . from_files ( vocab_dir ) [EOL] text_field2 = TextField ( tokens , { [string] : token_indexer } ) [EOL] text_field2 . index ( vocab2 ) [EOL] indexed_tokens2 = deepcopy ( text_field2 . _indexed_tokens ) [comment] [EOL] assert indexed_tokens == indexed_tokens2 [EOL] [EOL] def test_from_params ( self ) : [EOL] [comment] [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] vocab = Vocabulary ( non_padded_namespaces = [ [string] , [string] ] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . save_to_files ( vocab_dir ) [EOL] [EOL] params = Params ( { [string] : vocab_dir } ) [EOL] vocab2 = Vocabulary . from_params ( params ) [EOL] assert vocab . get_index_to_token_vocabulary ( [string] ) == vocab2 . get_index_to_token_vocabulary ( [string] ) [EOL] assert vocab . get_index_to_token_vocabulary ( [string] ) == vocab2 . get_index_to_token_vocabulary ( [string] ) [EOL] [EOL] [comment] [EOL] vocab2 = Vocabulary . from_params ( Params ( { } ) , self . dataset ) [EOL] assert vocab2 . get_index_to_token_vocabulary ( [string] ) == { [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] } [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( Params ( { } ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( Params ( { [string] : vocab_dir , [string] : { [string] : [number] } } ) ) [EOL] [EOL] def test_from_params_adds_tokens_to_vocab ( self ) : [EOL] vocab = Vocabulary . from_params ( Params ( { [string] : { [string] : [ [string] , [string] , [string] ] } } ) , self . dataset ) [EOL] assert vocab . get_index_to_token_vocabulary ( [string] ) == { [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] , [number] : [string] } [EOL] [EOL] def test_valid_vocab_extension ( self ) : [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] extension_ways = [ [string] , [string] ] [EOL] [comment] [EOL] non_padded_namespaces_list = [ [ ] , [ [string] ] ] [EOL] for non_padded_namespaces in non_padded_namespaces_list : [EOL] original_vocab = Vocabulary ( non_padded_namespaces = non_padded_namespaces ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] text_field = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] instances = Batch ( [ Instance ( { [string] : text_field } ) ] ) [EOL] for way in extension_ways : [EOL] if way == [string] : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] params = Params ( { [string] : non_padded_namespaces } ) [EOL] extended_vocab . extend_from_instances ( params , instances ) [EOL] else : [EOL] shutil . rmtree ( vocab_dir , ignore_errors = True ) [EOL] original_vocab . save_to_files ( vocab_dir ) [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : non_padded_namespaces } ) [EOL] extended_vocab = Vocabulary . from_params ( params , instances ) [EOL] [EOL] extra_count = [number] if extended_vocab . is_padded ( [string] ) else [number] [EOL] assert extended_vocab . get_token_index ( [string] , [string] ) == [number] + extra_count [EOL] assert extended_vocab . get_token_index ( [string] , [string] ) == [number] + extra_count [EOL] assert extended_vocab . get_token_index ( [string] , [string] ) == [number] + extra_count [EOL] [EOL] assert extended_vocab . get_token_index ( [string] , [string] ) [comment] [EOL] assert extended_vocab . get_token_index ( [string] , [string] ) [comment] [EOL] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] + extra_count [EOL] [EOL] [comment] [EOL] non_padded_namespaces_list = [ [ ] , [ [string] ] , [ [string] , [string] ] ] [EOL] for non_padded_namespaces in non_padded_namespaces_list : [EOL] original_vocab = Vocabulary ( non_padded_namespaces = non_padded_namespaces ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] text_field = TextField ( [ Token ( t ) for t in [ [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] instances = Batch ( [ Instance ( { [string] : text_field } ) ] ) [EOL] [EOL] for way in extension_ways : [EOL] if way == [string] : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] params = Params ( { [string] : non_padded_namespaces } ) [EOL] extended_vocab . extend_from_instances ( params , instances ) [EOL] else : [EOL] shutil . rmtree ( vocab_dir , ignore_errors = True ) [EOL] original_vocab . save_to_files ( vocab_dir ) [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : non_padded_namespaces } ) [EOL] extended_vocab = Vocabulary . from_params ( params , instances ) [EOL] [EOL] [comment] [EOL] assert len ( extended_vocab . _token_to_index ) == [number] [EOL] [EOL] extra_count = [number] if extended_vocab . is_padded ( [string] ) else [number] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] + extra_count [EOL] [EOL] extra_count = [number] if extended_vocab . is_padded ( [string] ) else [number] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] + extra_count [EOL] [EOL] def test_invalid_vocab_extension ( self ) : [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] original_vocab = Vocabulary ( non_padded_namespaces = [ [string] ] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . save_to_files ( vocab_dir ) [EOL] text_field1 = TextField ( [ Token ( t ) for t in [ [string] [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] text_field2 = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] instances = Batch ( [ Instance ( { [string] : text_field1 , [string] : text_field2 } ) ] ) [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : [ ] } ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( params , instances ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] params = Params ( { [string] : [ ] } ) [EOL] extended_vocab . extend_from_instances ( params , instances ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] extended_vocab . _extend ( non_padded_namespaces = [ ] , tokens_to_add = { [string] : [ [string] ] , [string] : [ [string] ] } ) [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : [ [string] ] } ) [EOL] Vocabulary . from_params ( params , instances ) [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] params = Params ( { [string] : [ [string] ] } ) [EOL] extended_vocab . extend_from_instances ( params , instances ) [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] extended_vocab . _extend ( non_padded_namespaces = [ [string] ] , tokens_to_add = { [string] : [ [string] ] , [string] : [ [string] ] } ) [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : [ [string] , [string] ] } ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( params , instances ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] params = Params ( { [string] : [ [string] , [string] ] } ) [EOL] extended_vocab . extend_from_instances ( params , instances ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] extended_vocab = copy . copy ( original_vocab ) [EOL] extended_vocab . _extend ( non_padded_namespaces = [ [string] , [string] ] , tokens_to_add = { [string] : [ [string] ] , [string] : [ [string] ] } ) [EOL] [EOL] def test_from_params_extend_config ( self ) : [EOL] [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] original_vocab = Vocabulary ( non_padded_namespaces = [ [string] ] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] original_vocab . save_to_files ( vocab_dir ) [EOL] [EOL] text_field = TextField ( [ Token ( t ) for t in [ [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] instances = Batch ( [ Instance ( { [string] : text_field } ) ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : vocab_dir , [string] : True } ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( params ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : True } ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = Vocabulary . from_params ( params , instances ) [EOL] [EOL] def test_from_params_valid_vocab_extension_thoroughly ( self ) : [EOL] [docstring] [EOL] [EOL] vocab_dir = self . TEST_DIR / [string] [EOL] original_vocab = Vocabulary ( non_padded_namespaces = [ [string] , [string] ] ) [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] original_vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] original_vocab . save_to_files ( vocab_dir ) [EOL] [EOL] text_field0 = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] text_field1 = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] text_field4 = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] text_field5 = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] instances = Batch ( [ Instance ( { [string] : text_field0 , [string] : text_field1 , [string] : text_field4 , [string] : text_field5 } ) ] ) [EOL] [EOL] params = Params ( { [string] : vocab_dir , [string] : True , [string] : [ [string] , [string] ] } ) [EOL] extended_vocab = Vocabulary . from_params ( params , instances ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] extended_namespaces = { * extended_vocab . _token_to_index } [EOL] assert extended_namespaces == { [string] . format ( i ) for i in range ( [number] ) } [EOL] [EOL] [comment] [EOL] assert extended_vocab . _non_padded_namespaces == { [string] , [string] , [string] } [EOL] [EOL] [comment] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == original_vocab . get_vocab_size ( [string] ) [EOL] assert extended_vocab . get_vocab_size ( [string] ) == original_vocab . get_vocab_size ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] [comment] [EOL] assert extended_vocab . get_vocab_size ( [string] ) == [number] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] for namespace , token2index in original_vocab . _token_to_index . items ( ) : [EOL] for token , _ in token2index . items ( ) : [EOL] vocab_index = original_vocab . get_token_index ( token , namespace ) [EOL] extended_vocab_index = extended_vocab . get_token_index ( token , namespace ) [EOL] assert vocab_index == extended_vocab_index [EOL] [comment] [EOL] for namespace , index2token in original_vocab . _index_to_token . items ( ) : [EOL] for index , _ in index2token . items ( ) : [EOL] vocab_token = original_vocab . get_token_from_index ( index , namespace ) [EOL] extended_vocab_token = extended_vocab . get_token_from_index ( index , namespace ) [EOL] assert vocab_token == extended_vocab_token [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def test_vocab_can_print ( self ) : [EOL] vocab = Vocabulary ( non_padded_namespaces = [ [string] , [string] ] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] print ( vocab ) [EOL] [EOL] def test_read_pretrained_words ( self ) : [EOL] [comment] [EOL] words = set ( [string] [string] . split ( [string] ) ) [EOL] [EOL] [comment] [EOL] base_path = str ( self . FIXTURES_ROOT / [string] ) [EOL] for ext in [ [string] , [string] , [string] , [string] , [string] , [string] ] : [EOL] file_path = base_path + ext [EOL] words_read = _read_pretrained_tokens ( file_path ) [EOL] assert words_read == words , f" [string] { file_path } [string] " f" [string] { sorted ( words_read ) } [string] " f" [string] { sorted ( words ) }" [EOL] [EOL] [comment] [EOL] base_path = str ( self . FIXTURES_ROOT / [string] ) [EOL] file_path = [string] [EOL] for ext in [ [string] , [string] ] : [EOL] archive_path = base_path + ext [EOL] embeddings_file_uri = format_embeddings_file_uri ( archive_path , file_path ) [EOL] words_read = _read_pretrained_tokens ( embeddings_file_uri ) [EOL] assert words_read == words , f" [string] { archive_path } [string] " f" [string] { sorted ( words_read ) } [string] " f" [string] { sorted ( words ) }" [EOL] [EOL] def test_from_instances_exclusive_embeddings_file_inside_archive ( self ) : [EOL] [docstring] [EOL] [comment] [EOL] archive_path = str ( self . TEST_DIR / [string] ) [EOL] [EOL] with zipfile . ZipFile ( archive_path , [string] ) as archive : [EOL] file_path = [string] [EOL] with archive . open ( file_path , [string] ) as embeddings_file : [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] embeddings_file . write ( [string] . encode ( [string] ) ) [EOL] [EOL] with archive . open ( [string] , [string] ) as dummy_file : [EOL] dummy_file . write ( [string] . encode ( [string] ) ) [EOL] [EOL] embeddings_file_uri = format_embeddings_file_uri ( archive_path , file_path ) [EOL] vocab = Vocabulary . from_instances ( self . dataset , min_count = { [string] : [number] } , pretrained_files = { [string] : embeddings_file_uri } , only_include_pretrained_words = True ) [EOL] [EOL] words = set ( vocab . get_index_to_token_vocabulary ( ) . values ( ) ) [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] assert [string] not in words [EOL] [EOL] vocab = Vocabulary . from_instances ( self . dataset , pretrained_files = { [string] : embeddings_file_uri } , only_include_pretrained_words = True ) [EOL] words = set ( vocab . get_index_to_token_vocabulary ( ) . values ( ) ) [EOL] assert [string] in words [EOL] assert [string] in words [EOL] assert [string] not in words [EOL] [EOL] def test_registrability ( self ) : [EOL] [EOL] @ Vocabulary . register ( [string] ) class MyVocabulary : [EOL] @ classmethod def from_params ( cls , params , instances = None ) : [EOL] [comment] [EOL] return MyVocabulary ( ) [EOL] [EOL] [EOL] params = Params ( { [string] : [string] } ) [EOL] [EOL] instance = Instance ( fields = { } ) [EOL] [EOL] vocab = Vocabulary . from_params ( params = params , instances = [ instance ] ) [EOL] [EOL] assert isinstance ( vocab , MyVocabulary ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Set[builtins.str]$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Set[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] from allennlp . data . dataset_readers import CcgBankDatasetReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestCcgBankReader ( AllenNlpTestCase ) : [EOL] [EOL] def test_read_from_file ( self ) : [EOL] [EOL] reader = CcgBankDatasetReader ( ) [EOL] instances = ensure_list ( reader . read ( self . FIXTURES_ROOT / [string] / [string] ) ) [EOL] [EOL] assert len ( instances ) == [number] [EOL] [EOL] instance = instances [ [number] ] [EOL] fields = instance . fields [EOL] tokens = [ token . text for token in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] ccg_categories = fields [ [string] ] . labels [EOL] assert ccg_categories == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] original_pos_tags = fields [ [string] ] . labels [EOL] assert original_pos_tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] modified_pos_tags = fields [ [string] ] . labels [EOL] assert modified_pos_tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] predicate_arg_categories = fields [ [string] ] . labels [EOL] assert predicate_arg_categories == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . data . dataset_readers . conll2003 import Conll2003DatasetReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] class TestConll2003Reader ( ) : [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) @ pytest . mark . parametrize ( [string] , ( [string] , [string] ) ) def test_read_from_file ( self , lazy , coding_scheme ) : [EOL] conll_reader = Conll2003DatasetReader ( lazy = lazy , coding_scheme = coding_scheme ) [EOL] instances = conll_reader . read ( str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] if coding_scheme == [string] : [EOL] expected_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] else : [EOL] expected_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == expected_labels [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . labels == expected_labels [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0
[comment] [EOL] from typing import List , Dict , Union , Any [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . data . dataset_readers import SnliReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] class TestSnliReader ( ) : [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file ( self , lazy ) : [EOL] reader = SnliReader ( lazy = lazy ) [EOL] instances = reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0
	0
[comment] [EOL] from typing import List , Tuple , Any [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataset_readers . reading_comprehension import util [EOL] from allennlp . data . tokenizers import WordTokenizer [EOL] [EOL] [EOL] class TestReadingComprehensionUtil ( AllenNlpTestCase ) : [EOL] def test_char_span_to_token_span_handles_easy_cases ( self ) : [EOL] [comment] [EOL] tokenizer = WordTokenizer ( ) [EOL] passage = [string] + [string] + [string] + [string] [EOL] tokens = tokenizer . tokenize ( passage ) [EOL] offsets = [ ( t . idx , t . idx + len ( t . text ) ) for t in tokens ] [EOL] [comment] [EOL] token_span = util . char_span_to_token_span ( offsets , ( [number] , [number] ) ) [ [number] ] [EOL] assert token_span == ( [number] , [number] ) [EOL] [comment] [EOL] token_span = util . char_span_to_token_span ( offsets , ( [number] , [number] ) ) [ [number] ] [EOL] assert token_span == ( [number] , [number] ) [EOL] [comment] [EOL] token_span = util . char_span_to_token_span ( offsets , ( [number] , [number] ) ) [ [number] ] [EOL] assert token_span == ( [number] , [number] ) [EOL] [EOL] def test_char_span_to_token_span_handles_hard_cases ( self ) : [EOL] [comment] [EOL] [comment] [EOL] tokenizer = WordTokenizer ( ) [EOL] passage = [string] + [string] + [string] + [string] + [string] + [string] + [string] + [string] + [string] + [string] + [string] + [string] [EOL] start = [number] [EOL] end = [number] + len ( [string] ) [EOL] tokens = tokenizer . tokenize ( passage ) [EOL] offsets = [ ( t . idx , t . idx + len ( t . text ) ) for t in tokens ] [EOL] token_span = util . char_span_to_token_span ( offsets , ( start , end ) ) [ [number] ] [EOL] assert token_span == ( [number] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Tuple[unknown,builtins.int]]$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
	0
	0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . token_indexers import ELMoTokenCharactersIndexer [EOL] [EOL] [EOL] class TestELMoTokenCharactersIndexer ( AllenNlpTestCase ) : [EOL] def test_bos_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . token_to_indices ( Token ( [string] ) , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert indices == expected_indices [EOL] [EOL] def test_eos_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . token_to_indices ( Token ( [string] ) , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert indices == expected_indices [EOL] [EOL] def test_unicode_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . token_to_indices ( Token ( chr ( [number] ) + [string] ) , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] assert indices == expected_indices [EOL] [EOL] def test_elmo_as_array_produces_token_sequence ( self ) : [comment] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = [ indexer . token_to_indices ( Token ( token ) , Vocabulary ( ) ) for token in [ [string] , [string] ] ] [EOL] padded_tokens = indexer . pad_token_sequence ( indices , desired_num_tokens = [number] , padding_lengths = { } ) [EOL] expected_padded_tokens = [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ] [EOL] [EOL] assert padded_tokens == expected_padded_tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[builtins.int]]$ 0
	0
	0
	0
[comment] [EOL] import os [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . training . metrics . wikitables_accuracy import SEMPRE_ABBREVIATIONS_PATH , SEMPRE_GRAMMAR_PATH [EOL] [EOL] class WikiTablesErmSemanticParserTest ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] self . should_remove_sempre_abbreviations = not os . path . exists ( SEMPRE_ABBREVIATIONS_PATH ) [EOL] self . should_remove_sempre_grammar = not os . path . exists ( SEMPRE_GRAMMAR_PATH ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . module_root_abbreviations_path = self . MODULE_ROOT / [string] / [string] [EOL] self . module_root_grammar_path = self . MODULE_ROOT / [string] / [string] [EOL] self . should_remove_root_sempre_abbreviations = not os . path . exists ( self . module_root_abbreviations_path ) [EOL] self . should_remove_root_sempre_grammar = not os . path . exists ( self . module_root_grammar_path ) [EOL] [EOL] super ( WikiTablesErmSemanticParserTest , self ) . setUp ( ) [EOL] self . set_up_model ( str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) , str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) ) [EOL] [EOL] def tearDown ( self ) : [EOL] super ( ) . tearDown ( ) [EOL] [comment] [EOL] if self . should_remove_sempre_abbreviations and os . path . exists ( SEMPRE_ABBREVIATIONS_PATH ) : [EOL] os . remove ( SEMPRE_ABBREVIATIONS_PATH ) [EOL] if self . should_remove_sempre_grammar and os . path . exists ( SEMPRE_GRAMMAR_PATH ) : [EOL] os . remove ( SEMPRE_GRAMMAR_PATH ) [EOL] if self . should_remove_root_sempre_abbreviations and os . path . exists ( self . module_root_abbreviations_path ) : [EOL] os . remove ( self . module_root_abbreviations_path ) [EOL] if self . should_remove_root_sempre_grammar and os . path . exists ( self . module_root_grammar_path ) : [EOL] os . remove ( self . module_root_grammar_path ) [EOL] [EOL] def test_model_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] [EOL] from typing import List , Dict , Tuple , Any [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] [EOL] [EOL] class CorefTest ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( CorefTest , self ) . setUp ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_coref_model_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_decode ( self ) : [EOL] [EOL] spans = torch . LongTensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] [EOL] antecedent_indices = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] spans = spans . unsqueeze ( [number] ) [EOL] antecedent_indices = antecedent_indices [EOL] [comment] [EOL] [comment] [EOL] predicted_antecedents = torch . LongTensor ( [ - [number] , [number] , - [number] , - [number] , [number] , [number] ] ) [EOL] predicted_antecedents = predicted_antecedents . unsqueeze ( [number] ) [EOL] output_dict = { [string] : spans , [string] : antecedent_indices , [string] : predicted_antecedents } [EOL] output = self . model . decode ( output_dict ) [EOL] [EOL] clusters = output [ [string] ] [ [number] ] [EOL] gold1 = [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] gold2 = [ ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] assert len ( clusters ) == [number] [EOL] assert gold1 in clusters [EOL] assert gold2 in clusters [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 $typing.Any$ 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 $typing.Any$ 0
	0
	0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import json [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . semparse import ParsingError , World [EOL] from allennlp . semparse . contexts import TableQuestionKnowledgeGraph [EOL] from allennlp . semparse . worlds import NlvrWorld , WikiTablesWorld [EOL] [EOL] [EOL] class FakeWorldWithoutRecursion ( World ) : [EOL] [comment] [EOL] @ overrides def all_possible_actions ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] actions = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] return actions [EOL] [EOL] [EOL] class FakeWorldWithRecursion ( FakeWorldWithoutRecursion ) : [EOL] [comment] [EOL] @ overrides def all_possible_actions ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] actions = super ( FakeWorldWithRecursion , self ) . all_possible_actions ( ) [EOL] actions . extend ( [ [string] , [string] ] ) [EOL] return actions [EOL] [EOL] [EOL] class TestWorld ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . world_without_recursion = FakeWorldWithoutRecursion ( ) [EOL] self . world_with_recursion = FakeWorldWithRecursion ( ) [EOL] [EOL] test_filename = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] data = [ json . loads ( line ) [ [string] ] for line in open ( test_filename ) . readlines ( ) ] [EOL] self . nlvr_world = NlvrWorld ( data [ [number] ] ) [EOL] [EOL] question_tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] table_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] table_kg = TableQuestionKnowledgeGraph . read_from_file ( table_file , question_tokens ) [EOL] self . wikitables_world = WikiTablesWorld ( table_kg ) [EOL] [EOL] def test_get_paths_to_root_without_recursion ( self ) : [EOL] argument_paths = self . world_without_recursion . get_paths_to_root ( [string] ) [EOL] assert argument_paths == [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] ] ] [EOL] unary_function_paths = self . world_without_recursion . get_paths_to_root ( [string] ) [EOL] assert unary_function_paths == [ [ [string] , [string] , [string] ] ] [EOL] binary_function_paths = self . world_without_recursion . get_paths_to_root ( [string] ) [EOL] assert binary_function_paths == [ [ [string] , [string] , [string] , [string] ] ] [EOL] [EOL] def test_get_paths_to_root_with_recursion ( self ) : [EOL] argument_paths = self . world_with_recursion . get_paths_to_root ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] assert argument_paths == [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] ] ] [EOL] identity_paths = self . world_with_recursion . get_paths_to_root ( [string] ) [EOL] [comment] [EOL] assert identity_paths == [ [ [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] ] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def test_get_action_sequence_removes_currying ( self ) : [EOL] world = self . wikitables_world [EOL] logical_form = ( [string] [string] ) [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] def test_get_action_sequence_removes_and_retains_var_correctly ( self ) : [EOL] world = self . wikitables_world [EOL] logical_form = ( [string] [string] [string] ) [EOL] parsed_logical_form_without_var = world . parse_logical_form ( logical_form ) [EOL] action_sequence_without_var = world . get_action_sequence ( parsed_logical_form_without_var ) [EOL] assert [string] not in action_sequence_without_var [EOL] [EOL] parsed_logical_form_with_var = world . parse_logical_form ( logical_form , remove_var_function = False ) [EOL] action_sequence_with_var = world . get_action_sequence ( parsed_logical_form_with_var ) [EOL] assert [string] in action_sequence_with_var [EOL] [EOL] def test_get_logical_form_handles_reverse ( self ) : [EOL] world = self . wikitables_world [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] reconstructed_logical_form = world . get_logical_form ( action_sequence ) [EOL] parsed_reconstructed_logical_form = world . parse_logical_form ( reconstructed_logical_form ) [EOL] assert parsed_logical_form == parsed_reconstructed_logical_form [EOL] [EOL] logical_form = ( [string] [string] [string] ) [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] reconstructed_logical_form = world . get_logical_form ( action_sequence ) [EOL] parsed_reconstructed_logical_form = world . parse_logical_form ( reconstructed_logical_form ) [EOL] assert parsed_logical_form == parsed_reconstructed_logical_form [EOL] [EOL] def test_get_logical_form_handles_greater_than ( self ) : [EOL] world = self . wikitables_world [EOL] action_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] logical_form = world . get_logical_form ( action_sequence ) [EOL] expected_logical_form = ( [string] [string] ) [EOL] assert logical_form == expected_logical_form [EOL] [EOL] def test_get_logical_form_handles_length_one_terminal_functions ( self ) : [EOL] world = self . wikitables_world [EOL] logical_form = ( [string] [string] ) [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] reconstructed_logical_form = world . get_logical_form ( action_sequence ) [EOL] parsed_reconstructed_logical_form = world . parse_logical_form ( reconstructed_logical_form ) [EOL] assert parsed_logical_form == parsed_reconstructed_logical_form [EOL] [EOL] def test_get_logical_form_with_real_logical_forms ( self ) : [EOL] nlvr_world = self . nlvr_world [EOL] logical_form = ( [string] ) [EOL] parsed_logical_form = nlvr_world . parse_logical_form ( logical_form ) [EOL] action_sequence = nlvr_world . get_action_sequence ( parsed_logical_form ) [EOL] reconstructed_logical_form = nlvr_world . get_logical_form ( action_sequence ) [EOL] parsed_reconstructed_logical_form = nlvr_world . parse_logical_form ( reconstructed_logical_form ) [EOL] [comment] [EOL] assert parsed_logical_form == parsed_reconstructed_logical_form [EOL] assert nlvr_world . execute ( logical_form ) == nlvr_world . execute ( reconstructed_logical_form ) [EOL] logical_form = [string] [EOL] parsed_logical_form = nlvr_world . parse_logical_form ( logical_form ) [EOL] action_sequence = nlvr_world . get_action_sequence ( parsed_logical_form ) [EOL] reconstructed_logical_form = nlvr_world . get_logical_form ( action_sequence ) [EOL] parsed_reconstructed_logical_form = nlvr_world . parse_logical_form ( reconstructed_logical_form ) [EOL] assert parsed_logical_form == parsed_reconstructed_logical_form [EOL] assert nlvr_world . execute ( logical_form ) == nlvr_world . execute ( reconstructed_logical_form ) [EOL] [EOL] def test_get_logical_form_fails_with_incomplete_action_sequence ( self ) : [EOL] nlvr_world = self . nlvr_world [EOL] action_sequence = [ [string] , [string] , [string] ] [EOL] with self . assertRaisesRegex ( ParsingError , [string] ) : [EOL] nlvr_world . get_logical_form ( action_sequence ) [EOL] [EOL] def test_get_logical_form_fails_with_extra_actions ( self ) : [EOL] nlvr_world = self . nlvr_world [EOL] action_sequence = [ [string] , [string] , [string] ] [EOL] with self . assertRaisesRegex ( ParsingError , [string] ) : [EOL] nlvr_world . get_logical_form ( action_sequence ) [EOL] [EOL] def test_get_logical_form_fails_with_action_sequence_in_wrong_order ( self ) : [EOL] nlvr_world = self . nlvr_world [EOL] action_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaisesRegex ( ParsingError , [string] ) : [EOL] nlvr_world . get_logical_form ( action_sequence ) [EOL] [EOL] def test_get_logical_form_adds_var_correctly ( self ) : [EOL] world = self . wikitables_world [EOL] action_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] logical_form = world . get_logical_form ( action_sequence ) [EOL] assert [string] in logical_form [EOL] expected_logical_form = ( [string] [string] [string] ) [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] parsed_expected_logical_form = world . parse_logical_form ( expected_logical_form ) [EOL] assert parsed_logical_form == parsed_expected_logical_form [EOL] [EOL] def test_get_logical_form_fails_with_unnecessary_add_var ( self ) : [EOL] world = self . wikitables_world [EOL] action_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaisesRegex ( ParsingError , [string] ) : [EOL] world . get_logical_form ( action_sequence ) [EOL] [EOL] def test_get_logical_form_with_multiple_negate_filters ( self ) : [EOL] world = self . nlvr_world [EOL] [comment] [EOL] action_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] logical_form = world . get_logical_form ( action_sequence ) [EOL] [comment] [EOL] expected_logical_form = ( [string] [string] ) [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] parsed_expected_logical_form = world . parse_logical_form ( expected_logical_form ) [EOL] assert parsed_logical_form == parsed_expected_logical_form [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0
[comment] [EOL] from typing import Dict , List , Union , Any [EOL] import typing [EOL] import json [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . semparse . worlds . world import ExecutionError [EOL] from allennlp . semparse . worlds . nlvr_world import NlvrWorld [EOL] [EOL] [EOL] class TestNlvrWorld ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] test_filename = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] data = [ json . loads ( line ) [ [string] ] for line in open ( test_filename ) . readlines ( ) ] [EOL] self . worlds = [ NlvrWorld ( rep ) for rep in data ] [EOL] [comment] [EOL] [comment] [EOL] custom_rep = [ [ { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : [string] } , { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : [string] } ] , [ { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : [string] } , { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : [string] } ] , [ { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : [string] } ] ] [EOL] self . custom_world = NlvrWorld ( custom_rep ) [EOL] [EOL] def test_get_action_sequence_removes_currying_for_all_nlvr_functions ( self ) : [EOL] world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] [comment] [EOL] logical_form = [string] [EOL] parsed_logical_form = world . parse_logical_form ( logical_form ) [EOL] action_sequence = world . get_action_sequence ( parsed_logical_form ) [EOL] assert [string] in action_sequence [EOL] [EOL] def test_logical_form_with_assert_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form_true = [string] [EOL] assert nlvr_world . execute ( logical_form_true ) is True [EOL] logical_form_false = [string] [EOL] assert nlvr_world . execute ( logical_form_false ) is False [EOL] [EOL] def test_logical_form_with_box_filter_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is False [EOL] [EOL] def test_logical_form_with_box_filter_within_object_filter_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is True [EOL] [EOL] def test_logical_form_with_same_color_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is True [EOL] [EOL] def test_logical_form_with_same_shape_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is False [EOL] [EOL] def test_logical_form_with_touch_wall_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is False [EOL] [EOL] def test_logical_form_with_not_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = ( [string] ) [EOL] assert nlvr_world . execute ( logical_form ) is True [EOL] [EOL] def test_logical_form_with_color_comparison_executes_correctly ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] assert nlvr_world . execute ( logical_form ) is True [EOL] [EOL] def test_logical_form_with_object_filter_returns_correct_action_sequence ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] logical_form = [string] [EOL] expression = nlvr_world . parse_logical_form ( logical_form ) [EOL] action_sequence = nlvr_world . get_action_sequence ( expression ) [EOL] assert action_sequence == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] def test_logical_form_with_box_filter_returns_correct_action_sequence ( self ) : [EOL] nlvr_world = self . worlds [ [number] ] [EOL] logical_form = [string] [EOL] expression = nlvr_world . parse_logical_form ( logical_form ) [EOL] action_sequence = nlvr_world . get_action_sequence ( expression ) [EOL] assert action_sequence == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] def test_spatial_relations_return_objects_in_the_same_box ( self ) : [EOL] [comment] [EOL] [comment] [EOL] world = self . custom_world [EOL] [comment] [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [comment] [EOL] logical_form = ( [string] [string] ) [EOL] assert world . execute ( logical_form ) is True [EOL] [EOL] [comment] [EOL] logical_form = ( [string] [string] ) [EOL] assert world . execute ( logical_form ) is True [EOL] [EOL] def test_touch_object_executes_correctly ( self ) : [EOL] world = self . custom_world [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [EOL] def test_spatial_relations_with_objects_from_different_boxes ( self ) : [EOL] [comment] [EOL] [comment] [EOL] world = self . custom_world [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [EOL] def test_count_with_all_equals_throws_execution_error ( self ) : [EOL] [comment] [EOL] [comment] [EOL] world = self . custom_world [EOL] with self . assertRaises ( ExecutionError ) : [EOL] world . execute ( [string] [string] ) [EOL] [EOL] def test_shape_with_equals_throws_execution_error ( self ) : [EOL] [comment] [EOL] [comment] [EOL] world = self . custom_world [EOL] with self . assertRaises ( ExecutionError ) : [EOL] world . execute ( [string] [string] ) [EOL] [EOL] def test_same_and_different_execute_correctly ( self ) : [EOL] world = self . custom_world [EOL] [comment] [EOL] assert world . execute ( [string] [string] [string] ) is True [EOL] [comment] [EOL] assert world . execute ( [string] [string] ) is True [EOL] [EOL] def test_get_agenda_for_sentence ( self ) : [EOL] world = self . worlds [ [number] ] [EOL] agenda = world . get_agenda_for_sentence ( [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert set ( agenda ) == set ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] def test_get_agenda_for_sentence_correctly_adds_object_filters ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] world = self . worlds [ [number] ] [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert [string] not in agenda [EOL] assert [string] in agenda [EOL] assert [string] not in agenda [EOL] assert [string] in agenda [EOL] assert [string] not in agenda [EOL] agenda = world . get_agenda_for_sentence ( [string] [string] ) [EOL] assert [string] in agenda [EOL] assert [string] not in agenda [EOL] assert [string] in agenda [EOL] assert [string] not in agenda [EOL] assert [string] in agenda [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
	0
	0
	0
	0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . nn . decoding import BeamSearch [EOL] from . simple_transition_system import SimpleDecoderState , SimpleDecoderStep [EOL] [EOL] [EOL] class TestBeamSearch ( AllenNlpTestCase ) : [EOL] def test_search ( self ) : [EOL] beam_search = BeamSearch . from_params ( Params ( { [string] : [number] } ) ) [EOL] initial_state = SimpleDecoderState ( [ [number] , [number] , [number] , [number] ] , [ [ ] , [ ] , [ ] , [ ] ] , [ torch . Tensor ( [ [number] ] ) , torch . Tensor ( [ [number] ] ) , torch . Tensor ( [ [number] ] ) , torch . Tensor ( [ [number] ] ) ] , [ - [number] , [number] , - [number] , [number] ] ) [EOL] decoder_step = SimpleDecoderStep ( include_value_in_score = True ) [EOL] best_states = beam_search . search ( [number] , initial_state , decoder_step , keep_final_unfinished_states = False ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert len ( best_states ) == [number] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ - [number] , [number] , [number] , [number] ] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ [number] , [number] ] [EOL] [EOL] best_states = beam_search . search ( [number] , initial_state , decoder_step , keep_final_unfinished_states = True ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert len ( best_states ) == [number] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ - [number] , [number] , [number] , [number] ] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ [number] , [number] ] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ - [number] , - [number] , - [number] , - [number] , - [number] ] [EOL] assert best_states [ [number] ] [ [number] ] . action_history [ [number] ] == [ [number] , [number] , [number] , [number] , [number] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import List , Tuple , Any [EOL] import typing [EOL] import torch [EOL] import numpy as np [EOL] from numpy . testing import assert_almost_equal [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . nn . decoding . decoder_trainers import ExpectedRiskMinimization [EOL] from . . simple_transition_system import SimpleDecoderState , SimpleDecoderStep [EOL] [EOL] [EOL] class TestExpectedRiskMinimization ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . initial_state = SimpleDecoderState ( [ [number] ] , [ [ [number] ] ] , [ torch . Tensor ( [ [number] ] ) ] ) [EOL] self . decoder_step = SimpleDecoderStep ( ) [EOL] [comment] [EOL] self . supervision = lambda state : torch . Tensor ( [ sum ( [ x % [number] != [number] for x in state . action_history [ [number] ] ] ) ] ) [EOL] [comment] [EOL] self . trainer = ExpectedRiskMinimization ( beam_size = [number] , normalize_by_length = False , max_decoding_steps = [number] ) [EOL] [EOL] def test_get_finished_states ( self ) : [EOL] finished_states = self . trainer . _get_finished_states ( self . initial_state , self . decoder_step ) [EOL] state_info = [ ( state . action_history [ [number] ] , state . score [ [number] ] . item ( ) ) for state in finished_states ] [EOL] [comment] [EOL] [comment] [EOL] assert len ( finished_states ) == [number] [EOL] assert ( [ [number] , [number] , [number] ] , - [number] ) in state_info [EOL] assert ( [ [number] , [number] , [number] , [number] ] , - [number] ) in state_info [EOL] assert ( [ [number] , [number] , [number] , [number] ] , - [number] ) in state_info [EOL] assert ( [ [number] , [number] , [number] , [number] ] , - [number] ) in state_info [EOL] assert ( [ [number] , [number] , [number] , [number] , [number] ] , - [number] ) in state_info [EOL] [EOL] def test_decode ( self ) : [EOL] decoded_info = self . trainer . decode ( self . initial_state , self . decoder_step , self . supervision ) [EOL] [comment] [EOL] assert decoded_info [ [string] ] [ [number] ] == [ [ [number] , [number] , [number] ] ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] partition = np . exp ( - [number] ) + np . exp ( - [number] ) + np . exp ( - [number] ) + np . exp ( - [number] ) + np . exp ( - [number] ) [EOL] expected_loss = ( ( np . exp ( - [number] ) * [number] ) + ( np . exp ( - [number] ) * [number] ) + ( np . exp ( - [number] ) * [number] ) + ( np . exp ( - [number] ) * [number] ) + ( np . exp ( - [number] ) * [number] ) ) / partition [EOL] assert_almost_equal ( decoded_info [ [string] ] . data . numpy ( ) , expected_loss ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import argparse [EOL] [EOL] from flaky import flaky [EOL] [EOL] from allennlp . commands . evaluate import evaluate_from_args , Evaluate [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestEvaluate ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [EOL] self . parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = self . parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Evaluate ( ) . add_subparser ( [string] , subparsers ) [EOL] [EOL] @ flaky def test_evaluate_from_args ( self ) : [EOL] kebab_args = [ [string] , str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) , [string] , str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] , [string] ] [EOL] [EOL] args = self . parser . parse_args ( kebab_args ) [EOL] metrics = evaluate_from_args ( args ) [EOL] assert metrics . keys ( ) == { [string] , [string] , [string] , [string] , [string] } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import List , Dict , Union , Any [EOL] import typing [EOL] import argparse [EOL] import re [EOL] import shutil [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . commands . fine_tune import FineTune , fine_tune_model_from_file_paths , fine_tune_model_from_args , fine_tune_model [EOL] from allennlp . common . params import Params [EOL] from allennlp . models import load_archive [EOL] [EOL] class TestFineTune ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . model_archive = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] self . config_file = str ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] self . serialization_dir = str ( self . TEST_DIR / [string] ) [EOL] [EOL] self . parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = self . parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] FineTune ( ) . add_subparser ( [string] , subparsers ) [EOL] [EOL] [EOL] def test_fine_tune_model_runs_from_file_paths ( self ) : [EOL] fine_tune_model_from_file_paths ( model_archive_path = self . model_archive , config_file = self . config_file , serialization_dir = self . serialization_dir ) [EOL] [EOL] def test_fine_tune_runs_from_parser_arguments ( self ) : [EOL] raw_args = [ [string] , [string] , self . model_archive , [string] , self . config_file , [string] , self . serialization_dir ] [EOL] [EOL] args = self . parser . parse_args ( raw_args ) [EOL] [EOL] assert args . func == fine_tune_model_from_args [EOL] assert args . model_archive == self . model_archive [EOL] assert args . config_file == self . config_file [EOL] assert args . serialization_dir == self . serialization_dir [EOL] fine_tune_model_from_args ( args ) [EOL] [EOL] def test_fine_tune_fails_without_required_args ( self ) : [EOL] [comment] [EOL] with self . assertRaises ( SystemExit ) as context : [EOL] self . parser . parse_args ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] assert context . exception . code == [number] [comment] [EOL] [EOL] [comment] [EOL] with self . assertRaises ( SystemExit ) as context : [EOL] self . parser . parse_args ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] assert context . exception . code == [number] [comment] [EOL] [EOL] [comment] [EOL] with self . assertRaises ( SystemExit ) as context : [EOL] self . parser . parse_args ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] assert context . exception . code == [number] [comment] [EOL] [EOL] def test_fine_tune_nograd_regex ( self ) : [EOL] original_model = load_archive ( self . model_archive ) . model [EOL] name_parameters_original = dict ( original_model . named_parameters ( ) ) [EOL] regex_lists = [ [ ] , [ [string] , [string] ] , [ [string] ] ] [EOL] for regex_list in regex_lists : [EOL] params = Params . from_file ( self . config_file ) [EOL] params [ [string] ] [ [string] ] = regex_list [EOL] shutil . rmtree ( self . serialization_dir , ignore_errors = True ) [EOL] tuned_model = fine_tune_model ( model = original_model , params = params , serialization_dir = self . serialization_dir ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for name , parameter in tuned_model . named_parameters ( ) : [EOL] if any ( re . search ( regex , name ) for regex in regex_list ) : [EOL] assert not parameter . requires_grad [EOL] else : [EOL] assert parameter . requires_grad == name_parameters_original [ name ] . requires_grad [EOL] [comment] [EOL] with pytest . raises ( Exception ) as _ : [EOL] params = Params . from_file ( self . config_file ) [EOL] params [ [string] ] [ [string] ] = [ [string] ] [EOL] shutil . rmtree ( self . serialization_dir , ignore_errors = True ) [EOL] tuned_model = fine_tune_model ( model = original_model , params = params , serialization_dir = self . serialization_dir ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Set , List , Tuple , Any [EOL] import typing [EOL] import itertools [EOL] import math [EOL] [EOL] from pytest import approx , raises [EOL] import torch [EOL] [EOL] from allennlp . modules import ConditionalRandomField [EOL] from allennlp . modules . conditional_random_field import allowed_transitions [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestConditionalRandomField ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . logits = torch . Tensor ( [ [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , ] ) [EOL] self . tags = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] self . transitions = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ - [number] , [number] , - [number] , [number] , [number] ] , [ [number] , [number] , [number] , - [number] , - [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] self . transitions_from_start = torch . Tensor ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] self . transitions_to_end = torch . Tensor ( [ - [number] , - [number] , [number] , - [number] , - [number] ] ) [EOL] [EOL] [comment] [EOL] self . crf = ConditionalRandomField ( [number] ) [EOL] self . crf . transitions = torch . nn . Parameter ( self . transitions ) [EOL] self . crf . start_transitions = torch . nn . Parameter ( self . transitions_from_start ) [EOL] self . crf . end_transitions = torch . nn . Parameter ( self . transitions_to_end ) [EOL] [EOL] def score ( self , logits , tags ) : [EOL] [docstring] [EOL] [comment] [EOL] total = self . transitions_from_start [ tags [ [number] ] ] + self . transitions_to_end [ tags [ - [number] ] ] [EOL] [comment] [EOL] for tag , next_tag in zip ( tags , tags [ [number] : ] ) : [EOL] total += self . transitions [ tag , next_tag ] [EOL] [comment] [EOL] for logit , tag in zip ( logits , tags ) : [EOL] total += logit [ tag ] [EOL] return total [EOL] [EOL] def test_forward_works_without_mask ( self ) : [EOL] log_likelihood = self . crf ( self . logits , self . tags ) . item ( ) [EOL] [EOL] [comment] [EOL] manual_log_likelihood = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for logits_i , tags_i in zip ( self . logits , self . tags ) : [EOL] numerator = self . score ( logits_i . detach ( ) , tags_i . detach ( ) ) [EOL] all_scores = [ self . score ( logits_i . detach ( ) , tags_j ) for tags_j in itertools . product ( range ( [number] ) , repeat = [number] ) ] [EOL] denominator = math . log ( sum ( math . exp ( score ) for score in all_scores ) ) [EOL] [comment] [EOL] manual_log_likelihood += numerator - denominator [EOL] [EOL] [comment] [EOL] assert manual_log_likelihood . item ( ) == approx ( log_likelihood ) [EOL] [EOL] [EOL] def test_forward_works_with_mask ( self ) : [EOL] [comment] [EOL] mask = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] log_likelihood = self . crf ( self . logits , self . tags , mask ) . item ( ) [EOL] [EOL] [comment] [EOL] manual_log_likelihood = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for logits_i , tags_i , mask_i in zip ( self . logits , self . tags , mask ) : [EOL] [comment] [EOL] sequence_length = torch . sum ( mask_i . detach ( ) ) [EOL] logits_i = logits_i . data [ : sequence_length ] [EOL] tags_i = tags_i . data [ : sequence_length ] [EOL] [EOL] numerator = self . score ( logits_i , tags_i ) [EOL] all_scores = [ self . score ( logits_i , tags_j ) for tags_j in itertools . product ( range ( [number] ) , repeat = sequence_length ) ] [EOL] denominator = math . log ( sum ( math . exp ( score ) for score in all_scores ) ) [EOL] [comment] [EOL] manual_log_likelihood += numerator - denominator [EOL] [EOL] [comment] [EOL] assert manual_log_likelihood . item ( ) == approx ( log_likelihood ) [EOL] [EOL] [EOL] def test_viterbi_tags ( self ) : [EOL] mask = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] viterbi_path = self . crf . viterbi_tags ( self . logits , mask ) [EOL] [EOL] [comment] [EOL] viterbi_tags = [ x for x , y in viterbi_path ] [EOL] viterbi_scores = [ y for x , y in viterbi_path ] [EOL] [EOL] [comment] [EOL] assert viterbi_tags == [ [ [number] , [number] , [number] ] , [ [number] , [number] ] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] most_likely_tags = [ ] [EOL] best_scores = [ ] [EOL] [EOL] for logit , mas in zip ( self . logits , mask ) : [EOL] sequence_length = torch . sum ( mas . detach ( ) ) [EOL] most_likely , most_likelihood = None , - float ( [string] ) [EOL] for tags in itertools . product ( range ( [number] ) , repeat = sequence_length ) : [EOL] score = self . score ( logit . data , tags ) [EOL] if score > most_likelihood : [EOL] most_likely , most_likelihood = tags , score [EOL] [comment] [EOL] most_likely_tags . append ( list ( most_likely ) ) [EOL] best_scores . append ( most_likelihood ) [EOL] [EOL] assert viterbi_tags == most_likely_tags [EOL] assert viterbi_scores == best_scores [EOL] [EOL] def test_constrained_viterbi_tags ( self ) : [EOL] constraints = { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) } [EOL] [EOL] [comment] [EOL] [comment] [EOL] for i in range ( [number] ) : [EOL] constraints . add ( ( [number] , i ) ) [EOL] constraints . add ( ( i , [number] ) ) [EOL] [EOL] crf = ConditionalRandomField ( num_tags = [number] , constraints = constraints ) [EOL] crf . transitions = torch . nn . Parameter ( self . transitions ) [EOL] crf . start_transitions = torch . nn . Parameter ( self . transitions_from_start ) [EOL] crf . end_transitions = torch . nn . Parameter ( self . transitions_to_end ) [EOL] [EOL] mask = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] viterbi_path = crf . viterbi_tags ( self . logits , mask ) [EOL] [EOL] [comment] [EOL] viterbi_tags = [ x for x , y in viterbi_path ] [EOL] [EOL] [comment] [EOL] assert viterbi_tags == [ [ [number] , [number] , [number] ] , [ [number] , [number] ] ] [EOL] [EOL] def test_allowed_transitions ( self ) : [EOL] [comment] [EOL] bio_labels = [ [string] , [string] , [string] , [string] , [string] ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( bio_labels ) ) ) [EOL] [EOL] [comment] [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) } [EOL] [EOL] bioul_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( bioul_labels ) ) ) [EOL] [EOL] [comment] [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) } [EOL] [EOL] with raises ( ConfigurationError ) : [EOL] allowed_transitions ( [string] , { } ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import allennlp [EOL] import numpy [EOL] from numpy . testing import assert_almost_equal [EOL] import torch [EOL] from torch . autograd import Variable [EOL] from torch . nn import Parameter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . modules . matrix_attention import LinearMatrixAttention [EOL] from allennlp . modules . matrix_attention . matrix_attention import MatrixAttention [EOL] [EOL] [EOL] class TestLinearMatrixAttention ( AllenNlpTestCase ) : [EOL] [EOL] def test_can_init_dot ( self ) : [EOL] legacy_attention = MatrixAttention . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] isinstance ( legacy_attention , LinearMatrixAttention ) [EOL] [EOL] def test_linear_similarity ( self ) : [EOL] linear = LinearMatrixAttention ( [number] , [number] ) [EOL] linear . _weight_vector = Parameter ( torch . FloatTensor ( [ - [number] , [number] , [number] , - [number] , [number] , [number] ] ) ) [EOL] linear . _bias = Parameter ( torch . FloatTensor ( [ [number] ] ) ) [EOL] output = linear ( Variable ( torch . FloatTensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ - [number] , - [number] , - [number] ] , [ [number] , [number] , [number] ] ] ] ) ) , Variable ( torch . FloatTensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ] ) ) ) [EOL] [EOL] assert_almost_equal ( output . data . numpy ( ) , numpy . array ( [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ - [number] , - [number] ] , [ [number] , [number] ] ] ] ) , decimal = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.matrix_attention.matrix_attention.MatrixAttention$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.matrix_attention.matrix_attention.MatrixAttention$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] [EOL] from allennlp . modules . span_extractors import SpanExtractor , EndpointSpanExtractor [EOL] from allennlp . common . params import Params [EOL] from allennlp . nn . util import batched_index_select [EOL] [EOL] class TestEndpointSpanExtractor : [EOL] def test_endpoint_span_extractor_can_build_from_params ( self ) : [EOL] params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] extractor = SpanExtractor . from_params ( params ) [EOL] assert isinstance ( extractor , EndpointSpanExtractor ) [EOL] assert extractor . get_output_dim ( ) == [number] [comment] [EOL] [EOL] def test_correct_sequence_elements_are_embedded ( self ) : [EOL] sequence_tensor = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] [comment] [EOL] extractor = EndpointSpanExtractor ( [number] , [string] ) [EOL] [EOL] indices = torch . LongTensor ( [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] ] ) [EOL] span_representations = extractor ( sequence_tensor , indices ) [EOL] [EOL] assert list ( span_representations . size ( ) ) == [ [number] , [number] , [number] ] [EOL] assert extractor . get_output_dim ( ) == [number] [EOL] assert extractor . get_input_dim ( ) == [number] [EOL] [EOL] start_indices , end_indices = indices . split ( [number] , - [number] ) [EOL] [comment] [EOL] [comment] [EOL] start_embeddings , end_embeddings = span_representations . split ( [number] , - [number] ) [EOL] [EOL] correct_start_embeddings = batched_index_select ( sequence_tensor , start_indices . squeeze ( ) ) [EOL] correct_end_embeddings = batched_index_select ( sequence_tensor , end_indices . squeeze ( ) ) [EOL] numpy . testing . assert_array_equal ( start_embeddings . data . numpy ( ) , correct_start_embeddings . data . numpy ( ) ) [EOL] numpy . testing . assert_array_equal ( end_embeddings . data . numpy ( ) , correct_end_embeddings . data . numpy ( ) ) [EOL] [EOL] def test_masked_indices_are_handled_correctly ( self ) : [EOL] sequence_tensor = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] [comment] [EOL] extractor = EndpointSpanExtractor ( [number] , [string] ) [EOL] [EOL] indices = torch . LongTensor ( [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] ] ) [EOL] span_representations = extractor ( sequence_tensor , indices ) [EOL] [EOL] [comment] [EOL] indices_mask = torch . LongTensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] [EOL] span_representations = extractor ( sequence_tensor , indices , span_indices_mask = indices_mask ) [EOL] start_embeddings , end_embeddings = span_representations . split ( [number] , - [number] ) [EOL] start_indices , end_indices = indices . split ( [number] , - [number] ) [EOL] [EOL] correct_start_embeddings = batched_index_select ( sequence_tensor , start_indices . squeeze ( ) ) . data [EOL] [comment] [EOL] correct_start_embeddings [ [number] , : , : ] . fill_ ( [number] ) [EOL] correct_end_embeddings = batched_index_select ( sequence_tensor , end_indices . squeeze ( ) ) . data [EOL] correct_end_embeddings [ [number] , : , : ] . fill_ ( [number] ) [EOL] numpy . testing . assert_array_equal ( start_embeddings . data . numpy ( ) , correct_start_embeddings . numpy ( ) ) [EOL] numpy . testing . assert_array_equal ( end_embeddings . data . numpy ( ) , correct_end_embeddings . numpy ( ) ) [EOL] [EOL] [EOL] def test_masked_indices_are_handled_correctly_with_exclusive_indices ( self ) : [EOL] sequence_tensor = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] [comment] [EOL] [comment] [EOL] extractor = EndpointSpanExtractor ( [number] , [string] , use_exclusive_start_indices = True ) [EOL] indices = torch . LongTensor ( [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] ] ) [EOL] sequence_mask = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] span_representations = extractor ( sequence_tensor , indices , sequence_mask = sequence_mask ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] start_embeddings , end_embeddings = span_representations . split ( [number] , - [number] ) [EOL] [EOL] [EOL] correct_start_indices = torch . LongTensor ( [ [ [number] , [number] ] , [ - [number] , - [number] ] ] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] correct_start_indices [ [number] , [number] ] = [number] [EOL] correct_start_indices [ [number] , [number] ] = [number] [EOL] [EOL] correct_end_indices = torch . LongTensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] [EOL] correct_start_embeddings = batched_index_select ( sequence_tensor . contiguous ( ) , correct_start_indices ) [EOL] [comment] [EOL] correct_start_embeddings [ [number] , [number] ] = extractor . _start_sentinel . data [EOL] correct_start_embeddings [ [number] , [number] ] = extractor . _start_sentinel . data [EOL] numpy . testing . assert_array_equal ( start_embeddings . data . numpy ( ) , correct_start_embeddings . data . numpy ( ) ) [EOL] [EOL] correct_end_embeddings = batched_index_select ( sequence_tensor . contiguous ( ) , correct_end_indices ) [EOL] numpy . testing . assert_array_equal ( end_embeddings . data . numpy ( ) , correct_end_embeddings . data . numpy ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0
	0
	0
	0
	0
	0
	0
	0
	0
[docstring] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from allennlp . data . tokenizers import Token as _ [EOL] from allennlp . semparse . worlds . world import ParsingError , World [EOL] from allennlp . semparse . action_space_walker import ActionSpaceWalker [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from allennlp . semparse . contexts . table_question_knowledge_graph import TableQuestionKnowledgeGraph [EOL]	0 0 0 0 0 0 0 0 0 0 0
from allennlp . semparse . worlds . nlvr_world import NlvrWorld [EOL] from allennlp . semparse . worlds . wikitables_world import WikiTablesWorld [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import List , Any [EOL] import argparse [EOL] import typing [EOL] import builtins [EOL] import os [EOL] import argparse [EOL] import sys [EOL] [EOL] sys . path . insert ( [number] , os . path . dirname ( os . path . abspath ( os . path . join ( __file__ , os . pardir ) ) ) ) [EOL] [EOL] import gzip [EOL] import numpy [EOL] import torch [EOL] [EOL] from allennlp . data . token_indexers import ELMoTokenCharactersIndexer [EOL] from allennlp . modules . elmo import _ElmoCharacterEncoder [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . vocabulary import DEFAULT_OOV_TOKEN [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] def main ( vocab_path , elmo_config_path , elmo_weights_path , output_dir , batch_size , device , use_custom_oov_token = False ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] with open ( vocab_path , [string] ) as vocab_file : [EOL] tokens = vocab_file . read ( ) . strip ( ) . split ( [string] ) [EOL] [EOL] [comment] [EOL] if tokens [ [number] ] != DEFAULT_OOV_TOKEN and not use_custom_oov_token : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] tokens = [ tokens [ [number] ] ] + [ [string] , [string] ] + tokens [ [number] : ] [EOL] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = [ indexer . token_to_indices ( Token ( token ) , Vocabulary ( ) ) for token in tokens ] [EOL] sentences = [ ] [EOL] for k in range ( ( len ( indices ) // [number] ) + [number] ) : [EOL] sentences . append ( indexer . pad_token_sequence ( indices [ ( k * [number] ) : ( ( k + [number] ) * [number] ) ] , desired_num_tokens = [number] , padding_lengths = { } ) ) [EOL] [EOL] last_batch_remainder = [number] - ( len ( indices ) % [number] ) [EOL] if device != - [number] : [EOL] elmo_token_embedder = _ElmoCharacterEncoder ( elmo_config_path , elmo_weights_path ) . cuda ( device ) [EOL] else : [EOL] elmo_token_embedder = _ElmoCharacterEncoder ( elmo_config_path , elmo_weights_path ) [EOL] [EOL] all_embeddings = [ ] [EOL] for i in range ( ( len ( sentences ) // batch_size ) + [number] ) : [EOL] array = numpy . array ( sentences [ i * batch_size : ( i + [number] ) * batch_size ] ) [EOL] if device != - [number] : [EOL] batch = torch . from_numpy ( array ) . cuda ( device ) [EOL] else : [EOL] batch = torch . from_numpy ( array ) [EOL] [EOL] token_embedding = elmo_token_embedder ( batch ) [ [string] ] . data [EOL] [EOL] [comment] [EOL] [comment] [EOL] per_word_embeddings = token_embedding [ : , [number] : - [number] , : ] . contiguous ( ) . view ( - [number] , token_embedding . size ( - [number] ) ) [EOL] [EOL] all_embeddings . append ( per_word_embeddings ) [EOL] [EOL] [comment] [EOL] all_embeddings [ - [number] ] = all_embeddings [ - [number] ] [ : - last_batch_remainder , : ] [EOL] [EOL] embedding_weight = torch . cat ( all_embeddings , [number] ) . cpu ( ) . numpy ( ) [EOL] [EOL] [comment] [EOL] os . makedirs ( output_dir , exist_ok = True ) [EOL] with gzip . open ( os . path . join ( output_dir , [string] ) , [string] ) as embeddings_file : [EOL] for i , word in enumerate ( tokens ) : [EOL] string_array = [string] . join ( [ str ( x ) for x in list ( embedding_weight [ i , : ] ) ] ) [EOL] embeddings_file . write ( f"{ word } [string] { string_array } [string] " . encode ( [string] ) ) [EOL] [EOL] [comment] [EOL] _ , vocab_file_name = os . path . split ( vocab_path ) [EOL] with open ( os . path . join ( output_dir , vocab_file_name ) , [string] ) as new_vocab_file : [EOL] for word in tokens : [EOL] new_vocab_file . write ( f"{ word } [string] " ) [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( description = [string] [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] [string] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , default = - [number] , help = [string] ) [EOL] parser . add_argument ( [string] , type = bool , default = False , help = [string] [string] [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] main ( args . vocab_path , args . elmo_config , args . elmo_weights , args . output_dir , args . batch_size , args . device , args . use_custom_oov_token ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0
[comment] [EOL] from typing import List , Dict , DefaultDict , Any [EOL] import argparse [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] [comment] [EOL] import json [EOL] import argparse [EOL] from collections import defaultdict [EOL] [EOL] [EOL] def group_dataset ( input_file , output_file ) : [EOL] instance_groups = defaultdict ( lambda : { [string] : [ ] , [string] : [ ] } ) [EOL] for line in open ( input_file ) : [EOL] data = json . loads ( line ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] identifier = data [ [string] ] . split ( [string] ) [ [number] ] [EOL] instance_groups [ identifier ] [ [string] ] = identifier [EOL] instance_groups [ identifier ] [ [string] ] = data [ [string] ] [EOL] instance_groups [ identifier ] [ [string] ] . append ( data [ [string] ] ) [EOL] instance_groups [ identifier ] [ [string] ] . append ( data [ [string] ] ) [EOL] [EOL] with open ( output_file , [string] ) as output : [EOL] for instance_group in instance_groups . values ( ) : [EOL] json . dump ( instance_group , output ) [EOL] output . write ( [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , help = [string] ) [EOL] args = parser . parse_args ( ) [EOL] group_dataset ( args . input_file , args . output_file ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0