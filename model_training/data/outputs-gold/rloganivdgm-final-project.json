from setuptools import setup [EOL] [EOL] [EOL] setup ( name = [string] , version = [string] , packages = [ [string] ] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , DefaultDict , Dict , Any , Tuple [EOL] import typing [EOL] import logging [EOL] from collections import defaultdict [EOL] import logging [EOL] [EOL] import torch [EOL] from torch . utils . data import Dataset [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [comment] [EOL] SPECIAL_TOKENS = [ [string] , [string] , [string] , [string] ] [EOL] NOTE_EVENTS = [ [string] % ( i , j ) for i in range ( [number] ) for j in [ [number] , [number] ] ] [EOL] WAIT_EVENTS = [ [string] % i for i in range ( [number] , [number] ) ] [EOL] [EOL] IDX_TO_TOKEN = [ * SPECIAL_TOKENS , * NOTE_EVENTS , * WAIT_EVENTS ] [EOL] TOKEN_TO_IDX = { token : i for i , token in enumerate ( IDX_TO_TOKEN ) } [EOL] [EOL] [EOL] def pad_and_combine_instances ( batch ) : [EOL] [docstring] [EOL] batch_size = len ( batch ) [EOL] max_field_lengths = defaultdict ( int ) [EOL] for instance in batch : [EOL] for field , tensor in instance . items ( ) : [EOL] if len ( tensor . size ( ) ) == [number] : [EOL] continue [EOL] elif len ( tensor . size ( ) ) == [number] : [EOL] max_field_lengths [ field ] = max ( max_field_lengths [ field ] , tensor . size ( ) [ [number] ] ) [EOL] elif len ( tensor . size ( ) ) > [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] out_dict = { } [EOL] for i , instance in enumerate ( batch ) : [EOL] for field , tensor in instance . items ( ) : [EOL] if field not in out_dict : [EOL] if field in max_field_lengths : [EOL] size = ( batch_size , max_field_lengths [ field ] ) [EOL] else : [EOL] size = ( batch_size , ) [EOL] out_dict [ field ] = tensor . new_zeros ( size ) [EOL] if field in max_field_lengths : [EOL] out_dict [ field ] [ i , : tensor . size ( ) [ [number] ] ] = tensor [EOL] else : [EOL] out_dict [ field ] [ i ] = tensor [EOL] [EOL] return out_dict [EOL] [EOL] [EOL] class MidiDataset ( Dataset ) : [EOL] def __init__ ( self , file_path , transforms = None , embedding_type = [string] ) : [EOL] [docstring] [EOL] assert embedding_type in { [string] , [string] } [EOL] self . _instances = self . read_instances ( file_path ) [EOL] self . _transforms = transforms [EOL] self . _embedding_type = embedding_type [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] [EOL] tokens = self . _instances [ idx ] [EOL] [EOL] if self . _transforms is not None : [EOL] for transform in self . _transforms : [EOL] tokens = transform ( tokens ) [EOL] [EOL] if self . _embedding_type == [string] : [EOL] timestamps = [ ] [EOL] current_time = [number] [EOL] for token in tokens : [EOL] timestamps . append ( current_time ) [EOL] token_type , * values = token . split ( [string] ) [EOL] if token_type == [string] : [EOL] current_time += int ( values [ [number] ] ) [EOL] elif self . _embedding_type == [string] : [EOL] timestamps = list ( range ( len ( tokens ) ) ) [EOL] else : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] tokens = [ TOKEN_TO_IDX [ x ] for x in tokens ] [EOL] instance = { [string] : torch . tensor ( tokens [ : - [number] ] , dtype = torch . int64 ) , [string] : torch . tensor ( tokens [ [number] : ] , dtype = torch . int64 ) , [string] : torch . tensor ( timestamps [ : - [number] ] , dtype = torch . float32 ) , } [EOL] [EOL] [EOL] return instance [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . _instances ) [EOL] [EOL] def read_instances ( self , file_path ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] [EOL] instances = [ ] [EOL] with open ( file_path , [string] ) as f : [EOL] for line in f : [EOL] tokens = line . strip ( ) . split ( ) [EOL] instances . append ( tokens ) [EOL] logger . debug ( [string] , max ( len ( x ) for x in instances ) ) [EOL] [EOL] return instances [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.str]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[builtins.str]$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0
from typing import List , Any , Dict [EOL] import typing [EOL] import squawkbox [EOL] import logging [EOL] import builtins [EOL] from collections import deque [EOL] import logging [EOL] [EOL] import squawkbox . midi as md [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] DEFAULT_TICKLEN = [number] / [number] [EOL] DEFAULT_HEADER = [string] [EOL] DEFAULT_TEMPO_MAP = [string] [EOL] [EOL] [EOL] def split_waits ( delta_time ) : [EOL] n_waits = delta_time // [number] [EOL] out = [ [string] ] * n_waits [EOL] remainder = delta_time % [number] [EOL] if remainder > [number] : [EOL] out . append ( f' [string] { remainder }' ) [EOL] return out [EOL] [EOL] [EOL] [comment] [EOL] class Tokenizer : [EOL] [EOL] def __init__ ( self , scale = [number] , max_tokens = None , max_wait_time = None ) : [EOL] self . _scale = scale [EOL] self . _max_tokens = max_tokens [EOL] self . _max_wait_time = max_wait_time [EOL] [EOL] def tokenize ( self , midi ) : [EOL] tokens = [ ] [EOL] [EOL] ppqn = midi . header . pulses_per_quarter_note [comment] [EOL] logger . debug ( [string] , ppqn ) [EOL] tempo = midi . tracks [ [number] ] . events [ [number] ] [ [number] ] . metadata [ [string] ] [comment] [EOL] logger . debug ( [string] , tempo ) [EOL] ticklen = tempo / ppqn [comment] [EOL] logger . debug ( [string] , ticklen ) [EOL] ticklen_mult = DEFAULT_TICKLEN / ticklen [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] tokens . append ( [string] ) [EOL] [EOL] cumulative_delta_time = [number] [EOL] for i , ( delta_time , event ) in enumerate ( midi . tracks [ [number] ] . events ) : [EOL] if self . _max_tokens is not None : [EOL] if len ( tokens ) >= self . _max_tokens : [EOL] yield [string] . join ( tokens [ : self . _max_tokens ] ) [EOL] tokens = [ [string] , * tokens [ self . _max_tokens : ] ] [EOL] cumulative_delta_time += round ( delta_time / ticklen_mult ) [EOL] if event . event_type == [string] : [EOL] metadata = event . metadata [EOL] if cumulative_delta_time > [number] : [EOL] if tokens [ - [number] ] != [string] : [EOL] tokens . extend ( split_waits ( cumulative_delta_time ) ) [EOL] cumulative_delta_time = [number] [EOL] if metadata [ [string] ] > [number] : [EOL] velocity = [number] [EOL] else : [EOL] velocity = [number] [EOL] tokens . append ( f' [string] { metadata [ [string] ] } [string] { velocity }' ) [EOL] [EOL] tokens . append ( [string] ) [EOL] if self . _max_tokens is not None : [EOL] if len ( tokens ) >= self . _max_tokens : [EOL] yield [string] . join ( tokens [ : self . _max_tokens ] ) [EOL] tokens = tokens [ self . _max_tokens : ] [EOL] yield [string] . join ( tokens ) [EOL] [EOL] def detokenize ( self , tokens ) : [EOL] tokens = deque ( tokens . split ( ) ) [EOL] events = [ ] [EOL] delta_time = [number] [EOL] while tokens : [EOL] current_token = tokens . popleft ( ) [EOL] token_type , * token_values = current_token . split ( [string] ) [EOL] if token_type == [string] : [EOL] delta_time += self . _scale * int ( token_values [ [number] ] ) [EOL] elif token_type == [string] : [EOL] metadata = { [string] : int ( token_values [ [number] ] ) , [string] : int ( token_values [ [number] ] ) } [EOL] event = md . MidiEvent ( [number] , [string] , [number] , metadata ) [EOL] events . append ( ( delta_time , event ) ) [EOL] delta_time = [number] [EOL] track = md . MidiTrack ( events ) [EOL] return DEFAULT_HEADER + DEFAULT_TEMPO_MAP + track . to_bytes ( ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Type [EOL] import typing [EOL] import squawkbox [EOL] from collections import defaultdict [EOL] [EOL] [EOL] class Registrable : [EOL] _registry = defaultdict ( dict ) [EOL] [EOL] @ classmethod def register ( cls , name ) : [EOL] registry = Registrable . _registry [ cls ] [EOL] def add_subclass_to_registry ( subclass ) : [EOL] if name in registry : [EOL] raise ValueError ( [string] % name ) [EOL] registry [ name ] = subclass [EOL] return subclass [EOL] return add_subclass_to_registry [EOL] [EOL] @ classmethod def get ( cls , name ) : [EOL] if name not in Registrable . _registry [ cls ] : [EOL] raise ValueError ( [string] % name ) [EOL] return Registrable . _registry [ cls ] . get ( name ) [EOL] [EOL] @ classmethod def from_config ( cls , config , ** kwargs ) : [EOL] constructor = cls . get ( config . pop ( [string] ) ) [EOL] return constructor ( ** config , ** kwargs ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0
from random import randint [EOL] from random import uniform [EOL] [EOL] from squawkbox . utils import Registrable [EOL] [EOL] [EOL] [comment] [EOL] class Transform ( Registrable ) : pass [EOL] [EOL] [EOL] [comment] [EOL] @ Transform . register ( [string] ) class TimeStretch ( Transform ) : [EOL] def __init__ ( self , min , max ) : [EOL] self . min = min [EOL] self . max = max [EOL] [EOL] def __call__ ( self , tokens ) : [EOL] k = uniform ( self . min , self . max ) [EOL] new_tokens = [ ] [EOL] for token in tokens : [EOL] token_type , * values = token . split ( [string] ) [EOL] if token_type == [string] : [EOL] wait_time = int ( values [ [number] ] ) [EOL] new_token = [string] % ( min ( [number] , max ( [number] , round ( wait_time * k ) ) ) ) [EOL] new_tokens . append ( new_token ) [EOL] else : [EOL] new_tokens . append ( token ) [EOL] return new_tokens [EOL] [EOL] [EOL] [comment] [EOL] @ Transform . register ( [string] ) class VolumeShift ( Transform ) : [EOL] def __init__ ( self , min , max ) : [EOL] self . min = min [EOL] self . max = max [EOL] [EOL] def __call__ ( self , tokens ) : [EOL] k = randint ( self . min , self . max ) [EOL] new_tokens = [ ] [EOL] for token in tokens : [EOL] token_type , * values = token . split ( [string] ) [EOL] if token_type == [string] : [EOL] pitch_value , volume_value = int ( values [ [number] ] ) , int ( values [ [number] ] ) [EOL] new_volume = volume_value + k [EOL] new_volume = max ( new_volume , [number] ) [EOL] new_volume = min ( new_volume , [number] ) [EOL] new_token = [string] % ( pitch_value , new_volume ) [EOL] new_tokens . append ( new_token ) [EOL] else : [EOL] new_tokens . append ( token ) [EOL] return new_tokens [EOL] [EOL] [EOL] [comment] [EOL] @ Transform . register ( [string] ) class PitchShift ( Transform ) : [EOL] def __init__ ( self , min , max ) : [EOL] self . min = min [EOL] self . max = max [EOL] [EOL] def __call__ ( self , tokens ) : [EOL] k = randint ( self . min , self . max ) [EOL] new_tokens = [ ] [EOL] for token in tokens : [EOL] token_type , * values = token . split ( [string] ) [EOL] if token_type == [string] : [EOL] pitch_value , volume_value = int ( values [ [number] ] ) , int ( values [ [number] ] ) [EOL] new_pitch = pitch_value + k [EOL] new_pitch = max ( new_pitch , [number] ) [EOL] new_pitch = min ( new_pitch , [number] ) [EOL] new_token = [string] % ( new_pitch , volume_value ) [EOL] new_tokens . append ( new_token ) [EOL] else : [EOL] new_tokens . append ( token ) [EOL] return new_tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.float$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0
	0
from typing import List , Any [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] [EOL] from squawkbox . data import TOKEN_TO_IDX , IDX_TO_TOKEN [EOL] [EOL] class Sampler ( nn . Module ) : [EOL] [EOL] def __init__ ( self , decoder , embedding_type , temp = None , top_k = None , top_p = None , max_length = [number] ) : [EOL] super ( Sampler , self ) . __init__ ( ) [EOL] [EOL] self . decoder = decoder [EOL] self . temp = temp [EOL] self . top_k = top_k [EOL] self . top_p = top_p [EOL] self . embedding_type = embedding_type [EOL] self . max_length = max_length [EOL] self . SOS = TOKEN_TO_IDX [ [string] ] [EOL] self . EOS = TOKEN_TO_IDX [ [string] ] [EOL] self . padding = TOKEN_TO_IDX [ [string] ] [EOL] self . cont = TOKEN_TO_IDX [ [string] ] [EOL] self . pos_adjustment = nn . Embedding ( len ( IDX_TO_TOKEN ) , [number] ) [EOL] [EOL] def _delta_time ( self , sample , dev ) : [EOL] tokens = [ IDX_TO_TOKEN [ x ] for x in sample . squeeze ( ) . tolist ( ) ] [EOL] waits = [ int ( x . split ( [string] ) [ - [number] ] ) if [string] in x else [number] for x in tokens ] [EOL] return torch . tensor ( waits , device = dev , dtype = torch . float32 ) . unsqueeze ( - [number] ) [EOL] [EOL] [EOL] def _temper ( self , logits ) : [EOL] [docstring] [EOL] if self . temp is None : [EOL] t = [number] [EOL] else : [EOL] t = self . temp [EOL] [EOL] return F . softmax ( logits / t , dim = - [number] ) [EOL] [EOL] def _sample ( self , probs ) : [EOL] [docstring] [EOL] [comment] [EOL] probs [ : , [number] ] = [number] [EOL] return torch . multinomial ( probs , [number] ) [EOL] [EOL] def _sample_top_k ( self , probs ) : [EOL] [docstring] [EOL] sorted_probs , sorted_indices = probs . sort ( dim = - [number] , descending = True ) [EOL] new_distribution = sorted_probs [ : , : self . top_k ] / sorted_probs . sum ( - [number] , keepdim = True ) [EOL] sample_indices = self . _sample ( new_distribution ) [EOL] sample = sorted_indices . gather ( - [number] , sample_indices ) [EOL] return sample [EOL] [EOL] [EOL] def _sample_top_p ( self , probs ) : [EOL] [docstring] [EOL] sorted_probs , sorted_indices = probs . sort ( dim = - [number] ) [EOL] cum_probs = probs . cumsum ( dim = - [number] ) [EOL] sorted_mask = ( sorted_probs <= self . top_p ) . type ( torch . float32 ) [EOL] masked_probs = probs * torch . zeros_like ( probs ) . scatter ( [number] , sorted_indices , sorted_mask ) [EOL] return self . _sample ( masked_probs ) [EOL] [EOL] def _to_tokens ( self , src ) : [EOL] out = [ ] [EOL] for sequence in src : [EOL] tokens = [ IDX_TO_TOKEN [ i ] for i in sequence . tolist ( ) ] [EOL] if [string] in tokens : [EOL] cutoff = tokens . index ( [string] ) [EOL] out . append ( tokens [ : cutoff ] ) [EOL] else : [EOL] out . append ( tokens ) [EOL] return out [EOL] [EOL] def forward ( self , src = None , timestamps = None , batch_size = None , dev = None , ** kwargs ) : [EOL] [docstring] [EOL] [EOL] if src is not None : [EOL] assert ( timestamps is not None ) [EOL] assert ( ( len ( src . shape ) == len ( timestamps . shape ) ) and ( src . shape [ [number] ] == timestamps . shape [ [number] ] ) and ( src . shape [ [number] ] == timestamps . shape [ [number] ] ) ) [EOL] batch_size = src . shape [ [number] ] [EOL] else : [EOL] assert ( batch_size is not None ) [EOL] src = torch . LongTensor ( [ [ self . SOS ] ] ) . expand ( batch_size , [number] ) . to ( dev ) [EOL] timestamps = torch . zeros ( batch_size , [number] , dtype = torch . float32 ) . to ( dev ) [EOL] [EOL] hidden = None [EOL] for _ in range ( self . max_length ) : [EOL] [EOL] with torch . no_grad ( ) : [EOL] output = self . decoder ( src , timestamps = timestamps ) [EOL] [EOL] [comment] [EOL] probs = self . _temper ( output [ [string] ] [ : , - [number] , : ] ) [EOL] [EOL] if self . top_k is not None : [EOL] sample = self . _sample_top_k ( probs ) [EOL] elif self . top_p is not None : [EOL] sample = self . _sample_top_p ( probs ) [EOL] else : [EOL] sample = self . _sample ( probs ) [EOL] [EOL] src = torch . cat ( ( src , sample ) , - [number] ) [EOL] [EOL] if self . embedding_type == [string] : [EOL] new_timestamp = timestamps [ : , - [number] ] . unsqueeze ( - [number] ) + [number] [EOL] elif self . embedding_type == [string] : [EOL] delta = self . _delta_time ( sample , dev ) [EOL] new_timestamp = timestamps [ : , - [number] ] . unsqueeze ( - [number] ) + delta [EOL] timestamps = torch . cat ( ( timestamps , new_timestamp ) , - [number] ) [EOL] [EOL] samples = self . _to_tokens ( src ) [EOL] [EOL] return samples [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] import squawkbox [EOL] import math [EOL] import torch [EOL] from torch import nn [EOL] [EOL] from . utils import Conv1D [EOL] [EOL] class Attention ( nn . Module ) : [EOL] def __init__ ( self , nx , n_ctx , config , scale = False ) : [EOL] super ( Attention , self ) . __init__ ( ) [EOL] n_state = nx [comment] [EOL] [comment] [EOL] assert n_state % config . n_head == [number] [EOL] self . register_buffer ( [string] , torch . tril ( torch . ones ( n_ctx , n_ctx ) ) . view ( [number] , [number] , n_ctx , n_ctx ) ) [EOL] self . n_ctx = n_ctx [EOL] self . n_head = config . n_head [EOL] self . split_size = n_state [EOL] self . scale = scale [EOL] self . c_attn = Conv1D ( n_state * [number] , nx ) [EOL] self . c_proj = Conv1D ( n_state , nx ) [EOL] [EOL] def _attn ( self , q , k , v ) : [EOL] w = torch . matmul ( q , k ) [EOL] if self . scale : [EOL] w = w / math . sqrt ( v . size ( - [number] ) ) [EOL] nd , ns = w . size ( - [number] ) , w . size ( - [number] ) [EOL] b = self . bias [ : , : , ns - nd : ns , : ns ] [EOL] w = w * b - [number] * ( [number] - b ) [EOL] [EOL] w = nn . Softmax ( dim = - [number] ) ( w ) [EOL] return torch . matmul ( w , v ) [EOL] [EOL] def merge_heads ( self , x ) : [EOL] x = x . permute ( [number] , [number] , [number] , [number] ) . contiguous ( ) [EOL] new_x_shape = x . size ( ) [ : - [number] ] + ( x . size ( - [number] ) * x . size ( - [number] ) , ) [EOL] return x . view ( * new_x_shape ) [comment] [EOL] [EOL] def split_heads ( self , x , k = False ) : [EOL] new_x_shape = x . size ( ) [ : - [number] ] + ( self . n_head , x . size ( - [number] ) // self . n_head ) [EOL] x = x . view ( * new_x_shape ) [comment] [EOL] if k : [EOL] return x . permute ( [number] , [number] , [number] , [number] ) [comment] [EOL] else : [EOL] return x . permute ( [number] , [number] , [number] , [number] ) [comment] [EOL] [EOL] def forward ( self , x , layer_past = None ) : [EOL] x = self . c_attn ( x ) [EOL] query , key , value = x . split ( self . split_size , dim = [number] ) [EOL] query = self . split_heads ( query ) [EOL] key = self . split_heads ( key , k = True ) [EOL] value = self . split_heads ( value ) [EOL] if layer_past is not None : [EOL] past_key , past_value = layer_past [ [number] ] . transpose ( - [number] , - [number] ) , layer_past [ [number] ] [comment] [EOL] key = torch . cat ( ( past_key , key ) , dim = - [number] ) [EOL] value = torch . cat ( ( past_value , value ) , dim = - [number] ) [EOL] present = torch . stack ( ( key . transpose ( - [number] , - [number] ) , value ) ) [comment] [EOL] a = self . _attn ( query , key , value ) [EOL] a = self . merge_heads ( a ) [EOL] a = self . c_proj ( a ) [EOL] return a , present [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $squawkbox.modules.utils.Conv1D$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $squawkbox.modules.utils.Conv1D$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] import math [EOL] import torch [EOL] from torch import nn [EOL] from torch . nn import Parameter [EOL] [EOL] def gelu ( x ) : [EOL] return [number] * x * ( [number] + torch . tanh ( math . sqrt ( [number] / math . pi ) * ( x + [number] * torch . pow ( x , [number] ) ) ) ) [EOL] [EOL] class Conv1D ( nn . Module ) : [EOL] def __init__ ( self , nf , nx ) : [EOL] super ( Conv1D , self ) . __init__ ( ) [EOL] self . nf = nf [EOL] w = torch . empty ( nx , nf ) [EOL] nn . init . normal_ ( w , std = [number] ) [EOL] self . weight = Parameter ( w ) [EOL] self . bias = Parameter ( torch . zeros ( nf ) ) [EOL] [EOL] def forward ( self , x ) : [EOL] size_out = x . size ( ) [ : - [number] ] + ( self . nf , ) [EOL] x = torch . addmm ( self . bias , x . view ( - [number] , x . size ( - [number] ) ) , self . weight ) [EOL] x = x . view ( * size_out ) [EOL] return x [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0
from typing import Any [EOL] import typing [EOL] import math [EOL] [EOL] import torch [EOL] [EOL] [EOL] class PositionalEmbedding ( torch . nn . Module ) : [EOL] def __init__ ( self , dim ) : [EOL] super ( PositionalEmbedding , self ) . __init__ ( ) [EOL] assert ( dim % [number] ) == [number] [EOL] self . dim = dim [EOL] half_dim = int ( dim / [number] ) [EOL] self . register_buffer ( [string] , [number] / [number] ** ( torch . arange ( [number] , half_dim , dtype = torch . float32 ) . view ( [number] , [number] , half_dim ) / half_dim ) ) [EOL] [EOL] def forward ( self , timestamp ) : [EOL] timestamp = timestamp . type ( torch . float32 ) . unsqueeze ( - [number] ) [EOL] x = torch . matmul ( timestamp , self . freq ) [comment] [EOL] sin = torch . sin ( x ) [EOL] cos = torch . cos ( x ) [EOL] return torch . cat ( ( sin , cos ) , - [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0
from squawkbox . modules . attention import * [EOL] from squawkbox . modules . positional_embedding import * [EOL] from squawkbox . modules . gpt2_modules import * [EOL] from squawkbox . modules . utils import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any , Dict [EOL] import typing [EOL] import squawkbox [EOL] import collections [EOL] import copy [EOL] import json [EOL] import logging [EOL] import math [EOL] import os [EOL] import shutil [EOL] import tarfile [EOL] import tempfile [EOL] import sys [EOL] from io import open [EOL] [EOL] import torch [EOL] import torch . nn as nn [EOL] from torch . nn import CrossEntropyLoss [EOL] from torch . nn . parameter import Parameter [EOL] from torch . utils . checkpoint import checkpoint [EOL] [EOL] from . utils import gelu , Conv1D [EOL] from . attention import Attention [EOL] from pytorch_pretrained_bert . file_utils import cached_path , CONFIG_NAME , WEIGHTS_NAME [EOL] from pytorch_pretrained_bert . modeling import BertLayerNorm as LayerNorm [EOL] [EOL] class GPT2Config ( object ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vocab_size_or_config_json_file = [number] , n_positions = [number] , n_ctx = [number] , n_embd = [number] , n_layer = [number] , n_head = [number] , layer_norm_epsilon = [number] , initializer_range = [number] , ) : [EOL] [docstring] [EOL] if isinstance ( vocab_size_or_config_json_file , str ) or ( sys . version_info [ [number] ] == [number] [EOL] and isinstance ( vocab_size_or_config_json_file , unicode ) ) : [EOL] with open ( vocab_size_or_config_json_file , [string] , encoding = [string] ) as reader : [EOL] json_config = json . loads ( reader . read ( ) ) [EOL] for key , value in json_config . items ( ) : [EOL] self . __dict__ [ key ] = value [EOL] elif isinstance ( vocab_size_or_config_json_file , int ) : [EOL] self . vocab_size = vocab_size_or_config_json_file [EOL] self . n_ctx = n_ctx [EOL] self . n_positions = n_positions [EOL] self . n_embd = n_embd [EOL] self . n_layer = n_layer [EOL] self . n_head = n_head [EOL] self . layer_norm_epsilon = layer_norm_epsilon [EOL] self . initializer_range = initializer_range [EOL] else : [EOL] raise ValueError ( [string] [string] ) [EOL] [EOL] @ classmethod def from_dict ( cls , json_object ) : [EOL] [docstring] [EOL] config = GPT2Config ( vocab_size_or_config_json_file = - [number] ) [EOL] for key , value in json_object . items ( ) : [EOL] config . __dict__ [ key ] = value [EOL] return config [EOL] [EOL] @ classmethod def from_json_file ( cls , json_file ) : [EOL] [docstring] [EOL] with open ( json_file , [string] , encoding = [string] ) as reader : [EOL] text = reader . read ( ) [EOL] return cls . from_dict ( json . loads ( text ) ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return str ( self . to_json_string ( ) ) [EOL] [EOL] def to_dict ( self ) : [EOL] [docstring] [EOL] output = copy . deepcopy ( self . __dict__ ) [EOL] return output [EOL] [EOL] def to_json_string ( self ) : [EOL] [docstring] [EOL] return json . dumps ( self . to_dict ( ) , indent = [number] , sort_keys = True ) + [string] [EOL] [EOL] def to_json_file ( self , json_file_path ) : [EOL] [docstring] [EOL] with open ( json_file_path , [string] , encoding = [string] ) as writer : [EOL] writer . write ( self . to_json_string ( ) ) [EOL] [EOL] [EOL] class MLP ( nn . Module ) : [EOL] def __init__ ( self , n_state , config ) : [comment] [EOL] super ( MLP , self ) . __init__ ( ) [EOL] nx = config . n_embd [EOL] self . c_fc = Conv1D ( n_state , nx ) [EOL] self . c_proj = Conv1D ( nx , n_state ) [EOL] self . act = gelu [EOL] [EOL] def forward ( self , x ) : [EOL] h = self . act ( self . c_fc ( x ) ) [EOL] h2 = self . c_proj ( h ) [EOL] return h2 [EOL] [EOL] [EOL] class Block ( nn . Module ) : [EOL] def __init__ ( self , n_ctx , config , scale = False ) : [EOL] super ( Block , self ) . __init__ ( ) [EOL] nx = config . n_embd [EOL] self . ln_1 = LayerNorm ( nx , eps = config . layer_norm_epsilon ) [EOL] self . attn = Attention ( nx , n_ctx , config , scale ) [EOL] self . ln_2 = LayerNorm ( nx , eps = config . layer_norm_epsilon ) [EOL] self . mlp = MLP ( [number] * nx , config ) [EOL] [EOL] def forward ( self , x , layer_past = None ) : [EOL] a , present = self . attn ( self . ln_1 ( x ) , layer_past = layer_past ) [EOL] x = x + a [EOL] m = self . mlp ( self . ln_2 ( x ) ) [EOL] x = x + m [EOL] return x , present [EOL] [EOL] [EOL] class GPT2LMHead ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_embeddings_weights , config ) : [EOL] super ( GPT2LMHead , self ) . __init__ ( ) [EOL] self . n_embd = config . n_embd [EOL] self . set_embeddings_weights ( model_embeddings_weights ) [EOL] [EOL] def set_embeddings_weights ( self , model_embeddings_weights ) : [EOL] embed_shape = model_embeddings_weights . shape [EOL] self . decoder = nn . Linear ( embed_shape [ [number] ] , embed_shape [ [number] ] , bias = False ) [EOL] self . decoder . weight = model_embeddings_weights [comment] [EOL] [EOL] def forward ( self , hidden_state ) : [EOL] [comment] [EOL] [comment] [EOL] lm_logits = self . decoder ( hidden_state ) [EOL] return lm_logits [EOL] [EOL] [EOL] class GPT2MultipleChoiceHead ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , config ) : [EOL] super ( GPT2MultipleChoiceHead , self ) . __init__ ( ) [EOL] self . n_embd = config . n_embd [EOL] self . linear = nn . Linear ( config . n_embd , [number] ) [EOL] [EOL] nn . init . normal_ ( self . linear . weight , std = [number] ) [EOL] nn . init . normal_ ( self . linear . bias , [number] ) [EOL] [EOL] def forward ( self , hidden_states , mc_token_ids ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] mc_token_ids = mc_token_ids . unsqueeze ( - [number] ) . unsqueeze ( - [number] ) . expand ( - [number] , - [number] , - [number] , hidden_states . size ( - [number] ) ) [EOL] [comment] [EOL] multiple_choice_h = hidden_states . gather ( [number] , mc_token_ids ) . squeeze ( [number] ) [EOL] [comment] [EOL] multiple_choice_logits = self . linear ( multiple_choice_h ) . squeeze ( - [number] ) [EOL] [comment] [EOL] return multiple_choice_logits [EOL] [EOL] [EOL] class GPT2PreTrainedModel ( nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , config , * inputs , ** kwargs ) : [EOL] super ( GPT2PreTrainedModel , self ) . __init__ ( ) [EOL] if not isinstance ( config , GPT2Config ) : [EOL] raise ValueError ( [string] [string] [string] . format ( self . __class__ . __name__ , self . __class__ . __name__ ) ) [EOL] self . config = config [EOL] [EOL] def set_tied ( self ) : [EOL] pass [EOL] [EOL] def init_weights ( self , module ) : [EOL] [docstring] [EOL] if isinstance ( module , ( nn . Linear , nn . Embedding ) ) : [EOL] [comment] [EOL] [comment] [EOL] module . weight . data . normal_ ( mean = [number] , std = self . config . initializer_range ) [EOL] elif isinstance ( module , LayerNorm ) : [EOL] module . bias . data . zero_ ( ) [EOL] module . weight . data . fill_ ( [number] ) [EOL] if isinstance ( module , nn . Linear ) and module . bias is not None : [EOL] module . bias . data . zero_ ( ) [EOL] [EOL] @ classmethod def from_pretrained ( cls , pretrained_model_name_or_path , state_dict = None , cache_dir = None , from_tf = False , * inputs , ** kwargs ) : [EOL] [docstring] [EOL] if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP : [EOL] archive_file = PRETRAINED_MODEL_ARCHIVE_MAP [ pretrained_model_name_or_path ] [EOL] config_file = PRETRAINED_CONFIG_ARCHIVE_MAP [ pretrained_model_name_or_path ] [EOL] else : [EOL] archive_file = os . path . join ( pretrained_model_name_or_path , WEIGHTS_NAME ) [EOL] config_file = os . path . join ( pretrained_model_name_or_path , CONFIG_NAME ) [EOL] [comment] [EOL] try : [EOL] resolved_archive_file = cached_path ( archive_file , cache_dir = cache_dir ) [EOL] resolved_config_file = cached_path ( config_file , cache_dir = cache_dir ) [EOL] except EnvironmentError : [EOL] logger . error ( [string] [string] [string] . format ( pretrained_model_name_or_path , [string] . join ( PRETRAINED_MODEL_ARCHIVE_MAP . keys ( ) ) , pretrained_model_name_or_path , archive_file , config_file ) ) [EOL] return None [EOL] if resolved_archive_file == archive_file and resolved_config_file == config_file : [EOL] logger . info ( [string] . format ( archive_file ) ) [EOL] logger . info ( [string] . format ( config_file ) ) [EOL] else : [EOL] logger . info ( [string] . format ( archive_file , resolved_archive_file ) ) [EOL] logger . info ( [string] . format ( config_file , resolved_config_file ) ) [EOL] [comment] [EOL] config = GPT2Config . from_json_file ( resolved_config_file ) [EOL] logger . info ( [string] . format ( config ) ) [EOL] [comment] [EOL] model = cls ( config , * inputs , ** kwargs ) [EOL] if state_dict is None and not from_tf : [EOL] state_dict = torch . load ( resolved_archive_file , map_location = [string] ) [EOL] if from_tf : [EOL] [comment] [EOL] return load_tf_weights_in_gpt2 ( model , resolved_archive_file ) [EOL] [EOL] old_keys = [ ] [EOL] new_keys = [ ] [EOL] for key in state_dict . keys ( ) : [EOL] new_key = None [EOL] if key . endswith ( [string] ) : [EOL] new_key = key [ : - [number] ] + [string] [EOL] elif key . endswith ( [string] ) : [EOL] new_key = key [ : - [number] ] + [string] [EOL] elif key . endswith ( [string] ) : [EOL] new_key = key [ : - [number] ] + [string] [EOL] if new_key : [EOL] old_keys . append ( key ) [EOL] new_keys . append ( new_key ) [EOL] for old_key , new_key in zip ( old_keys , new_keys ) : [EOL] state_dict [ new_key ] = state_dict . pop ( old_key ) [EOL] [EOL] missing_keys = [ ] [EOL] unexpected_keys = [ ] [EOL] error_msgs = [ ] [EOL] [comment] [EOL] metadata = getattr ( state_dict , [string] , None ) [EOL] state_dict = state_dict . copy ( ) [EOL] if metadata is not None : [EOL] state_dict . _metadata = metadata [EOL] [EOL] def load ( module , prefix = [string] ) : [EOL] local_metadata = { } if metadata is None else metadata . get ( prefix [ : - [number] ] , { } ) [EOL] module . _load_from_state_dict ( state_dict , prefix , local_metadata , True , missing_keys , unexpected_keys , error_msgs ) [EOL] for name , child in module . _modules . items ( ) : [EOL] if child is not None : [EOL] load ( child , prefix + name + [string] ) [EOL] [EOL] start_model = model [EOL] if hasattr ( model , [string] ) and all ( not s . startswith ( [string] ) for s in state_dict . keys ( ) ) : [EOL] start_model = model . transformer [EOL] load ( start_model , prefix = [string] ) [EOL] [EOL] if len ( missing_keys ) > [number] : [EOL] logger . info ( [string] . format ( model . __class__ . __name__ , missing_keys ) ) [EOL] if len ( unexpected_keys ) > [number] : [EOL] logger . info ( [string] . format ( model . __class__ . __name__ , unexpected_keys ) ) [EOL] if len ( error_msgs ) > [number] : [EOL] raise RuntimeError ( [string] . format ( model . __class__ . __name__ , [string] . join ( error_msgs ) ) ) [EOL] [EOL] [comment] [EOL] model . set_tied ( ) [EOL] return model [EOL] [EOL] class GPT2Model ( GPT2PreTrainedModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , config ) : [EOL] super ( GPT2Model , self ) . __init__ ( config ) [EOL] self . wte = nn . Embedding ( config . vocab_size , config . n_embd ) [EOL] self . wpe = nn . Embedding ( config . n_positions , config . n_embd ) [EOL] block = Block ( config . n_ctx , config , scale = True ) [EOL] self . h = nn . ModuleList ( [ copy . deepcopy ( block ) for _ in range ( config . n_layer ) ] ) [EOL] self . ln_f = LayerNorm ( config . n_embd , eps = config . layer_norm_epsilon ) [EOL] [EOL] self . apply ( self . init_weights ) [EOL] [EOL] def forward ( self , input_ids , position_ids = None , token_type_ids = None , past = None ) : [EOL] if past is None : [EOL] past_length = [number] [EOL] past = [ None ] * len ( self . h ) [EOL] else : [EOL] past_length = past [ [number] ] [ [number] ] . size ( - [number] ) [EOL] if position_ids is None : [EOL] position_ids = torch . arange ( past_length , input_ids . size ( - [number] ) + past_length , dtype = torch . long , device = input_ids . device ) [EOL] position_ids = position_ids . unsqueeze ( [number] ) . expand_as ( input_ids ) [EOL] [EOL] input_shape = input_ids . size ( ) [EOL] input_ids = input_ids . view ( - [number] , input_ids . size ( - [number] ) ) [EOL] position_ids = position_ids . view ( - [number] , position_ids . size ( - [number] ) ) [EOL] [EOL] inputs_embeds = self . wte ( input_ids ) [EOL] position_embeds = self . wpe ( position_ids ) [EOL] if token_type_ids is not None : [EOL] token_type_ids = token_type_ids . view ( - [number] , token_type_ids . size ( - [number] ) ) [EOL] token_type_embeds = self . wte ( token_type_ids ) [EOL] else : [EOL] token_type_embeds = [number] [EOL] hidden_states = inputs_embeds + position_embeds + token_type_embeds [EOL] presents = [ ] [EOL] for i , ( block , layer_past ) in enumerate ( zip ( self . h , past ) ) : [EOL] hidden_states , present = checkpoint ( block , hidden_states , layer_past ) [EOL] presents . append ( present ) [EOL] hidden_states = self . ln_f ( hidden_states ) [EOL] output_shape = input_shape + ( hidden_states . size ( - [number] ) , ) [EOL] return hidden_states . view ( * output_shape ) , presents [EOL] [EOL] [EOL] class GPT2LMHeadModel ( GPT2PreTrainedModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , config ) : [EOL] super ( GPT2LMHeadModel , self ) . __init__ ( config ) [EOL] self . transformer = GPT2Model ( config ) [EOL] self . lm_head = GPT2LMHead ( self . transformer . wte . weight , config ) [EOL] self . apply ( self . init_weights ) [EOL] [EOL] def set_tied ( self ) : [EOL] [docstring] [EOL] self . lm_head . set_embeddings_weights ( self . transformer . wte . weight ) [EOL] [EOL] def forward ( self , input_ids , position_ids = None , token_type_ids = None , lm_labels = None , past = None ) : [EOL] hidden_states , presents = self . transformer ( input_ids , position_ids , token_type_ids , past ) [EOL] lm_logits = checkpoint ( self . lm_head , hidden_states ) [EOL] if lm_labels is not None : [EOL] [comment] [EOL] shift_logits = lm_logits [ : , : - [number] ] . contiguous ( ) [EOL] shift_labels = lm_labels [ : , [number] : ] . contiguous ( ) [EOL] [EOL] [comment] [EOL] loss_fct = CrossEntropyLoss ( ignore_index = - [number] ) [EOL] loss = loss_fct ( shift_logits . view ( - [number] , shift_logits . size ( - [number] ) ) , shift_labels . view ( - [number] ) ) [EOL] return loss [EOL] return lm_logits , presents [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.utils.Conv1D$ 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.utils.Conv1D$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.attention.Attention$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.MLP$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2PreTrainedModel$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.Block$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.Block$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[None]$ 0 0 0 0 0 0 $typing.List[None]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[None]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Model$ 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2LMHead$ 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0
from typing import Dict , Any [EOL] import typing [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] from torch . nn . utils . rnn import pack_padded_sequence , pad_packed_sequence [EOL] [EOL] from squawkbox . models import Model [EOL] [EOL] [EOL] @ Model . register ( [string] ) class Baseline ( nn . Module ) : [EOL] def __init__ ( self , vocab_size , embedding_dim , num_lstm_units , num_lstm_layers , padding = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] self . vocab_size = vocab_size [EOL] self . padding = padding [EOL] [EOL] self . embedding = nn . Embedding ( num_embeddings = vocab_size , embedding_dim = embedding_dim ) [EOL] [EOL] self . lstm = nn . LSTM ( input_size = embedding_dim , hidden_size = num_lstm_units , num_layers = num_lstm_layers , batch_first = True ) [EOL] [EOL] self . h2o = nn . Linear ( num_lstm_units , vocab_size ) [EOL] [EOL] def forward ( self , src , tgt = None , hidden = None , ** kwargs ) : [EOL] [docstring] [EOL] [EOL] if tgt is not None : [EOL] masks = tgt . ne ( self . padding ) . float ( ) [EOL] else : [EOL] masks = src . ne ( self . padding ) . float ( ) [EOL] [EOL] seq_lens = masks . sum ( [number] ) [EOL] seq_lens , perm_idx = seq_lens . sort ( [number] , descending = True ) [EOL] embeddings = self . embedding ( src ) [ perm_idx ] [EOL] if tgt is not None : [EOL] tgt = tgt [ perm_idx ] [EOL] [EOL] [comment] [EOL] packed = pack_padded_sequence ( embeddings , lengths = seq_lens , batch_first = True ) [EOL] if hidden is None : [EOL] lstm_output , hidden_out = self . lstm ( packed ) [EOL] else : [EOL] lstm_output , hidden_out = self . lstm ( packed , hidden ) [EOL] [comment] [EOL] lstm_output , _ = pad_packed_sequence ( lstm_output , batch_first = True ) [EOL] [EOL] [comment] [EOL] logits = self . h2o ( lstm_output ) * masks . unsqueeze ( - [number] ) [comment] [EOL] output = { [string] : logits , [string] : hidden_out } [EOL] [EOL] if tgt is not None : [EOL] ll = F . log_softmax ( logits , dim = - [number] ) . gather ( [number] , tgt . unsqueeze ( - [number] ) ) [EOL] ll = ll * masks . unsqueeze ( - [number] ) [EOL] output [ [string] ] = - ll . mean ( dim = [number] ) . mean ( dim = [number] ) [EOL] [EOL] return output [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0
from typing import Dict , Any [EOL] import typing [EOL] import squawkbox [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from torch import nn [EOL] [EOL] [EOL] from squawkbox . modules import GPT2Model , GPT2LMHead , GPT2Config , PositionalEmbedding [EOL] from squawkbox . models import Model [EOL] [EOL] @ Model . register ( [string] ) class GPT2_Standard ( nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab_size , n_positions , n_ctx , n_embd , n_layers , n_head , layer_norm_epsilon , initializer_range , padding = [number] , ** kwargs ) : [EOL] [EOL] super ( GPT2_Standard , self ) . __init__ ( ) [EOL] config = GPT2Config ( vocab_size_or_config_json_file = vocab_size , n_positions = n_positions , n_ctx = n_ctx , n_embd = n_embd , n_layer = n_layers , n_head = n_head , layer_norm_epsilon = layer_norm_epsilon , initializer_range = initializer_range ) [EOL] [EOL] self . n_ctx = n_ctx [EOL] self . padding = padding [EOL] self . main_model = GPT2Model ( config = config ) [EOL] self . lm_head = GPT2LMHead ( model_embeddings_weights = self . main_model . wte . weight , config = config ) [EOL] [EOL] [comment] [EOL] self . main_model . wpe = PositionalEmbedding ( n_embd ) [EOL] [EOL] def forward ( self , src , tgt = None , timestamps = None , hidden = None , ** kwargs ) : [EOL] hidden_out , presents = self . main_model ( input_ids = src , position_ids = timestamps , token_type_ids = None ) [EOL] [EOL] logits = self . lm_head ( hidden_state = hidden_out ) [EOL] [EOL] [comment] [EOL] output = { [string] : logits } [EOL] if tgt is not None : [EOL] loss_function = nn . CrossEntropyLoss ( ignore_index = self . padding ) [EOL] ll = loss_function ( logits . view ( - [number] , logits . shape [ - [number] ] ) , tgt . view ( - [number] ) ) [EOL] output [ [string] ] = ll [EOL] [EOL] return output [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Model$ 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2LMHead$ 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Model$ 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 $squawkbox.modules.gpt2_modules.GPT2Config$ 0 0 0 0 0 0 0 $squawkbox.modules.gpt2_modules.GPT2Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0
import torch [EOL] [EOL] from squawkbox . utils import Registrable [EOL] [EOL] [EOL] class Model ( torch . nn . Module , Registrable ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from squawkbox . models . model import Model [EOL] from squawkbox . models . baseline import Baseline [EOL] from squawkbox . models . gpt2 import GPT2_Standard [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import logging [EOL] import argparse [EOL] import typing [EOL] import csv [EOL] import squawkbox [EOL] [docstring] [EOL] from collections import deque [EOL] import csv [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] from typing import Deque , IO , List , Tuple [EOL] [EOL] from overrides import overrides [EOL] from tqdm import tqdm [EOL] [EOL] from squawkbox . midi import Midi [EOL] from squawkbox . tokenizer import Tokenizer [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def _tokenize ( args ) : [EOL] tokenizer = Tokenizer ( scale = args . scale , max_tokens = args . max_tokens , max_wait_time = args . max_wait_time ) [EOL] with open ( args . input , [string] ) as midi_file : [EOL] midi = Midi . load ( midi_file ) [EOL] with open ( args . output , [string] ) as token_file : [EOL] for token_str in tokenizer . tokenize ( midi ) : [EOL] token_file . write ( token_str + [string] ) [EOL] [EOL] [EOL] def _process_maestro ( args ) : [EOL] tokenizer = Tokenizer ( scale = args . scale , max_tokens = args . max_tokens , max_wait_time = args . max_wait_time ) [EOL] [EOL] if not args . csv . exists ( ) : [EOL] raise IOError ( [string] , args . csv ) [EOL] [EOL] if args . root_dir is None : [EOL] root_dir = args . csv . parents [ [number] ] [EOL] else : [EOL] root_dir = args . root_dir [EOL] [EOL] if not args . output_dir . exists ( ) : [EOL] logger . info ( [string] , args . output_dir ) [EOL] args . output_dir . mkdir ( parents = True ) [EOL] [EOL] if ( args . output_dir / [string] ) . exists ( ) : [EOL] raise RuntimeError ( [string] ) [EOL] if ( args . output_dir / [string] ) . exists ( ) : [EOL] raise RuntimeError ( [string] ) [EOL] if ( args . output_dir / [string] ) . exists ( ) : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] with open ( args . csv ) as csv_file : [EOL] reader = csv . DictReader ( csv_file ) [EOL] for row in tqdm ( reader ) : [EOL] fname = root_dir / row [ [string] ] [EOL] split = row [ [string] ] [EOL] with open ( fname , [string] ) as midi_file : [EOL] midi = Midi . load ( midi_file ) [EOL] with open ( args . output_dir / ( split + [string] ) , [string] ) as token_file : [EOL] for token_str in tokenizer . tokenize ( midi ) : [EOL] token_file . write ( token_str + [string] ) [EOL] [EOL] [EOL] def _detokenize ( args ) : [EOL] logger . warning ( [string] ) [EOL] tokenizer = Tokenizer ( scale = args . scale ) [EOL] with open ( args . input , [string] ) as token_file : [EOL] tokens = token_file . readline ( ) [EOL] with open ( args . output , [string] ) as midi_file : [EOL] midi_file . write ( tokenizer . detokenize ( tokens ) ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import argparse [EOL] [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] [EOL] tokenize_description = [string] [EOL] tokenize_parser = subparsers . add_parser ( [string] , description = tokenize_description , help = tokenize_description ) [EOL] tokenize_parser . add_argument ( [string] , type = str , help = [string] ) [EOL] tokenize_parser . add_argument ( [string] , type = str , help = [string] ) [EOL] tokenize_parser . add_argument ( [string] , type = int , default = [number] , help = [string] ) [EOL] tokenize_parser . add_argument ( [string] , type = int , default = None , help = [string] ) [EOL] tokenize_parser . add_argument ( [string] , type = int , default = None , help = [string] ) [EOL] tokenize_parser . set_defaults ( func = _tokenize ) [EOL] [EOL] batch_tokenize_description = [string] [EOL] batch_tokenize_parser = subparsers . add_parser ( [string] , description = batch_tokenize_description , help = batch_tokenize_description ) [EOL] batch_tokenize_parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] batch_tokenize_parser . add_argument ( [string] , [string] , type = Path , default = Path ( [string] ) , help = [string] ) [EOL] batch_tokenize_parser . add_argument ( [string] , type = Path , default = None , help = [string] [string] [string] ) [EOL] batch_tokenize_parser . add_argument ( [string] , type = int , default = [number] , help = [string] ) [EOL] batch_tokenize_parser . add_argument ( [string] , type = int , default = None , help = [string] ) [EOL] batch_tokenize_parser . add_argument ( [string] , type = int , default = None , help = [string] ) [EOL] batch_tokenize_parser . set_defaults ( func = _process_maestro ) [EOL] [EOL] detokenize_description = [string] [EOL] detokenize_parser = subparsers . add_parser ( [string] , description = detokenize_description , help = detokenize_description ) [EOL] detokenize_parser . add_argument ( [string] , type = str , help = [string] ) [EOL] detokenize_parser . add_argument ( [string] , type = str , help = [string] ) [EOL] detokenize_parser . add_argument ( [string] , type = int , default = [number] , help = [string] ) [EOL] detokenize_parser . set_defaults ( func = _detokenize ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] if os . environ . get ( [string] ) : [EOL] level = logging . DEBUG [EOL] else : [EOL] level = logging . INFO [EOL] logging . basicConfig ( format = [string] , level = level ) [EOL] [EOL] if [string] in dir ( args ) : [EOL] args . func ( args ) [EOL] else : [EOL] parser . print_help ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0
from typing import Dict , Any [EOL] import argparse [EOL] import typing [EOL] import squawkbox [EOL] import logging [EOL] [docstring] [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] import sys [EOL] [EOL] import numpy as np [EOL] import torch [EOL] from torch . utils . data import DataLoader [EOL] from tqdm import tqdm [EOL] import yaml [EOL] [EOL] from squawkbox . data import MidiDataset , pad_and_combine_instances [EOL] from squawkbox . models import Model [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def _evaluate ( args ) : [EOL] if not args . config . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] [EOL] if not args . checkpoint . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] [EOL] if not args . data . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] [EOL] with open ( args . config , [string] ) as config_file : [EOL] config = yaml . load ( config_file ) [EOL] [EOL] torch . manual_seed ( config . get ( [string] , [number] ) ) [EOL] np . random . seed ( config . get ( [string] , [number] ) + [number] ) [EOL] [EOL] [comment] [EOL] model = Model . from_config ( config [ [string] ] ) [EOL] if args . cuda : [EOL] logger . info ( [string] ) [EOL] if args . cuda_device is not None : [EOL] model = model . cuda ( args . cuda_device ) [EOL] else : [EOL] model = model . cuda ( ) [EOL] [EOL] logger . info ( [string] ) [EOL] state_dict = torch . load ( args . checkpoint ) [EOL] model . load_state_dict ( state_dict [ [string] ] ) [EOL] [EOL] train_config = config [ [string] ] [EOL] embedding_type = train_config . get ( [string] , [string] ) [EOL] [EOL] dataset = MidiDataset ( args . data , embedding_type = embedding_type ) [EOL] data_loader = DataLoader ( dataset , batch_size = train_config [ [string] ] , shuffle = False , num_workers = train_config . get ( [string] , [number] ) , collate_fn = pad_and_combine_instances ) [EOL] [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] total_validation_loss = [number] [EOL] n_tokens = [number] [EOL] for instance in tqdm ( data_loader ) : [EOL] if args . cuda : [EOL] instance = { key : value . cuda ( args . cuda_device ) for key , value in instance . items ( ) } [EOL] with torch . no_grad ( ) : [EOL] output_dict = model ( ** instance ) [EOL] loss = output_dict [ [string] ] [EOL] n_tokens_in_batch = instance [ [string] ] . ne ( [number] ) . sum ( ) . item ( ) [EOL] total_validation_loss += loss . item ( ) * n_tokens_in_batch [EOL] n_tokens += n_tokens_in_batch [EOL] metric = total_validation_loss / n_tokens [EOL] logger . info ( [string] , metric ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import argparse [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = None ) [EOL] args , _ = parser . parse_known_args ( ) [EOL] [EOL] if os . environ . get ( [string] ) : [EOL] level = logging . DEBUG [EOL] else : [EOL] level = logging . INFO [EOL] logging . basicConfig ( format = [string] , level = level ) [EOL] [EOL] _evaluate ( args ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import argparse [EOL] import typing [EOL] import squawkbox [EOL] import logging [EOL] [docstring] [EOL] from collections import deque [EOL] import csv [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] from typing import Deque , IO , List , Tuple [EOL] import time [EOL] import yaml [EOL] import torch [EOL] from torch . utils . data import DataLoader [EOL] import numpy as np [EOL] import sys [EOL] [EOL] from squawkbox . midi import Midi [EOL] from squawkbox . tokenizer import Tokenizer [EOL] from squawkbox . models import Model [EOL] from squawkbox . modules . sampler import Sampler [EOL] from squawkbox . data import IDX_TO_TOKEN , MidiDataset , pad_and_combine_instances [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] def _sample ( args ) : [EOL] [EOL] model_path = args . model_dir / [string] [EOL] config_path = args . model_dir / [string] [EOL] samples_folder = args . model_dir / [string][comment] [EOL] [EOL] if not config_path . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] if not model_path . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] [EOL] with open ( config_path , [string] ) as config_file : [EOL] config = yaml . load ( config_file , Loader = yaml . SafeLoader ) [EOL] [EOL] if not samples_folder . exists ( ) : [EOL] logger . info ( [string] , samples_folder ) [EOL] samples_folder . mkdir ( ) [EOL] [EOL] samples_folder = args . out [EOL] [EOL] if not samples_folder . exists ( ) : [EOL] logger . info ( [string] , samples_folder ) [EOL] samples_folder . mkdir ( ) [EOL] [EOL] torch . manual_seed ( config . get ( [string] , [number] ) ) [EOL] np . random . seed ( config . get ( [string] , [number] ) + [number] ) [EOL] [EOL] model = Model . from_config ( config [ [string] ] ) [EOL] state_dict = torch . load ( model_path ) [EOL] model . load_state_dict ( state_dict [ [string] ] ) [EOL] if args . cuda : [EOL] logger . info ( [string] ) [EOL] if args . cuda_device is not None : [EOL] dev = torch . device ( [string] . format ( args . cuda_device ) ) [EOL] else : [EOL] dev = torch . device ( [string] ) [EOL] else : [EOL] dev = torch . device ( [string] ) [EOL] [EOL] model = model . to ( dev ) [EOL] [EOL] if args . conditional is not None : [EOL] dataset = MidiDataset ( args . conditional , transforms = None , embedding_type = config [ [string] ] . get ( [string] , [string] ) ) [EOL] loader = DataLoader ( dataset , batch_size = [number] , shuffle = False , num_workers = [number] , collate_fn = pad_and_combine_instances ) [EOL] instance = next ( iter ( loader ) ) [EOL] src = instance [ [string] ] [ : , : args . conditional_len ] . expand ( args . num_samples , - [number] ) . to ( dev ) [EOL] timestamps = instance [ [string] ] [ : , : args . conditional_len ] . expand ( args . num_samples , - [number] ) . to ( dev ) [EOL] else : [EOL] src = None [EOL] timestamps = None [EOL] [EOL] logger . info ( [string] ) [EOL] sampler = Sampler ( decoder = model , embedding_type = config [ [string] ] . get ( [string] , [string] ) , temp = args . temperature , top_k = args . top_k , top_p = args . top_p , max_length = args . max_length ) [EOL] sampler . to ( dev ) [EOL] [EOL] samples = sampler ( src = src , timestamps = timestamps , batch_size = args . num_samples , dev = dev ) [EOL] [EOL] tokenizer = Tokenizer ( scale = args . scale ) [EOL] logger . info ( [string] . format ( samples_folder ) ) [EOL] for i , sample in enumerate ( samples ) : [EOL] tokens = [string] . join ( sample ) [EOL] with open ( samples_folder / [string] . format ( i ) , [string] ) as midi_file : [EOL] midi_file . write ( tokenizer . detokenize ( tokens ) ) [EOL] [EOL] if args . conditional is not None : [EOL] original_ids = instance [ [string] ] . squeeze ( ) . tolist ( ) [EOL] original_toks = [ IDX_TO_TOKEN [ idx ] for idx in original_ids ] [EOL] tokens = [string] . join ( original_toks ) [EOL] with open ( samples_folder / [string] , [string] ) as midi_file : [EOL] midi_file . write ( tokenizer . detokenize ( tokens ) ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import argparse [EOL] [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = [number] ) [EOL] parser . add_argument ( [string] , type = float , help = [string] , default = None ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = None ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = None ) [EOL] parser . add_argument ( [string] , type = float , help = [string] , default = None ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = [number] ) [EOL] args , _ = parser . parse_known_args ( ) [EOL] [EOL] if os . environ . get ( [string] ) : [EOL] level = logging . DEBUG [EOL] else : [EOL] level = logging . INFO [EOL] logging . basicConfig ( format = [string] , level = level ) [EOL] [EOL] _sample ( args ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0
from typing import List , Any , Dict [EOL] import argparse [EOL] import typing [EOL] import squawkbox [EOL] import logging [EOL] [docstring] [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] import shutil [EOL] import sys [EOL] [EOL] from apex import amp [EOL] import numpy as np [EOL] import torch [EOL] from torch . utils . data import DataLoader [EOL] from tqdm import tqdm [EOL] import yaml [EOL] [EOL] from squawkbox . data import MidiDataset , pad_and_combine_instances [EOL] from squawkbox . models import Model [EOL] from squawkbox . optim import Optimizer , LRScheduler [EOL] from squawkbox . transform import Transform [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def _train ( args ) : [EOL] [docstring] [EOL] [comment] [EOL] if not args . config . exists ( ) : [EOL] logger . error ( [string] ) [EOL] sys . exit ( [number] ) [EOL] [EOL] with open ( args . config , [string] ) as config_file : [EOL] config = yaml . load ( config_file , Loader = yaml . SafeLoader ) [EOL] [EOL] if args . output_dir . exists ( ) and not ( args . resume or args . force ) : [EOL] logger . error ( [string] % str ( args . output_dir ) ) [EOL] sys . exit ( [number] ) [EOL] else : [EOL] logger . info ( [string] , args . output_dir ) [EOL] if not args . output_dir . exists ( ) : [EOL] args . output_dir . mkdir ( ) [EOL] shutil . copy ( args . config , args . output_dir / [string] ) [EOL] [EOL] [comment] [EOL] fh = logging . FileHandler ( args . output_dir / [string] ) [EOL] logging . getLogger ( ) . addHandler ( fh ) [EOL] [EOL] torch . manual_seed ( config . get ( [string] , [number] ) ) [EOL] np . random . seed ( config . get ( [string] , [number] ) + [number] ) [EOL] [EOL] [comment] [EOL] model = Model . from_config ( config [ [string] ] ) [EOL] [EOL] optimizer = Optimizer . from_config ( config [ [string] ] , params = model . parameters ( ) ) [EOL] if [string] in config : [EOL] lr_scheduler = LRScheduler . from_config ( config [ [string] ] ) [EOL] else : [EOL] lr_scheduler = None [EOL] [EOL] if args . cuda : [EOL] logger . info ( [string] ) [EOL] if args . cuda_device is not None : [EOL] model = model . cuda ( args . cuda_device ) [EOL] else : [EOL] model = model . cuda ( ) [EOL] [EOL] if args . fp16 : [EOL] model , optimizer = amp . initialize ( model , optimizer , opt_level = [string] ) [EOL] [EOL] [comment] [EOL] checkpoint_path = args . output_dir / [string] [EOL] best_checkpoint_path = args . output_dir / [string] [EOL] if ( args . output_dir / [string] ) . exists ( ) and args . resume : [EOL] logger . info ( [string] ) [EOL] state_dict = torch . load ( checkpoint_path ) [EOL] start_epoch = state_dict [ [string] ] [EOL] best_metric = state_dict [ [string] ] [EOL] model . load_state_dict ( state_dict [ [string] ] ) [EOL] optimizer . load_state_dict ( state_dict [ [string] ] ) [EOL] if lr_scheduler is not None : [EOL] lr_scheduler . load_state_dict ( state_dict [ [string] ] ) [EOL] else : [EOL] logger . info ( [string] ) [EOL] start_epoch = [number] [EOL] best_metric = float ( [string] ) [EOL] [EOL] train_config = config [ [string] ] [EOL] [EOL] transforms = train_config . get ( [string] , [ ] ) [EOL] transforms = [ Transform . from_config ( x ) for x in transforms ] [EOL] embedding_type = train_config . get ( [string] , [string] ) [EOL] [EOL] train_dataset = MidiDataset ( config [ [string] ] , transforms = transforms , embedding_type = embedding_type ) [EOL] validation_dataset = MidiDataset ( config [ [string] ] , embedding_type = embedding_type ) [EOL] [EOL] step = [number] [EOL] for epoch in range ( start_epoch , train_config [ [string] ] ) : [EOL] logger . info ( [string] , epoch ) [EOL] [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] model . train ( ) [EOL] train_loader = DataLoader ( train_dataset , batch_size = train_config [ [string] ] , shuffle = True , num_workers = train_config . get ( [string] , [number] ) , collate_fn = pad_and_combine_instances ) [EOL] train_tqdm = tqdm ( train_loader , desc = [string] ) [EOL] optimizer . zero_grad ( ) [EOL] for instance in train_tqdm : [EOL] if args . cuda : [EOL] instance = { key : value . cuda ( args . cuda_device ) for key , value in instance . items ( ) } [EOL] [EOL] output_dict = model ( ** instance ) [EOL] loss = output_dict [ [string] ] [EOL] if args . fp16 : [EOL] with amp . scale_loss ( loss , optimizer ) as scaled_loss : [EOL] scaled_loss . backward ( ) [EOL] else : [EOL] loss . backward ( ) [EOL] [EOL] step += [number] [EOL] if not step % train_config . get ( [string] , [number] ) : [EOL] optimizer . step ( ) [EOL] optimizer . zero_grad ( ) [EOL] [EOL] train_tqdm . set_description ( [string] % loss . item ( ) ) [EOL] [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] model . eval ( ) [EOL] validation_loader = DataLoader ( validation_dataset , batch_size = train_config [ [string] ] , shuffle = False , num_workers = train_config . get ( [string] , [number] ) , collate_fn = pad_and_combine_instances ) [EOL] validation_tqdm = tqdm ( validation_loader , desc = [string] ) [EOL] total_validation_loss = [number] [EOL] n_tokens = [number] [EOL] for instance in validation_tqdm : [EOL] if args . cuda : [EOL] instance = { key : value . cuda ( args . cuda_device ) for key , value in instance . items ( ) } [EOL] [EOL] output_dict = model ( ** instance ) [EOL] loss = output_dict [ [string] ] [EOL] n_tokens_in_batch = instance [ [string] ] . ne ( [number] ) . sum ( ) . item ( ) [EOL] total_validation_loss += loss . item ( ) * n_tokens_in_batch [EOL] n_tokens += n_tokens_in_batch [EOL] validation_tqdm . set_description ( [string] % ( loss . item ( ) , total_validation_loss / n_tokens ) ) [EOL] metric = total_validation_loss / n_tokens [EOL] logger . info ( [string] , metric ) [EOL] [EOL] [comment] [EOL] model_state_dict = model . state_dict ( ) [EOL] state_dict = { [string] : model_state_dict , [string] : optimizer . state_dict ( ) , [string] : epoch , [string] : best_metric } [EOL] if lr_scheduler : [EOL] state_dict [ [string] ] = lr_scheduler . state_dict ( ) [EOL] [EOL] [comment] [EOL] if metric < best_metric : [EOL] logger . info ( [string] ) [EOL] state_dict [ [string] ] = metric [EOL] best_metric = metric [EOL] torch . save ( state_dict , best_checkpoint_path ) [EOL] [EOL] [comment] [EOL] torch . save ( state_dict , checkpoint_path ) [EOL] [EOL] if lr_scheduler is not None : [EOL] lr_scheduler . step ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import argparse [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , type = Path , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , help = [string] , default = None ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , action = [string] , help = [string] ) [EOL] args , _ = parser . parse_known_args ( ) [EOL] [EOL] if os . environ . get ( [string] ) : [EOL] level = logging . DEBUG [EOL] else : [EOL] level = logging . INFO [EOL] logging . basicConfig ( format = [string] , level = level ) [EOL] [EOL] _train ( args ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0
from typing import List , Any , Dict [EOL] import typing [EOL] import squawkbox [EOL] import torch [EOL] [EOL] from squawkbox . data import TOKEN_TO_IDX , MidiDataset , pad_and_combine_instances [EOL] [EOL] [EOL] def test_read_instance ( ) : [EOL] midi_dataset = MidiDataset ( [string] ) [EOL] [EOL] [comment] [EOL] assert len ( midi_dataset ) == [number] [EOL] [EOL] [comment] [EOL] instance = midi_dataset [ [number] ] [EOL] [EOL] [comment] [EOL] expected_src = [ [string] , [string] , [string] ] [EOL] expected_src_tensor = torch . LongTensor ( [ TOKEN_TO_IDX [ x ] for x in expected_src ] ) [EOL] assert torch . equal ( instance [ [string] ] [ : [number] ] , expected_src_tensor ) [EOL] [EOL] [comment] [EOL] expected_tgt = [ [string] , [string] , [string] ] [EOL] expected_tgt_tensor = torch . LongTensor ( [ TOKEN_TO_IDX [ x ] for x in expected_tgt ] ) [EOL] assert torch . equal ( instance [ [string] ] [ : [number] ] , expected_tgt_tensor ) [EOL] [EOL] [comment] [EOL] expected_timestamp = [ [number] , [number] , [number] , [number] ] [EOL] expected_timestamp_tensor = torch . FloatTensor ( expected_timestamp ) [EOL] assert torch . equal ( instance [ [string] ] [ : [number] ] , expected_timestamp_tensor ) [EOL] [EOL] [EOL] def test_pad_and_combine_instances ( ) : [EOL] batch = [ { [string] : torch . tensor ( [ [number] , [number] , [number] ] , dtype = torch . int64 ) , [string] : torch . tensor ( [number] , dtype = torch . float32 ) } , { [string] : torch . tensor ( [ [number] , [number] ] , dtype = torch . int64 ) , [string] : torch . tensor ( [number] , dtype = torch . float32 ) } ] [EOL] out_dict = pad_and_combine_instances ( batch ) [EOL] [EOL] [comment] [EOL] assert set ( out_dict . keys ( ) ) == { [string] , [string] } [EOL] [EOL] [comment] [EOL] expected_a = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , dtype = torch . int64 ) [EOL] assert torch . equal ( out_dict [ [string] ] , expected_a ) [EOL] [EOL] [comment] [EOL] expected_b = torch . tensor ( [ [number] , [number] ] , dtype = torch . float32 ) [EOL] assert torch . equal ( out_dict [ [string] ] , expected_b ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import squawkbox [EOL] import torch [EOL] from squawkbox . models . baseline import Baseline [EOL] [EOL] [EOL] def test_baseline ( ) : [EOL] baseline = Baseline ( vocab_size = [number] , embedding_dim = [number] , num_lstm_units = [number] , num_lstm_layers = [number] ) [EOL] src = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] ) [EOL] tgt = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] ) [EOL] out_dict = baseline ( src = src , tgt = tgt ) [EOL] out_dict [ [string] ] . backward ( ) [EOL] for p in baseline . parameters ( ) : [EOL] assert p . grad is not None [EOL] assert out_dict [ [string] ] [ [number] , [number] : [number] ] . abs ( ) . sum ( ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import argparse [EOL] import argparse [EOL] from pathlib import Path [EOL] import shutil [EOL] import tempfile [EOL] from unittest import TestCase [EOL] [EOL] import torch [EOL] [EOL] from squawkbox . commands . train import _train [EOL] from squawkbox . models import Model [EOL] [EOL] [EOL] @ Model . register ( [string] ) class NullModel ( Model ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] self . weight = torch . nn . Parameter ( torch . tensor ( [number] ) ) [EOL] [EOL] def forward ( * args , ** kwargs ) : [EOL] return { [string] : torch . tensor ( [number] , requires_grad = True ) , [string] : None } [EOL] [EOL] [EOL] class TestTrainCommand ( TestCase ) : [EOL] def setUp ( self ) : [EOL] self . tmp_dir = Path ( tempfile . gettempdir ( ) ) / [string] [EOL] self . args = argparse . Namespace ( ) [EOL] self . args . config = Path ( [string] ) [EOL] self . args . output_dir = self . tmp_dir [EOL] self . args . cuda = False [EOL] self . args . cuda_device = None [EOL] self . args . fp16 = False [EOL] self . args . resume = False [EOL] [EOL] def test_runs ( self ) : [EOL] _train ( self . args ) [EOL] [EOL] def tearDown ( self ) : [EOL] shutil . rmtree ( self . tmp_dir ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import squawkbox [EOL] import tempfile [EOL] [EOL] from squawkbox . midi import Midi [EOL] from squawkbox . tokenizer import Tokenizer [EOL] [EOL] [EOL] def test_tokenize_and_detokenize ( ) : [EOL] tokenizer = Tokenizer ( ) [EOL] with open ( [string] , [string] ) as midi_file : [EOL] midi = Midi . load ( midi_file ) [EOL] midi_file . seek ( [number] ) [EOL] _bytes = midi_file . read ( ) [EOL] with open ( [string] , [string] ) as token_file : [EOL] tokens = token_file . read ( ) [EOL] [comment] [EOL] assert tokenizer . detokenize ( tokens ) == _bytes [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import squawkbox [EOL] from math import pi [EOL] [EOL] import torch [EOL] [EOL] from squawkbox . modules . positional_embedding import PositionalEmbedding [EOL] [EOL] [EOL] def test_positional_embedding ( ) : [EOL] [comment] [EOL] postional_embedding = PositionalEmbedding ( dim = [number] ) [EOL] expected_freq = torch . tensor ( [ [ [ [number] , [number] / [number] ] ] ] ) [EOL] assert torch . equal ( postional_embedding . freq , expected_freq ) [EOL] [EOL] [comment] [EOL] timestamp = torch . tensor ( [ [ [number] , [number] * pi , [number] * [number] * pi ] ] ) [EOL] output = postional_embedding ( timestamp ) [EOL] _ , seq_len , dim = output . shape [EOL] assert seq_len == [number] [EOL] assert dim == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL] assert output [ [number] , [number] , [number] ] == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , Union [EOL] import typing [EOL] from unittest import TestCase [EOL] from squawkbox . utils import Registrable [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] class Mock ( Registrable ) : [EOL] pass [EOL] [EOL] @ Mock . register ( [string] ) class NoArgs ( Mock ) : [EOL] pass [EOL] [EOL] @ Mock . register ( [string] ) class OneArg ( Mock ) : [EOL] def __init__ ( self , arg ) : [EOL] self . arg = arg [EOL] [EOL] [EOL] class TestRegistrable ( TestCase ) : [EOL] [EOL] def test_get ( self ) : [EOL] mock_no_args = Mock . get ( [string] ) [EOL] self . assertEqual ( mock_no_args , NoArgs ) [EOL] [EOL] mock_one_arg = Mock . get ( [string] ) [EOL] self . assertEqual ( mock_one_arg , OneArg ) [EOL] [EOL] with self . assertRaises ( ValueError ) : [EOL] Mock . get ( [string] ) [EOL] [EOL] def test_from_config ( self ) : [EOL] no_args_config = { [string] : [string] } [EOL] no_args = Mock . from_config ( no_args_config ) [EOL] self . assertIsInstance ( no_args , NoArgs ) [EOL] [EOL] one_arg_config = { [string] : [string] , [string] : [number] } [EOL] one_arg = Mock . from_config ( one_arg_config ) [EOL] self . assertIsInstance ( one_arg , OneArg ) [EOL] self . assertEqual ( one_arg . arg , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
from collections import deque [EOL] import squawkbox [EOL] import collections [EOL] from collections import deque [EOL] import unittest [EOL] [EOL] import squawkbox . midi as midi [EOL] [EOL] [EOL] def test_parse_variable_length_quantity ( ) : [EOL] [comment] [EOL] byte_string = [string] [EOL] expected = [number] [EOL] byte_queue = deque ( byte_string ) [EOL] observed = midi . _parse_variable_length_quantity ( byte_queue ) [EOL] assert expected == observed [EOL] [EOL] [comment] [EOL] byte_string = [string] [EOL] expected = [number] [EOL] byte_queue = deque ( byte_string ) [EOL] observed = midi . _parse_variable_length_quantity ( byte_queue ) [EOL] assert expected == observed [EOL] [EOL] byte_string = [string] [EOL] expected = [number] [EOL] byte_queue = deque ( byte_string ) [EOL] observed = midi . _parse_variable_length_quantity ( byte_queue ) [EOL] assert expected == observed [EOL] [EOL] [EOL] def test_as_variable_length_quantity ( ) : [EOL] value = [number] [EOL] expected = [string] [EOL] observed = midi . _as_variable_length_quantity ( value ) [EOL] assert expected == observed [EOL] [EOL] value = [number] [EOL] expected = [string] [EOL] observed = midi . _as_variable_length_quantity ( value ) [EOL] assert expected == observed [EOL] [EOL] value = [number] [EOL] expected = [string] [EOL] observed = midi . _as_variable_length_quantity ( value ) [EOL] assert expected == observed [EOL] [EOL] [EOL] class TestMidiHeader ( unittest . TestCase ) : [EOL] def setUp ( self ) : [EOL] self . format_type = [string] [EOL] self . ntracks = [string] [EOL] self . tickdiv_type_0 = [string] [EOL] self . tickdiv_type_1 = [string] [EOL] [EOL] def test_from_bytes ( self ) : [EOL] [comment] [EOL] good_combination = self . format_type + self . ntracks + self . tickdiv_type_0 [EOL] midi_header = midi . MidiHeader . from_bytes ( good_combination ) [EOL] self . assertEqual ( midi_header . format_type , [number] ) [EOL] self . assertEqual ( midi_header . ntracks , [number] ) [EOL] self . assertEqual ( midi_header . pulses_per_quarter_note , [number] ) [EOL] [EOL] def test_unsupported ( self ) : [EOL] [comment] [EOL] bad_tickdiv_type = self . format_type + self . ntracks + self . tickdiv_type_1 [EOL] with self . assertRaises ( NotImplementedError ) : [EOL] midi_header = midi . MidiHeader . from_bytes ( bad_tickdiv_type ) [EOL] [EOL] [EOL] class TestMidiTrack ( unittest . TestCase ) : [EOL] def test_from_bytes ( self ) : [EOL] [comment] [EOL] invalid_prefix = [string] [EOL] with self . assertRaises ( midi . MidiError ) : [EOL] midi_track = midi . MidiTrack . from_bytes ( invalid_prefix ) [EOL] [EOL] [comment] [EOL] sysex_event = [string] [EOL] midi_track = midi . MidiTrack . from_bytes ( sysex_event ) [EOL] self . assertEqual ( len ( midi_track . events ) , [number] ) [EOL] [EOL] delta_time , event = midi_track . events [ [number] ] [EOL] self . assertEqual ( delta_time , [number] ) [EOL] self . assertIsInstance ( event , midi . SysexEvent ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [string] ) [EOL] [EOL] [comment] [EOL] meta_event = [string] [EOL] midi_track = midi . MidiTrack . from_bytes ( meta_event ) [EOL] self . assertEqual ( len ( midi_track . events ) , [number] ) [EOL] [EOL] _ , event = midi_track . events [ [number] ] [EOL] self . assertIsInstance ( event , midi . MetaEvent ) [EOL] self . assertEqual ( event . event_type , [string] ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [number] ) [EOL] [EOL] [comment] [EOL] midi_events = [string] [EOL] midi_track = midi . MidiTrack . from_bytes ( midi_events ) [EOL] self . assertEqual ( len ( midi_track . events ) , [number] ) [EOL] [EOL] _ , event = midi_track . events [ [number] ] [EOL] self . assertIsInstance ( event , midi . MidiEvent ) [EOL] self . assertEqual ( event . event_type , [string] ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [number] ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [number] ) [EOL] [EOL] _ , event = midi_track . events [ [number] ] [EOL] self . assertIsInstance ( event , midi . MidiEvent ) [EOL] self . assertEqual ( event . event_type , [string] ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [number] ) [EOL] self . assertEqual ( event . metadata [ [string] ] , [number] ) [EOL] [EOL] [EOL] class TestMidiFile ( unittest . TestCase ) : [EOL] def test_loads ( self ) : [EOL] [comment] [EOL] with open ( [string] , [string] ) as f : [EOL] midi . Midi . load ( f ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiHeader$ 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 $squawkbox.midi.MidiHeader$ 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiHeader$ 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiHeader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiHeader$ 0 0 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $squawkbox.midi.MidiTrack$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0