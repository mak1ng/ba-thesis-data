import numpy as np [EOL] [EOL] [EOL] def test_forward_propagation ( forward_propagation_model , forward_training_data ) : [EOL] forward_propagation_model . execute_forward_propagation ( forward_training_data [ [number] ] , forward_training_data [ [number] ] ) [EOL] assert ( np . max ( np . abs ( forward_propagation_model . layers [ [number] ] . cache . preactivation - np . array ( [ [ [number] , [number] , [number] ] , [ - [number] , - [number] , - [number] ] , [ [number] , [number] , [number] ] , [ - [number] , - [number] , - [number] ] , ] ) ) ) < [number] ) [EOL] [EOL] assert ( np . max ( np . abs ( forward_propagation_model . layers [ [number] ] . cache . activation - np . array ( [ [ [number] , [number] , [number] ] , [ - [number] , - [number] , - [number] ] , [ [number] , [number] , [number] ] , [ - [number] , - [number] , - [number] ] , ] ) ) ) < [number] ) [EOL] [EOL] assert ( np . max ( np . abs ( forward_propagation_model . layers [ [number] ] . cache . preactivation - np . array ( [ [ - [number] , - [number] , - [number] ] ] ) ) ) < [number] ) [EOL] [EOL] assert ( np . max ( np . abs ( forward_propagation_model . layers [ [number] ] . cache . activation - np . array ( [ [ [number] , [number] , [number] ] ] ) ) ) < [number] ) [EOL] [EOL] [EOL] def test_backward_propagation ( backward_propagation_model , backward_training_data ) : [EOL] [EOL] [comment] [EOL] backward_propagation_model . execute_forward_propagation ( backward_training_data [ [number] ] , backward_training_data [ [number] ] ) [EOL] backward_propagation_model . execute_backward_propagation ( backward_training_data [ [number] ] ) [EOL] assert ( np . max ( np . abs ( backward_propagation_model . layers [ [number] ] . cache . d_W - np . array ( [ [ [number] , - [number] ] , [ [number] , - [number] ] , [ - [number] , [number] ] , [ - [number] , [number] ] , ] ) ) ) < [number] ) [EOL] assert ( [number] < np . abs ( np . max ( backward_propagation_model . layers [ [number] ] . cache . d_b - np . array ( [ [ [number] ] , [ [number] ] , [ - [number] ] , [ - [number] ] ] ) ) ) < [number] ) [EOL] [EOL] assert ( np . max ( np . abs ( backward_propagation_model . layers [ - [number] ] . cache . d_W - np . array ( [ [ [number] , [number] , - [number] , - [number] ] ] ) ) ) < [number] ) [EOL] assert ( np . max ( np . abs ( backward_propagation_model . layers [ - [number] ] . cache . d_b - np . array ( [ [ - [number] ] ] ) ) ) < [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pystork [EOL] import math [EOL] import numpy as np [EOL] [EOL] from pystork . activations import Sigmoid [EOL] from pystork . costs . binary_classfication import BinaryClassificationCost [EOL] [EOL] [EOL] def test_binary_classification_cost ( ) : [EOL] [EOL] labels = np . array ( [ [ [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] binary_classification_cost = BinaryClassificationCost ( ) [EOL] [EOL] eps = math . pow ( [number] , - [number] ) [EOL] good_predictions = np . array ( [ [ [number] - eps , eps , [number] - eps , eps ] ] ) [EOL] bad_predictions = np . array ( [ [ eps , [number] - eps , eps , [number] - eps ] ] ) [EOL] avg_prediction = np . array ( [ [ [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] good_cost = binary_classification_cost . compute ( good_predictions , labels ) [EOL] bad_cost = binary_classification_cost . compute ( bad_predictions , labels ) [EOL] avg_cost = binary_classification_cost . compute ( avg_prediction , labels ) [EOL] [EOL] assert [number] < good_cost < math . pow ( [number] , - [number] ) [EOL] assert bad_cost > [number] [EOL] assert [number] < avg_cost < [number] [EOL] [EOL] [EOL] def test_binary_classification_derivative ( ) : [EOL] [EOL] labels = np . array ( [ [ [number] ] ] ) [EOL] [EOL] preactivation = np . array ( [ [ [number] ] ] ) [EOL] y_pred = Sigmoid ( ) . get_value ( preactivation ) [EOL] [EOL] binary_classification_cost = BinaryClassificationCost ( ) [EOL] derivative = binary_classification_cost . compute_preactivation_derivative ( y_pred , labels ) [EOL] [EOL] eps = math . pow ( [number] , - [number] ) [EOL] approximated_derivative = ( binary_classification_cost . compute ( Sigmoid ( ) . get_value ( preactivation + eps ) , labels ) - binary_classification_cost . compute ( Sigmoid ( ) . get_value ( preactivation - eps ) , labels ) ) / ( [number] * eps ) [EOL] [EOL] assert np . all ( derivative <= approximated_derivative + eps ) [EOL] assert np . all ( approximated_derivative - eps <= derivative ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from pystork . data_generators import threshold_data [EOL] [EOL] [EOL] def test_threshold_data ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( [number] , [number] ) [EOL] [EOL] assert X . shape == ( [number] , [number] ) [EOL] assert Y . shape == ( [number] , [number] ) [EOL] for i in range ( [number] ) : [EOL] if X [ [number] , i ] >= [number] : [EOL] assert Y [ [number] , i ] == [number] [EOL] else : [EOL] assert Y [ [number] , i ] == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pystork [EOL] import math [EOL] [EOL] import numpy as np [EOL] import pytest [EOL] [EOL] from pystork . activations import Sigmoid , Tanh , Relu [EOL] [EOL] [EOL] def test_sigmoid_value ( ) : [EOL] [EOL] x = np . array ( [ [number] , [number] , - [number] , [number] ] ) [EOL] assert np . all ( Sigmoid ( ) . get_value ( x ) == np . array ( [ [number] , [number] / ( [number] + math . exp ( - [number] ) ) , [number] / ( [number] + math . exp ( [number] ) ) , [number] / ( [number] + math . exp ( - [number] ) ) ] ) ) [EOL] [EOL] [EOL] def test_sigmoid_derivative ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] ] ) [EOL] [EOL] [comment] [EOL] eps = math . pow ( [number] , - [number] ) [EOL] [EOL] approximated_derivative = Sigmoid ( ) . get_approximate_derivative ( x ) [EOL] derivative = Sigmoid ( ) . get_derivative ( x ) [EOL] [EOL] assert np . all ( approximated_derivative - eps <= derivative ) and np . all ( derivative <= approximated_derivative + eps ) [EOL] [EOL] [EOL] def test_sigmoid_derivative_when_value_present ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] ] ) [EOL] [EOL] [comment] [EOL] eps = math . pow ( [number] , - [number] ) [EOL] value_at_x = Sigmoid ( ) . get_value ( x ) [EOL] approximated_derivative = Sigmoid ( ) . get_approximate_derivative ( x ) [EOL] derivative = Sigmoid ( ) . get_derivative ( x , value_at_x ) [EOL] [EOL] assert np . all ( approximated_derivative - eps <= derivative ) and np . all ( derivative <= approximated_derivative + eps ) [EOL] [EOL] [EOL] def test_tanh_value ( ) : [EOL] [EOL] x = np . array ( [ [number] , [number] , - [number] , [number] ] ) [EOL] assert ( np . max ( np . abs ( Tanh ( ) . get_value ( x ) - np . array ( [ [number] , ( math . exp ( [number] ) - math . exp ( - [number] ) ) / ( math . exp ( [number] ) + math . exp ( - [number] ) ) , ( math . exp ( - [number] ) - math . exp ( [number] ) ) / ( math . exp ( - [number] ) + math . exp ( [number] ) ) , ( math . exp ( [number] ) - math . exp ( - [number] ) ) / ( math . exp ( [number] ) + math . exp ( - [number] ) ) , ] ) ) ) < [number] ) [EOL] [EOL] [EOL] def test_tanh_derivative ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] ] ) [EOL] eps = math . pow ( [number] , - [number] ) [EOL] [EOL] approximate_derivative = Tanh ( ) . get_approximate_derivative ( x , eps ) [EOL] derivative = Tanh ( ) . get_derivative ( x ) [EOL] [EOL] assert np . all ( approximate_derivative - eps <= derivative ) and np . all ( derivative <= approximate_derivative + eps ) [EOL] [EOL] [EOL] def test_tanh_derivative_with_value ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] ] ) [EOL] eps = math . pow ( [number] , - [number] ) [EOL] value_at_x = Tanh ( ) . get_value ( x ) [EOL] approximate_derivative = Tanh ( ) . get_approximate_derivative ( x , eps ) [EOL] derivative = Tanh ( ) . get_derivative ( x , value_at_x ) [EOL] [EOL] assert np . all ( approximate_derivative - eps <= derivative ) and np . all ( derivative <= approximate_derivative + eps ) [EOL] [EOL] [EOL] def test_relu_value ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] , [ - [number] ] , [ [number] ] , [ - [number] ] , [ [number] ] ] ) [EOL] relu = Relu ( ) [EOL] value = relu . get_value ( x ) [EOL] assert np . all ( value == np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] ] ) ) [EOL] [EOL] [EOL] def test_relu_derivative ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] , [ - [number] ] , [ [number] ] , [ - [number] ] , [ [number] ] ] ) [EOL] relu = Relu ( ) [EOL] derivative = relu . get_derivative ( x ) [EOL] expected_derivative = np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] assert np . all ( derivative == expected_derivative ) [EOL] [EOL] [EOL] def test_relu_approximative_derivative ( ) : [EOL] [EOL] x = np . array ( [ [ [number] ] , [ - [number] ] , [ [number] ] , [ - [number] ] , [ [number] ] ] ) [EOL] with pytest . raises ( Exception ) : [EOL] Relu ( ) . get_approximate_derivative ( x ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import pystork [EOL] import numpy as np [EOL] from pystork . initializers import RandomInitializer , ZerosInitializer [EOL] [EOL] [EOL] def test_zero_initializer ( ) : [EOL] [EOL] initializer = ZerosInitializer ( ) [EOL] assert np . all ( initializer . get_values ( [number] , [number] ) == np . array ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) ) [EOL] [EOL] [EOL] def test_random_initializer ( ) : [EOL] initializer = RandomInitializer ( ) [EOL] assert initializer . get_values ( x_dim = [number] , y_dim = [number] ) . shape == ( [number] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pystork [EOL] import numpy as np [EOL] [EOL] from pystork . layer import Layer [EOL] [EOL] [EOL] def test_get_preactivation_with_save ( ) : [EOL] [EOL] layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = None ) [EOL] layer . set_parameters ( np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , np . array ( [ [ [number] ] , [ [number] ] ] ) ) [EOL] preactivation = layer . compute_preactivation ( np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) , save = True ) [EOL] [EOL] expected_preactivation = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] assert np . all ( expected_preactivation == preactivation ) [EOL] assert np . all ( expected_preactivation == layer . cache . preactivation ) [EOL] [EOL] [EOL] def test_get_preactivation_without_save ( ) : [EOL] [EOL] layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = None ) [EOL] layer . set_parameters ( np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , np . array ( [ [ [number] ] , [ [number] ] ] ) ) [EOL] preactivation = layer . compute_preactivation ( np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) , save = False ) [EOL] [EOL] expected_preactivation = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] assert np . all ( expected_preactivation == preactivation ) [EOL] assert layer . cache . preactivation is None [EOL] [EOL] [EOL] def test_execute_forward_propagation_with_save ( layer ) : [EOL] [EOL] x = np . array ( [ [ - [number] ] , [ [number] ] ] ) [EOL] activation = layer . execute_forward_propagation ( x , save = True ) [EOL] [EOL] expected_activation = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] assert np . all ( activation == expected_activation ) [EOL] assert np . all ( layer . cache . activation == expected_activation ) [EOL] assert np . all ( layer . cache . forward_vector == x ) [EOL] [EOL] [EOL] def test_execute_forward_propagation_without_save ( layer ) : [EOL] [EOL] x = np . array ( [ [ - [number] ] , [ [number] ] ] ) [EOL] activation = layer . execute_forward_propagation ( x , save = False ) [EOL] [EOL] expected_activation = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] assert np . all ( activation == expected_activation ) [EOL] assert layer . cache . activation is None [EOL] assert layer . cache . forward_vector is None [EOL] [EOL] [EOL] def test_execute_last_unit_backward_propagation ( layer ) : [EOL] [EOL] x = np . array ( [ [ - [number] , [number] ] , [ [number] , - [number] ] ] ) [EOL] layer . execute_forward_propagation ( x , save = True ) [EOL] [EOL] d_current_layer_preactivation = np . array ( [ [ [number] , [number] ] , [ [number] , - [number] ] ] ) [EOL] d_preactivation = layer . execute_last_unit_backward_propagation ( d_current_layer_preactivation ) [EOL] [EOL] expected_d_preactivation = d_current_layer_preactivation [EOL] expected_d_W = np . array ( [ [ [number] , - [number] ] , [ - [number] , [number] ] ] ) [EOL] expected_d_bias = np . array ( [ [ [number] ] , [ - [number] ] ] ) [EOL] [EOL] assert np . all ( expected_d_preactivation == d_preactivation ) [EOL] assert np . all ( layer . cache . d_W == expected_d_W ) [EOL] assert np . all ( layer . cache . d_b == expected_d_bias ) [EOL] [EOL] [EOL] def test_execute_hidden_backward_propagation ( layer ) : [EOL] [EOL] x = np . array ( [ [ - [number] ] , [ [number] ] ] ) [EOL] layer . execute_forward_propagation ( x , save = True ) [EOL] [EOL] [comment] [EOL] next_layer_d_preactivation = np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] next_layer_weights = np . array ( [ [ [number] , [number] ] , [ - [number] , [number] ] , [ - [number] , [number] ] ] ) [EOL] d_preactivation = layer . execute_backward_propagation ( next_layer_d_preactivation = next_layer_d_preactivation , next_layer_weights = next_layer_weights , ) [EOL] [EOL] expected_d_preactivation = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] expected_d_W = np . array ( [ [ [number] , [number] ] , [ - [number] , [number] ] ] ) [EOL] expected_d_bias = np . array ( [ [ [number] ] , [ [number] ] ] ) [EOL] [EOL] assert np . all ( d_preactivation == expected_d_preactivation ) [EOL] assert np . all ( layer . cache . d_W == expected_d_W ) [EOL] assert np . all ( layer . cache . d_b == expected_d_bias ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple [EOL] import typing [EOL] import pystork [EOL] import numpy [EOL] import unittest [EOL] [docstring] [EOL] [EOL] from typing import Tuple [EOL] from unittest . mock import MagicMock [EOL] [EOL] [comment] [EOL] import numpy as np [EOL] import pytest [EOL] [EOL] from pystork . activations import Relu , Tanh , Sigmoid [EOL] from pystork . costs . binary_classfication import BinaryClassificationCost [EOL] from pystork . initializers import RandomInitializer [EOL] from pystork . layer import Layer [EOL] from pystork . model import Model [EOL] [EOL] [EOL] @ pytest . fixture def layer ( ) : [EOL] [docstring] [EOL] layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Relu ( ) ) [EOL] layer . set_parameters ( np . array ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) , np . array ( [ [ [number] ] , [ [number] ] ] ) ) [EOL] return layer [EOL] [EOL] [EOL] @ pytest . fixture def forward_propagation_model ( ) : [EOL] [docstring] [EOL] [EOL] hidden_layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] output_layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] [EOL] model = Model ( layers = [ hidden_layer , output_layer ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] hidden_layer . W = np . array ( [ [ - [number] , - [number] ] , [ - [number] , [number] ] , [ - [number] , - [number] ] , [ [number] , - [number] ] , ] ) [EOL] hidden_layer . b = np . array ( [ [ [number] ] , [ - [number] ] , [ [number] ] , [ - [number] ] ] ) [EOL] output_layer . W = np . array ( [ - [number] , - [number] , [number] , [number] ] ) [EOL] output_layer . b = np . array ( [ [ - [number] ] ] ) [EOL] [EOL] return model [EOL] [EOL] [EOL] @ pytest . fixture def forward_training_data ( ) : [EOL] [docstring] [EOL] training_inputs = np . array ( [ [ [number] , - [number] , - [number] ] , [ - [number] , [number] , - [number] ] ] ) [EOL] [EOL] training_labels = np . array ( [ [ True , False , False ] ] ) [EOL] [EOL] return training_inputs , training_labels [EOL] [EOL] [EOL] @ pytest . fixture def backward_propagation_model ( ) : [EOL] [docstring] [EOL] [EOL] hidden_layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] output_layer = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] model = Model ( layers = [ hidden_layer , output_layer ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] hidden_layer . W = np . array ( [ [ - [number] , - [number] ] , [ - [number] , [number] ] , [ - [number] , - [number] ] , [ [number] , - [number] ] , ] ) [EOL] hidden_layer . b = np . array ( [ [ [number] ] , [ [number] ] , [ [number] ] , [ [number] ] ] ) [EOL] output_layer . W = np . array ( [ [ - [number] , - [number] , [number] , [number] ] ] ) [EOL] output_layer . b = np . array ( [ [ [number] ] ] ) [EOL] [EOL] return model [EOL] [EOL] [EOL] @ pytest . fixture def backward_training_data ( ) : [EOL] [docstring] [EOL] training_inputs = np . array ( [ [ [number] , - [number] , - [number] ] , [ - [number] , [number] , - [number] ] ] ) [EOL] [EOL] training_labels = np . array ( [ [ True , False , True ] ] ) [EOL] [EOL] return training_inputs , training_labels [EOL] [EOL] [EOL] @ pytest . fixture def mock_permutations ( mocker ) : [EOL] [EOL] return mocker . patch ( [string] , return_value = [ [number] , [number] , [number] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.MagicMock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] from pystork import helpers [EOL] [EOL] [EOL] [comment] [EOL] def test_data_shuffle ( mock_permutations ) : [EOL] [EOL] X = np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] Y = np . array ( [ [ - [number] , - [number] , - [number] ] ] ) [EOL] [EOL] shuffled_X , shuffled_Y = helpers . shuffle_data ( X , Y ) [EOL] [EOL] assert np . all ( shuffled_X == np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) ) [EOL] assert np . all ( shuffled_Y == np . array ( [ [ - [number] , - [number] , - [number] ] ] ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] import pystork [EOL] import numpy as np [EOL] [EOL] from pystork . activations import Tanh , Sigmoid [EOL] from pystork . costs . binary_classfication import BinaryClassificationCost [EOL] from pystork . data_generators import threshold_data [EOL] from pystork . initializers import RandomInitializer [EOL] from pystork . model import Layer , Model [EOL] from pystork . optimizers . gradient_descent import ( GradientDescent , MiniBatchGradientDescent , Adam ) [EOL] [EOL] [EOL] def test_get_minibatches ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] [EOL] mini_batch = MiniBatchGradientDescent ( iterations_number = [number] , learning_rate = [number] , mini_batch_size = [number] , print_cost = True ) [EOL] [comment] [EOL] mini_batches = mini_batch . _get_minibatches ( X , Y ) [EOL] [EOL] assert len ( mini_batches ) == [number] [EOL] for i in range ( [number] ) : [EOL] assert mini_batches [ i ] [ [number] ] . shape [ [number] ] == [number] [EOL] assert mini_batches [ i ] [ [number] ] . shape [ [number] ] == [number] [EOL] [EOL] assert mini_batches [ [number] ] [ [number] ] . shape [ [number] ] == [number] [EOL] assert mini_batches [ [number] ] [ [number] ] . shape [ [number] ] == [number] [EOL] [EOL] [EOL] def test_gradient_descent ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] [comment] [EOL] layer_1 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] layer_2 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] [EOL] model = Model ( layers = [ layer_1 , layer_2 ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] gradient_descent = GradientDescent ( iterations_number = [number] , learning_rate = [number] , print_cost = True ) [EOL] costs = gradient_descent . optimize_cost ( model , X , Y ) [EOL] assert costs [ - [number] ] <= [number] [EOL] [EOL] X_test , Y_test = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] Y_pred = model . predict ( X_test ) >= [number] [EOL] precision = np . sum ( np . abs ( Y_pred == Y_test ) ) / [number] [EOL] assert precision > [number] [EOL] [EOL] [EOL] def test_mini_batch_gradient_descent ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] [comment] [EOL] layer_1 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] layer_2 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] [EOL] model = Model ( layers = [ layer_1 , layer_2 ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] mini_batch = MiniBatchGradientDescent ( iterations_number = [number] , learning_rate = [number] , mini_batch_size = [number] , print_cost = True ) [EOL] costs = mini_batch . optimize_cost ( model , X , Y ) [EOL] assert costs [ - [number] ] <= [number] [EOL] [EOL] X_test , Y_test = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] Y_pred = model . predict ( X_test ) >= [number] [EOL] [EOL] precision = np . sum ( np . abs ( Y_pred == Y_test ) ) / [number] [EOL] assert precision > [number] [EOL] [EOL] [EOL] def test_gradient_descent_adam ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] [comment] [EOL] layer_1 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] layer_2 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] [EOL] model = Model ( layers = [ layer_1 , layer_2 ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] gradient_descent = Adam ( iterations_number = [number] , learning_rate = [number] , mini_batch_size = [number] , gradient_moment = [number] , square_gradient_moment = [number] , print_cost = True ) [EOL] costs = gradient_descent . optimize_cost ( model , X , Y ) [EOL] assert costs [ - [number] ] <= [number] [EOL] [EOL] X_test , Y_test = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] Y_pred = model . predict ( X_test ) >= [number] [EOL] precision = np . sum ( np . abs ( Y_pred == Y_test ) ) / [number] [EOL] assert precision > [number] [EOL] [EOL] [EOL] def test_mini_batch_gradient_descent_adam ( ) : [EOL] [EOL] X , Y = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] [comment] [EOL] layer_1 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Tanh ( ) ) [EOL] layer_2 = Layer ( units_number = [number] , inputs_number = [number] , activation_function = Sigmoid ( ) ) [EOL] [EOL] model = Model ( layers = [ layer_1 , layer_2 ] , cost_function = BinaryClassificationCost ( ) , initializer = RandomInitializer ( ) , ) [EOL] [EOL] mini_batch = Adam ( iterations_number = [number] , learning_rate = [number] , mini_batch_size = [number] , print_cost = True , gradient_moment = [number] , square_gradient_moment = [number] ) [EOL] costs = mini_batch . optimize_cost ( model , X , Y ) [EOL] assert costs [ - [number] ] <= [number] [EOL] [EOL] X_test , Y_test = threshold_data . generate_data ( samples_number = [number] , threshold = [number] ) [EOL] Y_pred = model . predict ( X_test ) >= [number] [EOL] [EOL] precision = np . sum ( np . abs ( Y_pred == Y_test ) ) / [number] [EOL] assert precision > [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , Union [EOL] import typing [EOL] import pystork [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Union [EOL] import numpy as np [EOL] [EOL] from pystork . activations import AbstractActivation [EOL] from pystork . initializers import AbstractInitializer , ZerosInitializer [EOL] from pystork . layer_cache import LayerCache [EOL] [EOL] [EOL] class Layer : [EOL] def __init__ ( self , units_number , inputs_number , activation_function , ) : [EOL] [EOL] self . units_number = units_number [EOL] self . inputs_number = inputs_number [EOL] self . activation_function = activation_function [EOL] [comment] [EOL] self . W = None [EOL] [comment] [EOL] self . b = None [EOL] [EOL] self . cache = LayerCache ( self . units_number , self . inputs_number ) [EOL] [EOL] def compute_preactivation ( self , x , save ) : [EOL] [docstring] [EOL] assert self . W is not None and self . b is not None [EOL] assert self . inputs_number == x . shape [ [number] ] [EOL] preactivation = np . dot ( self . W , x ) + self . b [EOL] if save : [EOL] self . cache . preactivation = preactivation [EOL] [comment] [EOL] assert self . cache . preactivation . shape == ( self . units_number , x . shape [ [number] ] ) [EOL] [EOL] return preactivation [EOL] [EOL] def execute_forward_propagation ( self , x , save = True ) : [EOL] [docstring] [EOL] [EOL] preactivation = self . compute_preactivation ( x , save ) [EOL] [EOL] activation = self . activation_function . get_value ( preactivation ) [EOL] if save : [EOL] self . cache . forward_vector = x [EOL] self . cache . activation = activation [EOL] [EOL] [comment] [EOL] assert activation . shape == ( self . units_number , x . shape [ [number] ] ) [EOL] return activation [EOL] [EOL] def execute_last_unit_backward_propagation ( self , current_layer_d_preactivation ) : [EOL] [docstring] [EOL] [EOL] self . cache . d_preactivation = current_layer_d_preactivation [EOL] self . get_parameters_derivatives ( ) [EOL] self . check_derivatives_dimensions ( ) [EOL] return self . cache . d_preactivation [EOL] [EOL] def execute_backward_propagation ( self , next_layer_d_preactivation , next_layer_weights ) : [EOL] [docstring] [EOL] [EOL] self . cache . d_preactivation = np . dot ( next_layer_weights . T , next_layer_d_preactivation ) * self . activation_function . get_derivative ( self . cache . preactivation , value_at_x = self . cache . activation ) [EOL] self . get_parameters_derivatives ( ) [EOL] [EOL] self . check_derivatives_dimensions ( ) [EOL] return self . cache . d_preactivation [EOL] [EOL] def get_parameters_derivatives ( self ) : [EOL] [EOL] labels_number = self . cache . forward_vector . shape [ [number] ] [EOL] self . cache . d_W = ( [number] / labels_number ) * np . dot ( self . cache . d_preactivation , self . cache . forward_vector . T ) [EOL] self . cache . d_b = ( [number] / labels_number ) * np . sum ( self . cache . d_preactivation , axis = [number] , keepdims = True ) [EOL] [EOL] def initialize ( self , initializer ) : [EOL] if self . W is None : [EOL] weights_initialization = initializer . get_values ( x_dim = self . units_number , y_dim = self . inputs_number ) [EOL] self . W = weights_initialization [EOL] [comment] [EOL] if self . b is None : [EOL] bias_initialization = ZerosInitializer ( ) . get_values ( x_dim = self . units_number , y_dim = [number] ) [EOL] self . b = bias_initialization [EOL] [EOL] [comment] [EOL] [EOL] def set_parameters ( self , new_W , new_b ) : [EOL] [EOL] self . W = new_W [EOL] self . b = new_b [EOL] [EOL] def check_derivatives_dimensions ( self ) : [EOL] [EOL] assert self . cache . d_preactivation . shape [ [number] ] == self . units_number [EOL] assert self . cache . d_W . shape == ( self . units_number , self . inputs_number ) [EOL] assert self . cache . d_b . shape == ( self . units_number , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Union [EOL] import typing [EOL] import numpy [EOL] import builtins [EOL] from abc import ABC , abstractmethod [EOL] from typing import Union [EOL] import numpy as np [EOL] [EOL] [EOL] class AbstractInitializer ( ABC ) : [EOL] @ abstractmethod def get_values ( self , x_dim , y_dim ) : [EOL] [docstring] [EOL] [EOL] [EOL] class RandomInitializer ( AbstractInitializer ) : [EOL] def __init__ ( self , reduction = [number] ) : [EOL] [docstring] [EOL] self . reduction = reduction [EOL] [EOL] def get_values ( self , x_dim , y_dim ) : [EOL] [EOL] return np . random . randn ( x_dim , y_dim ) [EOL] [EOL] [EOL] class ZerosInitializer ( AbstractInitializer ) : [EOL] def get_values ( self , x_dim , y_dim ) : [EOL] initialization = np . zeros ( ( x_dim , y_dim ) ) [EOL] [EOL] return initialization [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Tuple [EOL] import typing [EOL] import pystork [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import List , Tuple [EOL] [EOL] import numpy as np [EOL] [EOL] from pystork . layer import Layer [EOL] from pystork . costs . abstract_cost import AbstractCostFunction [EOL] from pystork . initializers import AbstractInitializer [EOL] [EOL] [EOL] class Model : [EOL] def __init__ ( self , layers , cost_function , initializer , ) : [EOL] [docstring] [EOL] [EOL] self . layers = layers [EOL] self . layers_number = len ( layers ) [EOL] self . normalization_vector = np . ones ( ( layers [ [number] ] . inputs_number , [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . _check_dimensions_compatibility ( ) [EOL] [EOL] self . cost = cost_function [EOL] self . last_cost_computation = float ( [string] ) [EOL] self . initializer = initializer [EOL] self . forward_propagation_result = None [EOL] [EOL] def normalize_inputs ( self , training_inputs ) : [EOL] [docstring] [EOL] self . normalization_vector = np . linalg . norm ( training_inputs , axis = [number] , keepdims = True ) [EOL] [EOL] return training_inputs / self . normalization_vector [EOL] [EOL] def _check_dimensions_compatibility ( self ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] if len ( self . layers ) > [number] : [EOL] for i in range ( len ( self . layers ) - [number] ) : [EOL] assert self . layers [ i ] . units_number == self . layers [ i + [number] ] . inputs_number [EOL] [EOL] def initialize_layers ( self ) : [EOL] for layer in self . layers : [EOL] layer . initialize ( self . initializer ) [EOL] [EOL] def execute_forward_propagation ( self , training_inputs , training_labels ) : [EOL] [docstring] [EOL] [EOL] for layer_number in range ( self . layers_number ) : [EOL] if layer_number > [number] : [EOL] training_inputs = self . layers [ layer_number - [number] ] . cache . activation [EOL] [EOL] current_layer = self . layers [ layer_number ] [EOL] current_layer . execute_forward_propagation ( training_inputs , save = True ) [EOL] self . last_cost_computation = self . cost . compute ( y_pred = self . layers [ - [number] ] . cache . activation , y_labels = training_labels ) [EOL] return self . forward_propagation_result , self . last_cost_computation [EOL] [EOL] def execute_backward_propagation ( self , training_labels ) : [EOL] [EOL] [comment] [EOL] assert self . layers [ - [number] ] . cache . activation is not None [EOL] [EOL] cost_function_derivative = self . cost . compute_preactivation_derivative ( self . layers [ - [number] ] . cache . activation , training_labels ) [EOL] [EOL] [comment] [EOL] last_layer = self . layers [ - [number] ] [EOL] last_layer . execute_last_unit_backward_propagation ( current_layer_d_preactivation = cost_function_derivative ) [EOL] [EOL] if self . layers_number > [number] : [EOL] for i in reversed ( range ( self . layers_number - [number] ) ) : [EOL] next_layer_d_preactivation = self . layers [ i + [number] ] . cache . d_preactivation [EOL] next_layer_weights = self . layers [ i + [number] ] . W [EOL] self . layers [ i ] . execute_backward_propagation ( next_layer_d_preactivation , next_layer_weights ) [EOL] [EOL] def predict ( self , x ) : [EOL] [EOL] assert x . shape [ [number] ] == self . layers [ [number] ] . inputs_number [EOL] [EOL] x = x / self . normalization_vector [EOL] for layer in self . layers : [EOL] x = layer . execute_forward_propagation ( x , save = False ) [EOL] return x [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Union [EOL] import typing [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] from typing import Union [EOL] import numpy as np [EOL] [EOL] [EOL] class LayerCache : [EOL] def __init__ ( self , units_number , inputs_number ) : [EOL] [EOL] self . preactivation = None [EOL] self . activation = None [EOL] [EOL] [comment] [EOL] self . d_preactivation = None [EOL] [comment] [EOL] self . d_W = None [EOL] self . d_b = None [EOL] [EOL] [comment] [EOL] self . forward_vector = None [EOL] [EOL] self . d_W_momentum = np . zeros ( ( units_number , inputs_number ) ) [EOL] self . d_b_momentum = np . zeros ( ( units_number , [number] ) ) [EOL] [EOL] self . d_W_square_momentum = np . zeros ( ( units_number , inputs_number ) ) [EOL] self . d_b_square_momentum = np . zeros ( ( units_number , [number] ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.array$ 0 0 0 0 0 $numpy.array$ 0 0 0 0 0 0 0 0 $numpy.array$ 0 0 0 0 0 0 0 $typing.Union[numpy.array,builtins.float]$ 0 0 0 0 0 $typing.Union[numpy.array,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
name = [string] [EOL]	$builtins.str$ 0 0 0
from typing import List , Any , Tuple [EOL] import typing [EOL] import numpy [EOL] from typing import Tuple [EOL] import numpy as np [EOL] [EOL] [EOL] def shuffle_data ( training_inputs , training_labels ) : [EOL] [docstring] [EOL] [EOL] assert training_inputs . shape [ [number] ] == training_labels . shape [ [number] ] [EOL] [EOL] permutation = list ( np . random . permutation ( training_inputs . shape [ [number] ] ) ) [EOL] shuffled_training_inputs = training_inputs [ : , permutation ] [EOL] shuffled_training_labels = training_labels [ : , permutation ] [EOL] return shuffled_training_inputs , shuffled_training_labels [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import numpy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from abc import ABC , abstractmethod [EOL] import math [EOL] [EOL] import numpy as np [EOL] [EOL] [EOL] class AbstractActivation ( ABC ) : [EOL] @ abstractmethod def get_value ( self , x ) : [EOL] [docstring] [EOL] [EOL] @ abstractmethod def get_derivative ( self , x , value_at_x = None ) : [EOL] [docstring] [EOL] [EOL] def get_approximate_derivative ( self , x , eps = math . pow ( [number] , - [number] ) ) : [EOL] [docstring] [EOL] return ( self . get_value ( x + eps ) - self . get_value ( x - eps ) ) / ( [number] * eps ) [EOL] [EOL] [EOL] class Sigmoid ( AbstractActivation ) : [EOL] def get_value ( self , x ) : [EOL] [EOL] return [number] / ( [number] + np . exp ( - x ) ) [EOL] [EOL] def get_derivative ( self , x , value_at_x = None ) : [EOL] [EOL] [comment] [EOL] if value_at_x is None : [EOL] sigmoid_value = self . get_value ( x ) [EOL] else : [EOL] sigmoid_value = value_at_x [EOL] [EOL] return sigmoid_value * ( [number] - sigmoid_value ) [EOL] [EOL] [EOL] class Relu ( AbstractActivation ) : [EOL] def get_value ( self , x ) : [EOL] [EOL] return np . maximum ( x , [number] ) [EOL] [EOL] def get_derivative ( self , x , value_at_x = None ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] return x >= [number] [EOL] [EOL] def get_approximate_derivative ( self , x , eps = math . pow ( [number] , - [number] ) ) : [EOL] [EOL] [comment] [EOL] raise Exception ( [string] ) [EOL] [EOL] [EOL] class Tanh ( AbstractActivation ) : [EOL] def get_value ( self , x ) : [EOL] [EOL] return np . tanh ( x ) [EOL] [EOL] def get_derivative ( self , x , value_at_x = None ) : [EOL] [EOL] [comment] [EOL] if value_at_x is None : [EOL] tanh_value = self . get_value ( x ) [EOL] else : [EOL] tanh_value = value_at_x [EOL] return [number] - np . power ( tanh_value , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , Tuple [EOL] import typing [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] from typing import Tuple [EOL] import numpy as np [EOL] [EOL] [EOL] def generate_data ( samples_number , threshold = [number] ) : [EOL] [docstring] [EOL] [EOL] X = np . random . randn ( [number] , samples_number ) [EOL] Y = X >= threshold [EOL] return X , Y [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import numpy [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from abc import ABC , abstractmethod [EOL] import numpy as np [EOL] [EOL] [EOL] class AbstractCostFunction ( ABC ) : [EOL] @ abstractmethod def compute ( self , y_pred , y_labels ) : [EOL] [docstring] [EOL] [EOL] @ abstractmethod def compute_preactivation_derivative ( self , y_pred , y_labels ) : [EOL] [docstring] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] import math [EOL] [EOL] import numpy as np [EOL] [EOL] from pystork . costs . abstract_cost import AbstractCostFunction [EOL] [EOL] [EOL] class BinaryClassificationCost ( AbstractCostFunction ) : [EOL] def compute ( self , y_pred , y_labels ) : [EOL] [EOL] [comment] [EOL] assert y_pred . shape == y_labels . shape [EOL] [comment] [EOL] assert np . all ( y_labels <= [number] ) and np . all ( y_labels >= [number] ) [EOL] assert np . all ( y_pred <= [number] ) and np . all ( y_pred >= [number] ) [EOL] [comment] [EOL] [comment] [EOL] epsilon = math . pow ( [number] , - [number] ) [EOL] y_pred = np . maximum ( np . minimum ( y_pred , [number] - epsilon ) , epsilon ) [EOL] labels_number = y_labels . shape [ [number] ] [EOL] cost = - ( [number] / labels_number ) * np . sum ( np . multiply ( y_labels , np . log ( y_pred ) ) + np . multiply ( [number] - y_labels , np . log ( [number] - y_pred ) ) ) [EOL] return np . squeeze ( cost ) [EOL] [EOL] def compute_preactivation_derivative ( self , y_pred , y_labels ) : [EOL] [docstring] [EOL] assert y_pred . shape == y_labels . shape [EOL] [EOL] return y_pred - y_labels [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any , Tuple [EOL] import typing [EOL] import pystork [EOL] import numpy [EOL] import builtins [EOL] [docstring] [EOL] import math [EOL] from typing import List , Tuple [EOL] [EOL] import numpy as np [EOL] [EOL] from pystork . model import Model [EOL] from pystork . layer import Layer [EOL] from pystork . optimizers . abstract_optimizer import AbstractOptimizer [EOL] from pystork import helpers [EOL] [EOL] [EOL] class MiniBatchGradientDescent ( AbstractOptimizer ) : [EOL] def __init__ ( self , iterations_number = [number] , mini_batch_size = [number] , learning_rate = [number] , print_cost = True , ) : [EOL] [EOL] self . iterations_number = iterations_number [EOL] self . learning_rate = learning_rate [EOL] self . print_cost = print_cost [EOL] self . mini_batch_size = mini_batch_size [EOL] [EOL] def optimize_cost ( self , model , training_inputs , training_labels ) : [EOL] [EOL] self . check_dimensions ( model , training_inputs , training_labels ) [EOL] model . initialize_layers ( ) [EOL] training_inputs = model . normalize_inputs ( training_inputs ) [EOL] shuffled_training_inputs , shuffled_training_labels = helpers . shuffle_data ( training_inputs , training_labels ) [EOL] [EOL] costs = [ ] [EOL] [EOL] for i in range ( self . iterations_number ) : [EOL] self . _execute_one_iteration ( model , shuffled_training_inputs , shuffled_training_labels ) [EOL] cost = model . last_cost_computation [EOL] costs . append ( cost ) [EOL] [EOL] for layer in model . layers : [EOL] new_W = layer . W - self . learning_rate * layer . cache . d_W [EOL] new_b = layer . b - self . learning_rate * layer . cache . d_b [EOL] layer . set_parameters ( new_W , new_b ) [EOL] [EOL] if i % [number] == [number] and self . print_cost : [EOL] print ( [string] . format ( i , cost ) ) [EOL] if self . print_cost : [EOL] print ( [string] . format ( costs [ - [number] ] ) ) [EOL] return costs [EOL] [EOL] def _get_minibatches ( self , training_inputs , training_labels ) : [EOL] [docstring] [EOL] complete_mini_batches_number = math . floor ( training_inputs . shape [ [number] ] / self . mini_batch_size ) [EOL] mini_batches = [ ] [EOL] [EOL] for k in range ( complete_mini_batches_number ) : [EOL] [EOL] mini_batch_indexes = range ( k * self . mini_batch_size , ( k + [number] ) * self . mini_batch_size ) [EOL] mini_batch_X = training_inputs [ : , mini_batch_indexes ] [EOL] mini_batch_Y = training_labels [ : , mini_batch_indexes ] [EOL] [EOL] mini_batches . append ( ( mini_batch_X , mini_batch_Y ) ) [EOL] [EOL] if training_inputs . shape [ [number] ] % complete_mini_batches_number != [number] : [EOL] mini_batch_indexes = range ( complete_mini_batches_number * self . mini_batch_size , training_inputs . shape [ [number] ] , ) [EOL] mini_batch_X = training_inputs [ : , mini_batch_indexes ] [EOL] mini_batch_Y = training_labels [ : , mini_batch_indexes ] [EOL] mini_batches . append ( ( mini_batch_X , mini_batch_Y ) ) [EOL] [EOL] return mini_batches [EOL] [EOL] def _execute_one_iteration ( self , model , training_inputs , training_labels ) : [EOL] [EOL] for ( mini_batch_X , mini_batch_Y ) in self . _get_minibatches ( training_inputs , training_labels ) : [EOL] [EOL] model . execute_forward_propagation ( mini_batch_X , mini_batch_Y ) [EOL] model . execute_backward_propagation ( training_labels = mini_batch_Y ) [EOL] [EOL] [EOL] class GradientDescent ( MiniBatchGradientDescent ) : [EOL] def __init__ ( self , iterations_number = [number] , learning_rate = [number] , print_cost = True , ) : [EOL] [comment] [EOL] super ( ) . __init__ ( iterations_number = iterations_number , mini_batch_size = [number] , learning_rate = learning_rate , print_cost = print_cost , ) [EOL] [EOL] def optimize_cost ( self , model , training_inputs , training_labels ) : [EOL] self . mini_batch_size = training_inputs . shape [ [number] ] [EOL] [EOL] return super ( ) . optimize_cost ( model , training_inputs , training_labels ) [EOL] [EOL] [EOL] class Adam ( AbstractOptimizer ) : [EOL] def __init__ ( self , iterations_number = [number] , mini_batch_size = [number] , learning_rate = [number] , gradient_moment = [number] , square_gradient_moment = [number] , print_cost = True , ) : [EOL] [EOL] self . iterations_number = iterations_number [EOL] self . mini_batch_size = mini_batch_size [EOL] self . learning_rate = learning_rate [EOL] [EOL] assert gradient_moment < [number] [EOL] assert square_gradient_moment < [number] [EOL] self . gradient_moment = gradient_moment [EOL] self . square_gradient_moment = square_gradient_moment [EOL] self . print_cost = print_cost [EOL] [EOL] def _get_minibatches ( self , training_inputs , training_labels ) : [EOL] [docstring] [EOL] complete_mini_batches_number = math . floor ( training_inputs . shape [ [number] ] / self . mini_batch_size ) [EOL] mini_batches = [ ] [EOL] [EOL] for k in range ( complete_mini_batches_number ) : [EOL] [EOL] mini_batch_indexes = range ( k * self . mini_batch_size , ( k + [number] ) * self . mini_batch_size ) [EOL] mini_batch_X = training_inputs [ : , mini_batch_indexes ] [EOL] mini_batch_Y = training_labels [ : , mini_batch_indexes ] [EOL] [EOL] mini_batches . append ( ( mini_batch_X , mini_batch_Y ) ) [EOL] [EOL] if training_inputs . shape [ [number] ] % complete_mini_batches_number != [number] : [EOL] mini_batch_indexes = range ( complete_mini_batches_number * self . mini_batch_size , training_inputs . shape [ [number] ] , ) [EOL] mini_batch_X = training_inputs [ : , mini_batch_indexes ] [EOL] mini_batch_Y = training_labels [ : , mini_batch_indexes ] [EOL] mini_batches . append ( ( mini_batch_X , mini_batch_Y ) ) [EOL] [EOL] return mini_batches [EOL] [EOL] def optimize_cost ( self , model , training_inputs , training_labels ) : [EOL] [EOL] self . check_dimensions ( model , training_inputs , training_labels ) [EOL] model . initialize_layers ( ) [EOL] training_inputs = model . normalize_inputs ( training_inputs ) [EOL] shuffled_training_inputs , shuffled_training_labels = helpers . shuffle_data ( training_inputs , training_labels ) [EOL] [EOL] costs = [ ] [EOL] [EOL] for iteration_number in range ( self . iterations_number ) : [EOL] self . _execute_one_iteration ( model , shuffled_training_inputs , shuffled_training_labels ) [EOL] cost = model . last_cost_computation [EOL] costs . append ( cost ) [EOL] [EOL] for layer in model . layers : [EOL] self . _update_layer_parameters ( layer , iteration_number + [number] ) [EOL] [EOL] if iteration_number % [number] == [number] and self . print_cost : [EOL] print ( [string] . format ( iteration_number , cost ) ) [EOL] if self . print_cost : [EOL] print ( [string] . format ( costs [ - [number] ] ) ) [EOL] return costs [EOL] [EOL] def _update_layer_parameters ( self , layer , iteration_number ) : [EOL] [EOL] layer . cache . d_W_momentum = ( self . gradient_moment * layer . cache . d_W_momentum + ( [number] - self . gradient_moment ) * layer . cache . d_W ) [EOL] layer . cache . d_b_momentum = ( self . gradient_moment * layer . cache . d_b_momentum + ( [number] - self . gradient_moment ) * layer . cache . d_b ) [EOL] [EOL] layer . cache . d_W_square_momentum = self . square_gradient_moment * layer . cache . d_W_square_momentum + ( [number] - self . square_gradient_moment ) * np . power ( layer . cache . d_W , [number] ) [EOL] [EOL] layer . cache . d_b_square_momentum = self . square_gradient_moment * layer . cache . d_b_square_momentum + ( [number] - self . square_gradient_moment ) * np . power ( layer . cache . d_b , [number] ) [EOL] [EOL] gradient_momentum_decay = [number] / ( [number] - math . pow ( self . gradient_moment , iteration_number ) ) [EOL] if self . square_gradient_moment > [number] : [EOL] square_d_W_momentum_decay = ( [number] - math . pow ( self . square_gradient_moment , iteration_number ) ) / ( np . power ( layer . cache . d_W_square_momentum , [number] ) + math . pow ( [number] , - [number] ) ) [EOL] square_d_b_momentum_decay = ( [number] - math . pow ( self . square_gradient_moment , iteration_number ) ) / ( np . power ( layer . cache . d_b_square_momentum , [number] ) + math . pow ( [number] , - [number] ) ) [EOL] else : [EOL] square_d_W_momentum_decay = [number] [EOL] square_d_b_momentum_decay = [number] [EOL] [EOL] d_W_approximation = gradient_momentum_decay * square_d_W_momentum_decay * layer . cache . d_W_momentum [EOL] d_b_approximation = gradient_momentum_decay * square_d_b_momentum_decay * layer . cache . d_b_momentum [EOL] [EOL] new_W = layer . W - self . learning_rate * d_W_approximation [EOL] new_b = layer . b - self . learning_rate * d_b_approximation [EOL] layer . set_parameters ( new_W , new_b ) [EOL] [EOL] def _execute_one_iteration ( self , model , training_inputs , training_labels ) : [EOL] [EOL] for ( mini_batch_X , mini_batch_Y ) in self . _get_minibatches ( training_inputs , training_labels ) : [EOL] [EOL] model . execute_forward_propagation ( mini_batch_X , mini_batch_Y ) [EOL] model . execute_backward_propagation ( training_labels = mini_batch_Y ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
import pystork [EOL] import numpy [EOL] [docstring] [EOL] [EOL] from abc import ABC , abstractmethod [EOL] import numpy as np [EOL] from pystork . model import Model [EOL] [EOL] [EOL] class AbstractOptimizer ( ABC ) : [EOL] @ abstractmethod def optimize_cost ( self , model , training_inputs , training_labels ) : [EOL] [docstring] [EOL] [EOL] @ staticmethod def check_dimensions ( model , training_inputs , training_labels ) : [EOL] [docstring] [EOL] assert training_inputs . shape [ [number] ] == training_labels . shape [ [number] ] [EOL] assert training_labels . shape [ [number] ] == model . layers [ - [number] ] . units_number [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0