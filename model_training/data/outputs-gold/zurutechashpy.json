[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Tuple , Union , Dict , Any , Type , List [EOL] import pathlib [EOL] import typing [EOL] import tests [EOL] [docstring] [EOL] [EOL] import operator [EOL] import os [EOL] import shutil [EOL] from pathlib import Path [EOL] [EOL] import pytest [EOL] import tensorflow [comment] [EOL] [EOL] import ashpy [EOL] from ashpy . metrics import ( ClassifierLoss , InceptionScore , SlicedWassersteinDistance , SSIM_Multiscale , ) [EOL] from tests . utils . fake_training_loop import ( FakeAdversarialTraining , FakeClassifierTraining , ) [EOL] [EOL] [EOL] @ pytest . fixture ( autouse = True ) def add_common_namespaces ( doctest_namespace ) : [EOL] [docstring] [EOL] doctest_namespace [ [string] ] = tensorflow [EOL] doctest_namespace [ [string] ] = ashpy . trainers [EOL] doctest_namespace [ [string] ] = ashpy . models [EOL] doctest_namespace [ [string] ] = ashpy . metrics [EOL] doctest_namespace [ [string] ] = ashpy . layers [EOL] doctest_namespace [ [string] ] = ashpy . losses [EOL] doctest_namespace [ [string] ] = ashpy . callbacks [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def save_dir ( ) : [EOL] [docstring] [EOL] m_save_dir = Path ( [string] ) [EOL] [EOL] [comment] [EOL] if m_save_dir . exists ( ) : [EOL] shutil . rmtree ( m_save_dir ) [EOL] assert not m_save_dir . exists ( ) [EOL] [EOL] yield m_save_dir [EOL] [EOL] [comment] [EOL] if m_save_dir . exists ( ) : [EOL] shutil . rmtree ( m_save_dir ) [EOL] assert not m_save_dir . exists ( ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] TEST_MATRIX = { [string] : [ FakeAdversarialTraining , { [string] : [ [number] , [number] ] , [string] : ( [number] , [number] ) , [string] : ( [number] , [number] ) , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [ ashpy . callbacks . LogImageGANCallback ( event = ashpy . callbacks . Event . ON_BATCH_END , event_freq = [number] ) ] , } , ( SlicedWassersteinDistance ( resolution = [number] ) , SSIM_Multiscale ( ) , InceptionScore ( ashpy . models . gans . ConvDiscriminator ( layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = ( [number] , [number] ) , initial_filters = [number] , filters_cap = [number] , output_shape = [number] , ) ) , ) , ] , [string] : [ FakeClassifierTraining , { [string] : [number] } , ( ClassifierLoss ( model_selection_operator = operator . lt ) , ) , ] , } [EOL] [EOL] TRAINING_IDS = [ k for k in TEST_MATRIX ] [EOL] LOOPS = [ TEST_MATRIX [ k ] for k in TEST_MATRIX ] [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] , params = LOOPS , ids = TRAINING_IDS ) def fake_training_fn ( request ) : [EOL] [docstring] [EOL] training_loop , loop_args , metrics = request . param [EOL] assert len ( metrics ) in [ [number] , [number] ] [EOL] [EOL] return lambda logdir , ** kwargs : training_loop ( logdir = logdir , metrics = metrics , ** loop_args , ** kwargs ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Dict , List , Any [EOL] import typing [EOL] [docstring] [EOL] import re [EOL] [EOL] from setuptools import find_packages , setup [EOL] [EOL] [comment] [EOL] INIT_PY = open ( [string] ) . read ( ) [EOL] METADATA = dict ( re . findall ( [string] , INIT_PY ) ) [EOL] [EOL] [comment] [EOL] README = open ( [string] ) . read ( ) [EOL] [EOL] [comment] [EOL] INSTALL_REQUIREMENTS = [ [string] , [string] ] [EOL] [EOL] setup ( author_email = METADATA [ [string] ] , author = METADATA [ [string] ] , classifiers = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] , description = ( [string] [string] ) , include_package_data = True , install_requires = INSTALL_REQUIREMENTS , keywords = [ [string] , [string] , [string] , [string] , [string] ] , license = [string] , long_description_content_type = [string] , long_description = README , name = [string] , package_dir = { [string] : [string] } , packages = find_packages ( where = [string] ) , url = METADATA [ [string] ] , version = METADATA [ [string] ] , zip_safe = False , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import pathlib [EOL] import typing [EOL] import tests [EOL] import ashpy [EOL] [docstring] [EOL] [EOL] from pathlib import Path [EOL] from typing import List [EOL] [EOL] import ashpy [EOL] import pytest [EOL] import tensorflow as tf [EOL] from ashpy . trainers import AdversarialTrainer , ClassifierTrainer [EOL] [EOL] from tests . test_restorers import ModelNotConstructedError , _check_models_weights [EOL] from tests . utils . fake_training_loop import ( FakeAdversarialTraining , FakeClassifierTraining , FakeTraining , ) [EOL] [EOL] [EOL] def test_correct_trainer_restoration_on_restart ( fake_training_fn , tmpdir ) : [EOL] logdir = Path ( tmpdir ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] [EOL] if isinstance ( fake_training , FakeClassifierTraining ) : [EOL] assert isinstance ( fake_training . trainer , ClassifierTrainer ) [EOL] trained_model = fake_training . trainer . _model [EOL] [EOL] new_training = fake_training_fn ( logdir = logdir ) [EOL] new_training . trainer . _build_and_restore_models ( new_training . dataset ) [EOL] restored_model = new_training . trainer . _model [EOL] [EOL] _check_models_weights ( trained_model , restored_model ) [EOL] [EOL] if isinstance ( fake_training , FakeAdversarialTraining ) : [EOL] assert isinstance ( fake_training . trainer , AdversarialTrainer ) [EOL] trained_g = fake_training . trainer . _generator [EOL] trained_d = fake_training . trainer . _discriminator [EOL] [EOL] new_training = fake_training_fn ( logdir = logdir ) [EOL] new_training . trainer . _build_and_restore_models ( new_training . dataset ) [EOL] restored_g = new_training . trainer . _generator [EOL] restored_d = new_training . trainer . _discriminator [EOL] _check_models_weights ( trained_g , restored_g ) [EOL] _check_models_weights ( trained_d , restored_d ) [EOL] [EOL] [EOL] def test_generate_human_ckpt_dict ( fake_training_fn , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] [EOL] trainer = fake_training . trainer [EOL] [EOL] assert trainer . _checkpoint_map [EOL] assert ( Path ( trainer . _ckpts_dir ) / [string] ) . exists ( ) [EOL] metrics = trainer . _metrics [EOL] for metric in metrics : [EOL] assert metric . best_folder / [string] / [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL]	0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import tests [EOL] import pathlib [EOL] import tensorflow [EOL] import ashpy [EOL] [docstring] [EOL] from pathlib import Path [EOL] from typing import Union [EOL] [EOL] import pytest [EOL] import tensorflow as tf [EOL] from ashpy . callbacks import CounterCallback [EOL] from ashpy . restorers import ( AdversarialRestorer , ClassifierRestorer , ModelNotConstructedError , Restorer , ) [EOL] from ashpy . trainers import AdversarialTrainer , ClassifierTrainer [EOL] [EOL] from tests . utils . fake_training_loop import ( FakeAdversarialTraining , FakeClassifierTraining , ) [EOL] [EOL] DEFAULT_CKPT_DIR = [string] [EOL] [EOL] [EOL] def _check_models_weights ( trained , restored , i = [number] ) : [EOL] [docstring] [EOL] [EOL] try : [EOL] for i , element in enumerate ( trained . weights ) : [EOL] assert tf . reduce_all ( tf . equal ( element , restored . weights [ i ] ) ) [EOL] except AssertionError : [EOL] raise ModelNotConstructedError [EOL] [EOL] [EOL] def _test_restore_object ( restorer , placeholder , ckpt_id , capsys ) : [EOL] [docstring] [EOL] restorer . restore_object ( placeholder , ckpt_id ) [EOL] _check_log ( restorer , ckpt_id , capsys ) [EOL] [EOL] [EOL] def _check_log ( restorer , ckpt_id , capsys ) : [EOL] [docstring] [EOL] out , _ = capsys . readouterr ( ) [EOL] [comment] [EOL] assert restorer . _restored_log_msg . format ( ckpt_id , restorer . _ckpts_dir ) in out . split ( [string] ) [EOL] [EOL] [EOL] def test_restore_model ( fake_training_fn , capsys , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] _tmp_logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] restorer = Restorer ( logdir = logdir ) [EOL] [EOL] if isinstance ( trainer , ClassifierTrainer ) : [EOL] [EOL] new_loop = fake_training_fn ( logdir = _tmp_logdir ) [EOL] placeholder = new_loop . model [EOL] [EOL] [comment] [EOL] x , _ = next ( iter ( new_loop . dataset ) ) [EOL] placeholder ( x ) [EOL] [EOL] ckpt_id = trainer . ckpt_id_model [EOL] _test_restore_object ( restorer , placeholder , ckpt_id , capsys ) [EOL] _check_models_weights ( trainer . _model , placeholder ) [EOL] [EOL] elif isinstance ( trainer , AdversarialTrainer ) : [EOL] [EOL] new_loop = fake_training_fn ( logdir = _tmp_logdir ) [EOL] placeholder_g , placeholder_d = ( new_loop . generator , new_loop . discriminator ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ModelNotConstructedError ) : [EOL] _test_restore_object ( restorer , placeholder_g , trainer . ckpt_id_generator , capsys ) [EOL] _test_restore_object ( restorer , placeholder_d , trainer . ckpt_id_discriminator , capsys ) [EOL] [EOL] [comment] [EOL] ( x , _ ) , z = next ( iter ( new_loop . dataset ) ) [EOL] fake = placeholder_g ( z ) [EOL] assert tf . reduce_all ( tf . equal ( fake . shape , x . shape ) ) [EOL] placeholder_d ( x ) [EOL] [EOL] _test_restore_object ( restorer , placeholder_g , trainer . ckpt_id_generator , capsys ) [EOL] _check_models_weights ( trainer . _generator , placeholder_g ) [EOL] _test_restore_object ( restorer , placeholder_d , trainer . ckpt_id_discriminator , capsys ) [EOL] _check_models_weights ( trainer . _discriminator , placeholder_d ) [EOL] [EOL] [EOL] def test_restore_common_variables ( fake_training_fn , capsys , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] restorer = Restorer ( logdir = logdir ) [EOL] [EOL] [comment] [EOL] assert tf . equal ( trainer . _global_step , restorer . get_global_step ( ) ) [EOL] assert tf . equal ( trainer . _steps_per_epoch , restorer . get_steps_per_epoch ( ) ) [EOL] [EOL] out , _ = capsys . readouterr ( ) [EOL] [EOL] [comment] [EOL] for id_to_check in [ trainer . ckpt_id_global_step , trainer . ckpt_id_steps_per_epoch , ] : [EOL] [comment] [EOL] assert restorer . _restored_log_msg . format ( id_to_check , restorer . _ckpts_dir ) in out . split ( [string] ) [EOL] [EOL] [EOL] def test_restore_callbacks ( fake_training_fn , capsys , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] restorer = Restorer ( logdir = logdir ) [EOL] [EOL] if isinstance ( trainer , AdversarialTrainer ) : [EOL] placeholder_callbacks = fake_training . callbacks [EOL] for i , placeholder_callback in enumerate ( placeholder_callbacks ) : [EOL] [comment] [EOL] restorer . restore_callback ( placeholder_callback , placeholder_callback . name ) [EOL] [comment] [EOL] _check_log ( restorer , placeholder_callback . name , capsys ) [EOL] [comment] [EOL] if isinstance ( placeholder_callback , CounterCallback ) : [EOL] assert tf . equal ( placeholder_callbacks [ i ] . _event_counter , trainer . _callbacks [ i ] . _event_counter , ) [EOL] [EOL] [EOL] def test_read_checkpoint_map ( fake_training_fn , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] restorer = Restorer ( logdir = logdir ) [EOL] assert restorer . checkpoint_map == trainer . _generate_checkpoint_map ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] ckpt_map = restorer . _ckpts_dir / [string] [EOL] ckpt_map . unlink ( ) [EOL] assert not ckpt_map . exists ( ) [EOL] assert not Restorer ( logdir ) . checkpoint_map [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def _test_convenience_model_restorer ( restorer , convenience_method , placeholder_model , trained_model , ckpt_id , capsys , ) : [EOL] convenience_method ( placeholder_model ) [EOL] _check_log ( restorer , ckpt_id , capsys ) [EOL] _check_models_weights ( trained_model , placeholder_model ) [EOL] [EOL] [EOL] def _test_convenience_optimizer_restorer ( restorer , convenience_method , placeholder_optimizer , ckpt_id , capsys ) : [EOL] [docstring] [EOL] convenience_method ( placeholder_optimizer ) [EOL] _check_log ( restorer , ckpt_id , capsys ) [EOL] [EOL] [EOL] def test_convenience_restorer ( fake_training_fn , capsys , tmpdir ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] _tmp_logdir = Path ( tmpdir ) . joinpath ( [string] ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] restorer = Restorer ( logdir = logdir ) [EOL] [EOL] if isinstance ( trainer , ClassifierTrainer ) : [EOL] restorer = ClassifierRestorer ( logdir = logdir ) [EOL] [EOL] new_training = fake_training_fn ( _tmp_logdir ) [EOL] [EOL] placeholder_model = new_training . model [EOL] placeholder_opt = tf . keras . optimizers . Adam ( ) [EOL] [EOL] [comment] [EOL] x , _ = next ( iter ( new_training . dataset ) ) [EOL] placeholder_model ( x ) [EOL] [EOL] _test_convenience_model_restorer ( restorer , restorer . restore_model , placeholder_model , trainer . _model , trainer . ckpt_id_model , capsys , ) [EOL] _test_convenience_optimizer_restorer ( restorer , restorer . restore_optimizer , placeholder_opt , trainer . ckpt_id_optimizer , capsys , ) [EOL] [EOL] elif isinstance ( trainer , AdversarialTrainer ) : [EOL] restorer = AdversarialRestorer ( logdir = logdir ) [EOL] [EOL] new_training = fake_training_fn ( _tmp_logdir ) [EOL] [EOL] placeholder_g , placeholder_d = ( new_training . generator , new_training . discriminator , ) [EOL] placeholder_optimizer_g , placeholder_optimizer_d = ( tf . keras . optimizers . Adam ( ) , tf . keras . optimizers . Adam ( ) , ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ModelNotConstructedError ) : [EOL] _test_convenience_model_restorer ( restorer , restorer . restore_generator , placeholder_g , trainer . _generator , trainer . ckpt_id_generator , capsys , ) [EOL] [EOL] with pytest . raises ( ModelNotConstructedError ) : [EOL] _test_convenience_model_restorer ( restorer , restorer . restore_discriminator , placeholder_d , trainer . _discriminator , trainer . ckpt_id_discriminator , capsys , ) [EOL] [EOL] [comment] [EOL] ( x , _ ) , z = next ( iter ( new_training . dataset ) ) [EOL] fake = placeholder_g ( z ) [EOL] assert tf . reduce_all ( tf . equal ( fake . shape , x . shape ) ) [EOL] placeholder_d ( x ) [EOL] [EOL] _test_convenience_model_restorer ( restorer , restorer . restore_generator , placeholder_g , trainer . _generator , trainer . ckpt_id_generator , capsys , ) [EOL] _test_convenience_optimizer_restorer ( restorer , restorer . restore_generator_optimizer , placeholder_optimizer_g , trainer . ckpt_id_optimizer_generator , capsys , ) [EOL] _test_convenience_model_restorer ( restorer , restorer . restore_discriminator , placeholder_d , trainer . _discriminator , trainer . ckpt_id_discriminator , capsys , ) [EOL] _test_convenience_optimizer_restorer ( restorer , restorer . restore_discriminator_optimizer , placeholder_optimizer_d , trainer . ckpt_id_optimizer_discriminator , capsys , ) [EOL] [EOL] [EOL] def test_failings ( tmpdir ) : [EOL] [docstring] [EOL] [comment] [EOL] with pytest . raises ( FileNotFoundError ) : [EOL] Restorer ( Path ( tmpdir + ( [string] ) ) ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( FileNotFoundError ) : [EOL] restorer = Restorer ( tmpdir ) [EOL] restorer . _restore_checkpoint ( tf . train . Checkpoint ( ) ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( TypeError ) : [EOL] Restorer . _validate_placeholder ( placeholders = tf . keras . Model ( ) , placeholder_type = tf . Variable , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] import builtins [EOL] from typing import Callable , Union , Dict , Any , List [EOL] import pathlib [EOL] import operator [EOL] import typing [EOL] import tests [EOL] [docstring] [EOL] import json [EOL] import operator [EOL] import shutil [EOL] from pathlib import Path [EOL] [EOL] import pytest [EOL] import tensorflow as tf [EOL] from ashpy . metrics import ClassifierLoss , Metric [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining , FakeTraining [EOL] [EOL] DEFAULT_LOGDIR = [string] [EOL] OPERATOR_INITIAL_VALUE_MAP = { operator . gt : [string] , operator . lt : [string] } [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def cleanup ( ) : [EOL] [docstring] [EOL] default_log_dir = Path ( DEFAULT_LOGDIR ) [EOL] if default_log_dir . exists ( ) : [EOL] shutil . rmtree ( default_log_dir ) [EOL] yield [string] [EOL] if default_log_dir . exists ( ) : [EOL] shutil . rmtree ( default_log_dir ) [EOL] [EOL] [EOL] def get_metric_data ( metric , tmpdir ) : [EOL] [docstring] [EOL] metric_dir = Path ( tmpdir ) / [string] / metric . sanitized_name [EOL] assert metric_dir . exists ( ) [EOL] json_path = metric_dir / f"{ metric . sanitized_name } [string] " [EOL] assert json_path . exists ( ) [EOL] with open ( json_path , [string] ) as fp : [EOL] metric_data = json . load ( fp ) [EOL] [EOL] [comment] [EOL] assert metric . sanitized_name in metric_data [EOL] assert [string] in metric_data [EOL] return metric_data [ metric . sanitized_name ] [EOL] [EOL] [EOL] def test_metrics_log ( fake_training_fn , tmpdir , cleanup ) : [EOL] [docstring] [EOL] logdir = Path ( tmpdir ) [EOL] [EOL] fake_training = fake_training_fn ( logdir = logdir ) [EOL] assert fake_training ( ) [EOL] trainer = fake_training . trainer [EOL] [EOL] assert not Path ( DEFAULT_LOGDIR ) . exists ( ) [comment] [EOL] [comment] [EOL] for metric in trainer . _metrics : [EOL] [EOL] metric_value = get_metric_data ( metric , tmpdir ) [EOL] [comment] [EOL] [comment] [EOL] if metric . model_selection_operator : [EOL] try : [EOL] initial_value = OPERATOR_INITIAL_VALUE_MAP [ metric . model_selection_operator ] [EOL] except KeyError : [EOL] raise ValueError ( [string] [string] ) [EOL] [EOL] assert metric_value != initial_value [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def test_metrics_names_collision ( tmpdir ) : [EOL] [docstring] [EOL] metrics = [ ClassifierLoss ( name = [string] ) , ClassifierLoss ( name = [string] ) , ] [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] [comment] [EOL] [comment] [EOL] FakeAdversarialTraining ( tmpdir , metrics = metrics ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def test_metrics_on_restart ( fake_training_fn , tmpdir ) : [EOL] [docstring] [EOL] [EOL] fake_training = fake_training_fn ( tmpdir ) [EOL] assert fake_training ( ) [EOL] [EOL] t1_values = { metric . name : get_metric_data ( metric , tmpdir ) for metric in fake_training . trainer . _metrics } [EOL] print ( t1_values ) [EOL] [EOL] restart = fake_training_fn ( tmpdir ) [EOL] [EOL] t2_values = { metric . name : get_metric_data ( metric , tmpdir ) for metric in restart . trainer . _metrics } [EOL] print ( t2_values ) [EOL] assert t1_values == t2_values [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def test_metric_precision ( fake_training_fn , tmpdir , capsys ) : [EOL] [docstring] [EOL] [EOL] class FakeMetric ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , name = [string] , model_selection_operator = operator . gt ) : [EOL] super ( ) . __init__ ( name = name , model_selection_operator = model_selection_operator , metric = tf . metrics . Mean ( name = name , dtype = tf . float32 ) , ) [EOL] self . fake_score = ( tf . divide ( tf . exp ( tf . random . normal ( ( [number] , ) ) ) , ( tf . add ( tf . exp ( tf . random . normal ( ( [number] , ) ) ) , tf . exp ( tf . random . normal ( ( [number] , ) , [number] ) ) , ) ) , ) / [number] ) . numpy ( ) [ [number] ] [EOL] print ( [string] , self . fake_score ) [EOL] [EOL] def update_state ( self , context ) : [EOL] updater = lambda value : lambda : self . _metric . update_state ( value ) [EOL] self . _distribute_strategy . experimental_run_v2 ( updater ( self . fake_score ) ) [EOL] [EOL] fake_training = fake_training_fn ( tmpdir ) [EOL] fake_training . metrics = ( * fake_training . metrics , FakeMetric ( ) ) [EOL] fake_training . epochs = [number] [EOL] fake_training . build_trainer ( ) [EOL] assert fake_training ( ) [EOL] out , _ = capsys . readouterr ( ) [EOL] assert out . count ( [string] ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Union[typing.Callable[[typing.Any,typing.Any],typing.Any],typing.Callable[[typing.Any,typing.Any],typing.Any]],builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Union[typing.Callable[[typing.Any,typing.Any],typing.Any],typing.Callable[[typing.Any,typing.Any],typing.Any]],builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import ashpy [EOL] [docstring] [EOL] [EOL] import pytest [EOL] from ashpy . losses . gan import ( AdversarialLossType , get_adversarial_loss_discriminator , get_adversarial_loss_generator , ) [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , list ( AdversarialLossType ) ) def test_losses ( loss_type , tmpdir ) : [EOL] [docstring] [EOL] [comment] [EOL] generator_loss = get_adversarial_loss_generator ( loss_type ) ( ) [EOL] discriminator_loss = get_adversarial_loss_discriminator ( loss_type ) ( ) [EOL] [EOL] FakeAdversarialTraining ( tmpdir , generator_loss = generator_loss , discriminator_loss = discriminator_loss , ) ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List [EOL] import typing [EOL] import builtins [EOL] import tests [EOL] import ashpy [EOL] [docstring] [EOL] [EOL] import pytest [EOL] from ashpy . callbacks import Callback [EOL] from ashpy . callbacks . events import Event [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining [EOL] [EOL] [EOL] class MCallback ( Callback ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , event ) : [EOL] [docstring] [EOL] super ( MCallback , self ) . __init__ ( name = [string] ) [EOL] self . _event = event [EOL] self . counter = [number] [EOL] [EOL] def on_event ( self , event , context ) : [EOL] [docstring] [EOL] if event == self . _event : [EOL] self . counter += [number] [EOL] [EOL] [EOL] def get_n_events_from_epochs ( event , epochs , dataset_size , batch_size ) : [EOL] [docstring] [EOL] if event in [ Event . ON_TRAIN_START , Event . ON_TRAIN_END ] : [EOL] return [number] [EOL] if event in [ Event . ON_EPOCH_START , Event . ON_EPOCH_END ] : [EOL] return epochs [EOL] if event in [ Event . ON_BATCH_START , Event . ON_BATCH_END ] : [EOL] return ( dataset_size / batch_size ) * epochs [EOL] if event in [ Event . ON_EXCEPTION ] : [EOL] return [number] [EOL] raise ValueError ( [string] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , list ( Event ) ) def test_custom_callbacks ( tmpdir , event ) : [EOL] [docstring] [EOL] m_callback = MCallback ( event ) [EOL] callbacks = [ m_callback ] [EOL] [EOL] epochs = [number] [EOL] dataset_size = [number] [EOL] batch_size = [number] [EOL] [EOL] FakeAdversarialTraining ( logdir = tmpdir , callbacks = callbacks , epochs = epochs , dataset_size = dataset_size , batch_size = batch_size , ) ( ) [EOL] [EOL] [comment] [EOL] assert m_callback . counter == get_n_events_from_epochs ( event , epochs , dataset_size , batch_size ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List , Literal , Tuple [EOL] import typing [EOL] import builtins [EOL] import typing_extensions [EOL] import pathlib [EOL] import ashpy [EOL] [docstring] [EOL] import os [EOL] from pathlib import Path [EOL] from typing import Tuple [EOL] [EOL] import pytest [EOL] from ashpy . callbacks import SaveCallback , SaveFormat , SaveSubFormat [EOL] from ashpy . models . gans import ConvDiscriminator , ConvGenerator [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining [EOL] [EOL] COMPATIBLE_FORMAT_AND_SUB_FORMAT = [ ( SaveFormat . WEIGHTS , SaveSubFormat . TF ) , ( SaveFormat . WEIGHTS , SaveSubFormat . H5 ) , ( SaveFormat . MODEL , SaveSubFormat . TF ) , ] [EOL] [EOL] INCOMPATIBLE_FORMAT_AND_SUB_FORMAT = [ ( SaveFormat . MODEL , SaveSubFormat . H5 ) ] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , COMPATIBLE_FORMAT_AND_SUB_FORMAT ) def test_save_callback_compatible ( tmpdir , save_format_and_sub_format , save_dir , ) : [EOL] [docstring] [EOL] save_format , save_sub_format = save_format_and_sub_format [EOL] _test_save_callback_helper ( tmpdir , save_format , save_sub_format , save_dir ) [EOL] [EOL] save_dirs = os . listdir ( save_dir ) [EOL] [comment] [EOL] assert len ( save_dirs ) == [number] [EOL] [EOL] for model_dir in save_dirs : [EOL] assert save_format . name ( ) in [ x . split ( os . path . sep ) [ - [number] ] for x in os . listdir ( save_dir / model_dir ) ] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , INCOMPATIBLE_FORMAT_AND_SUB_FORMAT ) def test_save_callback_incompatible ( tmpdir , save_format_and_sub_format , save_dir , ) : [EOL] [docstring] [EOL] save_format , save_sub_format = save_format_and_sub_format [EOL] [EOL] with pytest . raises ( NotImplementedError ) : [EOL] _test_save_callback_helper ( tmpdir , save_format , save_sub_format , save_dir ) [EOL] [EOL] [comment] [EOL] assert not save_dir . exists ( ) [EOL] [EOL] [EOL] def _test_save_callback_helper ( tmpdir , save_format , save_sub_format , save_dir ) : [EOL] image_resolution = ( [number] , [number] ) [EOL] layer_spec_input_res = ( [number] , [number] ) [EOL] layer_spec_target_res = ( [number] , [number] ) [EOL] kernel_size = [number] [EOL] channels = [number] [EOL] [EOL] [comment] [EOL] generator = ConvGenerator ( layer_spec_input_res = layer_spec_input_res , layer_spec_target_res = image_resolution , kernel_size = kernel_size , initial_filters = [number] , filters_cap = [number] , channels = channels , ) [EOL] [EOL] discriminator = ConvDiscriminator ( layer_spec_input_res = image_resolution , layer_spec_target_res = layer_spec_target_res , kernel_size = kernel_size , initial_filters = [number] , filters_cap = [number] , output_shape = [number] , ) [EOL] [EOL] callbacks = [ SaveCallback ( models = [ generator , discriminator ] , save_dir = save_dir , verbose = [number] , save_format = save_format , save_sub_format = save_sub_format , ) ] [EOL] [EOL] FakeAdversarialTraining ( tmpdir , callbacks = callbacks , generator = generator , discriminator = discriminator , ) ( ) [EOL] [EOL] [EOL] def test_save_callback_type_error ( save_dir , ) : [EOL] [docstring] [EOL] with pytest . raises ( TypeError ) : [EOL] SaveCallback ( models = [ ] , save_dir = save_dir , verbose = [number] , save_format = [string] , save_sub_format = SaveSubFormat . TF , ) [EOL] [EOL] with pytest . raises ( TypeError ) : [EOL] SaveCallback ( models = [ ] , save_dir = save_dir , verbose = [number] , save_format = SaveFormat . WEIGHTS , save_sub_format = [string] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL]	0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List , Literal , Tuple [EOL] import typing [EOL] import tests [EOL] import typing_extensions [EOL] [docstring] [EOL] [EOL] import pytest [EOL] from ashpy . callbacks import CounterCallback , Event [EOL] from ashpy . models . gans import ConvDiscriminator , ConvGenerator [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining [EOL] [EOL] [EOL] class FakeCounterCallback ( CounterCallback ) : [EOL] def __init__ ( self , * args , ** kwargs ) : [EOL] super ( ) . __init__ ( * args , ** kwargs ) [EOL] self . fake_counter = [number] [EOL] [EOL] def on_event ( self , event , context ) : [EOL] if event == self . _event : [EOL] self . fake_counter += [number] [EOL] super ( ) . on_event ( event , context ) [EOL] [EOL] [EOL] @ pytest . fixture ( ) def _models ( ) : [EOL] image_resolution = ( [number] , [number] ) [EOL] layer_spec_input_res = ( [number] , [number] ) [EOL] layer_spec_target_res = ( [number] , [number] ) [EOL] kernel_size = [number] [EOL] channels = [number] [EOL] [EOL] [comment] [EOL] generator = ConvGenerator ( layer_spec_input_res = layer_spec_input_res , layer_spec_target_res = image_resolution , kernel_size = kernel_size , initial_filters = [number] , filters_cap = [number] , channels = channels , ) [EOL] [EOL] discriminator = ConvDiscriminator ( layer_spec_input_res = image_resolution , layer_spec_target_res = layer_spec_target_res , kernel_size = kernel_size , initial_filters = [number] , filters_cap = [number] , output_shape = [number] , ) [EOL] [EOL] return generator , discriminator [EOL] [EOL] [EOL] def test_counter_callback_multiple_events ( ) : [EOL] [docstring] [EOL] with pytest . raises ( TypeError ) : [EOL] FakeCounterCallback ( event = [ Event . ON_EPOCH_END ] , name = [string] , fn = lambda context : print ( [string] ) , ) [EOL] [EOL] [EOL] [comment] [EOL] def test_counter_callback ( _models , tmpdir ) : [EOL] clbk = FakeCounterCallback ( event = Event . ON_EPOCH_END , name = [string] , fn = lambda context : print ( [string] ) , ) [EOL] callbacks = [ clbk ] [EOL] generator , discriminator = _models [EOL] FakeAdversarialTraining ( logdir = tmpdir , callbacks = callbacks , generator = generator , discriminator = discriminator , epochs = [number] , ) ( ) [EOL] assert clbk . fake_counter == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] from ashpy . callbacks import LogImageGANCallback [EOL] from ashpy . callbacks . events import Event [EOL] [EOL] from tests . utils . fake_training_loop import FakeAdversarialTraining [EOL] [EOL] [EOL] def test_callbacks ( tmpdir ) : [EOL] [docstring] [EOL] callbacks = [ LogImageGANCallback ( event = Event . ON_BATCH_END , event_freq = [number] ) ] [EOL] FakeAdversarialTraining ( tmpdir , callbacks = callbacks ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any , Union [EOL] import pathlib [EOL] import typing [EOL] import builtins [EOL] import tensorflow [EOL] [docstring] [EOL] import operator [EOL] from pathlib import Path [EOL] from typing import List , Tuple , Union [EOL] [EOL] import ashpy [EOL] import tensorflow as tf [EOL] from ashpy . losses import DiscriminatorMinMax , GeneratorBCE [EOL] from ashpy . models . gans import ConvDiscriminator , ConvGenerator [EOL] from ashpy . trainers import AdversarialTrainer , ClassifierTrainer , Trainer [EOL] [EOL] from tests . utils . fake_datasets import ( fake_adversarial_dataset , fake_autoencoder_datasest , ) [EOL] from tests . utils . fake_models import basic_dcgan , conv_autoencoder [EOL] [EOL] __ALL__ = [ [string] , [string] , [string] ] [EOL] [EOL] [EOL] class FakeTraining : [EOL] dataset = ... [EOL] trainer = ... [EOL] measure_performance_freq = ... [EOL] metrics = ... [EOL] [EOL] [EOL] class FakeClassifierTraining ( FakeTraining ) : [EOL] def __init__ ( self , logdir = [string] , optimizer = tf . optimizers . Adam ( [number] ) , metrics = [ ashpy . metrics . ClassifierLoss ( model_selection_operator = operator . lt ) ] , epochs = [number] , dataset_size = [number] , image_resolution = ( [number] , [number] ) , batch_size = [number] , layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = [number] , initial_filters = [number] , filters_cap = [number] , encoding_dimension = [number] , channels = [number] , measure_performance_freq = [number] , ) : [EOL] [docstring] [EOL] self . logdir = logdir [EOL] self . epochs = epochs [EOL] self . measure_performance_freq = measure_performance_freq [EOL] [EOL] self . optimizer = optimizer [EOL] [EOL] self . metrics = metrics [EOL] [EOL] [comment] [EOL] self . model = conv_autoencoder ( layer_spec_input_res , layer_spec_target_res , kernel_size , initial_filters , filters_cap , encoding_dimension , channels , ) [EOL] [EOL] [comment] [EOL] self . dataset = fake_autoencoder_datasest ( dataset_size , image_resolution , channels , batch_size ) [EOL] [EOL] [comment] [EOL] self . reconstruction_error = ashpy . losses . ClassifierLoss ( tf . keras . losses . MeanSquaredError ( ) ) [EOL] [EOL] [comment] [EOL] self . trainer = ... [EOL] self . build_trainer ( ) [EOL] [EOL] def build_trainer ( self ) : [EOL] self . trainer = ClassifierTrainer ( model = self . model , optimizer = self . optimizer , loss = self . reconstruction_error , logdir = str ( self . logdir ) , epochs = self . epochs , metrics = self . metrics , ) [EOL] [EOL] def __call__ ( self ) : [EOL] self . trainer ( self . dataset , self . dataset , measure_performance_freq = self . measure_performance_freq , ) [EOL] return True [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] class FakeAdversarialTraining ( FakeTraining ) : [EOL] def __init__ ( self , logdir = [string] , kernel_size = ( [number] , [number] ) , metrics = None , callbacks = None , epochs = [number] , dataset_size = [number] , batch_size = [number] , generator_loss = GeneratorBCE ( ) , discriminator_loss = DiscriminatorMinMax ( ) , image_resolution = ( [number] , [number] ) , layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , channels = [number] , output_shape = [number] , latent_dim = [number] , measure_performance_freq = [number] , generator = None , discriminator = None , ) : [EOL] [docstring] [EOL] self . generator_loss = generator_loss [EOL] self . discriminator_loss = discriminator_loss [EOL] self . epochs = epochs [EOL] self . logdir = logdir [EOL] [EOL] self . measure_performance_freq = measure_performance_freq [EOL] [EOL] [comment] [EOL] if callbacks is None : [EOL] callbacks = [ ] [EOL] if metrics is None : [EOL] metrics = [ ] [EOL] [EOL] self . metrics = metrics [EOL] self . callbacks = callbacks [EOL] [EOL] [comment] [EOL] models = basic_dcgan ( image_resolution = image_resolution , layer_spec_input_res = layer_spec_input_res , layer_spec_target_res = layer_spec_target_res , kernel_size = kernel_size , channels = channels , output_shape = output_shape , ) [EOL] if not generator : [EOL] generator = models [ [number] ] [EOL] if not discriminator : [EOL] discriminator = models [ [number] ] [EOL] [EOL] self . generator = generator [EOL] self . discriminator = discriminator [EOL] [EOL] [comment] [EOL] self . trainer = ... [EOL] self . build_trainer ( ) [EOL] [EOL] self . dataset = fake_adversarial_dataset ( image_resolution = image_resolution , epochs = epochs , dataset_size = dataset_size , batch_size = batch_size , latent_dim = latent_dim , channels = channels , ) [EOL] [EOL] def __call__ ( self ) : [EOL] self . trainer ( self . dataset , measure_performance_freq = self . measure_performance_freq ) [EOL] return True [EOL] [EOL] def build_trainer ( self ) : [EOL] self . trainer = AdversarialTrainer ( generator = self . generator , discriminator = self . discriminator , generator_optimizer = tf . optimizers . Adam ( [number] ) , discriminator_optimizer = tf . optimizers . Adam ( [number] ) , generator_loss = self . generator_loss , discriminator_loss = self . discriminator_loss , epochs = self . epochs , metrics = self . metrics , callbacks = self . callbacks , logdir = self . logdir , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tensorflow.data.Dataset$ 0 0 0 $ashpy.trainers.Trainer$ 0 0 0 $builtins.int$ 0 0 0 $typing.Union[typing.List[ashpy.metrics.Metric],typing.Tuple[ashpy.metrics.Metric]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[pathlib.Path,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[pathlib.Path,builtins.str]$ 0 $typing.Union[pathlib.Path,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tensorflow.keras.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ashpy.trainers.ClassifierTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[pathlib.Path,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[pathlib.Path,builtins.str]$ 0 $typing.Union[pathlib.Path,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $ashpy.trainers.AdversarialTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL]	0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] import tensorflow as tf [EOL] [EOL] __ALL__ = [ [string] , [string] ] [EOL] [EOL] [EOL] def fake_autoencoder_datasest ( dataset_size = [number] , image_resolution = ( [number] , [number] ) , channels = [number] , batch_size = [number] , ** kwargs , ) : [EOL] [docstring] [EOL] inputs , labels = ( tf . zeros ( ( dataset_size , image_resolution [ [number] ] , image_resolution [ [number] ] , channels ) ) , tf . zeros ( ( dataset_size , image_resolution [ [number] ] , image_resolution [ [number] ] , channels ) ) , ) [EOL] dataset = ( tf . data . Dataset . from_tensor_slices ( ( inputs , labels ) ) . take ( dataset_size ) . batch ( batch_size ) . prefetch ( [number] ) ) [EOL] [EOL] return dataset [EOL] [EOL] [EOL] def fake_adversarial_dataset ( image_resolution = ( [number] , [number] ) , epochs = [number] , dataset_size = [number] , batch_size = [number] , latent_dim = [number] , channels = [number] , ** kwargs , ) : [EOL] [comment] [EOL] data_x , data_y = ( tf . zeros ( ( dataset_size , image_resolution [ [number] ] , image_resolution [ [number] ] , channels ) ) , tf . zeros ( ( dataset_size , [number] ) ) , ) [EOL] [comment] [EOL] [comment] [EOL] real_data = ( tf . data . Dataset . from_tensor_slices ( ( data_x , data_y ) ) . take ( dataset_size ) . batch ( batch_size ) . prefetch ( [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] dataset = real_data . map ( lambda x , y : ( ( x , y ) , tf . random . normal ( shape = ( batch_size , latent_dim ) ) ) ) [EOL] return dataset [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] import tensorflow [EOL] import tensorflow as tf [EOL] from ashpy . models . convolutional . autoencoders import Autoencoder [EOL] from ashpy . models . gans import ConvDiscriminator , ConvGenerator [EOL] [EOL] __ALL__ = [ [string] , [string] ] [EOL] [EOL] [EOL] def conv_autoencoder ( layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = [number] , initial_filters = [number] , filters_cap = [number] , encoding_dimension = [number] , channels = [number] , ** kwargs , ) : [EOL] [docstring] [EOL] autoencoder = Autoencoder ( layer_spec_input_res = layer_spec_input_res , layer_spec_target_res = layer_spec_target_res , kernel_size = kernel_size , initial_filters = initial_filters , filters_cap = filters_cap , encoding_dimension = encoding_dimension , channels = channels , ) [EOL] [comment] [EOL] inputs = tf . keras . layers . Input ( shape = ( [number] , [number] , [number] ) ) [EOL] _ , reconstruction = autoencoder ( inputs ) [EOL] model = tf . keras . Model ( inputs = inputs , outputs = reconstruction ) [EOL] return model [EOL] [EOL] [EOL] def basic_dcgan ( image_resolution = ( [number] , [number] ) , layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = ( [number] , [number] ) , initial_filters_g = [number] , initial_filters_d = [number] , filters_cap_g = [number] , filters_cap_d = [number] , output_shape = [number] , channels = [number] , generator = None , discriminator = None , ** kwargs , ) : [EOL] if generator is None : [EOL] generator = ConvGenerator ( layer_spec_input_res = layer_spec_input_res , layer_spec_target_res = image_resolution , kernel_size = kernel_size , initial_filters = initial_filters_g , filters_cap = filters_cap_g , channels = channels , ) [EOL] [EOL] if discriminator is None : [EOL] discriminator = ConvDiscriminator ( layer_spec_input_res = image_resolution , layer_spec_target_res = layer_spec_target_res , kernel_size = kernel_size , initial_filters = initial_filters_d , filters_cap = filters_cap_d , output_shape = output_shape , ) [EOL] return generator , discriminator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $tensorflow.keras.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import tensorflow as tf [EOL] [EOL] from ashpy . models . convolutional . unet import UNet [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] x = tf . ones ( ( [number] , [number] , [number] , [number] ) ) [EOL] u_net = UNet ( input_res = [number] , min_res = [number] , kernel_size = [number] , initial_filters = [number] , filters_cap = [number] , channels = [number] , ) [EOL] y = u_net ( x , training = True ) [EOL] print ( y . shape ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import operator [EOL] [EOL] import tensorflow as tf [EOL] [EOL] from ashpy . losses import ClassifierLoss [EOL] from ashpy . metrics import ClassifierMetric [EOL] from ashpy . trainers import ClassifierTrainer [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] strategy = tf . distribute . MirroredStrategy ( ) [EOL] with strategy . scope ( ) : [EOL] training_set , validation_set = tf . keras . datasets . mnist . load_data ( ) [EOL] [EOL] def process ( images , labels ) : [EOL] data_images = tf . data . Dataset . from_tensor_slices ( (images) ) . map ( lambda x : tf . reshape ( x , ( [number] * [number] , ) ) ) [EOL] data_images = data_images . map ( lambda x : tf . image . convert_image_dtype ( x , tf . float32 ) ) [EOL] data_labels = tf . data . Dataset . from_tensor_slices ( (labels) ) [EOL] dataset = tf . data . Dataset . zip ( ( data_images , data_labels ) ) [EOL] dataset = dataset . batch ( [number] * [number] ) [EOL] return dataset [EOL] [EOL] training_set , validation_set = ( process ( training_set [ [number] ] , training_set [ [number] ] ) , process ( validation_set [ [number] ] , validation_set [ [number] ] ) , ) [EOL] [EOL] model = tf . keras . Sequential ( [ tf . keras . layers . Dense ( [number] , activation = tf . nn . sigmoid ) , tf . keras . layers . Dense ( [number] ) , ] ) [EOL] optimizer = tf . optimizers . Adam ( [number] ) [EOL] loss = ClassifierLoss ( tf . losses . SparseCategoricalCrossentropy ( from_logits = True ) ) [EOL] logdir = [string] [EOL] epochs = [number] [EOL] [EOL] metrics = [ ClassifierMetric ( tf . metrics . Accuracy ( ) , model_selection_operator = operator . gt ) , ClassifierMetric ( tf . metrics . BinaryAccuracy ( ) , model_selection_operator = operator . gt ) , ] [EOL] [EOL] trainer = ClassifierTrainer ( model = model , optimizer = optimizer , loss = loss , epochs = epochs , metrics = metrics , logdir = logdir , ) [EOL] trainer ( training_set , validation_set ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import tensorflow as tf [EOL] from tensorflow import keras [comment] [EOL] [EOL] from ashpy . losses import DiscriminatorMinMax , GeneratorBCE [EOL] from ashpy . metrics import InceptionScore [EOL] from ashpy . models . gans import ConvDiscriminator , ConvGenerator [EOL] from ashpy . trainers import AdversarialTrainer [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] strategy = tf . distribute . MirroredStrategy ( ) [EOL] with strategy . scope ( ) : [EOL] [EOL] generator = ConvGenerator ( layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = ( [number] , [number] ) , initial_filters = [number] , filters_cap = [number] , channels = [number] , ) [EOL] [EOL] discriminator = ConvDiscriminator ( layer_spec_input_res = ( [number] , [number] ) , layer_spec_target_res = ( [number] , [number] ) , kernel_size = ( [number] , [number] ) , initial_filters = [number] , filters_cap = [number] , output_shape = [number] , ) [EOL] [EOL] [comment] [EOL] generator_bce = GeneratorBCE ( ) [EOL] minmax = DiscriminatorMinMax ( ) [EOL] [EOL] [comment] [EOL] logdir = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] metrics = [ ] [EOL] [EOL] epochs = [number] [EOL] trainer = AdversarialTrainer ( generator = generator , discriminator = discriminator , generator_optimizer = tf . optimizers . Adam ( [number] ) , discriminator_optimizer = tf . optimizers . Adam ( [number] ) , generator_loss = generator_bce , discriminator_loss = minmax , epochs = epochs , metrics = metrics , logdir = logdir , ) [EOL] [EOL] batch_size = [number] [EOL] [EOL] [comment] [EOL] mnist_x , mnist_y = keras . datasets . mnist . load_data ( ) [ [number] ] [EOL] [EOL] def iterator ( ) : [EOL] [docstring] [EOL] for image , label in zip ( mnist_x , mnist_y ) : [EOL] yield tf . image . convert_image_dtype ( tf . expand_dims ( image , - [number] ) , tf . float32 ) , tf . expand_dims ( label , - [number] ) [EOL] [EOL] real_data = ( tf . data . Dataset . from_generator ( iterator , ( tf . float32 , tf . int64 ) , ( ( [number] , [number] , [number] ) , ( [number] , ) ) ) . batch ( batch_size ) . prefetch ( [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] dataset = real_data . map ( lambda x , y : ( ( x , y ) , tf . random . normal ( shape = ( batch_size , [number] ) ) ) ) [EOL] [EOL] trainer ( dataset ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import pathlib [EOL] import typing [EOL] [docstring] [EOL] from pathlib import Path [EOL] [EOL] import tensorflow as tf [EOL] [EOL] from ashpy import LogEvalMode [EOL] from ashpy . losses . gan import ( AdversarialLossType , Pix2PixLoss , get_adversarial_loss_discriminator , ) [EOL] from ashpy . models . convolutional . discriminators import PatchDiscriminator [EOL] from ashpy . models . convolutional . unet import FUNet [EOL] from ashpy . trainers . gan import AdversarialTrainer [EOL] [EOL] _URL = [string] [EOL] [EOL] PATH_TO_ZIP = tf . keras . utils . get_file ( [string] , origin = _URL , extract = True ) [EOL] PATH = Path ( PATH_TO_ZIP ) . parent / [string] [EOL] [EOL] BUFFER_SIZE = [number] [EOL] BATCH_SIZE = [number] [EOL] IMG_WIDTH = [number] [EOL] IMG_HEIGHT = [number] [EOL] [EOL] [EOL] def load ( image_file ) : [EOL] [docstring] [EOL] image = tf . io . read_file ( image_file ) [EOL] image = tf . image . decode_jpeg ( image ) [EOL] [EOL] width = tf . shape ( image ) [ [number] ] [EOL] [EOL] width = width // [number] [EOL] real_image = image [ : , : width , : ] [EOL] input_image = image [ : , width : , : ] [EOL] [EOL] input_image = tf . cast ( input_image , tf . float32 ) [EOL] real_image = tf . cast ( real_image , tf . float32 ) [EOL] [EOL] return input_image , real_image [EOL] [EOL] [EOL] def resize ( input_image , real_image , height , width ) : [EOL] [docstring] [EOL] input_image = tf . image . resize ( input_image , [ height , width ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) [EOL] real_image = tf . image . resize ( real_image , [ height , width ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) [EOL] [EOL] return input_image , real_image [EOL] [EOL] [EOL] def random_crop ( input_image , real_image ) : [EOL] [docstring] [EOL] stacked_image = tf . stack ( [ input_image , real_image ] , axis = [number] ) [EOL] cropped_image = tf . image . random_crop ( stacked_image , size = [ [number] , IMG_HEIGHT , IMG_WIDTH , [number] ] ) [EOL] [EOL] return cropped_image [ [number] ] , cropped_image [ [number] ] [EOL] [EOL] [EOL] def normalize ( input_image , real_image ) : [EOL] [docstring] [EOL] input_image = ( input_image / [number] ) - [number] [EOL] real_image = ( real_image / [number] ) - [number] [EOL] [EOL] return input_image , real_image [EOL] [EOL] [EOL] def load_image_train ( image_file ) : [EOL] [docstring] [EOL] input_image , real_image = load ( image_file ) [EOL] input_image , real_image = random_jitter ( input_image , real_image ) [EOL] input_image , real_image = normalize ( input_image , real_image ) [EOL] [EOL] return input_image , real_image [EOL] [EOL] [EOL] @ tf . function def random_jitter ( input_image , real_image ) : [EOL] [docstring] [EOL] [comment] [EOL] input_image , real_image = resize ( input_image , real_image , [number] , [number] ) [EOL] [EOL] [comment] [EOL] input_image , real_image = random_crop ( input_image , real_image ) [EOL] [EOL] if tf . random . uniform ( ( ) ) > [number] : [EOL] [comment] [EOL] input_image = tf . image . flip_left_right ( input_image ) [EOL] real_image = tf . image . flip_left_right ( real_image ) [EOL] [EOL] return input_image , real_image [EOL] [EOL] [EOL] def main ( kernel_size = [number] , learning_rate_d = [number] , learning_rate_g = [number] , g_input_res = IMG_WIDTH , g_min_res = [number] , g_initial_filters = [number] , g_filters_cap = [number] , use_dropout_encoder = False , use_dropout_decoder = True , d_target_res = [number] , d_initial_filters = [number] , d_filters_cap = [number] , use_dropout_discriminator = False , dataset_name = [string] , resolution = [number] , epochs = [number] , dropout_prob = [number] , l1_loss_weight = [number] , gan_loss_weight = [number] , use_attention_d = False , use_attention_g = False , channels = [number] , gan_loss_type = AdversarialLossType . LSGAN , ) : [EOL] [docstring] [EOL] generator = FUNet ( input_res = g_input_res , min_res = g_min_res , kernel_size = kernel_size , initial_filters = g_initial_filters , filters_cap = g_filters_cap , channels = channels , use_dropout_encoder = use_dropout_encoder , use_dropout_decoder = use_dropout_decoder , dropout_prob = dropout_prob , use_attention = use_attention_g , ) [EOL] discriminator = PatchDiscriminator ( input_res = resolution , min_res = d_target_res , initial_filters = d_initial_filters , kernel_size = kernel_size , filters_cap = d_filters_cap , use_dropout = use_dropout_discriminator , dropout_prob = dropout_prob , use_attention = use_attention_d , ) [EOL] [EOL] discriminator_loss = get_adversarial_loss_discriminator ( gan_loss_type ) ( ) [EOL] generator_loss = Pix2PixLoss ( l1_loss_weight = l1_loss_weight , adversarial_loss_weight = gan_loss_weight , adversarial_loss_type = gan_loss_type , ) [EOL] [EOL] metrics = [ ] [EOL] logdir = Path ( [string] ) / dataset_name / [string] [EOL] [EOL] if not logdir . exists ( ) : [EOL] logdir . mkdir ( parents = True ) [EOL] [EOL] trainer = AdversarialTrainer ( generator = generator , discriminator = discriminator , generator_optimizer = tf . optimizers . Adam ( learning_rate_g , beta_1 = [number] ) , discriminator_optimizer = tf . optimizers . Adam ( learning_rate_d , beta_1 = [number] ) , generator_loss = generator_loss , discriminator_loss = discriminator_loss , epochs = epochs , metrics = metrics , logdir = logdir , log_eval_mode = LogEvalMode . TEST , ) [EOL] [EOL] train_dataset = tf . data . Dataset . list_files ( PATH + [string] ) [EOL] train_dataset = train_dataset . shuffle ( BUFFER_SIZE ) [EOL] train_dataset = train_dataset . map ( load_image_train ) [EOL] train_dataset = train_dataset . batch ( BATCH_SIZE ) [EOL] [EOL] train_dataset = train_dataset . map ( lambda x , y : ( ( y , x ) , x ) ) [EOL] [EOL] trainer ( train_dataset ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import operator [EOL] [EOL] import tensorflow as tf [EOL] from tensorflow import keras [EOL] [EOL] from ashpy . losses import DiscriminatorMinMax , EncoderBCE , GeneratorBCE [EOL] from ashpy . metrics import EncodingAccuracy [EOL] from ashpy . trainers import EncoderTrainer [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] [EOL] def real_gen ( ) : [EOL] [docstring] [EOL] for _ in tf . range ( [number] ) : [EOL] yield ( ( [number] , ) , ( [number] , ) ) [EOL] [EOL] num_classes = [number] [EOL] latent_dim = [number] [EOL] [EOL] generator = keras . Sequential ( [ keras . layers . Dense ( [number] ) ] ) [EOL] [EOL] left_input = tf . keras . layers . Input ( shape = ( [number] , ) ) [EOL] left = tf . keras . layers . Dense ( [number] , activation = tf . nn . elu ) ( left_input ) [EOL] [EOL] right_input = tf . keras . layers . Input ( shape = ( latent_dim , ) ) [EOL] right = tf . keras . layers . Dense ( [number] , activation = tf . nn . elu ) ( right_input ) [EOL] [EOL] net = tf . keras . layers . Concatenate ( ) ( [ left , right ] ) [EOL] out = tf . keras . layers . Dense ( [number] ) ( net ) [EOL] [EOL] discriminator = tf . keras . Model ( inputs = [ left_input , right_input ] , outputs = [ out ] ) [EOL] [EOL] encoder = keras . Sequential ( [ keras . layers . Dense ( latent_dim ) ] ) [EOL] generator_bce = GeneratorBCE ( ) [EOL] encoder_bce = EncoderBCE ( ) [EOL] minmax = DiscriminatorMinMax ( ) [EOL] [EOL] epochs = [number] [EOL] logdir = [string] [EOL] [EOL] [comment] [EOL] classifier = tf . keras . Sequential ( [ tf . keras . layers . Dense ( [number] ) , tf . keras . layers . Dense ( num_classes ) ] ) [EOL] [EOL] metrics = [ EncodingAccuracy ( classifier , model_selection_operator = operator . gt , logdir = logdir ) ] [EOL] [EOL] trainer = EncoderTrainer ( generator = generator , discriminator = discriminator , encoder = encoder , generator_optimizer = tf . optimizers . Adam ( [number] ) , discriminator_optimizer = tf . optimizers . Adam ( [number] ) , encoder_optimizer = tf . optimizers . Adam ( [number] ) , generator_loss = generator_bce , discriminator_loss = minmax , encoder_loss = encoder_bce , epochs = epochs , metrics = metrics , logdir = logdir , ) [EOL] [EOL] batch_size = [number] [EOL] discriminator_input = tf . data . Dataset . from_generator ( real_gen , ( tf . float32 , tf . int64 ) , ( [number] , [number] ) ) . batch ( batch_size ) [EOL] [EOL] dataset = discriminator_input . map ( lambda x , y : ( ( x , y ) , tf . random . normal ( shape = ( batch_size , latent_dim ) ) ) ) [EOL] [EOL] trainer ( dataset ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] import os [EOL] [EOL] import tensorflow as tf [EOL] [EOL] from ashpy import LogEvalMode [EOL] from ashpy . losses . gan import ( AdversarialLossType , Pix2PixLoss , get_adversarial_loss_discriminator , ) [EOL] from ashpy . models . convolutional . discriminators import PatchDiscriminator [EOL] from ashpy . models . convolutional . unet import FUNet [EOL] from ashpy . trainers . gan import AdversarialTrainer [EOL] [EOL] from . pix2pix_facades import BATCH_SIZE , BUFFER_SIZE , IMG_WIDTH , PATH , load_image_train [EOL] [EOL] [EOL] def main ( kernel_size = [number] , learning_rate_d = [number] , learning_rate_g = [number] , g_input_res = IMG_WIDTH , g_min_res = [number] , g_initial_filters = [number] , g_filters_cap = [number] , use_dropout_encoder = False , use_dropout_decoder = True , d_target_res = [number] , d_initial_filters = [number] , d_filters_cap = [number] , use_dropout_discriminator = False , dataset_name = [string] , resolution = [number] , epochs = [number] , dropout_prob = [number] , l1_loss_weight = [number] , gan_loss_weight = [number] , use_attention_d = False , use_attention_g = False , channels = [number] , gan_loss_type = AdversarialLossType . LSGAN , ) : [EOL] [docstring] [EOL] [comment] [EOL] strategy = tf . distribute . MirroredStrategy ( ) [EOL] with strategy . scope ( ) : [EOL] generator = FUNet ( input_res = g_input_res , min_res = g_min_res , kernel_size = kernel_size , initial_filters = g_initial_filters , filters_cap = g_filters_cap , channels = channels , use_dropout_encoder = use_dropout_encoder , use_dropout_decoder = use_dropout_decoder , dropout_prob = dropout_prob , use_attention = use_attention_g , ) [EOL] discriminator = PatchDiscriminator ( input_res = resolution , min_res = d_target_res , initial_filters = d_initial_filters , kernel_size = kernel_size , filters_cap = d_filters_cap , use_dropout = use_dropout_discriminator , dropout_prob = dropout_prob , use_attention = use_attention_d , ) [EOL] [EOL] discriminator_loss = get_adversarial_loss_discriminator ( gan_loss_type ) ( ) [EOL] generator_loss = Pix2PixLoss ( l1_loss_weight = l1_loss_weight , adversarial_loss_weight = gan_loss_weight , adversarial_loss_type = gan_loss_type , ) [EOL] [EOL] metrics = [ ] [EOL] logdir = f'{ [string] } [string] { dataset_name } [string] ' [EOL] [EOL] if not logdir . exists ( ) : [EOL] logdir . mkdir ( parents = True ) [EOL] [EOL] trainer = AdversarialTrainer ( generator = generator , discriminator = discriminator , generator_optimizer = tf . optimizers . Adam ( learning_rate_g * strategy . num_replicas_in_sync , beta_1 = [number] ) , discriminator_optimizer = tf . optimizers . Adam ( learning_rate_d * strategy . num_replicas_in_sync , beta_1 = [number] ) , generator_loss = generator_loss , discriminator_loss = discriminator_loss , epochs = epochs , metrics = metrics , logdir = logdir , log_eval_mode = LogEvalMode . TEST , ) [EOL] [EOL] train_dataset = tf . data . Dataset . list_files ( PATH + [string] ) [EOL] train_dataset = train_dataset . shuffle ( BUFFER_SIZE ) [EOL] train_dataset = train_dataset . map ( load_image_train ) [EOL] train_dataset = train_dataset . batch ( BATCH_SIZE ) [EOL] [EOL] train_dataset = train_dataset . map ( lambda x , y : ( ( y , x ) , x ) ) [EOL] [EOL] trainer ( train_dataset ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0