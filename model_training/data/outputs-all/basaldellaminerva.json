[comment] [EOL] nlp_backend = [string] [EOL]	0 0 $builtins.str$ 0 0 0
from . import config [EOL] [EOL] from . __version__ import __version__ [EOL] from minerva . nlp import * [EOL] from minerva . text . base import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
VERSION = ( [number] , [number] , [number] ) [EOL] [EOL] __version__ = [string] . join ( map ( str , VERSION ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Iterable , List , Dict , Any , Optional , Union [EOL] import typing [EOL] import minerva [EOL] import builtins [EOL] from abc import ABC , abstractmethod [EOL] from typing import Any , Dict , List , Iterable , Optional , overload , Union [EOL] [EOL] import minerva as mine [EOL] [EOL] [EOL] class Annotation ( ABC ) : [EOL] def __init__ ( self , value , score = None , ** kwargs ) : [EOL] [docstring] [EOL] self . value = value [EOL] self . __annos = { } [EOL] for key , value in kwargs : [EOL] self [ key ] = value [EOL] [EOL] if score : [EOL] self [ [string] ] = score [EOL] [EOL] def __setitem__ ( self , key , value ) : [EOL] [docstring] [EOL] [EOL] if key == [string] : [EOL] raise ValueError ( [string] ) [EOL] else : [EOL] self . __annos [ key ] = value [EOL] [EOL] def __getitem__ ( self , item ) : [EOL] [docstring] [EOL] if item == [string] : [EOL] return self . value [EOL] else : [EOL] if item in self . __annos : [EOL] return self . __annos [ item ] [EOL] else : [EOL] raise KeyError ( f"{ item } [string] " ) [EOL] [EOL] [EOL] class TokenSpan ( Annotation ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , value , start_token , end_token = None , score = None , ** kwargs , ) : [EOL] [docstring] [EOL] [EOL] if not end_token : [EOL] end_token = start_token [EOL] [EOL] super ( ) . __init__ ( value , score ) [EOL] self [ [string] ] = start_token [EOL] self [ [string] ] = end_token [EOL] for key , value in kwargs : [EOL] self [ key ] = value [EOL] [EOL] @ property def start_token ( self ) : [EOL] [docstring] [EOL] t = self [ [string] ] [EOL] return t [EOL] [EOL] @ property def end_token ( self ) : [EOL] [docstring] [EOL] t = self [ [string] ] [EOL] return t [EOL] [EOL] @ property def start_index ( self ) : [EOL] [docstring] [EOL] return self . start_token . index [EOL] [EOL] @ property def end_index ( self ) : [EOL] [docstring] [EOL] return self . end_token . index [EOL] [EOL] @ property def text ( self ) : [EOL] [docstring] [EOL] if self . start_token . parent : [EOL] return self . start_token . parent . text [ self . start_token . char_index : self . end_token . end_char_index ] [EOL] [EOL] raise ValueError ( [string] ) [EOL] [EOL] [EOL] class BaseEntity ( ABC ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , text , language = [string] ) : [EOL] [docstring] [EOL] self . text = text [EOL] self . language = language [EOL] [EOL] def __str__ ( self ) : [EOL] [docstring] [EOL] return self . text [EOL] [EOL] [EOL] class BaseTextualEntity ( BaseEntity ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , text , index = - [number] , char_index = - [number] , language = [string] , parent = None , ) : [EOL] [docstring] [EOL] super ( ) . __init__ ( text , language = language ) [EOL] self . index = index [EOL] self . char_index = char_index [EOL] self . end_char_index = char_index + len ( text ) if char_index >= [number] else - [number] [EOL] self . labels = { } [EOL] self . parent = parent [EOL] [EOL] [EOL] class Token ( BaseTextualEntity ) : [EOL] def __init__ ( self , text , index = - [number] , parent = None , char_index = - [number] , language = [string] , ) : [EOL] super ( ) . __init__ ( text , index = index , char_index = char_index , language = language , parent = parent ) [EOL] [EOL] def __contains__ ( self , item ) : [EOL] return item in self . labels . keys ( ) [EOL] [EOL] def __setitem__ ( self , key , value ) : [EOL] self . labels [ key ] = value [EOL] [EOL] def __getitem__ ( self , item ) : [EOL] return self . labels [ item ] [EOL] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . labels ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . text ) [EOL] [EOL] def __str__ ( self ) : [EOL] if self . parent and self . parent . index >= [number] : [EOL] index = ( f" [string] { self . parent . index } [string] { str ( self . index ) } [string] " if self . index >= [number] else [string] ) [EOL] else : [EOL] index = f" [string] { str ( self . index ) } [string] " if self . index >= [number] else [string] [EOL] return f" [string] { index } [string] { self . text }" [EOL] [EOL] [EOL] class Sentence ( BaseTextualEntity ) : [EOL] def __init__ ( self , text , index = - [number] , parent = None ) : [EOL] super ( ) . __init__ ( text , index = index , parent = parent ) [EOL] self . tokens = [ ] [EOL] self . annotations = { } [EOL] [EOL] [comment] [EOL] processed_text = text [EOL] for word in mine . word_tokenize ( text ) : [EOL] tok_pos = [number] if len ( self ) == [number] else self . tokens [ len ( self ) - [number] ] . end_char_index [EOL] tok_pos += processed_text . index ( word ) [EOL] self . tokens . append ( Token ( word , index = len ( self ) , parent = self , char_index = tok_pos ) ) [EOL] processed_text = text [ tok_pos + len ( word ) : ] [EOL] [EOL] def add_annotation ( self , key , value , begin = None , end = None , score = None , ** kwargs , ) : [EOL] if begin is not None and end is not None : [EOL] ann = TokenSpan ( value , self [ begin ] , self [ end - [number] ] , score , ** kwargs ) [EOL] for token in self [ begin : end ] : [EOL] token [ key ] = ann [EOL] [EOL] else : [EOL] self . annotations [ key ] = Annotation ( value , score , ** kwargs ) [EOL] [EOL] def get_annotation ( self , key ) : [EOL] [EOL] if key in self . annotations : [EOL] return self . annotations [ key ] [EOL] [EOL] anns = [ ] [EOL] [EOL] i = [number] [EOL] while i < len ( self ) : [EOL] [EOL] token = self [ i ] [EOL] if key in token : [EOL] if isinstance ( token [ key ] , TokenSpan ) : [EOL] i = token [ key ] . end_index [EOL] [EOL] anns . append ( token [ key ] ) [EOL] i += [number] [EOL] [EOL] if len ( anns ) > [number] : [EOL] return anns [EOL] else : [EOL] return None [EOL] [EOL] def token_at_char ( self , index ) : [EOL] [docstring] [EOL] [EOL] if index < [number] or index > len ( self . text ) : [EOL] raise IndexError ( f" [string] { index } [string] { self . text } [string] " ) [EOL] [EOL] token = None [EOL] [EOL] left = [number] [EOL] right = len ( self ) - [number] [EOL] [EOL] while left <= right : [EOL] middle = ( left + right ) // [number] [EOL] [EOL] if self [ middle ] . char_index <= index <= self [ middle ] . end_char_index - [number] : [EOL] token = self [ middle ] [EOL] break [EOL] elif index < self [ middle ] . char_index : [EOL] right = middle - [number] [EOL] else : [EOL] left = middle + [number] [EOL] [EOL] return token [EOL] [EOL] @ overload def __getitem__ ( self , idx ) : [EOL] pass [EOL] [EOL] @ overload def __getitem__ ( self , idx ) : [EOL] pass [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] return self . tokens [ idx ] [EOL] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . tokens ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . tokens ) [EOL] [EOL] def __str__ ( self ) : [EOL] index = f" [string] { str ( self . index ) } [string] " if self . index >= [number] else [string] [EOL] return f" [string] { index } [string] { self . text }" [EOL] [EOL] [EOL] class Document ( BaseEntity ) : [EOL] def __init__ ( self , text , _id = None , language = [string] ) : [EOL] [EOL] if isinstance ( text , str ) : [EOL] super ( ) . __init__ ( text = text , language = language ) [EOL] else : [EOL] super ( ) . __init__ ( text = [string] . join ( text ) , language = language ) [EOL] [EOL] sentences_txt = mine . sent_tokenize ( text ) if isinstance ( text , str ) else text [EOL] [EOL] self . sentences = [ Sentence ( sentences_txt [ i ] , index = i ) for i in range ( len ( sentences_txt ) ) ] [EOL] self . id = _id [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] return self . sentences [ idx ] [EOL] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . sentences ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . sentences ) [EOL] [EOL] def __str__ ( self ) : [EOL] return f" [string] { self . text }" [EOL] [EOL] [EOL] class Corpus : [EOL] def __init__ ( self , _id = None , items = None ) : [EOL] self . id = _id [EOL] self . items = ... [EOL] [EOL] if items : [EOL] self . items = items [EOL] else : [EOL] self . items = [ ] [EOL] [EOL] def add ( self , item ) : [EOL] self . items . append ( item ) [EOL] [EOL] def __add__ ( self , other ) : [EOL] new_id = self . id + [string] + other . id if self . id and other . id else [string] [EOL] return Corpus ( new_id , self . items + other . items ) [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] return self . items [ idx ] [EOL] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . items ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . items ) [EOL] [EOL] def __str__ ( self ) : [EOL] return f" [string] { self . id } [string] { len ( self ) } [string] " [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[Token]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[minerva.text.base.Token]$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Optional[minerva.text.base.Token]$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Optional[minerva.text.base.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[Sentence]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $Sentence$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Iterable$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[minerva.text.base.BaseEntity]$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $builtins.str$ 0 0 0 $typing.List[minerva.text.base.BaseEntity]$ 0 0 0 0 0 $typing.List[minerva.text.base.BaseEntity]$ 0 0 0 0 $typing.List[minerva.text.base.BaseEntity]$ 0 $typing.List[minerva.text.base.BaseEntity]$ 0 0 0 0 0 0 $typing.List[minerva.text.base.BaseEntity]$ 0 0 0 0 0 0 $None$ 0 0 0 $BaseEntity$ 0 0 0 0 0 0 0 0 0 $BaseEntity$ 0 0 0 0 $"Corpus"$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $BaseEntity$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Iterable$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import _importlib_modulespec [EOL] import typing [EOL] [docstring] [EOL] [EOL] import importlib [EOL] import types [EOL] [EOL] [EOL] class LazyLoader ( types . ModuleType ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] def __init__ ( self , name , local_name = None , parent_module_globals = globals ( ) , warning = None ) : [comment] [EOL] self . _local_name = local_name if local_name else name [EOL] self . _parent_module_globals = parent_module_globals [EOL] self . _warning = warning [EOL] [EOL] super ( LazyLoader , self ) . __init__ ( name ) [EOL] [EOL] def _load ( self ) : [EOL] [docstring] [EOL] [comment] [EOL] module = importlib . import_module ( self . __name__ ) [EOL] self . _parent_module_globals [ self . _local_name ] = module [EOL] [EOL] [comment] [EOL] if self . _warning : [EOL] [comment] [EOL] [comment] [EOL] self . _warning = None [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . __dict__ . update ( module . __dict__ ) [EOL] [EOL] return module [EOL] [EOL] def __getattr__ ( self , item ) : [EOL] module = self . _load ( ) [EOL] return getattr ( module , item ) [EOL] [EOL] def __dir__ ( self ) : [EOL] module = self . _load ( ) [EOL] return dir ( module ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $_importlib_modulespec.ModuleType$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $_importlib_modulespec.ModuleType$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $_importlib_modulespec.ModuleType$ 0 0 0 0 0 0 $_importlib_modulespec.ModuleType$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
import minerva . config as config [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] if config . nlp_backend == [string] : [EOL] [EOL] from minerva . nlp . wrappers . nltk import * [EOL] else : [EOL] raise Exception ( f" [string] { config . nlp_backend } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple , List [EOL] import typing [EOL] import builtins [EOL] import nltk [comment] [EOL] from typing import List , Tuple [EOL] [EOL] TOKENIZER = None [EOL] [EOL] [EOL] def sent_tokenize ( txt , language = [string] ) : [EOL] [EOL] return nltk . sent_tokenize ( txt , language ) [comment] [EOL] [EOL] [EOL] def word_tokenize ( txt , language = [string] ) : [EOL] [EOL] global TOKENIZER [EOL] [EOL] if not TOKENIZER : [EOL] TOKENIZER = nltk . tokenize . WordPunctTokenizer ( ) [EOL] [EOL] return TOKENIZER . tokenize ( txt ) [comment] [EOL] [EOL] [EOL] def pos_tag ( txt , language = [string] ) : [EOL] [EOL] ret = ... [EOL] if isinstance ( txt , str ) : [EOL] ret = nltk . pos_tag ( nltk . word_tokenize ( txt , language = language ) , lang = language ) [EOL] else : [EOL] ret = nltk . pos_tag ( txt , lang = language ) [EOL] [EOL] return ret [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 $None$ 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List [EOL] import typing [EOL] import minerva [EOL] import minerva as mine [EOL] import pytest [EOL] [EOL] [EOL] @ pytest . fixture def quotes_template ( ) : [EOL] return [ ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] , ) ] [EOL] [EOL] [EOL] @ pytest . fixture def contractions_template_en ( ) : [EOL] return [ ( [string] , [ [string] , [string] , [string] , [string] , [string] ] ) , ( [string] , [ [string] , [string] , [string] , [string] ] ) , ( [string] , [ [string] , [string] , [string] ] ) , ( [string] , [ [string] , [string] , [string] , [string] , [string] ] ) , ] [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] , params = [ [string] , [string] , [string] , ] , ) def sample_text ( request ) : [EOL] return request . param [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def sample_tokens ( ) : [EOL] return [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def sample_pos_tags ( ) : [EOL] return [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def lipsum_array ( ) : [EOL] return [ [string] + [string] , [string] , ] [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def lipsum_txt ( ) : [EOL] return ( [string] [string] [string] ) [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def sentence ( sample_text ) : [EOL] return mine . Sentence ( sample_text ) [EOL] [EOL] [EOL] def test_tokenization ( sentence , sample_text , sample_tokens ) : [EOL] assert len ( sentence ) == [number] [EOL] assert sentence [ [number] ] . text == [string] [EOL] [EOL] for i , token in enumerate ( sentence ) : [EOL] assert token . parent == sentence [EOL] assert token . text == sample_tokens [ i ] [EOL] assert token . char_index == sample_text . index ( sample_tokens [ i ] ) [EOL] [EOL] middle_token = sentence [ [number] ] [EOL] assert middle_token . text == [string] [EOL] assert middle_token . char_index == sample_text . index ( [string] ) [EOL] assert middle_token . end_char_index == sample_text . index ( [string] ) + len ( [string] ) [EOL] [EOL] assert sentence . token_at_char ( sample_text . index ( [string] ) ) . text == [string] [EOL] assert sentence . token_at_char ( sample_text . index ( [string] ) + [number] ) . text == [string] [EOL] assert sentence . token_at_char ( sample_text . index ( [string] ) + [number] ) . text == [string] [EOL] assert sentence . token_at_char ( sample_text . index ( [string] ) ) . text == [string] [EOL] assert ( sentence . token_at_char ( sample_text . index ( [string] ) + len ( [string] ) - [number] ) . text == [string] ) [EOL] assert sentence . token_at_char ( len ( sample_text ) - [number] ) . text == [string] [EOL] assert not sentence . token_at_char ( [number] ) [EOL] assert not ( sentence . token_at_char ( sample_text . index ( [string] ) + len ( [string] ) ) ) [EOL] assert not ( sentence . token_at_char ( sample_text . index ( [string] ) + len ( [string] ) ) ) [EOL] [EOL] [EOL] def test_document ( lipsum_array , lipsum_txt ) : [EOL] d1 = mine . Document ( lipsum_txt ) [EOL] d2 = mine . Document ( lipsum_array ) [EOL] [EOL] assert len ( d1 ) == len ( d2 ) == [number] [EOL] for i in range ( len ( d1 ) ) : [EOL] assert d1 [ i ] . text == d2 [ i ] . text [EOL] assert d1 [ i ] . index == d2 [ i ] . index == i [EOL] for j in range ( len ( d1 [ i ] ) ) : [EOL] assert d1 [ i ] [ j ] . text == d2 [ i ] [ j ] . text [EOL] [EOL] assert d1 [ [number] ] . text == lipsum_array [ [number] ] [EOL] assert d1 . text == d2 . text [EOL] assert d1 . text == lipsum_txt [EOL] [EOL] [EOL] def test_corpus ( lipsum_array ) : [EOL] [EOL] sentences = [ mine . Sentence ( lipsum ) for lipsum in lipsum_array ] [EOL] [EOL] c1 = mine . Corpus ( [string] , items = sentences ) [EOL] c2 = mine . Corpus ( [string] ) [EOL] for sentence in sentences : [EOL] c2 . add ( sentence ) [EOL] [EOL] assert len ( c1 ) == len ( c2 ) == len ( sentences ) [EOL] assert c1 [ [number] ] . text == c2 [ [number] ] . text [EOL] [EOL] c3 = c1 + c2 [EOL] assert len ( c3 ) == len ( c1 ) + len ( c2 ) [EOL] assert c3 [ [number] ] == c1 [ [number] ] [EOL] assert c3 [ - [number] ] == c2 [ - [number] ] [EOL] [EOL] [EOL] def test_tags ( sentence , sample_pos_tags ) : [EOL] [EOL] for i , token in enumerate ( sentence ) : [EOL] token [ [string] ] = mine . Annotation ( sample_pos_tags [ i ] ) [EOL] [EOL] annos = [ anno . value for anno in sentence . get_annotation ( [string] ) ] [EOL] assert annos == sample_pos_tags [EOL] [EOL] sentence . add_annotation ( [string] , [string] , begin = [number] , end = [number] , score = [number] ) [EOL] for t in sentence [ [number] : [number] ] : [EOL] assert t [ [string] ] . value == [string] [EOL] [EOL] with pytest . raises ( IndexError ) : [EOL] assert sentence . add_annotation ( [string] , [string] , begin = [number] , end = [number] , score = [number] ) [EOL] [EOL] sentence . add_annotation ( [string] , [string] , begin = [number] , end = [number] , score = [number] ) [EOL] for t in sentence [ [number] : [number] ] : [EOL] assert t [ [string] ] . value == [string] [EOL] [EOL] spans = sentence . get_annotation ( [string] ) [EOL] assert spans [ [number] ] . text == [string] [EOL] assert spans [ [number] ] . text == [string] [EOL] [EOL] with pytest . raises ( KeyError ) : [EOL] assert sentence . get_annotation ( [string] ) [EOL] [EOL] [EOL] def test_quotes_and_contraptions_en ( quotes_template , contractions_template_en ) : [EOL] [EOL] for sentence , tokens in quotes_template + contractions_template_en : [EOL] s = mine . Sentence ( sentence ) [EOL] for i in range ( len ( s ) ) : [EOL] assert s [ i ] . text == tokens [ i ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0