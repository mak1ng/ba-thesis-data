[comment] [EOL] [comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] import io [EOL] import sys [EOL] def instance2dict ( instance ) : [EOL] [comment] [EOL] tmp_dict = { } [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] [EOL] return tmp_dict [EOL] [EOL] def fetch_data ( file ) : [EOL] [comment] [EOL] lines = file . readlines ( ) [EOL] instance_list = [ ] [EOL] tmp_instance = [ ] [EOL] for line in lines : [EOL] if line . strip ( ) == [string] : [EOL] instance_list . append ( instance2dict ( tmp_instance ) ) [EOL] tmp_instance = [ ] [EOL] else : [EOL] tmp_instance . append ( line ) [EOL] return instance_list [EOL] [EOL] def main ( ) : [EOL] [comment] [EOL] print ( [string] % ( sys . argv [ [number] ] ) ) [EOL] print ( [string] % ( sys . argv [ [number] ] ) ) [EOL] predicted_file = open ( sys . argv [ [number] ] , [string] , encoding = [string] ) [EOL] gold_file = open ( sys . argv [ [number] ] , [string] , encoding = [string] ) [EOL] predicted_data = fetch_data ( predicted_file ) [EOL] gold_data = fetch_data ( gold_file ) [EOL] if len ( predicted_data ) != len ( gold_data ) : [EOL] print ( [string] ) [EOL] exit ( [number] ) [EOL] total_count = [number] [EOL] right_count = [number] [EOL] for i in range ( len ( gold_data ) ) : [EOL] total_count += [number] [EOL] if predicted_data [ i ] [ [string] ] == gold_data [ i ] [ [string] ] : [EOL] right_count += [number] [EOL] print ( [string] % ( float ( right_count ) / total_count ) ) [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import logging [EOL] import os [EOL] import sys [EOL] [EOL] if os . environ . get ( [string] ) : [EOL] LEVEL = logging . DEBUG [EOL] else : [EOL] LEVEL = logging . INFO [EOL] [EOL] sys . path . insert ( [number] , os . path . dirname ( os . path . abspath ( os . path . join ( __file__ , os . pardir ) ) ) ) [EOL] logging . basicConfig ( format = [string] , level = LEVEL ) [EOL] [EOL] from allennlp . commands import main [comment] [EOL] [EOL] def run ( ) : [EOL] main ( prog = [string] ) [EOL] [EOL] if __name__ == [string] : [EOL] run ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
_MAJOR = [string] [EOL] _MINOR = [string] [EOL] _REVISION = [string] [EOL] [EOL] VERSION_SHORT = [string] . format ( _MAJOR , _MINOR ) [EOL] VERSION = [string] . format ( _MAJOR , _MINOR , _REVISION ) [EOL]	$builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0
from typing import Dict , Union , List , Optional , Any [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import pytorch_pretrained_bert [EOL] import torch [EOL] from typing import Dict , List , Optional , Any , Union [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . modules import Linear , Dropout [EOL] import torch . nn . functional as F [EOL] from pytorch_pretrained_bert . modeling import BertModel [EOL] [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] from allennlp . nn . util import get_text_field_mask , sequence_cross_entropy_with_logits [EOL] from allennlp . nn . util import get_lengths_from_binary_sequence_mask , viterbi_decode [EOL] from allennlp . training . metrics import SpanBasedF1Measure [EOL] [EOL] @ Model . register ( [string] ) class SrlBert ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab , bert_model , embedding_dropout = [number] , initializer = InitializerApplicator ( ) , regularizer = None , label_smoothing = None , ignore_span_metric = False ) : [EOL] super ( SrlBert , self ) . __init__ ( vocab , regularizer ) [EOL] [EOL] if isinstance ( bert_model , str ) : [EOL] self . bert_model = BertModel . from_pretrained ( bert_model ) [EOL] else : [EOL] self . bert_model = bert_model [EOL] [EOL] self . num_classes = self . vocab . get_vocab_size ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] self . span_metric = SpanBasedF1Measure ( vocab , tag_namespace = [string] , ignore_classes = [ [string] ] ) [EOL] self . tag_projection_layer = Linear ( self . bert_model . config . hidden_size , self . num_classes ) [EOL] [EOL] self . embedding_dropout = Dropout ( p = embedding_dropout ) [EOL] self . _label_smoothing = label_smoothing [EOL] self . ignore_span_metric = ignore_span_metric [EOL] initializer ( self ) [EOL] [EOL] def forward ( self , tokens , verb_indicator , metadata , tags = None ) : [EOL] [comment] [EOL] [docstring] [EOL] mask = get_text_field_mask ( tokens ) [EOL] bert_embeddings , _ = self . bert_model ( input_ids = tokens [ [string] ] , token_type_ids = verb_indicator , attention_mask = mask , output_all_encoded_layers = False ) [EOL] [EOL] embedded_text_input = self . embedding_dropout ( bert_embeddings ) [EOL] batch_size , sequence_length , _ = embedded_text_input . size ( ) [EOL] logits = self . tag_projection_layer ( embedded_text_input ) [EOL] [EOL] reshaped_log_probs = logits . view ( - [number] , self . num_classes ) [EOL] class_probabilities = F . softmax ( reshaped_log_probs , dim = - [number] ) . view ( [ batch_size , sequence_length , self . num_classes ] ) [EOL] output_dict = { [string] : logits , [string] : class_probabilities } [EOL] if tags is not None : [EOL] loss = sequence_cross_entropy_with_logits ( logits , tags , mask , label_smoothing = self . _label_smoothing ) [EOL] if not self . ignore_span_metric : [EOL] self . span_metric ( class_probabilities , tags , mask ) [EOL] output_dict [ [string] ] = loss [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] output_dict [ [string] ] = mask [EOL] [EOL] [comment] [EOL] words , verbs , offsets = zip ( * [ ( x [ [string] ] , x [ [string] ] , x [ [string] ] ) for x in metadata ] ) [EOL] output_dict [ [string] ] = list ( words ) [EOL] output_dict [ [string] ] = list ( verbs ) [EOL] output_dict [ [string] ] = list ( offsets ) [EOL] return output_dict [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] all_predictions = output_dict [ [string] ] [EOL] sequence_lengths = get_lengths_from_binary_sequence_mask ( output_dict [ [string] ] ) . data . tolist ( ) [EOL] [EOL] if all_predictions . dim ( ) == [number] : [EOL] predictions_list = [ all_predictions [ i ] . detach ( ) . cpu ( ) for i in range ( all_predictions . size ( [number] ) ) ] [EOL] else : [EOL] predictions_list = [ all_predictions ] [EOL] wordpiece_tags = [ ] [EOL] word_tags = [ ] [EOL] transition_matrix = self . get_viterbi_pairwise_potentials ( ) [EOL] start_transitions = self . get_start_transitions ( ) [EOL] [comment] [EOL] [comment] [EOL] for predictions , length , offsets in zip ( predictions_list , sequence_lengths , output_dict [ [string] ] ) : [EOL] max_likelihood_sequence , _ = viterbi_decode ( predictions [ : length ] , transition_matrix , allowed_start_transitions = start_transitions ) [EOL] tags = [ self . vocab . get_token_from_index ( x , namespace = [string] ) for x in max_likelihood_sequence ] [EOL] [EOL] wordpiece_tags . append ( tags ) [EOL] word_tags . append ( [ tags [ i ] for i in offsets ] ) [EOL] output_dict [ [string] ] = wordpiece_tags [EOL] output_dict [ [string] ] = word_tags [EOL] return output_dict [EOL] [EOL] def get_metrics ( self , reset = False ) : [EOL] if self . ignore_span_metric : [EOL] [comment] [EOL] [comment] [EOL] return { } [EOL] [EOL] else : [EOL] metric_dict = self . span_metric . get_metric ( reset = reset ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] return { x : y for x , y in metric_dict . items ( ) if [string] in x } [EOL] [EOL] def get_viterbi_pairwise_potentials ( self ) : [EOL] [docstring] [EOL] all_labels = self . vocab . get_index_to_token_vocabulary ( [string] ) [EOL] num_labels = len ( all_labels ) [EOL] transition_matrix = torch . zeros ( [ num_labels , num_labels ] ) [EOL] [EOL] for i , previous_label in all_labels . items ( ) : [EOL] for j , label in all_labels . items ( ) : [EOL] [comment] [EOL] [comment] [EOL] if i != j and label [ [number] ] == [string] and not previous_label == [string] + label [ [number] : ] : [EOL] transition_matrix [ i , j ] = float ( [string] ) [EOL] return transition_matrix [EOL] [EOL] [EOL] def get_start_transitions ( self ) : [EOL] [docstring] [EOL] all_labels = self . vocab . get_index_to_token_vocabulary ( [string] ) [EOL] num_labels = len ( all_labels ) [EOL] [EOL] start_transitions = torch . zeros ( num_labels ) [EOL] [EOL] for i , label in all_labels . items ( ) : [EOL] if label [ [number] ] == [string] : [EOL] start_transitions [ i ] = float ( [string] ) [EOL] [EOL] return start_transitions [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
from typing import Dict , List , Optional , Any , Tuple , Set [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , Optional , Tuple [EOL] [EOL] import numpy [EOL] from overrides import overrides [EOL] [EOL] import torch [EOL] from torch . nn import Module , ModuleDict [EOL] from torch . nn . modules . rnn import GRUCell [EOL] from torch . nn . modules . linear import Linear [EOL] from torch import nn [EOL] import torch . nn . functional as F [EOL] [EOL] from allennlp . common . util import START_SYMBOL , END_SYMBOL [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . modules import Seq2VecEncoder , TextFieldEmbedder [EOL] from allennlp . modules . token_embedders import Embedding [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn . beam_search import BeamSearch [EOL] from allennlp . nn . util import get_text_field_mask , sequence_cross_entropy_with_logits [EOL] from allennlp . training . metrics import UnigramRecall [EOL] [EOL] [EOL] @ Model . register ( [string] ) class Event2Mind ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab , source_embedder , embedding_dropout , encoder , max_decoding_steps , beam_size = [number] , target_names = None , target_namespace = [string] , target_embedding_dim = None ) : [EOL] super ( ) . __init__ ( vocab ) [EOL] target_names = target_names or [ [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . _source_embedder = source_embedder [EOL] self . _embedding_dropout = nn . Dropout ( embedding_dropout ) [EOL] self . _encoder = encoder [EOL] self . _max_decoding_steps = max_decoding_steps [EOL] self . _target_namespace = target_namespace [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . _start_index = self . vocab . get_token_index ( START_SYMBOL , self . _target_namespace ) [EOL] self . _end_index = self . vocab . get_token_index ( END_SYMBOL , self . _target_namespace ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] num_classes = self . vocab . get_vocab_size ( self . _target_namespace ) [EOL] [comment] [EOL] [comment] [EOL] self . _decoder_output_dim = self . _encoder . get_output_dim ( ) [EOL] target_embedding_dim = target_embedding_dim or self . _source_embedder . get_output_dim ( ) [EOL] [EOL] self . _states = ModuleDict ( ) [EOL] for name in target_names : [EOL] self . _states [ name ] = StateDecoder ( num_classes , target_embedding_dim , self . _decoder_output_dim ) [EOL] [EOL] self . _beam_search = BeamSearch ( self . _end_index , beam_size = beam_size , max_steps = max_decoding_steps ) [EOL] [EOL] def _update_recall ( self , all_top_k_predictions , target_tokens , target_recall ) : [EOL] targets = target_tokens [ [string] ] [EOL] target_mask = get_text_field_mask ( target_tokens ) [EOL] [comment] [EOL] [comment] [EOL] relevant_targets = targets [ : , [number] : ] . contiguous ( ) [EOL] relevant_mask = target_mask [ : , [number] : ] . contiguous ( ) [EOL] target_recall ( all_top_k_predictions , relevant_targets , relevant_mask , self . _end_index ) [EOL] [EOL] def _get_num_decoding_steps ( self , target_tokens ) : [EOL] if target_tokens : [EOL] targets = target_tokens [ [string] ] [EOL] target_sequence_length = targets . size ( ) [ [number] ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return target_sequence_length - [number] [EOL] else : [EOL] return self . _max_decoding_steps [EOL] [EOL] @ overrides def forward ( self , source , ** target_tokens ) : [EOL] [comment] [EOL] [docstring] [EOL] [comment] [EOL] embedded_input = self . _embedding_dropout ( self . _source_embedder ( source ) ) [EOL] source_mask = get_text_field_mask ( source ) [EOL] [comment] [EOL] final_encoder_output = self . _encoder ( embedded_input , source_mask ) [EOL] output_dict = { } [EOL] [EOL] [comment] [EOL] if target_tokens : [EOL] if target_tokens . keys ( ) != self . _states . keys ( ) : [EOL] target_only = target_tokens . keys ( ) - self . _states . keys ( ) [EOL] states_only = self . _states . keys ( ) - target_tokens . keys ( ) [EOL] raise Exception ( [string] + f" [string] { target_only } [string] { states_only }" ) [EOL] total_loss = [number] [EOL] for name , state in self . _states . items ( ) : [EOL] loss = self . greedy_search ( final_encoder_output = final_encoder_output , target_tokens = target_tokens [ name ] , target_embedder = state . embedder , decoder_cell = state . decoder_cell , output_projection_layer = state . output_projection_layer ) [EOL] total_loss += loss [EOL] output_dict [ f"{ name } [string] " ] = loss [EOL] [EOL] [comment] [EOL] output_dict [ [string] ] = total_loss / len ( self . _states ) [EOL] [EOL] [comment] [EOL] if not self . training : [EOL] batch_size = final_encoder_output . size ( ) [ [number] ] [EOL] for name , state in self . _states . items ( ) : [EOL] start_predictions = final_encoder_output . new_full ( ( batch_size , ) , fill_value = self . _start_index , dtype = torch . long ) [EOL] start_state = { [string] : final_encoder_output } [EOL] [EOL] [comment] [EOL] all_top_k_predictions , log_probabilities = self . _beam_search . search ( start_predictions , start_state , state . take_step ) [EOL] [EOL] if target_tokens : [EOL] self . _update_recall ( all_top_k_predictions , target_tokens [ name ] , state . recall ) [EOL] output_dict [ f"{ name } [string] " ] = all_top_k_predictions [EOL] output_dict [ f"{ name } [string] " ] = log_probabilities [EOL] [EOL] return output_dict [EOL] [EOL] def greedy_search ( self , final_encoder_output , target_tokens , target_embedder , decoder_cell , output_projection_layer ) : [EOL] [docstring] [EOL] num_decoding_steps = self . _get_num_decoding_steps ( target_tokens ) [EOL] targets = target_tokens [ [string] ] [EOL] decoder_hidden = final_encoder_output [EOL] step_logits = [ ] [EOL] for timestep in range ( num_decoding_steps ) : [EOL] [comment] [EOL] input_choices = targets [ : , timestep ] [EOL] decoder_input = target_embedder ( input_choices ) [EOL] decoder_hidden = decoder_cell ( decoder_input , decoder_hidden ) [EOL] [comment] [EOL] output_projections = output_projection_layer ( decoder_hidden ) [EOL] [comment] [EOL] step_logits . append ( output_projections . unsqueeze ( [number] ) ) [EOL] [comment] [EOL] logits = torch . cat ( step_logits , [number] ) [EOL] target_mask = get_text_field_mask ( target_tokens ) [EOL] return self . _get_loss ( logits , targets , target_mask ) [EOL] [EOL] def greedy_predict ( self , final_encoder_output , target_embedder , decoder_cell , output_projection_layer ) : [EOL] [docstring] [EOL] num_decoding_steps = self . _max_decoding_steps [EOL] decoder_hidden = final_encoder_output [EOL] batch_size = final_encoder_output . size ( ) [ [number] ] [EOL] predictions = [ final_encoder_output . new_full ( ( batch_size , ) , fill_value = self . _start_index , dtype = torch . long ) ] [EOL] for _ in range ( num_decoding_steps ) : [EOL] input_choices = predictions [ - [number] ] [EOL] decoder_input = target_embedder ( input_choices ) [EOL] decoder_hidden = decoder_cell ( decoder_input , decoder_hidden ) [EOL] [comment] [EOL] output_projections = output_projection_layer ( decoder_hidden ) [EOL] class_probabilities = F . softmax ( output_projections , dim = - [number] ) [EOL] _ , predicted_classes = torch . max ( class_probabilities , [number] ) [EOL] predictions . append ( predicted_classes ) [EOL] all_predictions = torch . cat ( [ ps . unsqueeze ( [number] ) for ps in predictions ] , [number] ) [EOL] [comment] [EOL] return all_predictions [ : , [number] : ] [EOL] [EOL] @ staticmethod def _get_loss ( logits , targets , target_mask ) : [EOL] [docstring] [EOL] relevant_targets = targets [ : , [number] : ] . contiguous ( ) [comment] [EOL] relevant_mask = target_mask [ : , [number] : ] . contiguous ( ) [comment] [EOL] loss = sequence_cross_entropy_with_logits ( logits , relevant_targets , relevant_mask ) [EOL] return loss [EOL] [EOL] def decode_all ( self , predicted_indices ) : [EOL] if not isinstance ( predicted_indices , numpy . ndarray ) : [EOL] predicted_indices = predicted_indices . detach ( ) . cpu ( ) . numpy ( ) [EOL] all_predicted_tokens = [ ] [EOL] for indices in predicted_indices : [EOL] indices = list ( indices ) [EOL] [comment] [EOL] if self . _end_index in indices : [EOL] indices = indices [ : indices . index ( self . _end_index ) ] [EOL] predicted_tokens = [ self . vocab . get_token_from_index ( x , namespace = self . _target_namespace ) for x in indices ] [EOL] all_predicted_tokens . append ( predicted_tokens ) [EOL] return all_predicted_tokens [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] for name in self . _states : [EOL] top_k_predicted_indices = output_dict [ f"{ name } [string] " ] [ [number] ] [EOL] output_dict [ f"{ name } [string] " ] = [ self . decode_all ( top_k_predicted_indices ) ] [EOL] [EOL] return output_dict [EOL] [EOL] @ overrides def get_metrics ( self , reset = False ) : [EOL] all_metrics = { } [EOL] [comment] [EOL] if not self . training : [EOL] for name , state in self . _states . items ( ) : [EOL] all_metrics [ name ] = state . recall . get_metric ( reset = reset ) [EOL] return all_metrics [EOL] [EOL] [EOL] class StateDecoder ( Module ) : [EOL] [comment] [EOL] [docstring] [EOL] def __init__ ( self , num_classes , input_dim , output_dim ) : [EOL] super ( ) . __init__ ( ) [EOL] self . embedder = Embedding ( num_classes , input_dim ) [EOL] self . decoder_cell = GRUCell ( input_dim , output_dim ) [EOL] self . output_projection_layer = Linear ( output_dim , num_classes ) [EOL] self . recall = UnigramRecall ( ) [EOL] [EOL] def take_step ( self , last_predictions , state ) : [EOL] decoder_hidden = state [ [string] ] [EOL] decoder_input = self . embedder ( last_predictions ) [EOL] decoder_hidden = self . decoder_cell ( decoder_input , decoder_hidden ) [EOL] state [ [string] ] = decoder_hidden [EOL] output_projections = self . output_projection_layer ( decoder_hidden ) [EOL] class_log_probabilities = F . log_softmax ( output_projections , dim = - [number] ) [EOL] return class_log_probabilities , state [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[torch.Tensor,typing.Dict[builtins.str,torch.Tensor]]$ 0 0 0 $torch.Tensor$ 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,torch.Tensor]$ 0
from typing import Any , List , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . modules import Seq2SeqEncoder , Seq2VecEncoder , TextFieldEmbedder [EOL] from allennlp . nn import InitializerApplicator [EOL] from allennlp . nn . util import get_text_field_mask [EOL] from allennlp . training . metrics import CategoricalAccuracy [EOL] [EOL] [EOL] @ Model . register ( [string] ) class BasicClassifier ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab , text_field_embedder , seq2vec_encoder , seq2seq_encoder = None , dropout = None , num_labels = None , label_namespace = [string] , initializer = InitializerApplicator ( ) ) : [EOL] [EOL] super ( ) . __init__ ( vocab ) [EOL] self . _text_field_embedder = text_field_embedder [EOL] [EOL] if seq2seq_encoder : [EOL] self . _seq2seq_encoder = seq2seq_encoder [EOL] else : [EOL] self . _seq2seq_encoder = None [EOL] [EOL] self . _seq2vec_encoder = seq2vec_encoder [EOL] self . _classifier_input_dim = self . _seq2vec_encoder . get_output_dim ( ) [EOL] [EOL] if dropout : [EOL] self . _dropout = torch . nn . Dropout ( dropout ) [EOL] else : [EOL] self . _dropout = None [EOL] [EOL] self . _label_namespace = label_namespace [EOL] [EOL] if num_labels : [EOL] self . _num_labels = num_labels [EOL] else : [EOL] self . _num_labels = vocab . get_vocab_size ( namespace = self . _label_namespace ) [EOL] self . _classification_layer = torch . nn . Linear ( self . _classifier_input_dim , self . _num_labels ) [EOL] self . _accuracy = CategoricalAccuracy ( ) [EOL] self . _loss = torch . nn . CrossEntropyLoss ( ) [EOL] initializer ( self ) [EOL] [EOL] def forward ( self , tokens , label = None ) : [EOL] [comment] [EOL] [docstring] [EOL] embedded_text = self . _text_field_embedder ( tokens ) [EOL] mask = get_text_field_mask ( tokens ) . float ( ) [EOL] [EOL] if self . _seq2seq_encoder : [EOL] embedded_text = self . _seq2seq_encoder ( embedded_text , mask = mask ) [EOL] [EOL] embedded_text = self . _seq2vec_encoder ( embedded_text , mask = mask ) [EOL] [EOL] if self . _dropout : [EOL] embedded_text = self . _dropout ( embedded_text ) [EOL] [EOL] logits = self . _classification_layer ( embedded_text ) [EOL] probs = torch . nn . functional . softmax ( logits , dim = - [number] ) [EOL] [EOL] output_dict = { [string] : logits , [string] : probs } [EOL] [EOL] if label is not None : [EOL] loss = self . _loss ( logits , label . long ( ) . view ( - [number] ) ) [EOL] output_dict [ [string] ] = loss [EOL] self . _accuracy ( logits , label ) [EOL] [EOL] return output_dict [EOL] [EOL] @ overrides def decode ( self , output_dict ) : [EOL] [docstring] [EOL] predictions = output_dict [ [string] ] [EOL] if predictions . dim ( ) == [number] : [EOL] predictions_list = [ predictions [ i ] for i in range ( predictions . shape [ [number] ] ) ] [EOL] else : [EOL] predictions_list = [ predictions ] [EOL] classes = [ ] [EOL] for prediction in predictions_list : [EOL] label_idx = prediction . argmax ( dim = - [number] ) . item ( ) [EOL] label_str = ( self . vocab . get_index_to_token_vocabulary ( self . _label_namespace ) . get ( label_idx , str ( label_idx ) ) ) [EOL] classes . append ( label_str ) [EOL] output_dict [ [string] ] = classes [EOL] return output_dict [EOL] [EOL] def get_metrics ( self , reset = False ) : [EOL] metrics = { [string] : self . _accuracy . get_metric ( reset ) } [EOL] return metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0
from allennlp . models . encoder_decoders . simple_seq2seq import SimpleSeq2Seq [EOL] from allennlp . models . encoder_decoders . copynet_seq2seq import CopyNetSeq2Seq [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . models . coreference_resolution . coref import CoreferenceResolver [EOL]	0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
	0
from typing import Union , List , Optional , Literal , Any , Tuple [EOL] import typing [EOL] import torch [EOL] import typing_extensions [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import Optional , Tuple , Union [EOL] import torch [EOL] from torch . nn . utils . rnn import PackedSequence [EOL] from allennlp . modules . augmented_lstm import AugmentedLstm [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] class StackedAlternatingLstm ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , input_size , hidden_size , num_layers , recurrent_dropout_probability = [number] , use_highway = True , use_input_projection_bias = True ) : [EOL] super ( StackedAlternatingLstm , self ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] self . input_size = input_size [EOL] self . hidden_size = hidden_size [EOL] self . num_layers = num_layers [EOL] [EOL] layers = [ ] [EOL] lstm_input_size = input_size [EOL] for layer_index in range ( num_layers ) : [EOL] go_forward = True if layer_index % [number] == [number] else False [EOL] layer = AugmentedLstm ( lstm_input_size , hidden_size , go_forward , recurrent_dropout_probability = recurrent_dropout_probability , use_highway = use_highway , use_input_projection_bias = use_input_projection_bias ) [EOL] lstm_input_size = hidden_size [EOL] self . add_module ( [string] . format ( layer_index ) , layer ) [EOL] layers . append ( layer ) [EOL] self . lstm_layers = layers [EOL] [EOL] def forward ( self , inputs , initial_state = None ) : [EOL] [docstring] [EOL] if not initial_state : [EOL] hidden_states = [ None ] * len ( self . lstm_layers ) [EOL] elif initial_state [ [number] ] . size ( ) [ [number] ] != len ( self . lstm_layers ) : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] else : [EOL] hidden_states = list ( zip ( initial_state [ [number] ] . split ( [number] , [number] ) , initial_state [ [number] ] . split ( [number] , [number] ) ) ) [EOL] [EOL] output_sequence = inputs [EOL] final_states = [ ] [EOL] for i , state in enumerate ( hidden_states ) : [EOL] layer = getattr ( self , [string] . format ( i ) ) [EOL] [comment] [EOL] output_sequence , final_state = layer ( output_sequence , state ) [EOL] final_states . append ( final_state ) [EOL] [EOL] final_hidden_state , final_cell_state = tuple ( torch . cat ( state_list , [number] ) for state_list in zip ( * final_states ) ) [EOL] return output_sequence , ( final_hidden_state , final_cell_state ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $typing.Union[typing_extensions.Literal[False],typing_extensions.Literal[True]]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Tuple[typing.Union[torch.Tensor,torch.nn.utils.rnn.PackedSequence],typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 $torch.nn.utils.rnn.PackedSequence$ 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Tuple[torch.Tensor,torch.Tensor]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.nn.utils.rnn.PackedSequence$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] [EOL] class TimeDistributed ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , module ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _module = module [EOL] [EOL] @ overrides def forward ( self , * inputs , pass_through = None , ** kwargs ) : [EOL] [comment] [EOL] pass_through = pass_through or [ ] [EOL] [EOL] reshaped_inputs = [ self . _reshape_tensor ( input_tensor ) for input_tensor in inputs ] [EOL] [EOL] [comment] [EOL] some_input = None [EOL] if inputs : [EOL] some_input = inputs [ - [number] ] [EOL] [EOL] reshaped_kwargs = { } [EOL] for key , value in kwargs . items ( ) : [EOL] if isinstance ( value , torch . Tensor ) and key not in pass_through : [EOL] if some_input is None : [EOL] some_input = value [EOL] [EOL] value = self . _reshape_tensor ( value ) [EOL] [EOL] reshaped_kwargs [ key ] = value [EOL] [EOL] reshaped_outputs = self . _module ( * reshaped_inputs , ** reshaped_kwargs ) [EOL] [EOL] if some_input is None : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] new_size = some_input . size ( ) [ : [number] ] + reshaped_outputs . size ( ) [ [number] : ] [EOL] outputs = reshaped_outputs . contiguous ( ) . view ( new_size ) [EOL] [EOL] return outputs [EOL] [EOL] @ staticmethod def _reshape_tensor ( input_tensor ) : [EOL] input_size = input_tensor . size ( ) [EOL] if len ( input_size ) <= [number] : [EOL] raise RuntimeError ( f" [string] { input_size }" ) [EOL] [comment] [EOL] [comment] [EOL] squashed_shape = [ - [number] ] + list ( input_size [ [number] : ] ) [EOL] return input_tensor . contiguous ( ) . view ( * squashed_shape ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0
import torch [EOL] import builtins [EOL] import torch [EOL] [EOL] [EOL] class ResidualWithLayerDropout ( torch . nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , undecayed_dropout_prob = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] if undecayed_dropout_prob < [number] or undecayed_dropout_prob > [number] : [EOL] raise ValueError ( f" [string] " f" [string] { undecayed_dropout_prob }" ) [EOL] self . undecayed_dropout_prob = undecayed_dropout_prob [EOL] [EOL] def forward ( self , layer_input , layer_output , layer_index = None , total_layers = None ) : [EOL] [comment] [EOL] [docstring] [EOL] if layer_index is not None and total_layers is not None : [EOL] dropout_prob = [number] * self . undecayed_dropout_prob * layer_index / total_layers [EOL] else : [EOL] dropout_prob = [number] * self . undecayed_dropout_prob [EOL] if self . training : [EOL] if torch . rand ( [number] ) < dropout_prob : [EOL] return layer_input [EOL] else : [EOL] return layer_output + layer_input [EOL] else : [EOL] return ( [number] - dropout_prob ) * layer_output + layer_input [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0
[docstring] [EOL] [EOL] from allennlp . modules . conditional_random_field import ConditionalRandomField [EOL] from allennlp . modules . elmo import Elmo [EOL] from allennlp . modules . feedforward import FeedForward [EOL] from allennlp . modules . highway import Highway [EOL] from allennlp . modules . layer_norm import LayerNorm [EOL] from allennlp . modules . maxout import Maxout [EOL] from allennlp . modules . scalar_mix import ScalarMix [EOL] from allennlp . modules . seq2seq_encoders import Seq2SeqEncoder [EOL] from allennlp . modules . seq2vec_encoders import Seq2VecEncoder [EOL] from allennlp . modules . similarity_functions import SimilarityFunction [EOL] from allennlp . modules . pruner import Pruner [EOL] from allennlp . modules . text_field_embedders import TextFieldEmbedder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders import TokenEmbedder , Embedding [EOL] from allennlp . modules . matrix_attention import MatrixAttention [EOL] from allennlp . modules . attention import Attention [EOL] from allennlp . modules . input_variational_dropout import InputVariationalDropout [EOL] from allennlp . modules . bimpm_matching import BiMpmMatching [EOL] from allennlp . modules . residual_with_layer_dropout import ResidualWithLayerDropout [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] from overrides import overrides [EOL] [EOL] from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . token_embedders . embedding import Embedding [EOL] from allennlp . nn import util [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] @ SpanExtractor . register ( [string] ) class BidirectionalEndpointSpanExtractor ( SpanExtractor ) : [EOL] [docstring] [EOL] def __init__ ( self , input_dim , forward_combination = [string] , backward_combination = [string] , num_width_embeddings = None , span_width_embedding_dim = None , bucket_widths = False , use_sentinels = True ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _input_dim = input_dim [EOL] self . _forward_combination = forward_combination [EOL] self . _backward_combination = backward_combination [EOL] self . _num_width_embeddings = num_width_embeddings [EOL] self . _bucket_widths = bucket_widths [EOL] [EOL] if self . _input_dim % [number] != [number] : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] if num_width_embeddings is not None and span_width_embedding_dim is not None : [EOL] self . _span_width_embedding = Embedding ( num_width_embeddings , span_width_embedding_dim ) [EOL] elif not all ( [ num_width_embeddings is None , span_width_embedding_dim is None ] ) : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] else : [EOL] self . _span_width_embedding = None [EOL] [EOL] self . _use_sentinels = use_sentinels [EOL] if use_sentinels : [EOL] self . _start_sentinel = Parameter ( torch . randn ( [ [number] , [number] , int ( input_dim / [number] ) ] ) ) [EOL] self . _end_sentinel = Parameter ( torch . randn ( [ [number] , [number] , int ( input_dim / [number] ) ] ) ) [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . _input_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] unidirectional_dim = int ( self . _input_dim / [number] ) [EOL] forward_combined_dim = util . get_combined_dim ( self . _forward_combination , [ unidirectional_dim , unidirectional_dim ] ) [EOL] backward_combined_dim = util . get_combined_dim ( self . _backward_combination , [ unidirectional_dim , unidirectional_dim ] ) [EOL] if self . _span_width_embedding is not None : [EOL] return forward_combined_dim + backward_combined_dim + self . _span_width_embedding . get_output_dim ( ) [EOL] return forward_combined_dim + backward_combined_dim [EOL] [EOL] @ overrides def forward ( self , sequence_tensor , span_indices , sequence_mask = None , span_indices_mask = None ) : [EOL] [EOL] [comment] [EOL] forward_sequence , backward_sequence = sequence_tensor . split ( int ( self . _input_dim / [number] ) , dim = - [number] ) [EOL] forward_sequence = forward_sequence . contiguous ( ) [EOL] backward_sequence = backward_sequence . contiguous ( ) [EOL] [EOL] [comment] [EOL] span_starts , span_ends = [ index . squeeze ( - [number] ) for index in span_indices . split ( [number] , dim = - [number] ) ] [EOL] [EOL] if span_indices_mask is not None : [EOL] span_starts = span_starts * span_indices_mask [EOL] span_ends = span_ends * span_indices_mask [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclusive_span_starts = span_starts - [number] [EOL] [comment] [EOL] start_sentinel_mask = ( exclusive_span_starts == - [number] ) . long ( ) . unsqueeze ( - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclusive_span_ends = span_ends + [number] [EOL] [EOL] if sequence_mask is not None : [EOL] [comment] [EOL] sequence_lengths = util . get_lengths_from_binary_sequence_mask ( sequence_mask ) [EOL] else : [EOL] [comment] [EOL] sequence_lengths = ( torch . ones_like ( sequence_tensor [ : , [number] , [number] ] , dtype = torch . long ) * sequence_tensor . size ( [number] ) ) [EOL] [EOL] [comment] [EOL] end_sentinel_mask = ( exclusive_span_ends >= sequence_lengths . unsqueeze ( - [number] ) ) . long ( ) . unsqueeze ( - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclusive_span_ends = exclusive_span_ends * ( [number] - end_sentinel_mask . squeeze ( - [number] ) ) [EOL] exclusive_span_starts = exclusive_span_starts * ( [number] - start_sentinel_mask . squeeze ( - [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if ( exclusive_span_starts < [number] ) . any ( ) or ( exclusive_span_ends > sequence_lengths . unsqueeze ( - [number] ) ) . any ( ) : [EOL] raise ValueError ( f" [string] " f" [string] { exclusive_span_starts } [string] " f" [string] { exclusive_span_ends } [string] " f"{ sequence_lengths } [string] " ) [EOL] [EOL] [comment] [EOL] forward_start_embeddings = util . batched_index_select ( forward_sequence , exclusive_span_starts ) [EOL] [comment] [EOL] [comment] [EOL] forward_end_embeddings = util . batched_index_select ( forward_sequence , span_ends ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] backward_start_embeddings = util . batched_index_select ( backward_sequence , exclusive_span_ends ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] backward_end_embeddings = util . batched_index_select ( backward_sequence , span_starts ) [EOL] [EOL] if self . _use_sentinels : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] float_end_sentinel_mask = end_sentinel_mask . float ( ) [EOL] float_start_sentinel_mask = start_sentinel_mask . float ( ) [EOL] forward_start_embeddings = forward_start_embeddings * ( [number] - float_start_sentinel_mask ) + float_start_sentinel_mask * self . _start_sentinel [EOL] backward_start_embeddings = backward_start_embeddings * ( [number] - float_end_sentinel_mask ) + float_end_sentinel_mask * self . _end_sentinel [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] forward_spans = util . combine_tensors ( self . _forward_combination , [ forward_start_embeddings , forward_end_embeddings ] ) [EOL] [comment] [EOL] backward_spans = util . combine_tensors ( self . _backward_combination , [ backward_start_embeddings , backward_end_embeddings ] ) [EOL] [comment] [EOL] span_embeddings = torch . cat ( [ forward_spans , backward_spans ] , - [number] ) [EOL] [EOL] if self . _span_width_embedding is not None : [EOL] [comment] [EOL] if self . _bucket_widths : [EOL] span_widths = util . bucket_values ( span_ends - span_starts , num_total_buckets = self . _num_width_embeddings ) [EOL] else : [EOL] span_widths = span_ends - span_starts [EOL] [EOL] span_width_embeddings = self . _span_width_embedding ( span_widths ) [EOL] return torch . cat ( [ span_embeddings , span_width_embeddings ] , - [number] ) [EOL] [EOL] if span_indices_mask is not None : [EOL] return span_embeddings * span_indices_mask . float ( ) . unsqueeze ( - [number] ) [EOL] return span_embeddings [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . span_extractors . endpoint_span_extractor import EndpointSpanExtractor [EOL] from allennlp . modules . span_extractors . self_attentive_span_extractor import SelfAttentiveSpanExtractor [EOL] from allennlp . modules . span_extractors . bidirectional_endpoint_span_extractor import BidirectionalEndpointSpanExtractor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Union [EOL] import typing [EOL] import torch [EOL] import pytorch_pretrained_bert [EOL] import builtins [EOL] from typing import Union [EOL] [EOL] from overrides import overrides [EOL] [EOL] import torch [EOL] from pytorch_pretrained_bert import BertModel [EOL] [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] from allennlp . modules . token_embedders . bert_token_embedder import PretrainedBertModel [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class BertPooler ( Seq2VecEncoder ) : [EOL] [docstring] [EOL] def __init__ ( self , pretrained_model , requires_grad = True ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] if isinstance ( pretrained_model , str ) : [EOL] model = PretrainedBertModel . load ( pretrained_model ) [EOL] else : [EOL] model = pretrained_model [EOL] [EOL] self . pooler = model . pooler [EOL] for param in self . pooler . parameters ( ) : [EOL] param . requires_grad = requires_grad [EOL] self . _embedding_dim = model . config . hidden_size [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] def forward ( self , tokens , mask = None ) : [comment] [EOL] return self . pooler ( tokens ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Union[builtins.str,pytorch_pretrained_bert.BertModel]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.str,pytorch_pretrained_bert.BertModel]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.str,pytorch_pretrained_bert.BertModel]$ 0 0 0 0 0 0 0 $typing.Union[builtins.str,pytorch_pretrained_bert.BertModel]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0
from typing import Optional , Any , Literal [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import typing_extensions [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . modules . token_embedders . embedding import Embedding [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class TokenCharactersEncoder ( TokenEmbedder ) : [EOL] [docstring] [EOL] def __init__ ( self , embedding , encoder , dropout = [number] ) : [EOL] super ( TokenCharactersEncoder , self ) . __init__ ( ) [EOL] self . _embedding = TimeDistributed ( embedding ) [EOL] self . _encoder = TimeDistributed ( encoder ) [EOL] if dropout > [number] : [EOL] self . _dropout = torch . nn . Dropout ( p = dropout ) [EOL] else : [EOL] self . _dropout = lambda x : x [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _encoder . _module . get_output_dim ( ) [comment] [EOL] [EOL] def forward ( self , token_characters ) : [comment] [EOL] mask = ( token_characters != [number] ) . long ( ) [EOL] return self . _dropout ( self . _encoder ( self . _embedding ( token_characters ) , mask ) ) [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [comment] [EOL] [comment] [EOL] embedding_params = params . pop ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] default_namespace = None if embedding_params . get ( [string] , None ) else [string] [EOL] embedding_params . setdefault ( [string] , default_namespace ) [EOL] embedding = Embedding . from_params ( vocab , embedding_params ) [EOL] encoder_params = params . pop ( [string] ) [EOL] encoder = Seq2VecEncoder . from_params ( encoder_params ) [EOL] dropout = params . pop_float ( [string] , [number] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( embedding , encoder , dropout ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'TokenCharactersEncoder'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import List , Dict [EOL] import torch [EOL] [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import check_dimensions_match , ConfigurationError [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] from allennlp . modules . elmo import Elmo [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . data import Vocabulary [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class ElmoTokenEmbedderMultiLang ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , options_files , weight_files , do_layer_norm = False , dropout = [number] , requires_grad = False , projection_dim = None , vocab_to_cache = None , scalar_mix_parameters = None , aligning_files = None ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] if options_files . keys ( ) != weight_files . keys ( ) : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] aligning_files = aligning_files or { } [EOL] output_dim = None [EOL] for lang in weight_files . keys ( ) : [EOL] name = [string] % lang [EOL] elmo = Elmo ( options_files [ lang ] , weight_files [ lang ] , num_output_representations = [number] , do_layer_norm = do_layer_norm , dropout = dropout , requires_grad = requires_grad , vocab_to_cache = vocab_to_cache , scalar_mix_parameters = scalar_mix_parameters ) [EOL] self . add_module ( name , elmo ) [EOL] [EOL] output_dim_tmp = elmo . get_output_dim ( ) [EOL] if output_dim is not None : [EOL] [comment] [EOL] check_dimensions_match ( output_dim_tmp , output_dim , [string] % name , [string] ) [EOL] [EOL] output_dim = output_dim_tmp [EOL] [EOL] self . output_dim = output_dim [EOL] [EOL] if projection_dim : [EOL] self . _projection = torch . nn . Linear ( output_dim , projection_dim ) [EOL] self . output_dim = projection_dim [EOL] else : [EOL] self . _projection = None [EOL] [EOL] for lang in weight_files . keys ( ) : [EOL] name = [string] % lang [EOL] aligning_matrix = torch . eye ( output_dim ) [EOL] if lang in aligning_files and aligning_files [ lang ] != [string] : [EOL] aligninig_path = cached_path ( aligning_files [ lang ] ) [EOL] aligning_matrix = torch . FloatTensor ( torch . load ( aligninig_path ) ) [EOL] [EOL] aligning = torch . nn . Linear ( output_dim , output_dim , bias = False ) [EOL] aligning . weight = torch . nn . Parameter ( aligning_matrix , requires_grad = False ) [EOL] self . add_module ( name , aligning ) [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] def forward ( self , inputs , lang , word_inputs = None ) : [EOL] [docstring] [EOL] elmo = getattr ( self , [string] . format ( lang ) ) [EOL] elmo_output = elmo ( inputs , word_inputs ) [EOL] elmo_representations = elmo_output [ [string] ] [ [number] ] [EOL] aligning = getattr ( self , [string] . format ( lang ) ) [EOL] elmo_representations = aligning ( elmo_representations ) [EOL] if self . _projection : [EOL] projection = self . _projection [EOL] for _ in range ( elmo_representations . dim ( ) - [number] ) : [EOL] projection = TimeDistributed ( projection ) [EOL] elmo_representations = projection ( elmo_representations ) [EOL] return elmo_representations [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [EOL] [comment] [EOL] options_files = params . pop ( [string] ) [EOL] weight_files = params . pop ( [string] ) [EOL] for lang in options_files . keys ( ) : [EOL] options_files . add_file_to_archive ( lang ) [EOL] for lang in weight_files . keys ( ) : [EOL] weight_files . add_file_to_archive ( lang ) [EOL] requires_grad = params . pop ( [string] , False ) [EOL] do_layer_norm = params . pop_bool ( [string] , False ) [EOL] dropout = params . pop_float ( [string] , [number] ) [EOL] namespace_to_cache = params . pop ( [string] , None ) [EOL] if namespace_to_cache is not None : [EOL] vocab_to_cache = list ( vocab . get_token_to_index_vocabulary ( namespace_to_cache ) . keys ( ) ) [EOL] else : [EOL] vocab_to_cache = None [EOL] projection_dim = params . pop_int ( [string] , None ) [EOL] scalar_mix_parameters = params . pop ( [string] , None ) [EOL] aligning_files = params . pop ( [string] , { } ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( options_files = options_files , weight_files = weight_files , do_layer_norm = do_layer_norm , dropout = dropout , requires_grad = requires_grad , projection_dim = projection_dim , vocab_to_cache = vocab_to_cache , scalar_mix_parameters = scalar_mix_parameters , aligning_files = aligning_files ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $builtins.str$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $'ElmoTokenEmbedderMultiLang'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 $allennlp.data.Vocabulary$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $None$ 0 $None$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
import builtins [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class TokenEmbedder ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def get_output_dim ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . text_field_embedders . basic_text_field_embedder import BasicTextFieldEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . modules . attention . legacy_attention import Attention [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class CosineAttention ( Attention ) : [EOL] [docstring] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] a_norm = vector / ( vector . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] b_norm = matrix / ( matrix . norm ( p = [number] , dim = - [number] , keepdim = True ) + [number] ) [EOL] return torch . bmm ( a_norm . unsqueeze ( dim = [number] ) , b_norm . transpose ( - [number] , - [number] ) ) . squeeze ( [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] import argparse [EOL] [docstring] [EOL] import argparse [EOL] [EOL] class Subcommand : [EOL] [docstring] [EOL] def add_subparser ( self , name , parser ) : [EOL] [comment] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $builtins.str$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import argparse [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import logging [EOL] import os [EOL] import pathlib [EOL] [EOL] import pytest [EOL] [EOL] import allennlp [EOL] from allennlp . commands . subcommand import Subcommand [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] class TestInstall ( Subcommand ) : [EOL] def add_subparser ( self , name , parser ) : [EOL] [comment] [EOL] description = [string] [EOL] subparser = parser . add_parser ( name , description = description , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , action = [string] , help = [string] [string] ) [EOL] subparser . add_argument ( [string] , type = str , default = None , help = [string] ) [EOL] [EOL] subparser . set_defaults ( func = _run_test ) [EOL] [EOL] return subparser [EOL] [EOL] [EOL] def _get_module_root ( ) : [EOL] return pathlib . Path ( allennlp . __file__ ) . parent [EOL] [EOL] [EOL] def _run_test ( args ) : [EOL] initial_working_dir = os . getcwd ( ) [EOL] module_parent = _get_module_root ( ) . parent [EOL] logger . info ( [string] , module_parent ) [EOL] os . chdir ( module_parent ) [EOL] test_dir = os . path . join ( module_parent , [string] ) [EOL] logger . info ( [string] , test_dir ) [EOL] [EOL] if args . k : [EOL] pytest_k = [ [string] , args . k ] [EOL] pytest_m = [ [string] , [string] ] [EOL] if args . run_all : [EOL] logger . warning ( [string] ) [EOL] elif args . run_all : [EOL] pytest_k = [ ] [EOL] pytest_m = [ ] [EOL] else : [EOL] pytest_k = [ [string] , [string] ] [EOL] pytest_m = [ [string] , [string] ] [EOL] [EOL] exit_code = pytest . main ( [ test_dir , [string] ] + pytest_k + pytest_m ) [EOL] [EOL] [comment] [EOL] os . chdir ( initial_working_dir ) [EOL] exit ( exit_code ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $builtins.str$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . nlvr_parser import NlvrParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . wikitables_parser import WikiTablesParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . simple_seq2seq import SimpleSeq2SeqPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . constituency_parser import ConstituencyParserPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . sentence_tagger import SentenceTaggerPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . coref import CorefPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . bidaf import BidafPredictor [EOL] [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . semantic_role_labeler import SemanticRoleLabelerPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . decomposable_attention import DecomposableAttentionPredictor [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] import warnings [EOL] [EOL] from allennlp . predictors . predictor import Predictor , DEFAULT_PREDICTORS [EOL] warnings . warn ( [string] [string] , FutureWarning ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Tuple , Dict [EOL] import typing [EOL] import torch [EOL] import numpy [EOL] import builtins [EOL] from typing import Dict [EOL] [EOL] import numpy [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . fields . field import Field [EOL] [EOL] [EOL] class ArrayField ( Field [ numpy . ndarray ] ) : [EOL] [docstring] [EOL] def __init__ ( self , array , padding_value = [number] , dtype = numpy . float32 ) : [EOL] self . array = array [EOL] self . padding_value = padding_value [EOL] self . dtype = dtype [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] return { [string] + str ( i ) : shape for i , shape in enumerate ( self . array . shape ) } [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] max_shape = [ padding_lengths [ [string] . format ( i ) ] for i in range ( len ( padding_lengths ) ) ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return_array = numpy . asarray ( numpy . ones ( max_shape , dtype = self . dtype ) * self . padding_value , dtype = self . dtype ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] slicing_shape = list ( self . array . shape ) [EOL] if len ( self . array . shape ) < len ( max_shape ) : [EOL] slicing_shape = slicing_shape + [ [number] for _ in range ( len ( max_shape ) - len ( self . array . shape ) ) ] [EOL] slices = tuple ( [ slice ( [number] , x ) for x in slicing_shape ] ) [EOL] return_array [ slices ] = self . array [EOL] tensor = torch . from_numpy ( return_array ) [EOL] return tensor [EOL] [EOL] @ overrides def empty_field ( self ) : [comment] [EOL] [comment] [EOL] [comment] [EOL] return ArrayField ( numpy . array ( [ ] , dtype = self . dtype ) , padding_value = self . padding_value , dtype = self . dtype ) [EOL] [EOL] def __str__ ( self ) : [EOL] return f" [string] { self . array . shape } [string] { self . dtype } [string] " [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.slice,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Tuple[builtins.slice,...]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Any , Set , Callable [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] [docstring] [EOL] from typing import Callable , Dict , List , Set [EOL] from collections import defaultdict [EOL] [EOL] import editdistance [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common import util [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data . fields . field import Field [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer , TokenType [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . nn import util as nn_util [EOL] from allennlp . semparse . contexts . knowledge_graph import KnowledgeGraph [EOL] [EOL] TokenList = List [ TokenType ] [comment] [EOL] [EOL] [EOL] class KnowledgeGraphField ( Field [ Dict [ str , torch . Tensor ] ] ) : [EOL] [docstring] [EOL] def __init__ ( self , knowledge_graph , utterance_tokens , token_indexers , tokenizer = None , feature_extractors = None , entity_tokens = None , linking_features = None , include_in_vocab = True , max_table_tokens = None ) : [EOL] self . knowledge_graph = knowledge_graph [EOL] if not entity_tokens : [EOL] entity_texts = [ knowledge_graph . entity_text [ entity ] . lower ( ) for entity in knowledge_graph . entities ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . entity_texts = tokenizer . batch_tokenize ( entity_texts ) [EOL] else : [EOL] self . entity_texts = entity_tokens [EOL] self . utterance_tokens = utterance_tokens [EOL] self . _token_indexers = token_indexers [EOL] self . _include_in_vocab = include_in_vocab [EOL] self . _indexed_entity_texts = None [EOL] self . _max_table_tokens = max_table_tokens [EOL] [EOL] feature_extractors = feature_extractors if feature_extractors is not None else [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] self . _feature_extractors = [ ] [EOL] for feature_extractor_name in feature_extractors : [EOL] extractor = getattr ( self , [string] + feature_extractor_name , None ) [EOL] if not extractor : [EOL] raise ConfigurationError ( f" [string] { feature_extractor_name }" ) [EOL] self . _feature_extractors . append ( extractor ) [EOL] [EOL] if not linking_features : [EOL] [comment] [EOL] [comment] [EOL] self . _entity_text_map = { } [EOL] for entity , entity_text in zip ( knowledge_graph . entities , self . entity_texts ) : [EOL] self . _entity_text_map [ entity ] = entity_text [EOL] [EOL] self . _entity_text_exact_text = { } [EOL] for entity , entity_text in zip ( knowledge_graph . entities , self . entity_texts ) : [EOL] self . _entity_text_exact_text [ entity ] = set ( e . text for e in entity_text ) [EOL] [EOL] self . _entity_text_lemmas = { } [EOL] for entity , entity_text in zip ( knowledge_graph . entities , self . entity_texts ) : [EOL] self . _entity_text_lemmas [ entity ] = set ( e . lemma_ for e in entity_text ) [EOL] self . linking_features = self . _compute_linking_features ( ) [EOL] else : [EOL] self . linking_features = linking_features [EOL] [EOL] @ overrides def count_vocab_items ( self , counter ) : [EOL] if self . _include_in_vocab : [EOL] for indexer in self . _token_indexers . values ( ) : [EOL] for entity_text in self . entity_texts : [EOL] for token in entity_text : [EOL] indexer . count_vocab_items ( token , counter ) [EOL] [EOL] @ overrides def index ( self , vocab ) : [EOL] self . _indexed_entity_texts = { } [EOL] for indexer_name , indexer in self . _token_indexers . items ( ) : [EOL] indexer_arrays = defaultdict ( list ) [EOL] [EOL] for entity_text in self . entity_texts : [EOL] for index_name , indexed in indexer . tokens_to_indices ( entity_text , vocab , indexer_name ) . items ( ) : [EOL] indexer_arrays [ index_name ] . append ( indexed ) [EOL] [EOL] self . _indexed_entity_texts . update ( indexer_arrays ) [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] num_entities = len ( self . entity_texts ) [EOL] num_entity_tokens = max ( len ( entity_text ) for entity_text in self . entity_texts ) [EOL] [EOL] if self . _max_table_tokens : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if num_entities * num_entity_tokens > self . _max_table_tokens : [EOL] num_entity_tokens = int ( self . _max_table_tokens / num_entities ) [EOL] [EOL] padding_lengths = { [string] : num_entities , [string] : len ( self . utterance_tokens ) } [EOL] padding_lengths [ [string] ] = num_entity_tokens [EOL] lengths = [ ] [EOL] assert self . _indexed_entity_texts is not None , ( [string] [string] [string] ) [EOL] for indexer_name , indexer in self . _token_indexers . items ( ) : [EOL] indexer_lengths = { } [EOL] [EOL] [comment] [EOL] entity_lengths = [ indexer . get_padding_lengths ( token ) for entity_text in self . _indexed_entity_texts [ indexer_name ] for token in entity_text ] [EOL] [comment] [EOL] [comment] [EOL] for key in entity_lengths [ [number] ] . keys ( ) : [EOL] indexer_lengths [ key ] = max ( x [ key ] if key in x else [number] for x in entity_lengths ) [EOL] lengths . append ( indexer_lengths ) [EOL] [EOL] [comment] [EOL] padding_keys = { key for d in lengths for key in d . keys ( ) } [EOL] for padding_key in padding_keys : [EOL] padding_lengths [ padding_key ] = max ( x [ padding_key ] if padding_key in x else [number] for x in lengths ) [EOL] return padding_lengths [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] tensors = { } [EOL] desired_num_entities = padding_lengths [ [string] ] [EOL] desired_num_entity_tokens = padding_lengths [ [string] ] [EOL] desired_num_utterance_tokens = padding_lengths [ [string] ] [EOL] for indexer_name , indexer in self . _token_indexers . items ( ) : [EOL] padded_entities = util . pad_sequence_to_length ( self . _indexed_entity_texts [ indexer_name ] , desired_num_entities , default_value = lambda : [ ] ) [EOL] padded_arrays = [ ] [EOL] for padded_entity in padded_entities : [EOL] padded_array = indexer . pad_token_sequence ( { [string] : padded_entity } , { [string] : desired_num_entity_tokens } , padding_lengths ) [ [string] ] [EOL] padded_arrays . append ( padded_array ) [EOL] tensor = torch . LongTensor ( padded_arrays ) [EOL] tensors [ indexer_name ] = tensor [EOL] padded_linking_features = util . pad_sequence_to_length ( self . linking_features , desired_num_entities , default_value = lambda : [ ] ) [EOL] padded_linking_arrays = [ ] [EOL] default_feature_value = lambda : [ [number] ] * len ( self . _feature_extractors ) [EOL] for linking_features in padded_linking_features : [EOL] padded_features = util . pad_sequence_to_length ( linking_features , desired_num_utterance_tokens , default_value = default_feature_value ) [EOL] padded_linking_arrays . append ( padded_features ) [EOL] linking_features_tensor = torch . FloatTensor ( padded_linking_arrays ) [EOL] return { [string] : tensors , [string] : linking_features_tensor } [EOL] [EOL] def _compute_linking_features ( self ) : [EOL] linking_features = [ ] [EOL] for entity , entity_text in zip ( self . knowledge_graph . entities , self . entity_texts ) : [EOL] entity_features = [ ] [EOL] for token_index , token in enumerate ( self . utterance_tokens ) : [EOL] token_features = [ ] [EOL] for feature_extractor in self . _feature_extractors : [EOL] token_features . append ( feature_extractor ( entity , entity_text , token , token_index , self . utterance_tokens ) ) [EOL] entity_features . append ( token_features ) [EOL] linking_features . append ( entity_features ) [EOL] return linking_features [EOL] [EOL] @ overrides def empty_field ( self ) : [EOL] return KnowledgeGraphField ( KnowledgeGraph ( set ( ) , { } ) , [ ] , self . _token_indexers ) [EOL] [EOL] @ overrides def batch_tensors ( self , tensor_list ) : [EOL] [comment] [EOL] batched_text = nn_util . batch_tensor_dicts ( tensor [ [string] ] for tensor in tensor_list ) [comment] [EOL] batched_linking = torch . stack ( [ tensor [ [string] ] for tensor in tensor_list ] ) [EOL] return { [string] : batched_text , [string] : batched_linking } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def _number_token_match ( self , entity , entity_text , token , token_index , tokens ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if [string] in entity : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return [number] [EOL] return self . _contains_exact_token_match ( entity , entity_text , token , token_index , tokens ) [EOL] [EOL] def _exact_token_match ( self , entity , entity_text , token , token_index , tokens ) : [EOL] if len ( entity_text ) != [number] : [EOL] return [number] [EOL] return self . _contains_exact_token_match ( entity , entity_text , token , token_index , tokens ) [EOL] [EOL] def _contains_exact_token_match ( self , entity , entity_text , token , token_index , tokens ) : [EOL] if token . text in self . _entity_text_exact_text [ entity ] : [EOL] return [number] [EOL] return [number] [EOL] [EOL] def _lemma_match ( self , entity , entity_text , token , token_index , tokens ) : [EOL] if len ( entity_text ) != [number] : [EOL] return [number] [EOL] return self . _contains_lemma_match ( entity , entity_text , token , token_index , tokens ) [EOL] [EOL] def _contains_lemma_match ( self , entity , entity_text , token , token_index , tokens ) : [EOL] if token . text in self . _entity_text_exact_text [ entity ] : [EOL] return [number] [EOL] if token . lemma_ in self . _entity_text_lemmas [ entity ] : [EOL] return [number] [EOL] return [number] [EOL] [EOL] def _edit_distance ( self , entity , entity_text , token , token_index , tokens ) : [EOL] edit_distance = float ( editdistance . eval ( [string] . join ( e . text for e in entity_text ) , token . text ) ) [EOL] return [number] - edit_distance / len ( token . text ) [EOL] [EOL] def _related_column ( self , entity , entity_text , token , token_index , tokens ) : [EOL] [comment] [EOL] if not entity . startswith ( [string] ) and [string] not in entity : [EOL] return [number] [EOL] for neighbor in self . knowledge_graph . neighbors [ entity ] : [EOL] if token . text in self . _entity_text_exact_text [ neighbor ] : [EOL] return [number] [EOL] return [number] [EOL] [EOL] def _related_column_lemma ( self , entity , entity_text , token , token_index , tokens ) : [EOL] [comment] [EOL] if not entity . startswith ( [string] ) and [string] not in entity : [EOL] return [number] [EOL] for neighbor in self . knowledge_graph . neighbors [ entity ] : [EOL] if token . text in self . _entity_text_exact_text [ neighbor ] : [EOL] return [number] [EOL] if token . lemma_ in self . _entity_text_lemmas [ neighbor ] : [EOL] return [number] [EOL] return [number] [EOL] [EOL] def _span_overlap_fraction ( self , entity , entity_text , token , token_index , tokens ) : [EOL] entity_words = set ( entity_token . text for entity_token in entity_text ) [EOL] if not entity_words : [EOL] [comment] [EOL] return [number] [EOL] seen_entity_words = set ( ) [EOL] token_index_left = token_index [EOL] while token_index < len ( tokens ) and tokens [ token_index ] . text in entity_words : [EOL] seen_entity_words . add ( tokens [ token_index ] . text ) [EOL] token_index += [number] [EOL] while token_index_left >= [number] and tokens [ token_index_left ] . text in entity_words : [EOL] seen_entity_words . add ( tokens [ token_index_left ] . text ) [EOL] token_index_left -= [number] [EOL] return len ( seen_entity_words ) / len ( entity_words ) [EOL] [EOL] def _span_lemma_overlap_fraction ( self , entity , entity_text , token , token_index , tokens ) : [EOL] entity_lemmas = set ( entity_token . lemma_ for entity_token in entity_text ) [EOL] if not entity_lemmas : [EOL] [comment] [EOL] return [number] [EOL] seen_entity_lemmas = set ( ) [EOL] token_index_left = token_index [EOL] while token_index < len ( tokens ) and tokens [ token_index ] . lemma_ in entity_lemmas : [EOL] seen_entity_lemmas . add ( tokens [ token_index ] . lemma_ ) [EOL] token_index += [number] [EOL] while token_index_left >= [number] and tokens [ token_index_left ] . lemma_ in entity_lemmas : [EOL] seen_entity_lemmas . add ( tokens [ token_index_left ] . lemma_ ) [EOL] token_index_left -= [number] [EOL] return len ( seen_entity_lemmas ) / len ( entity_lemmas ) [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0
import builtins [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] [EOL] [EOL] class SequenceField ( Field [ DataArray ] ) : [EOL] [docstring] [EOL] def sequence_length ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def empty_field ( self ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $'SequenceField'$ 0 0 0 0 0 0 0 0
from typing import Any , List , Iterable , Dict [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] from typing import Dict , List , Sequence , Iterable [EOL] import itertools [EOL] import logging [EOL] import pickle [EOL] import re [EOL] [EOL] from overrides import overrides [EOL] [EOL] from nltk . tokenize import word_tokenize [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . dataset_readers . dataset_utils import to_bioul [EOL] from allennlp . data . fields import TextField , SequenceLabelField , Field , MetadataField , MultiLabelField , ListField , LabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import TokenIndexer , SingleIdTokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] def instance2dict ( instance ) : [EOL] [comment] [EOL] tmp_dict = { } [EOL] tmp_dict [ [string] ] = [ Token ( token ) for token in word_tokenize ( [string] . join ( instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) . split ( ) ) ) ] [EOL] [EOL] logical_form_rexpr_pattern = [string] [EOL] entity_rexpr_pattern = [string] [EOL] [EOL] logical_form = instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) [EOL] [EOL] try : [EOL] tmp_dict [ [string] ] = re . findall ( logical_form_rexpr_pattern , logical_form ) [ [number] ] [ [number] ] . split ( [string] ) [EOL] except : [EOL] tmp_dict [ [string] ] = None [EOL] [EOL] tmp_dict [ [string] ] = re . findall ( entity_rexpr_pattern , instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) ) [ [number] ] [ [number] ] [EOL] [EOL] ent_span = eval ( [string] + re . findall ( entity_rexpr_pattern , instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) ) [ [number] ] [ [number] ] + [string] ) [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) . split ( ) [ slice ( ent_span [ [number] ] , ent_span [ [number] ] + [number] ) ] [EOL] tmp_dict [ [string] ] = [ Token ( token ) for token in word_tokenize ( [string] . join ( tmp_dict [ [string] ] ) ) ] [EOL] [EOL] tmp_dict [ [string] ] = instance [ [number] ] . split ( [string] ) [ [number] ] . strip ( ) [EOL] return tmp_dict [EOL] [EOL] def fetch_data ( file ) : [EOL] [comment] [EOL] lines = file . readlines ( ) [EOL] instance_list = [ ] [EOL] tmp_instance = [ ] [EOL] for line in file . readlines ( ) : [EOL] if line . strip ( ) == [string] : [EOL] instance_list . append ( instance2dict ( tmp_instance ) ) [EOL] tmp_instance = [ ] [EOL] else : [EOL] tmp_instance . append ( line ) [EOL] return instance_list [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class EMNLPDatasetReader ( DatasetReader ) : [EOL] [EOL] def __init__ ( self , token_indexers = None , lazy = False , KB_path = [string] , label_namespace = [string] , question_type = [string] ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . KB_path = KB_path [EOL] assert question_type == [string] [EOL] self . label_namespace = label_namespace [EOL] self . question_type = question_type [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] [comment] [EOL] [comment] [EOL] KB_path = self . KB_path [EOL] file_path = cached_path ( file_path ) [EOL] dict_entity_lookup = pickle . load ( open ( KB_path , [string] ) ) [EOL] [EOL] with open ( file_path , [string] ) as data_file : [EOL] logger . info ( [string] , file_path ) [EOL] [EOL] tmp_instance = [ ] [EOL] for line in data_file . readlines ( ) : [EOL] if line . strip ( ) == [string] : [EOL] instance_dict = instance2dict ( tmp_instance ) [EOL] [EOL] question = instance_dict [ [string] ] [EOL] entity_surface = instance_dict [ [string] ] [EOL] [EOL] entity = instance_dict [ [string] ] [EOL] try : [EOL] KB_gloss = dict_entity_lookup [ entity ] [ [string] ] [ [number] ] [EOL] [comment] [EOL] e_type = KB_gloss [ [string] ] . get ( [string] , [ ] ) [EOL] e_descr = [ Token ( token ) for token in word_tokenize ( KB_gloss [ [string] ] . get ( [string] , [ ] ) ) ] [EOL] e_detail = [ Token ( token ) for token in word_tokenize ( KB_gloss [ [string] ] . get ( [string] , { } ) . get ( [string] , [ ] ) ) ] [EOL] except : [EOL] KB_gloss = None [EOL] e_type = [ ] [EOL] e_descr = [ ] [EOL] e_detail = [ ] [EOL] [EOL] logical_form = instance_dict [ [string] ] [EOL] if instance_dict [ [string] ] == self . question_type : [EOL] yield self . text_to_instance ( question , entity , entity_surface , e_type , e_descr , e_detail , logical_form ) [EOL] tmp_instance = [ ] [EOL] else : [EOL] tmp_instance . append ( line ) [EOL] [EOL] [EOL] def text_to_instance ( self , question , entity , entity_surface , e_type = None , e_descr = None , e_detail = None , logical_form = None ) : [EOL] [docstring] [EOL] [comment] [EOL] sequence = TextField ( question , self . _token_indexers ) [EOL] entity_sequence = TextField ( entity_surface , self . _token_indexers ) [EOL] description = TextField ( e_descr , self . _token_indexers ) [EOL] detail = TextField ( e_detail , self . _token_indexers ) [EOL] [EOL] instance_fields = { [string] : sequence , [string] : entity_sequence , [string] : description , [string] : detail } [EOL] instance_fields [ [string] ] = MetadataField ( { [string] : [ x . text for x in question ] , [string] : [ x . text for x in entity_sequence ] , [string] : [ x . text for x in description ] , [string] : [ x . text for x in detail ] } ) [EOL] [EOL] instance_fields [ [string] ] = MultiLabelField ( e_type , [string] ) [EOL] instance_fields [ [string] ] = LabelField ( entity , [string] ) [EOL] if logical_form is not None : [EOL] instance_fields [ [string] ] = LabelField ( logical_form [ [number] ] , [string] ) [EOL] instance_fields [ [string] ] = LabelField ( logical_form [ [number] ] , [string] ) [EOL] instance_fields [ [string] ] = LabelField ( logical_form [ [number] ] , [string] ) [EOL] else : [EOL] pass [EOL] [EOL] [EOL] [EOL] return Instance ( instance_fields ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $None$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 $builtins.str$ 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0
from typing import Dict , Union , List , Any , Iterable [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] from typing import Dict , Iterable , Union , Optional , List [EOL] import logging [EOL] import math [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] from allennlp . data . tokenizers import WordTokenizer [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class SimpleLanguageModelingDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , tokenizer = None , token_indexers = None , max_sequence_length = None , start_tokens = None , end_tokens = None ) : [EOL] super ( ) . __init__ ( True ) [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] if max_sequence_length is not None : [EOL] self . _max_sequence_length = max_sequence_length [EOL] else : [EOL] self . _max_sequence_length = math . inf [EOL] [EOL] self . _start_tokens = [ Token ( st ) for st in ( start_tokens or [ ] ) ] [EOL] self . _end_tokens = [ Token ( et ) for et in ( end_tokens or [ ] ) ] [EOL] [EOL] logger . info ( [string] ) [EOL] logger . info ( [string] , max_sequence_length ) [EOL] [EOL] @ overrides def text_to_instance ( self , sentence ) : [EOL] [comment] [EOL] tokenized = self . _tokenizer . tokenize ( sentence ) [EOL] tokenized_with_ends = [ ] [EOL] tokenized_with_ends . extend ( self . _start_tokens ) [EOL] tokenized_with_ends . extend ( tokenized ) [EOL] tokenized_with_ends . extend ( self . _end_tokens ) [EOL] return_instance = Instance ( { [string] : TextField ( tokenized_with_ends , self . _token_indexers ) , } ) [EOL] return return_instance [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] [comment] [EOL] logger . info ( [string] , file_path ) [EOL] dropped_instances = [number] [EOL] [EOL] with open ( file_path ) as file : [EOL] for sentence in file : [EOL] instance = self . text_to_instance ( sentence ) [EOL] if instance . fields [ [string] ] . sequence_length ( ) <= self . _max_sequence_length : [EOL] yield instance [EOL] else : [EOL] dropped_instances += [number] [EOL] if not dropped_instances : [EOL] logger . info ( f" [string] { file_path } [string] " ) [EOL] else : [EOL] logger . warning ( f" [string] { dropped_instances } [string] { file_path } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.data.tokenizers.tokenizer.Tokenizer$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.token_indexer.TokenIndexer]$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.tokenizer.Tokenizer$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.token_indexer.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Union[None,builtins.float,builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Union[None,builtins.float,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.str$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 $logging.Logger$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 0 0
from typing import Dict , List , Any , Tuple , Iterator [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] from typing import Dict , Tuple , List , Iterator , Any [EOL] import logging [EOL] import itertools [EOL] import glob [EOL] import os [EOL] import numpy as np [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import Field , TextField , SequenceLabelField , MetadataField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . data . dataset_readers . universal_dependencies import lazy_parse [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] def get_file_paths ( pathname , languages ) : [EOL] [docstring] [EOL] paths = [ ] [EOL] for file_path in glob . glob ( pathname ) : [EOL] base = os . path . splitext ( os . path . basename ( file_path ) ) [ [number] ] [EOL] lang_id = base . split ( [string] ) [ [number] ] [EOL] if lang_id in languages : [EOL] paths . append ( ( lang_id , file_path ) ) [EOL] [EOL] if not paths : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] return paths [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class UniversalDependenciesMultiLangDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , languages , token_indexers = None , use_language_specific_pos = False , lazy = False , alternate = True , is_first_pass_for_vocab = True , instances_per_file = [number] ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _languages = languages [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . _use_language_specific_pos = use_language_specific_pos [EOL] [EOL] self . _is_first_pass_for_vocab = is_first_pass_for_vocab [EOL] self . _alternate = alternate [EOL] self . _instances_per_file = instances_per_file [EOL] [EOL] self . _is_first_pass = True [EOL] self . _iterators = None [EOL] [EOL] def _read_one_file ( self , lang , file_path ) : [EOL] with open ( file_path , [string] ) as conllu_file : [EOL] logger . info ( [string] , lang , file_path ) [EOL] [EOL] for annotation in lazy_parse ( conllu_file . read ( ) ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] annotation = [ x for x in annotation if x [ [string] ] is not None ] [EOL] [EOL] heads = [ x [ [string] ] for x in annotation ] [EOL] tags = [ x [ [string] ] for x in annotation ] [EOL] words = [ x [ [string] ] for x in annotation ] [EOL] if self . _use_language_specific_pos : [EOL] pos_tags = [ x [ [string] ] for x in annotation ] [EOL] else : [EOL] pos_tags = [ x [ [string] ] for x in annotation ] [EOL] yield self . text_to_instance ( lang , words , pos_tags , list ( zip ( tags , heads ) ) ) [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] file_paths = get_file_paths ( file_path , self . _languages ) [EOL] if ( self . _is_first_pass and self . _is_first_pass_for_vocab ) or ( not self . _alternate ) : [EOL] iterators = [ iter ( self . _read_one_file ( lang , file_path ) ) for ( lang , file_path ) in file_paths ] [EOL] self . _is_first_pass = False [EOL] for inst in itertools . chain ( * iterators ) : [EOL] yield inst [EOL] [EOL] else : [EOL] if self . _iterators is None : [EOL] self . _iterators = [ ( lang , iter ( self . _read_one_file ( lang , file_path ) ) ) for ( lang , file_path ) in file_paths ] [EOL] num_files = len ( file_paths ) [EOL] while True : [EOL] ind = np . random . randint ( num_files ) [EOL] lang , lang_iter = self . _iterators [ ind ] [EOL] for _ in range ( self . _instances_per_file ) : [EOL] try : [EOL] yield lang_iter . __next__ ( ) [EOL] except StopIteration : [EOL] lang , file_path = file_paths [ ind ] [EOL] lang_iter = iter ( self . _read_one_file ( lang , file_path ) ) [EOL] self . _iterators [ ind ] = ( lang , lang_iter ) [EOL] yield lang_iter . __next__ ( ) [EOL] [EOL] @ overrides def text_to_instance ( self , lang , words , upos_tags , dependencies = None ) : [EOL] [comment] [EOL] [docstring] [EOL] fields = { } [EOL] [EOL] tokens = TextField ( [ Token ( w ) for w in words ] , self . _token_indexers ) [EOL] fields [ [string] ] = tokens [EOL] fields [ [string] ] = SequenceLabelField ( upos_tags , tokens , label_namespace = [string] ) [EOL] if dependencies is not None : [EOL] [comment] [EOL] [comment] [EOL] fields [ [string] ] = SequenceLabelField ( [ x [ [number] ] for x in dependencies ] , tokens , label_namespace = [string] ) [EOL] fields [ [string] ] = SequenceLabelField ( [ int ( x [ [number] ] ) for x in dependencies ] , tokens , label_namespace = [string] ) [EOL] [EOL] fields [ [string] ] = MetadataField ( { [string] : words , [string] : upos_tags , [string] : lang } ) [EOL] return Instance ( fields ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Iterator[typing.Any]]]$ 0 $typing.Any$ 0 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 $typing.List[typing.Tuple[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0
	0
[docstring] [EOL] [EOL] from allennlp . data . dataset_readers . coreference_resolution . conll import ConllCorefReader [EOL] from allennlp . data . dataset_readers . coreference_resolution . winobias import WinobiasReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Union , List , Any , Tuple , Set , Counter [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import collections [EOL] [docstring] [EOL] [EOL] from collections import Counter , defaultdict [EOL] import logging [EOL] import string [EOL] from typing import Any , Dict , List , Tuple [EOL] [EOL] from allennlp . data . fields import Field , TextField , IndexField , MetadataField , LabelField , ListField , SequenceLabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import TokenIndexer [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] IGNORED_TOKENS = { [string] , [string] , [string] } [EOL] STRIPPED_CHARACTERS = string . punctuation + [string] . join ( [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] [EOL] def normalize_text ( text ) : [EOL] [docstring] [EOL] return [string] . join ( [ token for token in text . lower ( ) . strip ( STRIPPED_CHARACTERS ) . split ( ) if token not in IGNORED_TOKENS ] ) [EOL] [EOL] [EOL] def char_span_to_token_span ( token_offsets , character_span ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] error = False [EOL] start_index = [number] [EOL] while start_index < len ( token_offsets ) and token_offsets [ start_index ] [ [number] ] < character_span [ [number] ] : [EOL] start_index += [number] [EOL] [comment] [EOL] if token_offsets [ start_index ] [ [number] ] > character_span [ [number] ] : [EOL] [comment] [EOL] [comment] [EOL] logger . debug ( [string] ) [EOL] start_index -= [number] [EOL] if token_offsets [ start_index ] [ [number] ] != character_span [ [number] ] : [EOL] error = True [EOL] end_index = start_index [EOL] while end_index < len ( token_offsets ) and token_offsets [ end_index ] [ [number] ] < character_span [ [number] ] : [EOL] end_index += [number] [EOL] if end_index == start_index and token_offsets [ end_index ] [ [number] ] > character_span [ [number] ] : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] logger . debug ( [string] ) [EOL] elif token_offsets [ end_index ] [ [number] ] > character_span [ [number] ] : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] logger . debug ( [string] ) [EOL] if token_offsets [ end_index ] [ [number] ] != character_span [ [number] ] : [EOL] error = True [EOL] return ( start_index , end_index ) , error [EOL] [EOL] [EOL] def find_valid_answer_spans ( passage_tokens , answer_texts ) : [EOL] [docstring] [EOL] normalized_tokens = [ token . text . lower ( ) . strip ( STRIPPED_CHARACTERS ) for token in passage_tokens ] [EOL] [comment] [EOL] [comment] [EOL] word_positions = defaultdict ( list ) [EOL] for i , token in enumerate ( normalized_tokens ) : [EOL] word_positions [ token ] . append ( i ) [EOL] spans = [ ] [EOL] for answer_text in answer_texts : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] answer_tokens = answer_text . lower ( ) . strip ( STRIPPED_CHARACTERS ) . split ( ) [EOL] num_answer_tokens = len ( answer_tokens ) [EOL] for span_start in word_positions [ answer_tokens [ [number] ] ] : [EOL] span_end = span_start [comment] [EOL] answer_index = [number] [EOL] while answer_index < num_answer_tokens and span_end + [number] < len ( normalized_tokens ) : [EOL] token = normalized_tokens [ span_end + [number] ] [EOL] if answer_tokens [ answer_index ] == token : [EOL] answer_index += [number] [EOL] span_end += [number] [EOL] elif token in IGNORED_TOKENS : [EOL] span_end += [number] [EOL] else : [EOL] break [EOL] if num_answer_tokens == answer_index : [EOL] spans . append ( ( span_start , span_end ) ) [EOL] return spans [EOL] [EOL] [EOL] def make_reading_comprehension_instance ( question_tokens , passage_tokens , token_indexers , passage_text , token_spans = None , answer_texts = None , additional_metadata = None ) : [EOL] [docstring] [EOL] additional_metadata = additional_metadata or { } [EOL] fields = { } [EOL] passage_offsets = [ ( token . idx , token . idx + len ( token . text ) ) for token in passage_tokens ] [EOL] [EOL] [comment] [EOL] passage_field = TextField ( passage_tokens , token_indexers ) [EOL] fields [ [string] ] = passage_field [EOL] fields [ [string] ] = TextField ( question_tokens , token_indexers ) [EOL] metadata = { [string] : passage_text , [string] : passage_offsets , [string] : [ token . text for token in question_tokens ] , [string] : [ token . text for token in passage_tokens ] , } [EOL] if answer_texts : [EOL] metadata [ [string] ] = answer_texts [EOL] [EOL] if token_spans : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] candidate_answers = Counter ( ) [EOL] for span_start , span_end in token_spans : [EOL] candidate_answers [ ( span_start , span_end ) ] += [number] [EOL] span_start , span_end = candidate_answers . most_common ( [number] ) [ [number] ] [ [number] ] [EOL] [EOL] fields [ [string] ] = IndexField ( span_start , passage_field ) [EOL] fields [ [string] ] = IndexField ( span_end , passage_field ) [EOL] [EOL] metadata . update ( additional_metadata ) [EOL] fields [ [string] ] = MetadataField ( metadata ) [EOL] return Instance ( fields ) [EOL] [EOL] [EOL] def make_reading_comprehension_instance_quac ( question_list_tokens , passage_tokens , token_indexers , passage_text , token_span_lists = None , yesno_list = None , followup_list = None , additional_metadata = None , num_context_answers = [number] ) : [EOL] [docstring] [EOL] additional_metadata = additional_metadata or { } [EOL] fields = { } [EOL] passage_offsets = [ ( token . idx , token . idx + len ( token . text ) ) for token in passage_tokens ] [EOL] [comment] [EOL] passage_field = TextField ( passage_tokens , token_indexers ) [EOL] fields [ [string] ] = passage_field [EOL] fields [ [string] ] = ListField ( [ TextField ( q_tokens , token_indexers ) for q_tokens in question_list_tokens ] ) [EOL] metadata = { [string] : passage_text , [string] : passage_offsets , [string] : [ [ token . text for token in question_tokens ] for question_tokens in question_list_tokens ] , [string] : [ token . text for token in passage_tokens ] , } [EOL] p1_answer_marker_list = [ ] [EOL] p2_answer_marker_list = [ ] [EOL] p3_answer_marker_list = [ ] [EOL] [EOL] def get_tag ( i , i_name ) : [EOL] [comment] [EOL] return [string] . format ( i , i_name ) [EOL] [EOL] def mark_tag ( span_start , span_end , passage_tags , prev_answer_distance ) : [EOL] try : [EOL] assert span_start >= [number] [EOL] assert span_end >= [number] [EOL] except : [EOL] raise ValueError ( [string] . format ( prev_answer_distance ) ) [EOL] [comment] [EOL] if span_start == span_end : [EOL] passage_tags [ prev_answer_distance ] [ span_start ] = get_tag ( prev_answer_distance , [string] ) [EOL] else : [EOL] passage_tags [ prev_answer_distance ] [ span_start ] = get_tag ( prev_answer_distance , [string] ) [EOL] passage_tags [ prev_answer_distance ] [ span_end ] = get_tag ( prev_answer_distance , [string] ) [EOL] for passage_index in range ( span_start + [number] , span_end ) : [EOL] passage_tags [ prev_answer_distance ] [ passage_index ] = get_tag ( prev_answer_distance , [string] ) [EOL] [EOL] if token_span_lists : [EOL] span_start_list = [ ] [EOL] span_end_list = [ ] [EOL] p1_span_start , p1_span_end , p2_span_start = - [number] , - [number] , - [number] [EOL] p2_span_end , p3_span_start , p3_span_end = - [number] , - [number] , - [number] [EOL] [comment] [EOL] for question_index , answer_span_lists in enumerate ( token_span_lists ) : [EOL] span_start , span_end = answer_span_lists [ - [number] ] [comment] [EOL] span_start_list . append ( IndexField ( span_start , passage_field ) ) [EOL] span_end_list . append ( IndexField ( span_end , passage_field ) ) [EOL] prev_answer_marker_lists = [ [ [string] ] * len ( passage_tokens ) , [ [string] ] * len ( passage_tokens ) , [ [string] ] * len ( passage_tokens ) , [ [string] ] * len ( passage_tokens ) ] [EOL] if question_index > [number] and num_context_answers > [number] : [EOL] mark_tag ( p1_span_start , p1_span_end , prev_answer_marker_lists , [number] ) [EOL] if question_index > [number] and num_context_answers > [number] : [EOL] mark_tag ( p2_span_start , p2_span_end , prev_answer_marker_lists , [number] ) [EOL] if question_index > [number] and num_context_answers > [number] : [EOL] mark_tag ( p3_span_start , p3_span_end , prev_answer_marker_lists , [number] ) [EOL] p3_span_start = p2_span_start [EOL] p3_span_end = p2_span_end [EOL] p2_span_start = p1_span_start [EOL] p2_span_end = p1_span_end [EOL] p1_span_start = span_start [EOL] p1_span_end = span_end [EOL] if num_context_answers > [number] : [EOL] p3_answer_marker_list . append ( SequenceLabelField ( prev_answer_marker_lists [ [number] ] , passage_field , label_namespace = [string] ) ) [EOL] if num_context_answers > [number] : [EOL] p2_answer_marker_list . append ( SequenceLabelField ( prev_answer_marker_lists [ [number] ] , passage_field , label_namespace = [string] ) ) [EOL] if num_context_answers > [number] : [EOL] p1_answer_marker_list . append ( SequenceLabelField ( prev_answer_marker_lists [ [number] ] , passage_field , label_namespace = [string] ) ) [EOL] fields [ [string] ] = ListField ( span_start_list ) [EOL] fields [ [string] ] = ListField ( span_end_list ) [EOL] if num_context_answers > [number] : [EOL] fields [ [string] ] = ListField ( p1_answer_marker_list ) [EOL] if num_context_answers > [number] : [EOL] fields [ [string] ] = ListField ( p2_answer_marker_list ) [EOL] if num_context_answers > [number] : [EOL] fields [ [string] ] = ListField ( p3_answer_marker_list ) [EOL] fields [ [string] ] = ListField ( [ LabelField ( yesno , label_namespace = [string] ) for yesno in yesno_list ] ) [EOL] fields [ [string] ] = ListField ( [ LabelField ( followup , label_namespace = [string] ) for followup in followup_list ] ) [EOL] metadata . update ( additional_metadata ) [EOL] fields [ [string] ] = MetadataField ( metadata ) [EOL] return Instance ( fields ) [EOL] [EOL] [EOL] def handle_cannot ( reference_answers ) : [EOL] [docstring] [EOL] num_cannot = [number] [EOL] num_spans = [number] [EOL] for ref in reference_answers : [EOL] if ref == [string] : [EOL] num_cannot += [number] [EOL] else : [EOL] num_spans += [number] [EOL] if num_cannot >= num_spans : [EOL] reference_answers = [ [string] ] [EOL] else : [EOL] reference_answers = [ x for x in reference_answers if x != [string] ] [EOL] return reference_answers [EOL] [EOL] [EOL] def split_token_by_delimiter ( token , delimiter ) : [EOL] split_tokens = [ ] [EOL] char_offset = token . idx [EOL] for sub_str in token . text . split ( delimiter ) : [EOL] if sub_str : [EOL] split_tokens . append ( Token ( text = sub_str , idx = char_offset ) ) [EOL] char_offset += len ( sub_str ) [EOL] split_tokens . append ( Token ( text = delimiter , idx = char_offset ) ) [EOL] char_offset += len ( delimiter ) [EOL] if split_tokens : [EOL] split_tokens . pop ( - [number] ) [EOL] char_offset -= len ( delimiter ) [EOL] return split_tokens [EOL] else : [EOL] return [ token ] [EOL] [EOL] [EOL] def split_tokens_by_hyphen ( tokens ) : [EOL] hyphens = [ [string] , [string] , [string] ] [EOL] new_tokens = [ ] [EOL] [EOL] for token in tokens : [EOL] if any ( hyphen in token . text for hyphen in hyphens ) : [EOL] unsplit_tokens = [ token ] [EOL] split_tokens = [ ] [EOL] for hyphen in hyphens : [EOL] for unsplit_token in unsplit_tokens : [EOL] if hyphen in token . text : [EOL] split_tokens += split_token_by_delimiter ( unsplit_token , hyphen ) [EOL] else : [EOL] split_tokens . append ( unsplit_token ) [EOL] unsplit_tokens , split_tokens = split_tokens , [ ] [EOL] new_tokens += unsplit_tokens [EOL] else : [EOL] new_tokens . append ( token ) [EOL] [EOL] return new_tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Optional , IO , Any , Tuple [EOL] import logging [EOL] import tarfile [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import json [EOL] import logging [EOL] import os [EOL] import tarfile [EOL] from typing import Dict , List , Tuple [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . dataset_readers . reading_comprehension import util [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Token , Tokenizer , WordTokenizer [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class TriviaQaReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , base_tarball_path , unfiltered_tarball_path = None , tokenizer = None , token_indexers = None , lazy = False ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _base_tarball_path = base_tarball_path [EOL] self . _unfiltered_tarball_path = unfiltered_tarball_path [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] logger . info ( [string] , self . _base_tarball_path ) [EOL] base_tarball = tarfile . open ( cached_path ( self . _base_tarball_path ) , [string] ) [EOL] if [string] in file_path : [EOL] logger . info ( [string] , self . _unfiltered_tarball_path ) [EOL] unfiltered_tarball = tarfile . open ( cached_path ( self . _unfiltered_tarball_path ) , [string] ) [EOL] logger . info ( [string] ) [EOL] data_json = json . loads ( unfiltered_tarball . extractfile ( file_path ) . read ( ) . decode ( [string] ) ) [EOL] else : [EOL] logger . info ( [string] ) [EOL] path = os . path . join ( [string] , file_path ) [EOL] data_json = json . loads ( base_tarball . extractfile ( path ) . read ( ) . decode ( [string] ) ) [EOL] [EOL] logger . info ( [string] ) [EOL] for question_json in data_json [ [string] ] : [EOL] question_text = question_json [ [string] ] [EOL] question_tokens = self . _tokenizer . tokenize ( question_text ) [EOL] [EOL] evidence_files = [ ] [comment] [EOL] if [string] in file_path : [EOL] for result in question_json [ [string] ] : [EOL] filename = result [ [string] ] [EOL] evidence_file = base_tarball . extractfile ( os . path . join ( [string] , [string] , filename ) ) [EOL] evidence_files . append ( [ line . decode ( [string] ) for line in evidence_file . readlines ( ) ] ) [EOL] else : [EOL] for result in question_json [ [string] ] : [EOL] filename = result [ [string] ] [EOL] evidence_file = base_tarball . extractfile ( os . path . join ( [string] , [string] , filename ) ) [EOL] evidence_files . append ( [ line . decode ( [string] ) for line in evidence_file . readlines ( ) ] ) [EOL] [EOL] answer_json = question_json [ [string] ] [EOL] human_answers = [ util . normalize_text ( answer ) for answer in answer_json . get ( [string] , [ ] ) ] [EOL] answer_texts = answer_json [ [string] ] + human_answers [EOL] for paragraph in self . pick_paragraphs ( evidence_files , question_text , answer_texts ) : [EOL] paragraph_tokens = self . _tokenizer . tokenize ( paragraph ) [EOL] token_spans = util . find_valid_answer_spans ( paragraph_tokens , answer_texts ) [EOL] if not token_spans : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] instance = self . text_to_instance ( question_text , paragraph , token_spans , answer_texts , question_tokens , paragraph_tokens ) [EOL] yield instance [EOL] [EOL] def pick_paragraphs ( self , evidence_files , question = None , answer_texts = None ) : [EOL] [docstring] [EOL] [comment] [EOL] paragraphs = [ ] [EOL] for evidence_file in evidence_files : [EOL] whole_document = [string] . join ( evidence_file ) [EOL] tokens = whole_document . split ( [string] ) [EOL] paragraph = [string] . join ( tokens [ : [number] ] ) [EOL] paragraphs . append ( paragraph ) [EOL] return paragraphs [EOL] [EOL] @ overrides def text_to_instance ( self , question_text , passage_text , token_spans = None , answer_texts = None , question_tokens = None , passage_tokens = None ) : [EOL] [comment] [EOL] if not question_tokens : [EOL] question_tokens = self . _tokenizer . tokenize ( question_text ) [EOL] if not passage_tokens : [EOL] passage_tokens = self . _tokenizer . tokenize ( passage_text ) [EOL] return util . make_reading_comprehension_instance ( question_tokens , passage_tokens , self . _token_indexers , passage_text , token_spans , answer_texts ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 $typing.List[builtins.str]$ 0 0
from typing import List , TypeVar , Dict [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , TypeVar , Generic [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] TokenType = TypeVar ( [string] , int , List [ int ] ) [comment] [EOL] [EOL] [EOL] class TokenIndexer ( Generic [ TokenType ] , Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __init__ ( self , token_min_padding_length = [number] ) : [EOL] self . _token_min_padding_length = token_min_padding_length [EOL] [EOL] def count_vocab_items ( self , token , counter ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def tokens_to_indices ( self , tokens , vocabulary , index_name ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_token ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_lengths ( self , token ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_token_min_padding_length ( self ) : [EOL] [docstring] [EOL] return self . _token_min_padding_length [EOL] [EOL] def pad_token_sequence ( self , tokens , desired_num_tokens , padding_lengths ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_keys ( self , index_name ) : [EOL] [docstring] [EOL] [comment] [EOL] return [ index_name ] [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( self , other . __class__ ) : [EOL] return self . __dict__ == other . __dict__ [EOL] return NotImplemented [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $TokenType$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 0 0 $typing.Dict[builtins.str,typing.List[TokenType]]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List [EOL] import itertools [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] [EOL] [EOL] @ TokenIndexer . register ( [string] ) class SingleIdTokenIndexer ( TokenIndexer [ int ] ) : [EOL] [docstring] [EOL] [comment] [EOL] def __init__ ( self , namespace = [string] , lowercase_tokens = False , start_tokens = None , end_tokens = None , token_min_padding_length = [number] ) : [EOL] super ( ) . __init__ ( token_min_padding_length ) [EOL] self . namespace = namespace [EOL] self . lowercase_tokens = lowercase_tokens [EOL] [EOL] self . _start_tokens = [ Token ( st ) for st in ( start_tokens or [ ] ) ] [EOL] self . _end_tokens = [ Token ( et ) for et in ( end_tokens or [ ] ) ] [EOL] [EOL] @ overrides def count_vocab_items ( self , token , counter ) : [EOL] [comment] [EOL] [comment] [EOL] if getattr ( token , [string] , None ) is None : [EOL] text = token . text [EOL] if self . lowercase_tokens : [EOL] text = text . lower ( ) [EOL] counter [ self . namespace ] [ text ] += [number] [EOL] [EOL] @ overrides def tokens_to_indices ( self , tokens , vocabulary , index_name ) : [EOL] indices = [ ] [EOL] [EOL] for token in itertools . chain ( self . _start_tokens , tokens , self . _end_tokens ) : [EOL] if getattr ( token , [string] , None ) is not None : [EOL] [comment] [EOL] [comment] [EOL] indices . append ( token . text_id ) [EOL] else : [EOL] text = token . text [EOL] if self . lowercase_tokens : [EOL] text = text . lower ( ) [EOL] indices . append ( vocabulary . get_token_index ( text , self . namespace ) ) [EOL] [EOL] return { index_name : indices } [EOL] [EOL] @ overrides def get_padding_token ( self ) : [EOL] return [number] [EOL] [EOL] @ overrides def get_padding_lengths ( self , token ) : [comment] [EOL] return { } [EOL] [EOL] @ overrides def pad_token_sequence ( self , tokens , desired_num_tokens , padding_lengths ) : [comment] [EOL] return { key : pad_sequence_to_length ( val , desired_num_tokens [ key ] ) for key , val in tokens . items ( ) } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.int]]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.int]]$ 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.int]]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.int]]$ 0 0 0 0 0 0
from typing import List , Deque , Any , Iterable , Iterator , DefaultDict [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] from collections import deque , defaultdict [EOL] from typing import Iterable , Deque [EOL] import logging [EOL] import random [EOL] [EOL] from allennlp . common . util import lazy_groups_of [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . iterators . bucket_iterator import BucketIterator [EOL] from allennlp . data . dataset import Batch [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] def split_by_language ( instance_list ) : [EOL] insts_by_lang = defaultdict ( lambda : [ ] ) [EOL] for inst in instance_list : [EOL] inst_lang = inst . fields [ [string] ] . metadata [ [string] ] [EOL] insts_by_lang [ inst_lang ] . append ( inst ) [EOL] [EOL] return iter ( insts_by_lang . values ( ) ) [EOL] [EOL] @ DataIterator . register ( [string] ) class SameLanguageIterator ( BucketIterator ) : [EOL] [docstring] [EOL] def _create_batches ( self , instances , shuffle ) : [EOL] [comment] [EOL] for instance_list in self . _memory_sized_lists ( instances ) : [EOL] if shuffle : [EOL] random . shuffle ( instance_list ) [EOL] instance_list = split_by_language ( instance_list ) [EOL] for same_lang_batch in instance_list : [EOL] iterator = iter ( same_lang_batch ) [EOL] excess = deque ( ) [EOL] [comment] [EOL] for batch_instances in lazy_groups_of ( iterator , self . _batch_size ) : [EOL] for poss_smaller_batches in self . _ensure_batch_is_sufficiently_small ( batch_instances , excess ) : [EOL] batch = Batch ( poss_smaller_batches ) [EOL] yield batch [EOL] if excess : [EOL] yield Batch ( excess ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.dataset.Batch]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 $typing.Deque[allennlp.data.instance.Instance]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Deque[allennlp.data.instance.Instance]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Deque[allennlp.data.instance.Instance]$ 0 0 0 0 0 $typing.Deque[allennlp.data.instance.Instance]$ 0 0
from typing import Dict , Union , List , Optional , Any , Iterable [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] import logging [EOL] import time [EOL] import datetime [EOL] from typing import Dict , Optional , List , Union , Any , Iterable [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import parse_cuda_device [EOL] from allennlp . common . tqdm import Tqdm [EOL] from allennlp . data . iterators . data_iterator import TensorDict [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn import util as nn_util [EOL] from allennlp . training import util as training_util [EOL] from allennlp . training . callbacks . callback import Callback [EOL] from allennlp . training . callbacks . callback_handler import CallbackHandler [EOL] from allennlp . training . callbacks . events import Events [EOL] from allennlp . training . optimizers import Optimizer [EOL] from allennlp . training . trainer_pieces import TrainerPieces [EOL] from allennlp . training . trainer_base import TrainerBase [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ TrainerBase . register ( [string] ) class CallbackTrainer ( TrainerBase ) : [EOL] def __init__ ( self , model , optimizer , num_epochs = [number] , serialization_dir = None , model_save_interval = None , cuda_device = - [number] , callbacks = None ) : [EOL] [docstring] [EOL] super ( ) . __init__ ( serialization_dir , cuda_device ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . model = model [EOL] self . optimizer = optimizer [EOL] self . validate = False [EOL] [EOL] [comment] [EOL] self . train_metrics = { } [EOL] self . val_metrics = { } [EOL] self . latest_val_metric = [number] [EOL] self . train_loss = [number] [EOL] [EOL] [comment] [EOL] self . metrics = { } [EOL] [EOL] self . batch_num_total = [number] [EOL] self . batch_group = [ ] [EOL] self . batches_this_epoch = [number] [EOL] [EOL] self . training_batches = ( ) [EOL] self . num_training_batches = [number] [EOL] [EOL] self . should_stop_early = False [EOL] self . num_epochs = num_epochs [EOL] [EOL] self . training_start_time = [number] [EOL] self . checkpoint_epoch = [number] [EOL] self . model_save_interval = model_save_interval [EOL] [EOL] self . last_log = [number] [EOL] self . epoch_number = [number] [EOL] self . batch_grad_norm = None [EOL] self . handler = CallbackHandler ( callbacks , self ) [EOL] [EOL] def batch_loss ( self , batch_group , for_training ) : [EOL] [docstring] [EOL] if self . _multiple_gpu : [EOL] output_dict = training_util . data_parallel ( batch_group , self . model , self . _cuda_devices ) [EOL] else : [EOL] assert len ( batch_group ) == [number] [EOL] batch = batch_group [ [number] ] [EOL] batch = nn_util . move_to_device ( batch , self . _cuda_devices [ [number] ] ) [EOL] output_dict = self . model ( ** batch ) [EOL] [EOL] try : [EOL] loss = output_dict [ [string] ] [EOL] if for_training : [EOL] loss += self . model . get_regularization_penalty ( ) [EOL] except KeyError : [EOL] if for_training : [EOL] raise RuntimeError ( [string] [string] ) [EOL] loss = None [EOL] [EOL] return loss [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] self . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] starting_epoch = self . epoch_number [EOL] [EOL] logger . info ( [string] ) [EOL] self . handler . fire_event ( Events . TRAINING_START ) [EOL] [EOL] self . training_start_time = time . time ( ) [EOL] [EOL] for self . epoch_number in range ( starting_epoch , self . num_epochs ) : [EOL] epoch_start_time = time . time ( ) [EOL] [comment] [EOL] self . handler . fire_event ( Events . EPOCH_START ) [EOL] [EOL] self . train_loss = [number] [EOL] [comment] [EOL] self . model . train ( ) [EOL] [EOL] self . last_log = time . time ( ) [EOL] last_save_time = time . time ( ) [EOL] [EOL] logger . info ( [string] ) [EOL] self . batches_this_epoch = [number] [EOL] [EOL] batch_groups_tqdm = Tqdm . tqdm ( self . training_batches , total = self . num_training_batches ) [EOL] [EOL] for self . batch_group in batch_groups_tqdm : [EOL] self . handler . fire_event ( Events . BATCH_START ) [EOL] [EOL] self . batches_this_epoch += [number] [EOL] self . batch_num_total += [number] [EOL] [EOL] self . handler . fire_event ( Events . FORWARD ) [EOL] self . handler . fire_event ( Events . BACKWARD ) [EOL] [EOL] description = training_util . description_from_metrics ( self . train_metrics ) [EOL] [EOL] batch_groups_tqdm . set_description ( description , refresh = False ) [EOL] [EOL] [comment] [EOL] if self . model_save_interval is not None and ( time . time ( ) - last_save_time > self . model_save_interval ) : [EOL] last_save_time = time . time ( ) [EOL] self . checkpoint_epoch = f"{ self . epoch_number } [string] { training_util . time_to_str ( int ( last_save_time ) ) }" [EOL] self . handler . fire_event ( Events . SAVE_CHECKPOINT ) [EOL] [EOL] self . handler . fire_event ( Events . BATCH_END ) [EOL] [EOL] self . handler . fire_event ( Events . VALIDATE ) [EOL] [EOL] epoch_elapsed_time = time . time ( ) - epoch_start_time [EOL] logger . info ( [string] , datetime . timedelta ( seconds = epoch_elapsed_time ) ) [EOL] [EOL] if self . epoch_number < self . num_epochs - [number] : [EOL] training_elapsed_time = time . time ( ) - self . training_start_time [EOL] estimated_time_remaining = training_elapsed_time * ( ( self . num_epochs - starting_epoch ) / float ( self . epoch_number - starting_epoch + [number] ) - [number] ) [EOL] formatted_time = str ( datetime . timedelta ( seconds = int ( estimated_time_remaining ) ) ) [EOL] logger . info ( [string] , formatted_time ) [EOL] [EOL] self . handler . fire_event ( Events . EPOCH_END ) [EOL] [EOL] self . checkpoint_epoch = self . epoch_number [EOL] self . handler . fire_event ( Events . SAVE_CHECKPOINT ) [EOL] [EOL] if self . should_stop_early : [EOL] logger . info ( [string] ) [EOL] break [EOL] [EOL] self . handler . fire_event ( Events . TRAINING_END ) [EOL] [EOL] return self . metrics [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , params , serialization_dir , recover = False ) : [EOL] pieces = TrainerPieces . from_params ( params , serialization_dir , recover ) [comment] [EOL] model = pieces . model [EOL] params = pieces . params [EOL] validation_iterator = pieces . validation_iterator or pieces . iterator [EOL] [EOL] shuffle = params . pop_bool ( [string] , True ) [EOL] num_epochs = params . pop_int ( [string] , [number] ) [EOL] cuda_device = parse_cuda_device ( params . pop ( [string] , - [number] ) ) [EOL] [EOL] if isinstance ( cuda_device , list ) : [EOL] model_device = cuda_device [ [number] ] [EOL] else : [EOL] model_device = cuda_device [EOL] if model_device >= [number] : [EOL] [comment] [EOL] [comment] [EOL] model = model . cuda ( model_device ) [EOL] [EOL] parameters = [ [ n , p ] for n , p in model . named_parameters ( ) if p . requires_grad ] [EOL] optimizer = Optimizer . from_params ( parameters , params . pop ( [string] ) ) [EOL] [EOL] model_save_interval = params . pop_float ( [string] , None ) [EOL] [EOL] callbacks_params = params . pop ( [string] , [ ] ) [EOL] callbacks = [ Callback . from_params ( params = callback_params , model = model , optimizer = optimizer , instances = pieces . train_dataset , iterator = pieces . iterator , shuffle = shuffle , validation_data = pieces . validation_dataset , validation_iterator = validation_iterator , serialization_dir = serialization_dir ) for callback_params in callbacks_params ] [EOL] [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( model , optimizer , num_epochs = num_epochs , serialization_dir = serialization_dir , cuda_device = cuda_device , model_save_interval = model_save_interval , callbacks = callbacks ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.float]$ 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 $typing.List[allennlp.training.callbacks.callback.Callback]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[allennlp.data.iterators.data_iterator.TensorDict]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Iterable[typing.List[allennlp.data.iterators.data_iterator.TensorDict]]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.Union[builtins.int,builtins.str]$ 0 0 0 0 0 $typing.Optional[builtins.float]$ 0 $typing.Optional[builtins.float]$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Optional[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.training.callbacks.callback.Callback]$ 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.List[allennlp.data.iterators.data_iterator.TensorDict]$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[allennlp.data.iterators.data_iterator.TensorDict]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.iterators.data_iterator.TensorDict]$ 0 0 0 0 $typing.Any$ 0 $typing.List[allennlp.data.iterators.data_iterator.TensorDict]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $None$ 0 $typing.Any$ 0 0 0 0 0 $builtins.bool$ 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.training.callbacks.callback.Callback]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[allennlp.training.callbacks.callback.Callback]$ 0 $typing.List[allennlp.training.callbacks.callback.Callback]$ 0 0
from typing import Any , Dict [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] import os [EOL] from typing import Dict , Any [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . models import Model [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . trainer_base import TrainerBase [EOL] from allennlp . training . trainer_pieces import TrainerPieces [EOL] [EOL] [EOL] @ TrainerBase . register ( [string] ) class NoOpTrainer ( TrainerBase ) : [EOL] def __init__ ( self , serialization_dir , model ) : [EOL] [docstring] [EOL] [EOL] super ( ) . __init__ ( serialization_dir , cuda_device = - [number] ) [EOL] self . model = model [EOL] [EOL] @ classmethod def from_params ( cls , params , serialization_dir , recover = False ) : [EOL] [comment] [EOL] pieces = TrainerPieces . from_params ( params , serialization_dir , recover ) [comment] [EOL] return NoOpTrainer ( serialization_dir , pieces . model ) [EOL] [EOL] def train ( self ) : [EOL] self . model . vocab . save_to_files ( os . path . join ( self . _serialization_dir , [string] ) ) [EOL] [EOL] checkpointer = Checkpointer ( self . _serialization_dir ) [EOL] checkpointer . save_checkpoint ( epoch = [number] , model_state = self . model . state_dict ( ) , training_states = { } , is_best_so_far = True ) [EOL] return { } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , Any [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] import logging [EOL] import os [EOL] import re [EOL] from typing import Iterable , NamedTuple [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import get_frozen_and_tunable_parameter_names [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators . data_iterator import DataIterator [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . training import util as training_util [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class TrainerPieces ( NamedTuple ) : [EOL] [docstring] [EOL] model = ... [EOL] iterator = ... [EOL] train_dataset = ... [EOL] validation_dataset = ... [EOL] test_dataset = ... [EOL] validation_iterator = ... [EOL] params = ... [EOL] [EOL] @ staticmethod def from_params ( params , serialization_dir , recover = False , cache_directory = None , cache_prefix = None ) : [EOL] all_datasets = training_util . datasets_from_params ( params , cache_directory , cache_prefix ) [EOL] datasets_for_vocab_creation = set ( params . pop ( [string] , all_datasets ) ) [EOL] [EOL] for dataset in datasets_for_vocab_creation : [EOL] if dataset not in all_datasets : [EOL] raise ConfigurationError ( f" [string] { dataset }" ) [EOL] [EOL] logger . info ( [string] , [string] . join ( datasets_for_vocab_creation ) ) [EOL] [EOL] if recover and os . path . exists ( os . path . join ( serialization_dir , [string] ) ) : [EOL] vocab = Vocabulary . from_files ( os . path . join ( serialization_dir , [string] ) ) [EOL] params . pop ( [string] , { } ) [EOL] else : [EOL] vocab = Vocabulary . from_params ( params . pop ( [string] , { } ) , ( instance for key , dataset in all_datasets . items ( ) for instance in dataset if key in datasets_for_vocab_creation ) ) [EOL] [EOL] model = Model . from_params ( vocab = vocab , params = params . pop ( [string] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] model . extend_embedder_vocab ( ) [EOL] [EOL] [comment] [EOL] vocab . save_to_files ( os . path . join ( serialization_dir , [string] ) ) [EOL] [EOL] iterator = DataIterator . from_params ( params . pop ( [string] ) ) [EOL] iterator . index_with ( model . vocab ) [EOL] validation_iterator_params = params . pop ( [string] , None ) [EOL] if validation_iterator_params : [EOL] validation_iterator = DataIterator . from_params ( validation_iterator_params ) [EOL] validation_iterator . index_with ( model . vocab ) [EOL] else : [EOL] validation_iterator = None [EOL] [EOL] train_data = all_datasets [ [string] ] [EOL] validation_data = all_datasets . get ( [string] ) [EOL] test_data = all_datasets . get ( [string] ) [EOL] [EOL] trainer_params = params . pop ( [string] ) [EOL] no_grad_regexes = trainer_params . pop ( [string] , ( ) ) [EOL] for name , parameter in model . named_parameters ( ) : [EOL] if any ( re . search ( regex , name ) for regex in no_grad_regexes ) : [EOL] parameter . requires_grad_ ( False ) [EOL] [EOL] frozen_parameter_names , tunable_parameter_names = get_frozen_and_tunable_parameter_names ( model ) [EOL] logger . info ( [string] ) [EOL] for name in frozen_parameter_names : [EOL] logger . info ( name ) [EOL] logger . info ( [string] ) [EOL] for name in tunable_parameter_names : [EOL] logger . info ( name ) [EOL] [EOL] return TrainerPieces ( model , iterator , train_data , validation_data , test_data , validation_iterator , trainer_params ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $allennlp.data.iterators.data_iterator.DataIterator$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $'TrainerPieces'$ 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Set[typing.Any]$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $allennlp.common.Params$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 0 $None$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $None$ 0 $typing.Any$ 0 0
from typing import Any , List , Dict , Union [EOL] import typing [EOL] import allennlp [EOL] import logging [EOL] import builtins [EOL] [docstring] [EOL] [comment] [EOL] [EOL] import logging [EOL] from typing import Dict , List , Union , Any [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] from allennlp . common . checks import ConfigurationError , check_for_gpu [EOL] from allennlp . models . model import Model [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class TrainerBase ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __init__ ( self , serialization_dir , cuda_device = - [number] ) : [EOL] check_for_gpu ( cuda_device ) [EOL] [EOL] self . _serialization_dir = serialization_dir [EOL] [EOL] [comment] [EOL] if not isinstance ( cuda_device , int ) and not isinstance ( cuda_device , list ) : [EOL] raise ConfigurationError ( [string] . format ( cuda_device ) ) [EOL] [EOL] if isinstance ( cuda_device , list ) : [EOL] self . _multiple_gpu = True [EOL] self . _cuda_devices = cuda_device [EOL] else : [EOL] self . _multiple_gpu = False [EOL] self . _cuda_devices = [ cuda_device ] [EOL] [EOL] def _move_to_gpu ( self , model ) : [EOL] if self . _cuda_devices [ [number] ] != - [number] : [EOL] return model . cuda ( self . _cuda_devices [ [number] ] ) [EOL] else : [EOL] return model [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ classmethod def from_params ( cls , params , serialization_dir , recover = False ) : [EOL] [comment] [EOL] typ3 = params . get ( [string] , { } ) . pop ( [string] , [string] ) [EOL] [EOL] if typ3 == [string] : [EOL] [comment] [EOL] from allennlp . training . trainer import Trainer [EOL] from allennlp . training . trainer_pieces import TrainerPieces [EOL] [EOL] pieces = TrainerPieces . from_params ( params , serialization_dir , recover ) [comment] [EOL] return Trainer . from_params ( model = pieces . model , serialization_dir = serialization_dir , iterator = pieces . iterator , train_data = pieces . train_dataset , validation_data = pieces . validation_dataset , params = pieces . params , validation_iterator = pieces . validation_iterator ) [EOL] else : [EOL] klass = TrainerBase . by_name ( typ3 ) [EOL] [comment] [EOL] is_overriden = klass . from_params . __func__ != TrainerBase . from_params . __func__ [comment] [EOL] assert is_overriden , f" [string] { klass . __name__ } [string] " [EOL] return klass . from_params ( params , serialization_dir , recover ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.int,typing.List]$ 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.Params$ 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.Params$ 0 $builtins.str$ 0 $builtins.bool$ 0 0
from allennlp . training . callback_trainer import CallbackTrainer [EOL] from allennlp . training . no_op_trainer import NoOpTrainer [EOL] from allennlp . training . trainer import Trainer [EOL] from allennlp . training . trainer_base import TrainerBase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional [EOL] import typing [EOL] import torch [EOL] import logging [EOL] import builtins [EOL] from typing import Optional , TYPE_CHECKING [EOL] import logging [EOL] [EOL] import torch [EOL] [EOL] from allennlp . training import util as training_util [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class TrainSupervised ( Callback ) : [EOL] [docstring] [EOL] def __init__ ( self , grad_norm = None , grad_clipping = None ) : [EOL] self . loss = [number] [EOL] self . grad_norm = grad_norm [EOL] self . grad_clipping = grad_clipping [EOL] [EOL] @ handle_event ( Events . TRAINING_START ) def enable_gradient_clipping ( self , trainer ) : [EOL] training_util . enable_gradient_clipping ( trainer . model , self . grad_clipping ) [EOL] [EOL] @ handle_event ( Events . BATCH_START ) def zero_grad ( self , trainer ) : [EOL] [comment] [EOL] [comment] [EOL] trainer . optimizer . zero_grad ( ) [EOL] [EOL] @ handle_event ( Events . FORWARD ) def compute_loss ( self , trainer ) : [EOL] self . loss = trainer . batch_loss ( trainer . batch_group , for_training = True ) [EOL] [EOL] if torch . isnan ( self . loss ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] @ handle_event ( Events . BACKWARD ) def backpropagate_errors ( self , trainer ) : [EOL] self . loss . backward ( ) [EOL] trainer . train_loss += self . loss . item ( ) [EOL] [EOL] @ handle_event ( Events . BACKWARD , priority = [number] ) def optimize ( self , trainer ) : [EOL] trainer . batch_grad_norm = training_util . rescale_gradients ( trainer . model , self . grad_norm ) [EOL] trainer . optimizer . step ( ) [EOL] [EOL] [comment] [EOL] trainer . train_metrics . update ( training_util . get_metrics ( trainer . model , trainer . train_loss , trainer . batches_this_epoch ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import TYPE_CHECKING [EOL] [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class UpdateLearningRate ( Callback ) : [EOL] [docstring] [EOL] def __init__ ( self , learning_rate_scheduler ) : [EOL] self . learning_rate_scheduler = learning_rate_scheduler [EOL] [EOL] @ handle_event ( Events . BACKWARD , priority = [number] ) def step_batch ( self , trainer ) : [EOL] self . learning_rate_scheduler . step_batch ( trainer . batch_num_total ) [EOL] [EOL] @ handle_event ( Events . EPOCH_END ) def step ( self , trainer ) : [EOL] self . learning_rate_scheduler . step ( trainer . latest_val_metric , trainer . epoch_number ) [EOL] [EOL] def get_training_state ( self ) : [EOL] [docstring] [EOL] return { [string] : self . learning_rate_scheduler . state_dict ( ) } [EOL] [EOL] def restore_training_state ( self , training_state ) : [EOL] state_dict = training_state . pop ( [string] , None ) [EOL] [EOL] if state_dict : [EOL] self . learning_rate_scheduler . load_state_dict ( state_dict ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.dict$ 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . callback_handler import CallbackHandler [EOL] from allennlp . training . callbacks . events import Events [EOL] [EOL] from allennlp . training . callbacks . log_to_tensorboard import LogToTensorboard [EOL] from allennlp . training . callbacks . update_learning_rate import UpdateLearningRate [EOL] from allennlp . training . callbacks . update_momentum import UpdateMomentum [EOL] from allennlp . training . callbacks . checkpoint import Checkpoint [EOL] from allennlp . training . callbacks . compute_moving_average import ComputeMovingAverage [EOL] from allennlp . training . callbacks . validate import Validate [EOL] from allennlp . training . callbacks . track_metrics import TrackMetrics [EOL] from allennlp . training . callbacks . train_supervised import TrainSupervised [EOL] from allennlp . training . callbacks . generate_training_batches import GenerateTrainingBatches [EOL] from allennlp . training . callbacks . post_to_url import PostToUrl [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import List , TYPE_CHECKING [EOL] import traceback [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class Checkpoint ( Callback ) : [EOL] [docstring] [EOL] def __init__ ( self , checkpointer , state_dict_attrs = None , other_attrs = None ) : [EOL] self . checkpointer = checkpointer [EOL] self . state_dict_attrs = state_dict_attrs or [ [string] ] [EOL] self . other_attrs = other_attrs or [ [string] ] [EOL] [EOL] @ handle_event ( Events . SAVE_CHECKPOINT ) def save_checkpoint ( self , trainer ) : [EOL] training_states = { } [EOL] [EOL] [comment] [EOL] for attr in self . state_dict_attrs : [EOL] state_attr = getattr ( trainer , attr ) [EOL] if state_attr is not None : [EOL] training_states [ attr ] = state_attr . state_dict ( ) [EOL] [EOL] [comment] [EOL] for attr in self . other_attrs : [EOL] training_states [ attr ] = getattr ( trainer , attr ) [EOL] [EOL] [comment] [EOL] for callback in trainer . handler . callbacks ( ) : [EOL] training_states . update ( callback . get_training_state ( ) ) [EOL] [EOL] is_best_so_far = training_states . pop ( [string] , True ) [EOL] self . checkpointer . save_checkpoint ( model_state = trainer . model . state_dict ( ) , epoch = trainer . checkpoint_epoch , training_states = training_states , is_best_so_far = is_best_so_far ) [EOL] [EOL] @ handle_event ( Events . RESTORE_CHECKPOINT ) def restore_checkpoint ( self , trainer ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] try : [EOL] model_state , training_state = self . checkpointer . restore_checkpoint ( ) [EOL] except RuntimeError : [EOL] traceback . print_exc ( ) [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] [EOL] if not training_state : [EOL] [comment] [EOL] trainer . epoch_number = [number] [EOL] return [EOL] [EOL] trainer . model . load_state_dict ( model_state ) [EOL] [EOL] [comment] [EOL] for attr in self . state_dict_attrs : [EOL] state_attr = getattr ( trainer , attr ) [EOL] if state_attr is not None : [EOL] state_attr . load_state_dict ( training_state [ attr ] ) [EOL] [EOL] [comment] [EOL] for attr in self . other_attrs : [EOL] setattr ( trainer , attr , training_state [ attr ] ) [EOL] [EOL] [comment] [EOL] for callback in trainer . handler . callbacks ( ) : [EOL] callback . restore_training_state ( training_state ) [EOL] [EOL] if isinstance ( training_state [ [string] ] , int ) : [EOL] trainer . epoch_number = training_state [ [string] ] + [number] [EOL] else : [EOL] trainer . epoch_number = int ( training_state [ [string] ] . split ( [string] ) [ [number] ] ) + [number] [EOL] [EOL] @ handle_event ( Events . TRAINING_END ) def load_best_model_state ( self , trainer ) : [EOL] [comment] [EOL] best_model_state = self . checkpointer . best_model_state ( ) [EOL] if best_model_state : [EOL] trainer . model . load_state_dict ( best_model_state ) [EOL] [EOL] @ classmethod def from_params ( cls , params , serialization_dir ) : [comment] [EOL] [comment] [EOL] checkpointer_params = params . pop ( [string] , None ) [EOL] if checkpointer_params : [EOL] checkpointer = Checkpointer . from_params ( checkpointer_params , serialization_dir = serialization_dir ) [EOL] else : [EOL] checkpointer = Checkpointer ( serialization_dir = serialization_dir ) [EOL] [EOL] state_dict_attrs = params . pop ( [string] , None ) [EOL] other_attrs = params . pop ( [string] , None ) [EOL] [EOL] return Checkpoint ( checkpointer , state_dict_attrs , other_attrs ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $'CheckpointCallback'$ 0 0 0 $allennlp.common.params.Params$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import TYPE_CHECKING [EOL] [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] from allennlp . training . momentum_schedulers import MomentumScheduler [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class UpdateMomentum ( Callback ) : [EOL] [docstring] [EOL] def __init__ ( self , momentum_scheduler ) : [EOL] self . momentum_scheduler = momentum_scheduler [EOL] [EOL] @ handle_event ( Events . BACKWARD , priority = [number] ) def step_batch ( self , trainer ) : [EOL] self . momentum_scheduler . step_batch ( trainer . batch_num_total ) [EOL] [EOL] @ handle_event ( Events . EPOCH_END ) def step ( self , trainer ) : [EOL] self . momentum_scheduler . step ( trainer . latest_val_metric , trainer . epoch_number ) [EOL] [EOL] def get_training_state ( self ) : [EOL] return { [string] : self . momentum_scheduler . state_dict ( ) } [EOL] [EOL] def restore_training_state ( self , training_state ) : [EOL] state_dict = training_state . pop ( [string] , None ) [EOL] [EOL] if state_dict : [EOL] self . momentum_scheduler . load_state_dict ( state_dict ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.dict$ 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
class Events : [EOL] TRAINING_START = [string] [EOL] [EOL] EPOCH_START = [string] [EOL] [EOL] BATCH_START = [string] [EOL] [EOL] FORWARD = [string] [EOL] [EOL] BACKWARD = [string] [EOL] [EOL] BATCH_END = [string] [EOL] [EOL] VALIDATE = [string] [EOL] [EOL] EPOCH_END = [string] [EOL] [EOL] TRAINING_END = [string] [EOL] [EOL] SAVE_CHECKPOINT = [string] [EOL] [EOL] RESTORE_CHECKPOINT = [string] [EOL]	0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0
[comment] [EOL] from typing import Iterable , Any [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Iterable , TYPE_CHECKING [EOL] import logging [EOL] import math [EOL] [EOL] from allennlp . common . util import lazy_groups_of [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators import DataIterator [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class GenerateTrainingBatches ( Callback ) : [EOL] [docstring] [EOL] def __init__ ( self , instances , iterator , shuffle = True ) : [EOL] self . instances = instances [EOL] self . iterator = iterator [EOL] self . shuffle = shuffle [EOL] [EOL] @ handle_event ( Events . EPOCH_START ) def generate_batches ( self , trainer ) : [EOL] [comment] [EOL] num_gpus = len ( trainer . _cuda_devices ) [EOL] [EOL] raw_train_generator = self . iterator ( self . instances , num_epochs = [number] , shuffle = self . shuffle ) [EOL] trainer . training_batches = lazy_groups_of ( raw_train_generator , num_gpus ) [EOL] trainer . num_training_batches = math . ceil ( self . iterator . get_num_batches ( self . instances ) / num_gpus ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $builtins.int$ 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import allennlp [EOL] from typing import TYPE_CHECKING [EOL] [EOL] from allennlp . common . params import Params [EOL] from allennlp . models import Model [EOL] from allennlp . training . callbacks . callback import Callback , handle_event [EOL] from allennlp . training . callbacks . events import Events [EOL] from allennlp . training . moving_average import MovingAverage [EOL] [EOL] if TYPE_CHECKING : [EOL] from allennlp . training . callback_trainer import CallbackTrainer [comment] [EOL] [EOL] [EOL] @ Callback . register ( [string] ) class ComputeMovingAverage ( Callback ) : [EOL] [docstring] [EOL] [comment] [EOL] def __init__ ( self , moving_average ) : [EOL] self . moving_average = moving_average [EOL] [EOL] @ handle_event ( Events . BATCH_END ) def apply_moving_average ( self , trainer ) : [EOL] self . moving_average . apply ( trainer . batch_num_total ) [EOL] [EOL] @ handle_event ( Events . SAVE_CHECKPOINT , priority = - [number] ) def assign_average_value_before_saving ( self , trainer ) : [EOL] [comment] [EOL] [comment] [EOL] self . moving_average . assign_average_value ( ) [EOL] [EOL] @ handle_event ( Events . VALIDATE , priority = - [number] ) def assign_average_value_before_validation ( self , trainer ) : [EOL] [comment] [EOL] [comment] [EOL] self . moving_average . assign_average_value ( ) [EOL] [EOL] @ handle_event ( Events . SAVE_CHECKPOINT , priority = [number] ) def restore_values_after_saving ( self , trainer ) : [EOL] [comment] [EOL] self . moving_average . restore ( ) [EOL] [EOL] @ handle_event ( Events . VALIDATE , priority = [number] ) def restore_values_after_validation ( self , _trainer ) : [EOL] [comment] [EOL] self . moving_average . restore ( ) [EOL] [EOL] @ classmethod def from_params ( cls , params , model ) : [comment] [EOL] parameters = [ [ n , p ] for n , p in model . named_parameters ( ) if p . requires_grad ] [EOL] moving_average = MovingAverage . from_params ( params . pop ( [string] ) , parameters = parameters ) [EOL] return ComputeMovingAverage ( moving_average ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'CallbackTrainer'$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $'MovingAverageCallback'$ 0 0 0 $allennlp.common.params.Params$ 0 $allennlp.models.Model$ 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.Model$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.params.Params$ 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 $typing.Any$ 0 0
from allennlp . training . momentum_schedulers . momentum_scheduler import MomentumScheduler [EOL] from allennlp . training . momentum_schedulers . inverted_triangular import InvertedTriangular [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , Any [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . training . scheduler import Scheduler [EOL] [EOL] [EOL] class LearningRateScheduler ( Scheduler , Registrable ) : [EOL] [EOL] def __init__ ( self , optimizer , last_epoch = - [number] ) : [EOL] super ( ) . __init__ ( optimizer , [string] , last_epoch ) [EOL] [EOL] def get_values ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , optimizer , params ) : [comment] [EOL] [comment] [EOL] scheduler_type = params . pop_choice ( [string] , LearningRateScheduler . list_available ( ) ) [EOL] scheduler = LearningRateScheduler . by_name ( scheduler_type ) ( optimizer , ** params . as_dict ( ) ) [comment] [EOL] if isinstance ( scheduler , torch . optim . lr_scheduler . ReduceLROnPlateau ) : [EOL] return _PyTorchLearningRateSchedulerWithMetricsWrapper ( scheduler ) [EOL] elif isinstance ( scheduler , torch . optim . lr_scheduler . _LRScheduler ) : [comment] [EOL] return _PyTorchLearningRateSchedulerWrapper ( scheduler ) [EOL] else : [EOL] return scheduler [EOL] [EOL] [EOL] class _PyTorchLearningRateSchedulerWrapper ( LearningRateScheduler ) : [EOL] [EOL] def __init__ ( self , lr_scheduler ) : [comment] [EOL] self . lr_scheduler = lr_scheduler [EOL] [EOL] def get_values ( self ) : [EOL] return self . lr_scheduler . get_lr ( ) [EOL] [EOL] @ overrides def step ( self , metric = None , epoch = None ) : [EOL] self . lr_scheduler . step ( epoch ) [EOL] [EOL] @ overrides def state_dict ( self ) : [EOL] return self . lr_scheduler . state_dict ( ) [EOL] [EOL] @ overrides def load_state_dict ( self , state_dict ) : [EOL] self . lr_scheduler . load_state_dict ( state_dict ) [EOL] [EOL] [EOL] class _PyTorchLearningRateSchedulerWithMetricsWrapper ( _PyTorchLearningRateSchedulerWrapper ) : [EOL] [EOL] @ overrides def step ( self , metric = None , epoch = None ) : [EOL] if metric is None : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] self . lr_scheduler . step ( metric , epoch ) [EOL] [EOL] [EOL] [comment] [EOL] Registrable . _registry [ LearningRateScheduler ] = { [string] : torch . optim . lr_scheduler . StepLR , [string] : torch . optim . lr_scheduler . MultiStepLR , [string] : torch . optim . lr_scheduler . ExponentialLR , [string] : torch . optim . lr_scheduler . ReduceLROnPlateau , } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import builtins [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . training . metrics . average import Average [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Perplexity ( Average ) : [EOL] [docstring] [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] average_loss = super ( ) . get_metric ( reset ) [EOL] if average_loss == [number] : [EOL] return [number] [EOL] [EOL] [comment] [EOL] return float ( torch . exp ( average_loss ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0
from typing import Tuple , Any [EOL] import typing [EOL] import builtins [EOL] from typing import Tuple [EOL] [EOL] from allennlp . training . metrics . metric import Metric [EOL] from allennlp . training . metrics . fbeta_measure import FBetaMeasure [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class F1Measure ( FBetaMeasure ) : [EOL] [docstring] [EOL] def __init__ ( self , positive_label ) : [EOL] super ( ) . __init__ ( beta = [number] , labels = [ positive_label ] ) [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] metric = super ( ) . get_metric ( reset = reset ) [EOL] [comment] [EOL] [comment] [EOL] precision = metric [ [string] ] [ [number] ] [EOL] recall = metric [ [string] ] [ [number] ] [EOL] fscore = metric [ [string] ] [ [number] ] [EOL] return precision , recall , fscore [EOL] [EOL] @ property def _true_positives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_positive_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _true_positive_sum [ [number] ] [EOL] [EOL] @ property def _true_negatives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_negative_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _true_negative_sum [ [number] ] [EOL] [EOL] @ property def _false_positives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _pred_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _pred_sum [ [number] ] - self . _true_positives [EOL] [EOL] @ property def _false_negatives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _true_sum [ [number] ] - self . _true_positives [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Tuple[builtins.float,builtins.float,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , Tuple , Iterable , Set , Generator [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from collections import Counter [EOL] import math [EOL] from typing import Iterable , Tuple , Dict , Set [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class BLEU ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , ngram_weights = ( [number] , [number] , [number] , [number] ) , exclude_indices = None ) : [EOL] self . _ngram_weights = ngram_weights [EOL] self . _exclude_indices = exclude_indices or set ( ) [EOL] self . _precision_matches = Counter ( ) [EOL] self . _precision_totals = Counter ( ) [EOL] self . _prediction_lengths = [number] [EOL] self . _reference_lengths = [number] [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _precision_matches = Counter ( ) [EOL] self . _precision_totals = Counter ( ) [EOL] self . _prediction_lengths = [number] [EOL] self . _reference_lengths = [number] [EOL] [EOL] def _ngrams ( self , tensor , ngram_size ) : [EOL] ngram_counts = Counter ( ) [EOL] if ngram_size > tensor . size ( - [number] ) : [EOL] return ngram_counts [EOL] for start_position in range ( ngram_size ) : [EOL] for tensor_slice in tensor [ start_position : ] . split ( ngram_size , dim = - [number] ) : [EOL] if tensor_slice . size ( - [number] ) < ngram_size : [EOL] break [EOL] ngram = tuple ( x . item ( ) for x in tensor_slice ) [EOL] if any ( x in self . _exclude_indices for x in ngram ) : [EOL] continue [EOL] ngram_counts [ ngram ] += [number] [EOL] return ngram_counts [EOL] [EOL] def _get_modified_precision_counts ( self , predicted_tokens , reference_tokens , ngram_size ) : [EOL] [docstring] [EOL] clipped_matches = [number] [EOL] total_predicted = [number] [EOL] for batch_num in range ( predicted_tokens . size ( [number] ) ) : [EOL] predicted_row = predicted_tokens [ batch_num , : ] [EOL] reference_row = reference_tokens [ batch_num , : ] [EOL] predicted_ngram_counts = self . _ngrams ( predicted_row , ngram_size ) [EOL] reference_ngram_counts = self . _ngrams ( reference_row , ngram_size ) [EOL] for ngram , count in predicted_ngram_counts . items ( ) : [EOL] clipped_matches += min ( count , reference_ngram_counts [ ngram ] ) [EOL] total_predicted += count [EOL] return clipped_matches , total_predicted [EOL] [EOL] def _get_brevity_penalty ( self ) : [EOL] if self . _prediction_lengths > self . _reference_lengths : [EOL] return [number] [EOL] if self . _reference_lengths == [number] or self . _prediction_lengths == [number] : [EOL] return [number] [EOL] return math . exp ( [number] - self . _reference_lengths / self . _prediction_lengths ) [EOL] [EOL] def _get_valid_tokens_mask ( self , tensor ) : [EOL] valid_tokens_mask = torch . ones ( tensor . size ( ) , dtype = torch . uint8 ) [EOL] for index in self . _exclude_indices : [EOL] valid_tokens_mask = valid_tokens_mask & ( tensor != index ) [EOL] return valid_tokens_mask [EOL] [EOL] @ overrides def __call__ ( self , predictions , gold_targets ) : [EOL] [docstring] [EOL] predictions , gold_targets = self . unwrap_to_tensors ( predictions , gold_targets ) [EOL] for ngram_size , _ in enumerate ( self . _ngram_weights , start = [number] ) : [EOL] precision_matches , precision_totals = self . _get_modified_precision_counts ( predictions , gold_targets , ngram_size ) [EOL] self . _precision_matches [ ngram_size ] += precision_matches [EOL] self . _precision_totals [ ngram_size ] += precision_totals [EOL] if not self . _exclude_indices : [EOL] self . _prediction_lengths += predictions . size ( [number] ) * predictions . size ( [number] ) [EOL] self . _reference_lengths += gold_targets . size ( [number] ) * gold_targets . size ( [number] ) [EOL] else : [EOL] valid_predictions_mask = self . _get_valid_tokens_mask ( predictions ) [EOL] self . _prediction_lengths += valid_predictions_mask . sum ( ) . item ( ) [EOL] valid_gold_targets_mask = self . _get_valid_tokens_mask ( gold_targets ) [EOL] self . _reference_lengths += valid_gold_targets_mask . sum ( ) . item ( ) [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] brevity_penalty = self . _get_brevity_penalty ( ) [EOL] ngram_scores = ( weight * ( math . log ( self . _precision_matches [ n ] + [number] ) - math . log ( self . _precision_totals [ n ] + [number] ) ) for n , weight in enumerate ( self . _ngram_weights , start = [number] ) ) [EOL] bleu = brevity_penalty * math . exp ( sum ( ngram_scores ) ) [EOL] if reset : [EOL] self . reset ( ) [EOL] return { [string] : bleu } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Generator[builtins.float,None,None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Generator[builtins.float,None,None]$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0
from typing import Dict , List , Optional , Type , Any , Set , Callable [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , List , Optional , Set , Callable [EOL] from collections import defaultdict [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . nn . util import get_lengths_from_binary_sequence_mask [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . training . metrics . metric import Metric [EOL] from allennlp . data . dataset_readers . dataset_utils . span_utils import ( bio_tags_to_spans , bioul_tags_to_spans , iob1_tags_to_spans , bmes_tags_to_spans , TypedStringSpan ) [EOL] [EOL] [EOL] TAGS_TO_SPANS_FUNCTION_TYPE = Callable [ [ List [ str ] , Optional [ List [ str ] ] ] , List [ TypedStringSpan ] ] [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class SpanBasedF1Measure ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self , vocabulary , tag_namespace = [string] , ignore_classes = None , label_encoding = [string] , tags_to_spans_function = None ) : [EOL] [docstring] [EOL] if label_encoding and tags_to_spans_function : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] if label_encoding : [EOL] if label_encoding not in [ [string] , [string] , [string] , [string] ] : [EOL] raise ConfigurationError ( [string] ) [EOL] elif tags_to_spans_function is None : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] self . _label_encoding = label_encoding [EOL] self . _tags_to_spans_function = tags_to_spans_function [EOL] self . _label_vocabulary = vocabulary . get_index_to_token_vocabulary ( tag_namespace ) [EOL] self . _ignore_classes = ignore_classes or [ ] [EOL] [EOL] [comment] [EOL] self . _true_positives = defaultdict ( int ) [EOL] self . _false_positives = defaultdict ( int ) [EOL] self . _false_negatives = defaultdict ( int ) [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , prediction_map = None ) : [EOL] [docstring] [EOL] if mask is None : [EOL] mask = torch . ones_like ( gold_labels ) [EOL] [EOL] predictions , gold_labels , mask , prediction_map = self . unwrap_to_tensors ( predictions , gold_labels , mask , prediction_map ) [EOL] [EOL] num_classes = predictions . size ( - [number] ) [EOL] if ( gold_labels >= num_classes ) . any ( ) : [EOL] raise ConfigurationError ( [string] [string] . format ( num_classes ) ) [EOL] [EOL] sequence_lengths = get_lengths_from_binary_sequence_mask ( mask ) [EOL] argmax_predictions = predictions . max ( - [number] ) [ [number] ] [EOL] [EOL] if prediction_map is not None : [EOL] argmax_predictions = torch . gather ( prediction_map , [number] , argmax_predictions ) [EOL] gold_labels = torch . gather ( prediction_map , [number] , gold_labels . long ( ) ) [EOL] [EOL] argmax_predictions = argmax_predictions . float ( ) [EOL] [EOL] [comment] [EOL] batch_size = gold_labels . size ( [number] ) [EOL] for i in range ( batch_size ) : [EOL] sequence_prediction = argmax_predictions [ i , : ] [EOL] sequence_gold_label = gold_labels [ i , : ] [EOL] length = sequence_lengths [ i ] [EOL] [EOL] if length == [number] : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] [EOL] predicted_string_labels = [ self . _label_vocabulary [ label_id ] for label_id in sequence_prediction [ : length ] . tolist ( ) ] [EOL] gold_string_labels = [ self . _label_vocabulary [ label_id ] for label_id in sequence_gold_label [ : length ] . tolist ( ) ] [EOL] [EOL] tags_to_spans_function = None [EOL] [comment] [EOL] if self . _label_encoding is None and self . _tags_to_spans_function : [EOL] tags_to_spans_function = self . _tags_to_spans_function [EOL] [comment] [EOL] elif self . _label_encoding == [string] : [EOL] tags_to_spans_function = bio_tags_to_spans [EOL] elif self . _label_encoding == [string] : [EOL] tags_to_spans_function = iob1_tags_to_spans [EOL] elif self . _label_encoding == [string] : [EOL] tags_to_spans_function = bioul_tags_to_spans [EOL] elif self . _label_encoding == [string] : [EOL] tags_to_spans_function = bmes_tags_to_spans [EOL] [EOL] predicted_spans = tags_to_spans_function ( predicted_string_labels , self . _ignore_classes ) [EOL] gold_spans = tags_to_spans_function ( gold_string_labels , self . _ignore_classes ) [EOL] [EOL] predicted_spans = self . _handle_continued_spans ( predicted_spans ) [EOL] gold_spans = self . _handle_continued_spans ( gold_spans ) [EOL] [EOL] for span in predicted_spans : [EOL] if span in gold_spans : [EOL] self . _true_positives [ span [ [number] ] ] += [number] [EOL] gold_spans . remove ( span ) [EOL] else : [EOL] self . _false_positives [ span [ [number] ] ] += [number] [EOL] [comment] [EOL] for span in gold_spans : [EOL] self . _false_negatives [ span [ [number] ] ] += [number] [EOL] [EOL] @ staticmethod def _handle_continued_spans ( spans ) : [EOL] [docstring] [EOL] span_set = set ( spans ) [EOL] continued_labels = [ label [ [number] : ] for ( label , span ) in span_set if label . startswith ( [string] ) ] [EOL] for label in continued_labels : [EOL] continued_spans = { span for span in span_set if label in span [ [number] ] } [EOL] [EOL] span_start = min ( span [ [number] ] [ [number] ] for span in continued_spans ) [EOL] span_end = max ( span [ [number] ] [ [number] ] for span in continued_spans ) [EOL] replacement_span = ( label , ( span_start , span_end ) ) [EOL] [EOL] span_set . difference_update ( continued_spans ) [EOL] span_set . add ( replacement_span ) [EOL] [EOL] return list ( span_set ) [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] all_tags = set ( ) [EOL] all_tags . update ( self . _true_positives . keys ( ) ) [EOL] all_tags . update ( self . _false_positives . keys ( ) ) [EOL] all_tags . update ( self . _false_negatives . keys ( ) ) [EOL] all_metrics = { } [EOL] for tag in all_tags : [EOL] precision , recall , f1_measure = self . _compute_metrics ( self . _true_positives [ tag ] , self . _false_positives [ tag ] , self . _false_negatives [ tag ] ) [EOL] precision_key = [string] + [string] + tag [EOL] recall_key = [string] + [string] + tag [EOL] f1_key = [string] + [string] + tag [EOL] all_metrics [ precision_key ] = precision [EOL] all_metrics [ recall_key ] = recall [EOL] all_metrics [ f1_key ] = f1_measure [EOL] [EOL] [comment] [EOL] precision , recall , f1_measure = self . _compute_metrics ( sum ( self . _true_positives . values ( ) ) , sum ( self . _false_positives . values ( ) ) , sum ( self . _false_negatives . values ( ) ) ) [EOL] all_metrics [ [string] ] = precision [EOL] all_metrics [ [string] ] = recall [EOL] all_metrics [ [string] ] = f1_measure [EOL] if reset : [EOL] self . reset ( ) [EOL] return all_metrics [EOL] [EOL] @ staticmethod def _compute_metrics ( true_positives , false_positives , false_negatives ) : [EOL] precision = float ( true_positives ) / float ( true_positives + false_positives + [number] ) [EOL] recall = float ( true_positives ) / float ( true_positives + false_negatives + [number] ) [EOL] f1_measure = [number] * ( ( precision * recall ) / ( precision + recall + [number] ) ) [EOL] return precision , recall , f1_measure [EOL] [EOL] def reset ( self ) : [EOL] self . _true_positives = defaultdict ( int ) [EOL] self . _false_positives = defaultdict ( int ) [EOL] self . _false_negatives = defaultdict ( int ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 $typing.List[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 $typing.List[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 $allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 $typing.Set[typing.Any]$ 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 $allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan$ 0 0 0 0 0 0 $typing.Set[allennlp.data.dataset_readers.dataset_utils.span_utils.TypedStringSpan]$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0
from typing import Optional , Any [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Optional [EOL] import math [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . training . metrics . covariance import Covariance [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class PearsonCorrelation ( Metric ) : [EOL] [docstring] [EOL] def __init__ ( self ) : [EOL] self . _predictions_labels_covariance = Covariance ( ) [EOL] self . _predictions_variance = Covariance ( ) [EOL] self . _labels_variance = Covariance ( ) [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . unwrap_to_tensors ( predictions , gold_labels , mask ) [EOL] self . _predictions_labels_covariance ( predictions , gold_labels , mask ) [EOL] self . _predictions_variance ( predictions , predictions , mask ) [EOL] self . _labels_variance ( gold_labels , gold_labels , mask ) [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] covariance = self . _predictions_labels_covariance . get_metric ( reset = reset ) [EOL] predictions_variance = self . _predictions_variance . get_metric ( reset = reset ) [EOL] labels_variance = self . _labels_variance . get_metric ( reset = reset ) [EOL] if reset : [EOL] self . reset ( ) [EOL] pearson_r = covariance / ( math . sqrt ( predictions_variance ) * math . sqrt ( labels_variance ) ) [EOL] return pearson_r [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _predictions_labels_covariance . reset ( ) [EOL] self . _predictions_variance . reset ( ) [EOL] self . _labels_variance . reset ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Set , Any , List [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Instance [EOL] from allennlp . data . fields import TextField , LabelField [EOL] from allennlp . data . tokenizers import Token [EOL] [EOL] class TestInstance ( AllenNlpTestCase ) : [EOL] def test_instance_implements_mutable_mapping ( self ) : [EOL] words_field = TextField ( [ Token ( [string] ) ] , { } ) [EOL] label_field = LabelField ( [number] , skip_indexing = True ) [EOL] instance = Instance ( { [string] : words_field , [string] : label_field } ) [EOL] [EOL] assert instance [ [string] ] == words_field [EOL] assert instance [ [string] ] == label_field [EOL] assert len ( instance ) == [number] [EOL] [EOL] keys = { k for k , v in instance . items ( ) } [EOL] assert keys == { [string] , [string] } [EOL] [EOL] values = [ v for k , v in instance . items ( ) ] [EOL] assert words_field in values [EOL] assert label_field in values [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0
	0
from typing import List , Any , Set , Counter [EOL] import typing [EOL] import collections [EOL] from collections import Counter [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataset_readers import InterleavingDatasetReader [EOL] from allennlp . data . iterators import HomogeneousBatchIterator [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . tests . data . dataset_readers . interleaving_dataset_reader_test import PlainTextReader [EOL] [EOL] [EOL] class TestHomogeneousBatchIterator ( AllenNlpTestCase ) : [EOL] def test_batches ( self ) : [EOL] readers = { [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) } [EOL] [EOL] reader = InterleavingDatasetReader ( readers ) [EOL] data_dir = self . FIXTURES_ROOT / [string] [EOL] [EOL] file_path = f""" [string] { data_dir / [string] } [string] { data_dir / [string] } [string] { data_dir / [string] } [string] """ [EOL] [EOL] instances = list ( reader . read ( file_path ) ) [EOL] vocab = Vocabulary . from_instances ( instances ) [EOL] [EOL] actual_instance_type_counts = Counter ( instance . fields [ [string] ] . metadata for instance in instances ) [EOL] [EOL] iterator = HomogeneousBatchIterator ( batch_size = [number] ) [EOL] iterator . index_with ( vocab ) [EOL] [EOL] observed_instance_type_counts = Counter ( ) [EOL] [EOL] for batch in iterator ( instances , num_epochs = [number] , shuffle = True ) : [EOL] [comment] [EOL] instance_types = set ( batch [ [string] ] ) [EOL] assert len ( instance_types ) == [number] [EOL] [EOL] observed_instance_type_counts . update ( batch [ [string] ] ) [EOL] [EOL] assert observed_instance_type_counts == actual_instance_type_counts [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $collections.Counter[typing.Any]$ 0 $collections.Counter[typing.Any]$ 0
[comment] [EOL] from typing import List , Tuple , Any [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data . iterators import BucketIterator [EOL] from allennlp . tests . data . iterators . basic_iterator_test import IteratorTest [EOL] [EOL] [EOL] class TestBucketIterator ( IteratorTest ) : [EOL] [comment] [EOL] def test_create_batches_groups_correctly ( self ) : [EOL] iterator = BucketIterator ( batch_size = [number] , padding_noise = [number] , sorting_keys = [ ( [string] , [string] ) ] ) [EOL] iterator . index_with ( self . vocab ) [EOL] batches = list ( iterator . _create_batches ( self . instances , shuffle = False ) ) [EOL] grouped_instances = [ batch . instances for batch in batches ] [EOL] assert grouped_instances == [ [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] ] ] [EOL] [EOL] def test_create_batches_groups_correctly_with_max_instances ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] iterator = BucketIterator ( batch_size = [number] , padding_noise = [number] , sorting_keys = [ ( [string] , [string] ) ] , max_instances_in_memory = [number] ) [EOL] iterator . index_with ( self . vocab ) [EOL] for test_instances in ( self . instances , self . lazy_instances ) : [EOL] batches = list ( iterator . _create_batches ( test_instances , shuffle = False ) ) [EOL] grouped_instances = [ batch . instances for batch in batches ] [EOL] assert grouped_instances == [ [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] ] [EOL] [EOL] def test_biggest_batch_first_works ( self ) : [EOL] iterator = BucketIterator ( batch_size = [number] , padding_noise = [number] , sorting_keys = [ ( [string] , [string] ) ] , biggest_batch_first = True ) [EOL] iterator . index_with ( self . vocab ) [EOL] batches = list ( iterator . _create_batches ( self . instances , shuffle = False ) ) [EOL] grouped_instances = [ batch . instances for batch in batches ] [EOL] assert grouped_instances == [ [ self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] ] [EOL] [EOL] def test_from_params ( self ) : [EOL] [comment] [EOL] params = Params ( { } ) [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] iterator = BucketIterator . from_params ( params ) [EOL] [EOL] sorting_keys = [ ( [string] , [string] ) , ( [string] , [string] ) ] [EOL] params [ [string] ] = sorting_keys [EOL] iterator = BucketIterator . from_params ( params ) [EOL] [EOL] assert iterator . _sorting_keys == sorting_keys [EOL] assert iterator . _padding_noise == [number] [EOL] assert not iterator . _biggest_batch_first [EOL] assert iterator . _batch_size == [number] [EOL] [EOL] params = Params ( { [string] : sorting_keys , [string] : [number] , [string] : True , [string] : [number] } ) [EOL] [EOL] iterator = BucketIterator . from_params ( params ) [EOL] assert iterator . _sorting_keys == sorting_keys [EOL] assert iterator . _padding_noise == [number] [EOL] assert iterator . _biggest_batch_first [EOL] assert iterator . _batch_size == [number] [EOL] [EOL] def test_bucket_iterator_maximum_samples_per_batch ( self ) : [EOL] iterator = BucketIterator ( batch_size = [number] , padding_noise = [number] , sorting_keys = [ ( [string] , [string] ) ] , maximum_samples_per_batch = [ [string] , [number] ] ) [EOL] iterator . index_with ( self . vocab ) [EOL] batches = list ( iterator . _create_batches ( self . instances , shuffle = False ) ) [EOL] stats = self . get_batches_stats ( batches ) [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == len ( self . instances ) [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == [ [number] , [number] , [number] ] [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == [ [number] , [number] , [number] ] [EOL] [EOL] def test_maximum_samples_per_batch_packs_tightly ( self ) : [EOL] token_counts = [ [number] , [number] , [number] ] [EOL] test_instances = self . create_instances_from_token_counts ( token_counts ) [EOL] [EOL] iterator = BucketIterator ( batch_size = [number] , padding_noise = [number] , sorting_keys = [ ( [string] , [string] ) ] , maximum_samples_per_batch = [ [string] , [number] ] ) [EOL] iterator . index_with ( self . vocab ) [EOL] batches = list ( iterator . _create_batches ( test_instances , shuffle = False ) ) [EOL] stats = self . get_batches_stats ( batches ) [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == len ( test_instances ) [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == [ [number] , [number] ] [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == [ [number] , [number] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import json [EOL] import tarfile [EOL] import spacy [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token [EOL] from allennlp . data . token_indexers import OpenaiTransformerBytePairIndexer [EOL] from allennlp . data . token_indexers . openai_transformer_byte_pair_indexer import text_standardize [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] [EOL] class TestOpenaiTransformerBytePairIndexer ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [EOL] encoder_path = self . TEST_DIR / [string] [EOL] bpe_path = self . TEST_DIR / [string] [EOL] transformer_model_path = self . TEST_DIR / [string] [EOL] [EOL] symbols = [ [string] , [string] , [string] , [string] , [string] ] [EOL] byte_pairs = [ ( sym1 , sym2 + end ) for sym1 in symbols for sym2 in symbols for end in ( [string] , [string] ) ] [comment] [EOL] encoding = { f"{ sym1 }{ sym2 }" : idx + [number] for idx , ( sym1 , sym2 ) in enumerate ( byte_pairs ) } [EOL] [EOL] [EOL] with open ( encoder_path , [string] ) as encoder_file : [EOL] json . dump ( encoding , encoder_file ) [EOL] [EOL] with open ( bpe_path , [string] ) as bpe_file : [EOL] bpe_file . write ( [string] ) [EOL] for sym1 , sym2 in byte_pairs : [EOL] bpe_file . write ( f"{ sym1 } [string] { sym2 } [string] " ) [EOL] bpe_file . write ( [string] ) [EOL] [EOL] with tarfile . open ( transformer_model_path , [string] ) as tf : [EOL] tf . add ( encoder_path , [string] ) [EOL] tf . add ( bpe_path , [string] ) [EOL] [EOL] self . encoding = encoding [EOL] self . byte_pairs = byte_pairs [EOL] [EOL] def _create_indexer_vocab ( self , tokens_to_add = None ) : [EOL] [comment] [EOL] self . indexer = OpenaiTransformerBytePairIndexer ( self . encoding , self . byte_pairs , tokens_to_add = tokens_to_add ) [EOL] self . vocab = Vocabulary ( non_padded_namespaces = [ [string] ] ) [EOL] [EOL] def test_bpe ( self ) : [EOL] self . _create_indexer_vocab ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] token = Token ( [string] ) [EOL] assert self . indexer . byte_pair_encode ( token ) == [ [string] , [string] ] [EOL] [EOL] [comment] [EOL] token = Token ( [string] ) [EOL] assert self . indexer . byte_pair_encode ( token ) == [ [string] , [string] ] [EOL] [EOL] [comment] [EOL] token = Token ( [string] ) [EOL] assert self . indexer . byte_pair_encode ( token ) == [ [string] , [string] ] [EOL] [EOL] [comment] [EOL] token = Token ( [string] ) [EOL] assert self . indexer . byte_pair_encode ( token ) == [ [string] ] [EOL] [EOL] def test_tokens_to_indices ( self ) : [EOL] self . _create_indexer_vocab ( ) [EOL] [EOL] tokens = [ Token ( [string] ) , Token ( [string] ) , Token ( [string] ) , Token ( [string] ) ] [EOL] [EOL] [comment] [EOL] assert [string] not in self . vocab . _index_to_token [EOL] assert [string] not in self . vocab . _token_to_index [EOL] [EOL] indices = self . indexer . tokens_to_indices ( tokens , self . vocab , [string] ) [EOL] [EOL] [comment] [EOL] i2t = self . vocab . _index_to_token . get ( [string] ) [EOL] t2i = self . vocab . _token_to_index . get ( [string] ) [EOL] assert len ( i2t ) == [number] * [number] * [number] [EOL] assert len ( t2i ) == [number] * [number] * [number] [EOL] [EOL] assert set ( indices . keys ( ) ) == { [string] , [string] , [string] } [EOL] [EOL] text_tokens = indices [ [string] ] [EOL] offsets = indices [ [string] ] [EOL] [EOL] assert text_tokens [ : [number] ] == [ self . indexer . encoder . get ( symbol , [number] ) for symbol in [ [string] , [string] ] + [ [string] ] + [ [string] , [string] ] + [ [string] ] ] [EOL] [EOL] assert offsets == [ [number] , [number] , [number] , [number] , ] [EOL] [EOL] def test_raises_with_too_long_sentence ( self ) : [EOL] self . _create_indexer_vocab ( ) [EOL] [EOL] tokens = [ Token ( [string] ) for _ in range ( [number] ) ] [EOL] [EOL] with pytest . raises ( RuntimeError ) : [EOL] self . indexer . tokens_to_indices ( tokens , self . vocab , [string] ) [EOL] [EOL] def test_with_extra_tokens ( self ) : [EOL] self . _create_indexer_vocab ( tokens_to_add = [ [string] , [string] ] ) [EOL] tokens = [ Token ( [string] ) , Token ( [string] ) , Token ( [string] ) , Token ( [string] ) , Token ( [string] ) , Token ( [string] ) ] [EOL] indices = self . indexer . tokens_to_indices ( tokens , self . vocab , [string] ) [EOL] assert indices [ [string] ] [ : [number] ] == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] @ pytest . mark . skip ( ) def test_for_correctness_with_fixture ( self ) : [EOL] bpe_path = [string] [EOL] indexer = OpenaiTransformerBytePairIndexer ( model_path = bpe_path ) [EOL] [EOL] with open ( self . FIXTURES_ROOT / [string] / [string] , [string] ) as fin : [EOL] sentences = fin . read ( ) . strip ( ) . split ( [string] ) [EOL] with open ( self . FIXTURES_ROOT / [string] / [string] , [string] ) as fin : [EOL] expected_indices = json . load ( fin ) [EOL] [EOL] [comment] [EOL] nlp = spacy . load ( [string] ) [EOL] [EOL] for k , sentence in enumerate ( sentences ) : [EOL] tokens = [ token . text for token in nlp ( text_standardize ( sentence ) ) if not token . is_space ] [EOL] indices = indexer . tokens_to_indices ( [ Token ( token ) for token in tokens ] , Vocabulary ( ) , [string] ) [EOL] non_padded_indices = [ i for i in indices [ [string] ] if i != [number] ] [EOL] assert non_padded_indices == expected_indices [ k ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0
	0
[comment] [EOL] from typing import List , Any , DefaultDict [EOL] import typing [EOL] from collections import defaultdict [EOL] [EOL] import pytest [EOL] import numpy [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . fields import TextField , SequenceLabelField [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] [EOL] [EOL] class TestSequenceLabelField ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( TestSequenceLabelField , self ) . setUp ( ) [EOL] self . text = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } ) [EOL] [EOL] def test_tag_length_mismatch_raises ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] wrong_tags = [ [string] , [string] , [string] ] [EOL] _ = SequenceLabelField ( wrong_tags , self . text ) [EOL] [EOL] def test_count_vocab_items_correctly_indexes_tags ( self ) : [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sequence_label_field = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] sequence_label_field . count_vocab_items ( counter ) [EOL] [EOL] assert counter [ [string] ] [ [string] ] == [number] [EOL] assert counter [ [string] ] [ [string] ] == [number] [EOL] assert counter [ [string] ] [ [string] ] == [number] [EOL] assert set ( counter . keys ( ) ) == { [string] } [EOL] [EOL] def test_index_converts_field_correctly ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] b_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] i_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] o_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sequence_label_field = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] sequence_label_field . index ( vocab ) [EOL] [EOL] [comment] [EOL] assert sequence_label_field . _indexed_labels == [ b_index , i_index , o_index , o_index , o_index ] [EOL] [comment] [EOL] [EOL] def test_as_tensor_produces_integer_targets ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sequence_label_field = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] sequence_label_field . index ( vocab ) [EOL] padding_lengths = sequence_label_field . get_padding_lengths ( ) [EOL] tensor = sequence_label_field . as_tensor ( padding_lengths ) . detach ( ) . cpu ( ) . numpy ( ) [EOL] numpy . testing . assert_array_almost_equal ( tensor , numpy . array ( [ [number] , [number] , [number] , [number] , [number] ] ) ) [EOL] [EOL] def test_sequence_label_field_raises_on_incorrect_type ( self ) : [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = SequenceLabelField ( [ [ ] , [ ] , [ ] , [ ] , [ ] ] , self . text ) [EOL] [EOL] def test_class_variables_for_namespace_warnings_work_correctly ( self ) : [EOL] [comment] [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] assert [string] not in SequenceLabelField . _already_warned_namespaces [EOL] with self . assertLogs ( logger = [string] , level = [string] ) : [EOL] _ = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] [EOL] [comment] [EOL] assert [string] in SequenceLabelField . _already_warned_namespaces [EOL] with pytest . raises ( AssertionError ) : [EOL] with self . assertLogs ( logger = [string] , level = [string] ) : [EOL] _ = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] [EOL] [comment] [EOL] assert [string] not in SequenceLabelField . _already_warned_namespaces [EOL] with self . assertLogs ( logger = [string] , level = [string] ) : [EOL] _ = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] [EOL] def test_printing_doesnt_crash ( self ) : [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sequence_label_field = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] print ( sequence_label_field ) [EOL] [EOL] def test_sequence_methods ( self ) : [EOL] tags = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sequence_label_field = SequenceLabelField ( tags , self . text , label_namespace = [string] ) [EOL] [EOL] assert len ( sequence_label_field ) == [number] [EOL] assert sequence_label_field [ [number] ] == [string] [EOL] assert [ label for label in sequence_label_field ] == tags [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[builtins.str]$ 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import cast [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . data . dataset_readers import Event2MindDatasetReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . instance import Instance [EOL] [EOL] def get_text ( key , instance ) : [EOL] return [ t . text for t in cast ( TextField , instance . fields [ key ] ) . tokens ] [EOL] [EOL] class TestEvent2MindDatasetReader : [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read ( self , lazy ) : [EOL] reader = Event2MindDatasetReader ( lazy = lazy ) [EOL] instances = reader . read ( str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] assert len ( instances ) == [number] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [comment] [EOL] [comment] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_with_dummy_instances_for_vocab_generation ( self , lazy ) : [EOL] reader = Event2MindDatasetReader ( lazy = lazy , dummy_instances_for_vocab_generation = True ) [EOL] instances = reader . read ( str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] assert len ( instances ) == [number] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL] assert get_text ( [string] , instance ) == [ [string] , [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from allennlp . data . dataset_readers . semantic_dependency_parsing import SemanticDependenciesDatasetReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestSemanticDependencyParsingDatasetReader : [EOL] def test_read_from_file ( self ) : [EOL] reader = SemanticDependenciesDatasetReader ( ) [EOL] instances = reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance = instances [ [number] ] [EOL] arcs = instance . fields [ [string] ] [EOL] tokens = [ x . text for x in instance . fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert arcs . indices == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] assert arcs . labels == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] instance = instances [ [number] ] [EOL] arcs = instance . fields [ [string] ] [EOL] tokens = [ x . text for x in instance . fields [ [string] ] . tokens ] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert arcs . indices == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] assert arcs . labels == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any , List , Dict , Union [EOL] import typing [EOL] import pytest [EOL] import spacy [EOL] [EOL] from allennlp . data . dataset_readers import TextClassificationJsonReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestTextClassificationJsonReader : [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_set_skip_indexing_true ( self , lazy ) : [EOL] reader = TextClassificationJsonReader ( lazy = lazy , skip_label_indexing = True ) [EOL] ag_path = AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [number] } [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [number] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] [EOL] with pytest . raises ( ValueError ) as exec_info : [EOL] ag_path = AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] [EOL] ensure_list ( reader . read ( ag_path ) ) [EOL] assert str ( exec_info . value ) == [string] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file_ag_news_corpus ( self , lazy ) : [EOL] reader = TextClassificationJsonReader ( lazy = lazy ) [EOL] ag_path = AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file_ag_news_corpus_and_truncates_properly ( self , lazy ) : [EOL] reader = TextClassificationJsonReader ( lazy = lazy , max_sequence_length = [number] ) [EOL] ag_path = AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL] [EOL] @ pytest . mark . skipif ( spacy . __version__ < [string] , reason = [string] ) @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file_ag_news_corpus_and_segments_sentences_properly ( self , lazy ) : [EOL] reader = TextClassificationJsonReader ( lazy = lazy , segment_sentences = True ) [EOL] ag_path = AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [ [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , [string] : [string] } [EOL] instance2 = { [string] : [ [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] text = [ [ token . text for token in sentence . tokens ] for sentence in fields [ [string] ] ] [EOL] assert text == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] text = [ [ token . text for token in sentence . tokens ] for sentence in fields [ [string] ] ] [EOL] assert text == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] text = [ [ token . text for token in sentence . tokens ] for sentence in fields [ [string] ] ] [EOL] assert text == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[typing.List[builtins.str]],builtins.str]]$ 0 0 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import os [EOL] import shutil [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataset_readers import SnliReader [EOL] from allennlp . data . dataset_readers . dataset_reader import _LazyInstances [EOL] [EOL] [EOL] class DatasetReaderTest ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . cache_directory = str ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def tearDown ( self ) : [EOL] super ( ) . tearDown ( ) [EOL] shutil . rmtree ( self . cache_directory ) [EOL] [EOL] def test_read_creates_cache_file_when_not_present ( self ) : [EOL] snli_file = self . FIXTURES_ROOT / [string] / [string] [EOL] reader = SnliReader ( ) [EOL] reader . cache_data ( self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( snli_file ) [EOL] assert not os . path . exists ( cache_file ) [EOL] reader . read ( snli_file ) [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] def test_read_uses_existing_cache_file_when_present ( self ) : [EOL] snli_file = self . FIXTURES_ROOT / [string] / [string] [EOL] snli_copy_file = str ( snli_file ) + [string] [EOL] shutil . copyfile ( snli_file , snli_copy_file ) [EOL] reader = SnliReader ( ) [EOL] reader . cache_data ( self . cache_directory ) [EOL] [EOL] [comment] [EOL] instances = reader . read ( snli_copy_file ) [EOL] [comment] [EOL] os . remove ( snli_copy_file ) [EOL] cached_instances = reader . read ( snli_copy_file ) [EOL] [comment] [EOL] assert len ( instances ) == len ( cached_instances ) [EOL] for instance , cached_instance in zip ( instances , cached_instances ) : [EOL] assert instance . fields == cached_instance . fields [EOL] [EOL] def test_read_only_creates_cache_file_once ( self ) : [EOL] snli_file = self . FIXTURES_ROOT / [string] / [string] [EOL] reader = SnliReader ( ) [EOL] reader . cache_data ( self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( snli_file ) [EOL] [EOL] [comment] [EOL] reader . read ( snli_file ) [EOL] assert os . path . exists ( cache_file ) [EOL] with open ( cache_file , [string] ) as in_file : [EOL] cache_contents = in_file . read ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] reader . read ( snli_file ) [EOL] reader . read ( snli_file ) [EOL] reader . read ( snli_file ) [EOL] reader . read ( snli_file ) [EOL] with open ( cache_file , [string] ) as in_file : [EOL] final_cache_contents = in_file . read ( ) [EOL] assert cache_contents == final_cache_contents [EOL] [EOL] def test_caching_works_with_lazy_reading ( self ) : [EOL] snli_file = self . FIXTURES_ROOT / [string] / [string] [EOL] snli_copy_file = str ( snli_file ) + [string] [EOL] shutil . copyfile ( snli_file , snli_copy_file ) [EOL] reader = SnliReader ( lazy = True ) [EOL] reader . cache_data ( self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( snli_copy_file ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] instances = reader . read ( snli_copy_file ) [EOL] assert isinstance ( instances , _LazyInstances ) [EOL] [EOL] [comment] [EOL] assert not os . path . exists ( cache_file ) [EOL] first_pass_instances = [ ] [EOL] for instance in instances : [EOL] first_pass_instances . append ( instance ) [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] os . remove ( snli_copy_file ) [EOL] second_pass_instances = [ ] [EOL] for instance in instances : [EOL] second_pass_instances . append ( instance ) [EOL] [EOL] [comment] [EOL] assert len ( first_pass_instances ) == len ( second_pass_instances ) [EOL] for instance , cached_instance in zip ( first_pass_instances , second_pass_instances ) : [EOL] assert instance . fields == cached_instance . fields [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] reader = SnliReader ( lazy = False ) [EOL] reader . cache_data ( self . cache_directory ) [EOL] cached_instances = reader . read ( snli_copy_file ) [EOL] assert len ( first_pass_instances ) == len ( cached_instances ) [EOL] for instance , cached_instance in zip ( first_pass_instances , cached_instances ) : [EOL] assert instance . fields == cached_instance . fields [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . data import DatasetReader [EOL] from allennlp . data . vocabulary import Vocabulary , DEFAULT_OOV_TOKEN [EOL] [EOL] [EOL] class TestCopyNetReader ( AllenNlpTestCase ) : [EOL] [EOL] def setUp ( self ) : [EOL] super ( TestCopyNetReader , self ) . setUp ( ) [EOL] params = Params . from_file ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] self . reader = DatasetReader . from_params ( params [ [string] ] ) [EOL] instances = self . reader . read ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] self . instances = ensure_list ( instances ) [EOL] self . vocab = Vocabulary . from_params ( params = params [ [string] ] , instances = instances ) [EOL] [EOL] def test_vocab_namespaces ( self ) : [EOL] assert self . vocab . get_vocab_size ( [string] ) > [number] [EOL] [EOL] def test_instances ( self ) : [EOL] assert len ( self . instances ) == [number] [EOL] assert set ( self . instances [ [number] ] . fields . keys ( ) ) == set ( ( [string] , [string] , [string] , [string] , [string] , [string] ) ) [EOL] [EOL] def test_tokens ( self ) : [EOL] fields = self . instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] [ [string] ] == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] def test_source_and_target_token_ids ( self ) : [EOL] source_token_ids = self . instances [ [number] ] . fields [ [string] ] . array [EOL] target_token_ids = self . instances [ [number] ] . fields [ [string] ] . array [EOL] assert list ( source_token_ids ) == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [comment] [EOL] assert list ( target_token_ids ) == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [comment] [EOL] [EOL] def test_source_to_target ( self ) : [EOL] source_to_target_field = self . instances [ [number] ] . fields [ [string] ] [EOL] source_to_target_field . index ( self . vocab ) [EOL] tensor = source_to_target_field . as_tensor ( source_to_target_field . get_padding_lengths ( ) ) [EOL] check = np . array ( [ self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) , self . vocab . get_token_index ( [string] , [string] ) ] ) [EOL] np . testing . assert_equal ( tensor . numpy ( ) , check ) [EOL] assert tensor [ [number] ] . item ( ) != self . vocab . get_token_index ( DEFAULT_OOV_TOKEN , [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . data . dataset_readers import DropReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestDropReader : [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file ( self , lazy ) : [EOL] reader = DropReader ( lazy = lazy ) [EOL] instances = ensure_list ( reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] instance = instances [ [number] ] [EOL] assert set ( instance . fields . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ - [number] : ] ] == [ [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert [ f . sequence_index for f in instance [ [string] ] ] == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , - [number] ] [EOL] assert len ( instance [ [string] ] ) == [number] [EOL] assert instance [ [string] ] [ [number] ] == ( [number] , [number] ) [EOL] assert len ( instance [ [string] ] ) == [number] [EOL] assert instance [ [string] ] [ [number] ] == ( [number] , [number] ) [EOL] assert len ( instance [ [string] ] ) == [number] [EOL] assert instance [ [string] ] [ [number] ] . labels == [ [number] , ] * [number] [EOL] assert len ( instance [ [string] ] ) == [number] [EOL] assert instance [ [string] ] [ [number] ] . label == - [number] [EOL] assert set ( instance [ [string] ] . metadata . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def test_read_in_bert_format ( self ) : [EOL] reader = DropReader ( instance_format = [string] ) [EOL] instances = ensure_list ( reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] print ( instances [ [number] ] ) [EOL] instance = instances [ [number] ] [EOL] assert set ( instance . fields . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ - [number] : ] ] == [ [string] , [string] , [string] ] [EOL] question_length = len ( instance [ [string] ] ) [EOL] passage_length = len ( instance [ [string] ] ) [EOL] assert len ( instance [ [string] ] ) == question_length + passage_length + [number] [EOL] [EOL] assert len ( instance [ [string] ] ) == [number] [EOL] assert instance [ [string] ] [ [number] ] == ( question_length + [number] + [number] , question_length + [number] + [number] ) [EOL] assert set ( instance [ [string] ] . metadata . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def test_read_in_squad_format ( self ) : [EOL] reader = DropReader ( instance_format = [string] ) [EOL] instances = ensure_list ( reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] print ( instances [ [number] ] ) [EOL] instance = instances [ [number] ] [EOL] assert set ( instance . fields . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instance [ [string] ] [ - [number] : ] ] == [ [string] , [string] , [string] ] [EOL] [EOL] assert instance [ [string] ] == [number] [EOL] assert instance [ [string] ] == [number] [EOL] assert set ( instance [ [string] ] . metadata . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def test_can_build_from_params ( self ) : [EOL] reader = DropReader . from_params ( Params ( { } ) ) [EOL] assert reader . _tokenizer . __class__ . __name__ == [string] [EOL] assert reader . _token_indexers [ [string] ] . __class__ . __name__ == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . data . dataset_readers import QangarooReader [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestQangarooReader : [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file ( self , lazy ) : [EOL] reader = QangarooReader ( lazy = lazy ) [EOL] instances = ensure_list ( reader . read ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] assert [ t . text for t in instances [ [number] ] . fields [ [string] ] [ [number] ] ] == [ [string] , [string] ] [EOL] assert [ t . text for t in instances [ [number] ] . fields [ [string] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instances [ [number] ] . fields [ [string] ] [ [number] ] [ : [number] ] ] == [ [string] , [string] , [string] ] [EOL] assert [ t . text for t in instances [ [number] ] . fields [ [string] ] ] == [ [string] , [string] ] [EOL] assert instances [ [number] ] . fields [ [string] ] . sequence_index == [number] [EOL] [EOL] def test_can_build_from_params ( self ) : [EOL] reader = QangarooReader . from_params ( Params ( { } ) ) [EOL] [comment] [EOL] assert reader . _token_indexers [ [string] ] . __class__ . __name__ == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] from allennlp . data . dataset_readers import TemplateText2SqlDatasetReader [EOL] from allennlp . common . util import ensure_list [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestTemplateText2SqlDatasetReader ( AllenNlpTestCase ) : [EOL] def test_reader ( self ) : [EOL] reader = TemplateText2SqlDatasetReader ( ) [EOL] [EOL] instances = reader . read ( str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] tags = fields [ [string] ] . labels [EOL] [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . label == [string] [string] [string] [string] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] tags = fields [ [string] ] . labels [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . label == [string] [string] [string] [string] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] tags = fields [ [string] ] . labels [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . label == [string] [string] [string] [string] [EOL] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] tags = fields [ [string] ] . labels [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . label == [string] [string] [string] [string] [EOL] fields = instances [ [number] ] . fields [EOL] tokens = [ t . text for t in fields [ [string] ] . tokens ] [EOL] tags = fields [ [string] ] . labels [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert tags == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert fields [ [string] ] . label == [string] [string] [string] [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import allennlp [EOL] from typing import List [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataset_readers . dataset_utils import span_utils [EOL] from allennlp . data . tokenizers . word_tokenizer import SpacyWordSplitter [EOL] from allennlp . data . tokenizers . token import Token [EOL] [EOL] class SpanUtilsTest ( AllenNlpTestCase ) : [EOL] [EOL] def test_bio_tags_to_spans_extracts_correct_spans ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_bio_tags_to_spans_extracts_correct_spans_without_labels ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_bio_tags_to_spans_ignores_specified_tags ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bio_tags_to_spans ( tag_sequence , [ [string] , [string] ] ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_iob1_tags_to_spans_extracts_correct_spans_without_labels ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_iob1_tags_to_spans_extracts_correct_spans ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . iob1_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_enumerate_spans_enumerates_all_spans ( self ) : [EOL] tokenizer = SpacyWordSplitter ( pos_tags = True ) [EOL] sentence = tokenizer . split_words ( [string] ) [EOL] [EOL] spans = span_utils . enumerate_spans ( sentence ) [EOL] assert spans == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] spans = span_utils . enumerate_spans ( sentence , max_span_width = [number] , min_span_width = [number] ) [EOL] assert spans == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] spans = span_utils . enumerate_spans ( sentence , max_span_width = [number] , min_span_width = [number] , offset = [number] ) [EOL] assert spans == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] def no_prefixed_punctuation ( tokens ) : [EOL] [comment] [EOL] return tokens [ [number] ] . pos_ != [string] and tokens [ - [number] ] . pos_ != [string] [EOL] [EOL] spans = span_utils . enumerate_spans ( sentence , max_span_width = [number] , min_span_width = [number] , filter_function = no_prefixed_punctuation ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert spans == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] [EOL] def test_bioul_tags_to_spans ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bioul_tags_to_spans ( tag_sequence ) [EOL] assert spans == [ ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) ] [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . bioul_tags_to_spans ( tag_sequence ) [EOL] [EOL] def test_bioul_tags_to_spans_without_labels ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bioul_tags_to_spans ( tag_sequence ) [EOL] assert spans == [ ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) ] [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] spans = span_utils . bioul_tags_to_spans ( tag_sequence ) [EOL] [EOL] def test_iob1_to_bioul ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] ] [EOL] bioul_sequence = span_utils . to_bioul ( tag_sequence , encoding = [string] ) [EOL] assert bioul_sequence == [ [string] , [string] , [string] , [string] ] [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] bioul_sequence = span_utils . to_bioul ( tag_sequence , encoding = [string] ) [EOL] assert bioul_sequence == [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] def test_bio_to_bioul ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] bioul_sequence = span_utils . to_bioul ( tag_sequence , encoding = [string] ) [EOL] assert bioul_sequence == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] with self . assertRaises ( span_utils . InvalidTagSequence ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] bioul_sequence = span_utils . to_bioul ( tag_sequence , encoding = [string] ) [EOL] [EOL] def test_bmes_tags_to_spans_extracts_correct_spans ( self ) : [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] def test_bmes_tags_to_spans_extracts_correct_spans_without_labels ( self ) : [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] [comment] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) } [EOL] [EOL] tag_sequence = [ [string] , [string] ] [EOL] spans = span_utils . bmes_tags_to_spans ( tag_sequence ) [EOL] assert set ( spans ) == { ( [string] , ( [number] , [number] ) ) , ( [string] , ( [number] , [number] ) ) }	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Any , Optional , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict , Optional [EOL] [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . nn import InitializerApplicator , Initializer [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . params import Params [EOL] [EOL] class _Net1 ( torch . nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] self . linear_1 = torch . nn . Linear ( [number] , [number] ) [EOL] self . linear_2 = torch . nn . Linear ( [number] , [number] ) [EOL] [EOL] def forward ( self , inputs ) : [comment] [EOL] pass [EOL] [EOL] class _Net2 ( torch . nn . Module ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] self . linear_1 = torch . nn . Linear ( [number] , [number] ) [EOL] self . linear_3 = torch . nn . Linear ( [number] , [number] ) [EOL] [EOL] def forward ( self , inputs ) : [comment] [EOL] pass [EOL] [EOL] [EOL] class TestPretrainedModelInitializer ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . net1 = _Net1 ( ) [EOL] self . net2 = _Net2 ( ) [EOL] self . temp_file = self . TEST_DIR / [string] [EOL] torch . save ( self . net2 . state_dict ( ) , self . temp_file ) [EOL] [EOL] def _are_equal ( self , linear1 , linear2 ) : [EOL] return torch . equal ( linear1 . weight , linear2 . weight ) and torch . equal ( linear1 . bias , linear2 . bias ) [EOL] [EOL] def _get_applicator ( self , regex , weights_file_path , parameter_name_overrides = None ) : [EOL] parameter_name_overrides = parameter_name_overrides or { } [EOL] initializer_params = Params ( { [string] : [string] , [string] : weights_file_path , [string] : parameter_name_overrides } ) [EOL] params = Params ( { [string] : [ ( regex , initializer_params ) ] } ) [EOL] return InitializerApplicator . from_params ( params [ [string] ] ) [EOL] [EOL] def test_random_initialization ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert not self . _are_equal ( self . net1 . linear_1 , self . net2 . linear_1 ) [EOL] assert not self . _are_equal ( self . net1 . linear_2 , self . net2 . linear_3 ) [EOL] [EOL] def test_from_params ( self ) : [EOL] params = Params ( { [string] : [string] , [string] : self . temp_file } ) [EOL] initializer = Initializer . from_params ( params ) [EOL] assert initializer . weights [EOL] assert initializer . parameter_name_overrides == { } [EOL] [EOL] name_overrides = { [string] : [string] , [string] : [string] } [EOL] params = Params ( { [string] : [string] , [string] : self . temp_file , [string] : name_overrides } ) [EOL] initializer = Initializer . from_params ( params ) [EOL] assert initializer . weights [EOL] assert initializer . parameter_name_overrides == name_overrides [EOL] [EOL] def test_default_parameter_names ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] applicator = self . _get_applicator ( [string] , self . temp_file ) [EOL] applicator ( self . net1 ) [EOL] assert self . _are_equal ( self . net1 . linear_1 , self . net2 . linear_1 ) [EOL] assert not self . _are_equal ( self . net1 . linear_2 , self . net2 . linear_3 ) [EOL] [EOL] def test_parameter_name_overrides ( self ) : [EOL] [comment] [EOL] [comment] [EOL] name_overrides = { [string] : [string] , [string] : [string] } [EOL] applicator = self . _get_applicator ( [string] , self . temp_file , name_overrides ) [EOL] applicator ( self . net1 ) [EOL] assert self . _are_equal ( self . net1 . linear_1 , self . net2 . linear_1 ) [EOL] assert self . _are_equal ( self . net1 . linear_2 , self . net2 . linear_3 ) [EOL] [EOL] def test_size_mismatch ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] name_overrides = { [string] : [string] } [EOL] applicator = self . _get_applicator ( [string] , self . temp_file , name_overrides ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] applicator ( self . net1 ) [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_load_to_gpu_from_gpu ( self ) : [EOL] [comment] [EOL] self . net1 . cuda ( device = [number] ) [EOL] self . net2 . cuda ( device = [number] ) [EOL] [EOL] [comment] [EOL] assert self . net1 . linear_1 . weight . is_cuda is True [EOL] assert self . net1 . linear_1 . bias . is_cuda is True [EOL] assert self . net2 . linear_1 . weight . is_cuda is True [EOL] assert self . net2 . linear_1 . bias . is_cuda is True [EOL] [EOL] [comment] [EOL] [comment] [EOL] temp_file = self . TEST_DIR / [string] [EOL] torch . save ( self . net2 . state_dict ( ) , temp_file ) [EOL] [EOL] applicator = self . _get_applicator ( [string] , temp_file ) [EOL] applicator ( self . net1 ) [EOL] [EOL] [comment] [EOL] assert self . net1 . linear_1 . weight . is_cuda is True [EOL] assert self . net1 . linear_1 . bias . is_cuda is True [EOL] assert self . net2 . linear_1 . weight . is_cuda is True [EOL] assert self . net2 . linear_1 . bias . is_cuda is True [EOL] [EOL] [comment] [EOL] assert self . _are_equal ( self . net1 . linear_1 , self . net2 . linear_1 ) [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_load_to_cpu_from_gpu ( self ) : [EOL] [comment] [EOL] [comment] [EOL] self . net2 . cuda ( device = [number] ) [EOL] [EOL] [comment] [EOL] assert self . net2 . linear_1 . weight . is_cuda is True [EOL] assert self . net2 . linear_1 . bias . is_cuda is True [EOL] [EOL] temp_file = self . TEST_DIR / [string] [EOL] torch . save ( self . net2 . state_dict ( ) , temp_file ) [EOL] [EOL] applicator = self . _get_applicator ( [string] , temp_file ) [EOL] applicator ( self . net1 ) [EOL] [EOL] [comment] [EOL] assert self . net1 . linear_1 . weight . is_cuda is False [EOL] assert self . net1 . linear_1 . bias . is_cuda is False [EOL] [EOL] [comment] [EOL] assert self . _are_equal ( self . net1 . linear_1 , self . net2 . linear_1 . cpu ( ) ) [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_load_to_gpu_from_cpu ( self ) : [EOL] [comment] [EOL] [comment] [EOL] self . net1 . cuda ( device = [number] ) [EOL] [EOL] [comment] [EOL] assert self . net1 . linear_1 . weight . is_cuda is True [EOL] assert self . net1 . linear_1 . bias . is_cuda is True [EOL] [EOL] [comment] [EOL] applicator = self . _get_applicator ( [string] , self . temp_file ) [EOL] applicator ( self . net1 ) [EOL] [EOL] [comment] [EOL] assert self . net1 . linear_1 . weight . is_cuda is True [EOL] assert self . net1 . linear_1 . bias . is_cuda is True [EOL] [EOL] [comment] [EOL] assert self . _are_equal ( self . net1 . linear_1 . cpu ( ) , self . net2 . linear_1 ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $torch.nn.Linear$ 0 $torch.nn.Linear$ 0 0 0 0 0 0 0 0 $torch.nn.Linear$ 0 0 0 $torch.nn.Linear$ 0 0 0 0 0 0 0 0 $torch.nn.Linear$ 0 0 0 $torch.nn.Linear$ 0 0 0 0 0 0 $allennlp.nn.Initializer.Applicator$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $typing.Optional[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 $typing.Optional[typing.Dict[builtins.str,builtins.str]]$ 0 $typing.Optional[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import re [EOL] import torch [EOL] [EOL] from allennlp . common . params import Params [EOL] from allennlp . nn import InitializerApplicator , Initializer [EOL] from allennlp . nn . regularizers import L1Regularizer , L2Regularizer , RegularizerApplicator [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestRegularizers ( AllenNlpTestCase ) : [EOL] def test_l1_regularization ( self ) : [EOL] model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) , torch . nn . Linear ( [number] , [number] ) ) [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : - [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( model ) [EOL] value = RegularizerApplicator ( [ ( [string] , L1Regularizer ( [number] ) ) ] ) ( model ) [EOL] [comment] [EOL] assert value . data . numpy ( ) == [number] [EOL] [EOL] def test_l2_regularization ( self ) : [EOL] model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) , torch . nn . Linear ( [number] , [number] ) ) [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( model ) [EOL] value = RegularizerApplicator ( [ ( [string] , L2Regularizer ( [number] ) ) ] ) ( model ) [EOL] assert value . data . numpy ( ) == [number] [EOL] [EOL] def test_regularizer_applicator_respects_regex_matching ( self ) : [EOL] model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) , torch . nn . Linear ( [number] , [number] ) ) [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( model ) [EOL] value = RegularizerApplicator ( [ ( [string] , L2Regularizer ( [number] ) ) , ( [string] , L1Regularizer ( [number] ) ) ] ) ( model ) [EOL] assert value . data . numpy ( ) == [number] [EOL] [EOL] def test_from_params ( self ) : [EOL] params = Params ( { [string] : [ ( [string] , [string] ) , ( [string] , { [string] : [string] , [string] : [number] } ) ] } ) [EOL] regularizer_applicator = RegularizerApplicator . from_params ( params . pop ( [string] ) ) [EOL] regularizers = regularizer_applicator . _regularizers [comment] [EOL] [EOL] conv = linear = None [EOL] for regex , regularizer in regularizers : [EOL] if regex == [string] : [EOL] conv = regularizer [EOL] elif regex == [string] : [EOL] linear = regularizer [EOL] [EOL] assert isinstance ( conv , L1Regularizer ) [EOL] assert isinstance ( linear , L2Regularizer ) [EOL] assert linear . alpha == [number] [EOL] [EOL] def test_frozen_params ( self ) : [EOL] model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) , torch . nn . Linear ( [number] , [number] ) ) [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : - [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( model ) [EOL] [comment] [EOL] for name , param in model . named_parameters ( ) : [EOL] if re . search ( [string] , name ) : [EOL] param . requires_grad = False [EOL] value = RegularizerApplicator ( [ ( [string] , L1Regularizer ( [number] ) ) ] ) ( model ) [EOL] [comment] [EOL] assert value . data . numpy ( ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import allennlp [EOL] import builtins [EOL] from typing import List [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . tokenizers import Token [EOL] from allennlp . data . tokenizers import WordTokenizer [EOL] from allennlp . semparse . contexts import TableQuestionContext [EOL] from allennlp . semparse import ExecutionError [EOL] from allennlp . semparse . common import Date [EOL] from allennlp . semparse . domain_languages . wikitables_language import WikiTablesLanguage [EOL] from allennlp . tests . semparse . domain_languages . domain_language_test import check_productions_match [EOL] [EOL] [EOL] class TestWikiTablesLanguage ( AllenNlpTestCase ) : [EOL] [comment] [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [comment] [EOL] question_tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] self . table_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . table_context = TableQuestionContext . read_from_file ( self . table_file , question_tokens ) [EOL] self . language = WikiTablesLanguage ( self . table_context ) [EOL] [EOL] def _get_world_with_question_tokens ( self , tokens ) : [EOL] table_context = TableQuestionContext . read_from_file ( self . table_file , tokens ) [EOL] world = WikiTablesLanguage ( table_context ) [EOL] return world [EOL] [EOL] def _get_world_with_question_tokens_and_table_file ( self , tokens , table_file ) : [EOL] table_context = TableQuestionContext . read_from_file ( table_file , tokens ) [EOL] world = WikiTablesLanguage ( table_context ) [EOL] return world [EOL] [EOL] def test_execute_fails_with_unknown_function ( self ) : [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_select ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert set ( cell_list ) == { [string] , [string] } [EOL] [EOL] def test_execute_works_with_select_number ( self ) : [EOL] logical_form = [string] [EOL] selected_number = self . language . execute ( logical_form ) [EOL] assert selected_number == [number] [EOL] [EOL] def test_execute_works_with_argmax ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_argmax_on_dates ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_argmin ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == Date ( [number] , [number] , - [number] ) [EOL] [EOL] def test_execute_works_with_argmin_on_dates ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_filter_number_greater ( self ) : [EOL] [comment] [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_date_greater ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_number_greater_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] count_result = self . language . execute ( logical_form ) [EOL] assert count_result == [number] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_date_greater_equals ( self ) : [EOL] [comment] [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_number_lesser ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_filter_date_lesser ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_number_lesser_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] count_result = self . language . execute ( logical_form ) [EOL] assert count_result == [number] [EOL] [EOL] def test_execute_works_with_filter_date_lesser_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_number_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] count_result = self . language . execute ( logical_form ) [EOL] assert count_result == [number] [EOL] [EOL] def test_execute_works_with_filter_date_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_number_not_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] count_result = self . language . execute ( logical_form ) [EOL] assert count_result == [number] [EOL] [EOL] def test_execute_works_with_filter_date_not_equals ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_value_list = self . language . execute ( logical_form ) [EOL] assert cell_value_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_filter_in ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_select_nested_in_filter_in ( self ) : [EOL] logical_form = [string] [EOL] row_list = self . language . execute ( logical_form ) [EOL] assert row_list == self . language . execute ( [string] ) [EOL] [EOL] def test_execute_works_with_filter_not_in ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [comment] [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_execute_works_with_first ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_logs_warning_with_first_on_empty_list ( self ) : [EOL] [comment] [EOL] with self . assertLogs ( [string] ) as log : [EOL] logical_form = [string] [EOL] self . language . execute ( logical_form ) [EOL] self . assertEqual ( log . output , [ [string] [string] ] ) [EOL] [EOL] def test_execute_works_with_last ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_logs_warning_with_last_on_empty_list ( self ) : [EOL] [comment] [EOL] with self . assertLogs ( [string] ) as log : [EOL] logical_form = [string] [EOL] self . language . execute ( logical_form ) [EOL] self . assertEqual ( log . output , [ [string] [string] ] ) [EOL] [EOL] def test_execute_works_with_previous ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_next ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] ] [EOL] [EOL] def test_execute_works_with_max_date ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert str ( cell_list ) == [string] [EOL] [EOL] def test_execute_works_with_min_date ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert str ( cell_list ) == [string] [EOL] [EOL] def test_execute_works_with_mode_number ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [number] [EOL] [EOL] def test_execute_works_with_mode_string ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] [comment] [EOL] assert cell_list == [ [string] , [string] ] [EOL] [EOL] def test_execute_works_with_mode_date ( self ) : [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert str ( cell_list ) == [string] [EOL] [EOL] def test_execute_works_with_same_as ( self ) : [EOL] [comment] [EOL] [comment] [EOL] logical_form = [string] [EOL] cell_list = self . language . execute ( logical_form ) [EOL] assert cell_list == [ [string] , [string] ] [EOL] [EOL] def test_execute_works_with_sum ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] sum_value = self . language . execute ( logical_form ) [EOL] assert sum_value == [number] [EOL] [comment] [EOL] logical_form = [string] [EOL] sum_value = self . language . execute ( logical_form ) [EOL] assert sum_value == [number] [EOL] [EOL] def test_execute_works_with_average ( self ) : [EOL] [comment] [EOL] logical_form = [string] [EOL] avg_value = self . language . execute ( logical_form ) [EOL] assert avg_value == [number] [EOL] [comment] [EOL] logical_form = [string] [EOL] avg_value = self . language . execute ( logical_form ) [EOL] assert avg_value == [number] [EOL] [EOL] def test_execute_works_with_diff ( self ) : [EOL] [comment] [EOL] [comment] [EOL] logical_form = [string] [EOL] avg_value = self . language . execute ( logical_form ) [EOL] assert avg_value == [number] [EOL] [EOL] def test_execute_fails_with_diff_on_non_numerical_columns ( self ) : [EOL] logical_form = [string] [EOL] with pytest . raises ( ExecutionError ) : [EOL] self . language . execute ( logical_form ) [EOL] [EOL] def test_number_comparison_works ( self ) : [EOL] [comment] [EOL] [comment] [EOL] tokens = WordTokenizer ( ) . tokenize ( [string] ) [EOL] tagged_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] language = self . _get_world_with_question_tokens_and_table_file ( tokens , tagged_file ) [EOL] result = language . execute ( [string] ) [EOL] assert result == Date ( - [number] , [number] , [number] ) [EOL] [EOL] def test_evaluate_logical_form ( self ) : [EOL] logical_form = [string] [EOL] assert self . language . evaluate_logical_form ( logical_form , [ [string] , [string] ] ) [EOL] [EOL] def test_evaluate_logical_form_with_invalid_logical_form ( self ) : [EOL] logical_form = [string] [EOL] assert not self . language . evaluate_logical_form ( logical_form , [ [string] , [string] ] ) [EOL] [EOL] def test_get_nonterminal_productions_all_column_types ( self ) : [EOL] [comment] [EOL] [comment] [EOL] productions = self . language . get_nonterminal_productions ( ) [EOL] assert set ( productions . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] ] ) [EOL] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] check_productions_match ( productions [ [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] def test_world_processes_logical_forms_correctly ( self ) : [EOL] logical_form = ( [string] [string] ) [EOL] action_sequence = self . language . logical_form_to_action_sequence ( logical_form ) [EOL] assert self . language . action_sequence_to_logical_form ( action_sequence ) == logical_form [EOL] [EOL] def test_world_gets_correct_actions ( self ) : [EOL] logical_form = [string] [EOL] expected_sequence = [ [string] , [string] , [string] , [string] [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert self . language . logical_form_to_action_sequence ( logical_form ) == expected_sequence [EOL] [EOL] def test_world_processes_logical_forms_with_number_correctly ( self ) : [EOL] logical_form = ( [string] [string] ) [EOL] action_sequence = self . language . logical_form_to_action_sequence ( logical_form ) [EOL] assert self . language . action_sequence_to_logical_form ( action_sequence ) == logical_form [EOL] [EOL] def test_world_processes_logical_forms_with_date_correctly ( self ) : [EOL] logical_form = ( [string] [string] ) [EOL] action_sequence = self . language . logical_form_to_action_sequence ( logical_form ) [EOL] assert self . language . action_sequence_to_logical_form ( action_sequence ) == logical_form [EOL] [EOL] def test_get_agenda ( self ) : [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] [comment] [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [comment] [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [comment] [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] [comment] [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] [comment] [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] , [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] , [string] , [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL] [EOL] tokens = [ Token ( x ) for x in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] world = self . _get_world_with_question_tokens ( tokens ) [EOL] assert set ( world . get_agenda ( ) ) == { [string] , [string] , [string] , [string] } [EOL] assert set ( world . get_agenda ( conservative = True ) ) == { [string] } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.semparse.domain_languages.wikitables_language.WikiTablesLanguage$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $allennlp.semparse.domain_languages.wikitables_language.WikiTablesLanguage$ 0 0 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 $typing.List[allennlp.data.tokenizers.Token]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
[comment] [EOL] from typing import Any , List , Iterable , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] import copy [EOL] import glob [EOL] import json [EOL] import os [EOL] import re [EOL] import time [EOL] from typing import Dict , Iterable [EOL] [EOL] import torch [EOL] import responses [EOL] import pytest [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . iterators import DataIterator [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . common . params import Params [EOL] from allennlp . models . simple_tagger import SimpleTagger [EOL] from allennlp . data . iterators import BasicIterator [EOL] from allennlp . data . dataset_readers import SequenceTaggingDatasetReader , WikiTablesDatasetReader [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . models . model import Model [EOL] from allennlp . training . callback_trainer import CallbackTrainer [EOL] from allennlp . training . callbacks import ( Events , LogToTensorboard , Checkpoint , ComputeMovingAverage , Validate , PostToUrl , UpdateLearningRate , UpdateMomentum , TrackMetrics , TrainSupervised , GenerateTrainingBatches ) [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] from allennlp . training . momentum_schedulers import MomentumScheduler [EOL] from allennlp . training . moving_average import ExponentialMovingAverage [EOL] from allennlp . training . tensorboard_writer import TensorboardWriter [EOL] [EOL] [EOL] class TestCallbackTrainer ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def metric_tracker ( self ) : [EOL] for callback in self . handler . callbacks ( ) : [EOL] if isinstance ( callback , TrackMetrics ) : [EOL] return callback . metric_tracker [EOL] return None [EOL] [EOL] setattr ( CallbackTrainer , [string] , property ( metric_tracker ) ) [EOL] [EOL] self . instances = SequenceTaggingDatasetReader ( ) . read ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] vocab = Vocabulary . from_instances ( self . instances ) [EOL] self . vocab = vocab [EOL] self . model_params = Params ( { [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } } ) [EOL] self . model = SimpleTagger . from_params ( vocab = self . vocab , params = self . model_params ) [EOL] self . optimizer = torch . optim . SGD ( self . model . parameters ( ) , [number] , momentum = [number] ) [EOL] [EOL] def tearDown ( self ) : [EOL] super ( ) . tearDown ( ) [EOL] delattr ( CallbackTrainer , [string] ) [EOL] [EOL] def default_callbacks ( self , validation_metric = [string] , patience = None , max_checkpoints = [number] , checkpoint_every = None , serialization_dir = [string] , iterator = None , validation_data = None , validation_iterator = None , batch_size = [number] ) : [EOL] if serialization_dir == [string] : [EOL] serialization_dir = self . TEST_DIR [EOL] checkpointer = Checkpointer ( serialization_dir , checkpoint_every , max_checkpoints ) [EOL] tensorboard = TensorboardWriter ( get_batch_num_total = lambda : None ) [EOL] [EOL] if iterator is None : [EOL] iterator = BasicIterator ( batch_size = batch_size ) [EOL] iterator . index_with ( self . vocab ) [EOL] [EOL] return [ LogToTensorboard ( log_batch_size_period = [number] , tensorboard = tensorboard ) , Checkpoint ( checkpointer ) , Validate ( validation_data = self . instances if validation_data is None else validation_data , validation_iterator = iterator if validation_iterator is None else validation_iterator ) , TrackMetrics ( patience , validation_metric ) , TrainSupervised ( ) , GenerateTrainingBatches ( self . instances , iterator , True ) ] [EOL] [EOL] def test_end_to_end ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_trainer_can_run_from_params ( self ) : [EOL] [comment] [EOL] from allennlp . commands . train import train_model [EOL] [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : [string] , [string] : [number] , [string] : [number] } , [string] : [number] , [string] : [ [string] , [string] , [string] , [string] , [string] , { [string] : [string] , [string] : [number] } ] } , [string] : { [string] : [string] } , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } } , [string] : { [string] : [string] , [string] : [number] } } ) [EOL] [EOL] train_model ( params , self . TEST_DIR ) [EOL] with open ( self . TEST_DIR / [string] ) as f : [EOL] metrics = json . load ( f ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] [EOL] def test_trainer_can_run ( self ) : [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , callbacks = self . default_callbacks ( serialization_dir = None ) , num_epochs = [number] ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] assert [string] in metrics [EOL] [EOL] [comment] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , callbacks = self . default_callbacks ( validation_metric = [string] , serialization_dir = None ) , num_epochs = [number] ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert metrics [ [string] ] > [number] [EOL] [EOL] @ responses . activate def test_trainer_posts_to_url ( self ) : [EOL] url = [string] [EOL] responses . add ( responses . POST , url ) [EOL] post_to_url = PostToUrl ( url , message = [string] ) [EOL] callbacks = self . default_callbacks ( ) + [ post_to_url ] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , num_epochs = [number] , callbacks = callbacks ) [EOL] trainer . train ( ) [EOL] [EOL] assert len ( responses . calls ) == [number] [EOL] assert responses . calls [ [number] ] . response . request . body == [string] [EOL] [EOL] [EOL] def test_trainer_can_run_exponential_moving_average ( self ) : [EOL] moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) , decay = [number] ) [EOL] callbacks = self . default_callbacks ( ) + [ ComputeMovingAverage ( moving_average ) ] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , num_epochs = [number] , callbacks = callbacks ) [EOL] trainer . train ( ) [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_trainer_can_run_cuda ( self ) : [EOL] self . model . cuda ( ) [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , callbacks = self . default_callbacks ( ) , cuda_device = [number] ) [EOL] trainer . train ( ) [EOL] [EOL] @ pytest . mark . skipif ( torch . cuda . device_count ( ) < [number] , reason = [string] ) def test_trainer_can_run_multiple_gpu ( self ) : [EOL] self . model . cuda ( ) [EOL] class MetaDataCheckWrapper ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , model ) : [EOL] super ( ) . __init__ ( model . vocab ) [EOL] self . model = model [EOL] [EOL] def forward ( self , ** kwargs ) : [comment] [EOL] assert [string] in kwargs and [string] in kwargs , f' [string] { kwargs . keys ( ) } [string] ' [EOL] batch_size = kwargs [ [string] ] [ [string] ] . size ( ) [ [number] ] [EOL] assert len ( kwargs [ [string] ] ) == batch_size , f' [string] { batch_size } [string] ' f" [string] { len ( kwargs [ [string] ] ) } [string] " [EOL] return self . model . forward ( ** kwargs ) [EOL] [EOL] multigpu_iterator = BasicIterator ( batch_size = [number] ) [EOL] multigpu_iterator . index_with ( self . vocab ) [EOL] trainer = CallbackTrainer ( MetaDataCheckWrapper ( self . model ) , self . optimizer , num_epochs = [number] , callbacks = self . default_callbacks ( iterator = multigpu_iterator ) , cuda_device = [ [number] , [number] ] ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert metrics [ [string] ] > [number] [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] [EOL] @ pytest . mark . skipif ( torch . cuda . device_count ( ) < [number] , reason = [string] ) def test_production_rule_field_with_multiple_gpus ( self ) : [EOL] wikitables_dir = [string] [EOL] wikitables_reader = WikiTablesDatasetReader ( tables_directory = wikitables_dir , dpd_output_directory = wikitables_dir + [string] ) [EOL] instances = wikitables_reader . read ( wikitables_dir + [string] ) [EOL] archive_path = self . FIXTURES_ROOT / [string] / [string] / [string] / [string] [EOL] model = load_archive ( archive_path ) . model [EOL] model . cuda ( ) [EOL] [EOL] multigpu_iterator = BasicIterator ( batch_size = [number] ) [EOL] multigpu_iterator . index_with ( model . vocab ) [EOL] [EOL] [EOL] trainer = CallbackTrainer ( model , self . optimizer , num_epochs = [number] , cuda_device = [ [number] , [number] ] , callbacks = [ GenerateTrainingBatches ( instances , multigpu_iterator ) , TrainSupervised ( ) ] ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_can_resume_training ( self ) : [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] [EOL] [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] [EOL] new_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] [EOL] assert new_trainer . epoch_number == [number] [EOL] [EOL] tracker = new_trainer . metric_tracker [EOL] [EOL] assert tracker is not None [EOL] assert tracker . is_best_so_far ( ) [EOL] assert tracker . _best_so_far is not None [EOL] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_can_resume_training_for_exponential_moving_average ( self ) : [EOL] moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) ) [EOL] callbacks = self . default_callbacks ( ) + [ ComputeMovingAverage ( moving_average ) ] [EOL] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = callbacks ) [EOL] trainer . train ( ) [EOL] [EOL] new_moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) ) [EOL] new_callbacks = self . default_callbacks ( ) + [ ComputeMovingAverage ( new_moving_average ) ] [EOL] [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = new_callbacks ) [EOL] [EOL] new_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [comment] [EOL] assert new_trainer . epoch_number == [number] [EOL] [EOL] tracker = trainer . metric_tracker [comment] [EOL] assert tracker . is_best_so_far ( ) [EOL] assert tracker . _best_so_far is not None [comment] [EOL] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_increasing_metric ( self ) : [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( [string] , patience = [number] ) ) [EOL] tracker = new_trainer . metric_tracker [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metric ( [number] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_decreasing_metric ( self ) : [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( patience = [number] ) ) [EOL] tracker = new_trainer . metric_tracker [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metric ( [number] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] def test_should_stop_early_with_increasing_metric ( self ) : [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( patience = [number] , validation_metric = [string] ) ) [EOL] [EOL] tracker = new_trainer . metric_tracker [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . should_stop_early ( ) [EOL] [EOL] [EOL] def test_should_stop_early_with_decreasing_metric ( self ) : [EOL] new_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( patience = [number] ) ) [EOL] tracker = new_trainer . metric_tracker [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] def test_should_stop_early_with_early_stopping_disabled ( self ) : [EOL] [comment] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , callbacks = self . default_callbacks ( validation_metric = [string] ) ) [EOL] tracker = trainer . metric_tracker [EOL] tracker . add_metrics ( [ float ( i ) for i in reversed ( range ( [number] ) ) ] ) [EOL] assert not tracker . should_stop_early ( ) [EOL] [EOL] [comment] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , callbacks = self . default_callbacks ( validation_metric = [string] ) ) [EOL] tracker = trainer . metric_tracker [EOL] tracker . add_metrics ( [ float ( i ) for i in range ( [number] ) ] ) [EOL] assert not tracker . should_stop_early ( ) [EOL] [EOL] def test_should_stop_early_with_invalid_patience ( self ) : [EOL] for patience in [ [number] , - [number] , - [number] , [number] , [string] ] : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , callbacks = self . default_callbacks ( patience = patience , validation_metric = [string] ) ) [EOL] [EOL] def test_trainer_can_run_and_resume_with_momentum_scheduler ( self ) : [EOL] scheduler = MomentumScheduler . from_params ( self . optimizer , Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] callbacks = self . default_callbacks ( ) + [ UpdateMomentum ( scheduler ) ] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , num_epochs = [number] , callbacks = callbacks , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] [EOL] new_scheduler = MomentumScheduler . from_params ( self . optimizer , Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] new_callbacks = self . default_callbacks ( ) + [ UpdateMomentum ( new_scheduler ) ] [EOL] new_trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , num_epochs = [number] , callbacks = new_callbacks , serialization_dir = self . TEST_DIR ) [EOL] new_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] assert new_trainer . epoch_number == [number] [EOL] assert new_scheduler . last_epoch == [number] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_can_run_with_lr_scheduler ( self ) : [EOL] lr_params = Params ( { [string] : [string] } ) [EOL] lr_scheduler = LearningRateScheduler . from_params ( self . optimizer , lr_params ) [EOL] callbacks = self . default_callbacks ( ) + [ UpdateLearningRate ( lr_scheduler ) ] [EOL] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , callbacks = callbacks , num_epochs = [number] ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_can_resume_with_lr_scheduler ( self ) : [EOL] lr_scheduler = LearningRateScheduler . from_params ( self . optimizer , Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] callbacks = self . default_callbacks ( ) + [ UpdateLearningRate ( lr_scheduler ) ] [EOL] [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , callbacks = callbacks , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] [EOL] new_lr_scheduler = LearningRateScheduler . from_params ( self . optimizer , Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] callbacks = self . default_callbacks ( ) + [ UpdateLearningRate ( new_lr_scheduler ) ] [EOL] [EOL] new_trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , callbacks = callbacks , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] new_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] assert new_trainer . epoch_number == [number] [EOL] assert new_lr_scheduler . lr_scheduler . last_epoch == [number] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_raises_on_model_with_no_loss_key ( self ) : [EOL] class FakeModel ( Model ) : [EOL] def forward ( self , ** kwargs ) : [comment] [EOL] return { } [EOL] with pytest . raises ( RuntimeError ) : [EOL] trainer = CallbackTrainer ( FakeModel ( None ) , self . optimizer , callbacks = self . default_callbacks ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_can_log_histograms ( self ) : [EOL] [comment] [EOL] for module in self . model . modules ( ) : [EOL] module . should_log_activations = True [EOL] [EOL] callbacks = [ cb for cb in self . default_callbacks ( ) if not isinstance ( cb , LogToTensorboard ) ] [EOL] [comment] [EOL] tensorboard = TensorboardWriter ( lambda : None , histogram_interval = [number] ) [EOL] callbacks . append ( LogToTensorboard ( tensorboard ) ) [EOL] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = callbacks ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_respects_num_serialized_models_to_keep ( self ) : [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( max_checkpoints = [number] ) ) [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] for prefix in [ [string] , [string] ] : [EOL] file_names = glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) [EOL] epochs = [ int ( re . search ( [string] , fname ) . group ( [number] ) ) for fname in file_names ] [EOL] assert sorted ( epochs ) == [ [number] , [number] , [number] ] [EOL] [EOL] def test_trainer_saves_metrics_every_epoch ( self ) : [EOL] trainer = CallbackTrainer ( model = self . model , optimizer = self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( max_checkpoints = [number] ) ) [EOL] trainer . train ( ) [EOL] [EOL] for epoch in range ( [number] ) : [EOL] epoch_file = self . TEST_DIR / f' [string] { epoch } [string] ' [EOL] assert epoch_file . exists ( ) [EOL] metrics = json . load ( open ( epoch_file ) ) [EOL] assert [string] in metrics [EOL] assert [string] in metrics [EOL] assert metrics . get ( [string] ) == epoch [EOL] [EOL] def test_trainer_respects_keep_serialized_model_every_num_seconds ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] class WaitingIterator ( BasicIterator ) : [EOL] [comment] [EOL] def _create_batches ( self , * args , ** kwargs ) : [EOL] time . sleep ( [number] ) [EOL] return super ( WaitingIterator , self ) . _create_batches ( * args , ** kwargs ) [EOL] [EOL] iterator = WaitingIterator ( batch_size = [number] ) [EOL] iterator . index_with ( self . vocab ) [EOL] [EOL] [comment] [EOL] viterator = BasicIterator ( batch_size = [number] ) [EOL] viterator . index_with ( self . vocab ) [EOL] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( max_checkpoints = [number] , checkpoint_every = [number] , iterator = iterator , validation_iterator = viterator ) ) [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] for prefix in [ [string] , [string] ] : [EOL] file_names = glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) [EOL] epochs = [ int ( re . search ( [string] , fname ) . group ( [number] ) ) for fname in file_names ] [EOL] [comment] [EOL] assert sorted ( epochs ) == [ [number] , [number] , [number] , [number] ] [EOL] [EOL] def test_trainer_can_log_learning_rates_tensorboard ( self ) : [EOL] callbacks = [ cb for cb in self . default_callbacks ( ) if not isinstance ( cb , LogToTensorboard ) ] [EOL] [comment] [EOL] tensorboard = TensorboardWriter ( lambda : None , should_log_learning_rate = True , summary_interval = [number] ) [EOL] callbacks . append ( LogToTensorboard ( tensorboard ) ) [EOL] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = callbacks ) [EOL] [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_saves_models_at_specified_interval ( self ) : [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( batch_size = [number] ) , model_save_interval = [number] ) [EOL] [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] prefix = [string] [EOL] file_names = sorted ( glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) ) [EOL] epochs = [ re . search ( [string] , fname ) . group ( [number] ) for fname in file_names ] [EOL] [comment] [EOL] [comment] [EOL] assert len ( epochs ) == [number] [EOL] assert epochs [ [number] ] == [string] [EOL] assert [string] in epochs [ [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for k in range ( [number] ) : [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] . format ( k ) ) ) [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] . format ( k ) ) ) [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] restore_trainer = CallbackTrainer ( self . model , self . optimizer , num_epochs = [number] , serialization_dir = self . TEST_DIR , callbacks = self . default_callbacks ( ) , model_save_interval = [number] ) [EOL] restore_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] assert restore_trainer . epoch_number == [number] [EOL] [comment] [EOL] assert restore_trainer . batch_num_total == [number] [EOL] [EOL] def test_trainer_saves_and_loads_best_validation_metrics_correctly_1 ( self ) : [EOL] [comment] [EOL] [comment] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] _ = trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] best_epoch_1 = trainer . metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_1 = trainer . metric_tracker . best_epoch_metrics [EOL] [comment] [EOL] assert isinstance ( best_validation_metrics_epoch_1 , dict ) [EOL] assert [string] in best_validation_metrics_epoch_1 [EOL] [EOL] [comment] [EOL] restore_trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] restore_trainer . train ( ) [EOL] _ = restore_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] best_epoch_2 = restore_trainer . metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_2 = restore_trainer . metric_tracker . best_epoch_metrics [EOL] [EOL] [comment] [EOL] assert best_epoch_1 == [number] and best_epoch_2 == [number] [EOL] assert best_validation_metrics_epoch_2 != best_validation_metrics_epoch_1 [EOL] [EOL] def test_trainer_saves_and_loads_best_validation_metrics_correctly_2 ( self ) : [EOL] [comment] [EOL] [comment] [EOL] trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( validation_metric = [string] ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] trainer . train ( ) [EOL] [EOL] _ = trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] best_epoch_1 = trainer . metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_1 = trainer . metric_tracker . best_epoch_metrics [EOL] [comment] [EOL] assert isinstance ( best_validation_metrics_epoch_1 , dict ) [EOL] assert [string] in best_validation_metrics_epoch_1 [EOL] [EOL] [comment] [EOL] restore_trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( validation_metric = [string] ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] restore_trainer . train ( ) [EOL] _ = restore_trainer . handler . fire_event ( Events . RESTORE_CHECKPOINT ) [EOL] best_epoch_2 = restore_trainer . metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_2 = restore_trainer . metric_tracker . best_epoch_metrics [EOL] [EOL] [comment] [EOL] assert best_epoch_1 == best_epoch_2 == [number] [EOL] assert best_validation_metrics_epoch_2 == best_validation_metrics_epoch_1 [EOL] [EOL] def test_restored_training_returns_best_epoch_metrics_even_if_no_better_epoch_is_found_after_restoring ( self ) : [EOL] [comment] [EOL] [comment] [EOL] original_trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( validation_metric = [string] ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] training_metrics = original_trainer . train ( ) [EOL] [EOL] [comment] [EOL] restored_trainer = CallbackTrainer ( self . model , self . optimizer , callbacks = self . default_callbacks ( validation_metric = [string] ) , num_epochs = [number] , serialization_dir = self . TEST_DIR ) [EOL] restored_metrics = restored_trainer . train ( ) [EOL] [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] [EOL] [comment] [EOL] assert training_metrics [ [string] ] == restored_metrics [ [string] ] [EOL] assert training_metrics [ [string] ] == [number] [EOL] assert training_metrics [ [string] ] > restored_metrics [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.allennlp.tests.training.callback_trainer_test.TestCallbackTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds.WaitingIterator$ 0 0 0 0 0 0 0 0 $allennlp.allennlp.tests.training.callback_trainer_test.TestCallbackTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds.WaitingIterator$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.allennlp.tests.training.callback_trainer_test.TestCallbackTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds.WaitingIterator$ 0 $allennlp.allennlp.tests.training.callback_trainer_test.TestCallbackTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds.WaitingIterator$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import builtins [EOL] import os [EOL] import re [EOL] import time [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . common . params import Params [EOL] from allennlp . training . trainer import Trainer [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] class TestCheckpointer ( AllenNlpTestCase ) : [EOL] def retrieve_and_delete_saved ( self ) : [EOL] [docstring] [EOL] serialization_files = os . listdir ( self . TEST_DIR ) [EOL] model_checkpoints = [ x for x in serialization_files if [string] in x ] [EOL] found_model_epochs = [ int ( re . search ( [string] , x ) . group ( [number] ) ) for x in model_checkpoints ] [EOL] for f in model_checkpoints : [EOL] os . remove ( os . path . join ( self . TEST_DIR , f ) ) [EOL] training_checkpoints = [ x for x in serialization_files if [string] in x ] [EOL] found_training_epochs = [ int ( re . search ( [string] , x ) . group ( [number] ) ) for x in training_checkpoints ] [EOL] for f in training_checkpoints : [EOL] os . remove ( os . path . join ( self . TEST_DIR , f ) ) [EOL] return sorted ( found_model_epochs ) , sorted ( found_training_epochs ) [EOL] [EOL] def test_default ( self ) : [EOL] [docstring] [EOL] default_num_to_keep = [number] [EOL] num_epochs = [number] [EOL] target = list ( range ( num_epochs - default_num_to_keep , num_epochs ) ) [EOL] [EOL] checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR ) [EOL] [EOL] for e in range ( num_epochs ) : [EOL] checkpointer . save_checkpoint ( epoch = e , model_state = { [string] : e } , training_states = { [string] : e } , is_best_so_far = False ) [EOL] models , training = self . retrieve_and_delete_saved ( ) [EOL] assert models == training == target [EOL] [EOL] def test_with_time ( self ) : [EOL] [docstring] [EOL] num_to_keep = [number] [EOL] num_epochs = [number] [EOL] target = list ( range ( num_epochs - num_to_keep , num_epochs ) ) [EOL] pauses = [ [number] , [number] , [number] ] [EOL] target = sorted ( set ( target + pauses ) ) [EOL] checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = num_to_keep , keep_serialized_model_every_num_seconds = [number] ) [EOL] for e in range ( num_epochs ) : [EOL] if e in pauses : [EOL] time . sleep ( [number] ) [EOL] checkpointer . save_checkpoint ( epoch = e , model_state = { [string] : e } , training_states = { [string] : e } , is_best_so_far = False ) [EOL] models , training = self . retrieve_and_delete_saved ( ) [EOL] assert models == training == target [EOL] [EOL] def test_configuration_error_when_passed_as_conflicting_argument_to_trainer ( self ) : [EOL] [docstring] [EOL] with self . assertRaises ( ConfigurationError ) : [EOL] Trainer ( None , None , None , None , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = None , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = [number] ) ) [EOL] with self . assertRaises ( ConfigurationError ) : [EOL] Trainer ( None , None , None , None , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = [number] , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = [number] ) ) [EOL] try : [EOL] Trainer ( None , None , None , None , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = [number] ) ) [EOL] except ConfigurationError : [EOL] self . fail ( [string] ) [EOL] [EOL] def test_registered_subclass ( self ) : [EOL] [docstring] [EOL] [EOL] @ Checkpointer . register ( [string] ) class CheckpointerSubclass ( Checkpointer ) : [EOL] def __init__ ( self , x , y ) : [EOL] super ( ) . __init__ ( ) [EOL] self . x = x [EOL] self . y = y [EOL] [EOL] sub_inst = Checkpointer . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] assert sub_inst . __class__ == CheckpointerSubclass [EOL] assert sub_inst . x == [number] and sub_inst . y == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] from math import isclose [EOL] import torch [EOL] [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . momentum_schedulers import MomentumScheduler [EOL] from allennlp . training . optimizers import Optimizer [EOL] [EOL] [EOL] class InvertedTriangularTest ( AllenNlpTestCase ) : [EOL] [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) ) [EOL] self . base_momentum = [number] [EOL] [EOL] def _get_optimizer ( self ) : [EOL] return Optimizer . from_params ( self . model . named_parameters ( ) , Params ( { [string] : [string] , [string] : [number] , [string] : self . base_momentum } ) ) [EOL] [EOL] def test_from_params ( self ) : [EOL] optimizer = self . _get_optimizer ( ) [EOL] scheduler = MomentumScheduler . from_params ( optimizer , Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] assert scheduler . cool_down == [number] [EOL] assert scheduler . warm_up == [number] [EOL] assert scheduler . ratio == [number] [EOL] assert scheduler . last_epoch == - [number] [EOL] [EOL] def test_basic_schedule ( self ) : [EOL] optimizer = self . _get_optimizer ( ) [EOL] scheduler = MomentumScheduler . from_params ( optimizer , Params ( { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } ) ) [EOL] [comment] [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == self . base_momentum [EOL] [comment] [EOL] [comment] [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum - ( self . base_momentum - self . base_momentum / [number] ) * ( [number] / [number] ) ) [EOL] [comment] [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum - ( self . base_momentum - self . base_momentum / [number] ) * ( [number] / [number] ) ) [EOL] [comment] [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum / [number] ) [EOL] [comment] [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum / [number] + ( self . base_momentum - self . base_momentum / [number] ) * ( [number] / [number] ) ) [EOL] [comment] [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum ) [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum ) [EOL] scheduler . step ( epoch = [number] ) [EOL] assert isclose ( optimizer . param_groups [ [number] ] [ [string] ] , self . base_momentum ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import torch [EOL] from sklearn import metrics [EOL] from numpy . testing import assert_almost_equal [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . metrics import Auc [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] class AucTest ( AllenNlpTestCase ) : [EOL] def test_auc_computation ( self ) : [EOL] auc = Auc ( ) [EOL] all_predictions = [ ] [EOL] all_labels = [ ] [EOL] for _ in range ( [number] ) : [EOL] predictions = torch . randn ( [number] ) . float ( ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) ) . long ( ) [EOL] [EOL] auc ( predictions , labels ) [EOL] [EOL] all_predictions . append ( predictions ) [EOL] all_labels . append ( labels ) [EOL] [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( torch . cat ( all_labels , dim = [number] ) . numpy ( ) , torch . cat ( all_predictions , dim = [number] ) . numpy ( ) ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_almost_equal ( real_auc_value , computed_auc_value ) [EOL] [EOL] [comment] [EOL] predictions = torch . randn ( [number] ) . float ( ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) ) . long ( ) [EOL] [EOL] auc ( predictions , labels ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . numpy ( ) , predictions . numpy ( ) ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_almost_equal ( real_auc_value , computed_auc_value ) [EOL] [EOL] def test_auc_gold_labels_behaviour ( self ) : [EOL] [comment] [EOL] auc = Auc ( positive_label = [number] ) [EOL] [EOL] predictions = torch . randn ( [number] ) . float ( ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) ) . long ( ) [EOL] [EOL] auc ( predictions , labels ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . numpy ( ) , predictions . numpy ( ) , pos_label = [number] ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_almost_equal ( real_auc_value , computed_auc_value ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) as _ : [EOL] labels = torch . LongTensor ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] auc ( predictions , labels ) [EOL] [EOL] def test_auc_with_mask ( self ) : [EOL] auc = Auc ( ) [EOL] [EOL] predictions = torch . randn ( [number] ) . float ( ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) ) . long ( ) [EOL] mask = torch . ByteTensor ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] auc ( predictions , labels , mask ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels [ : [number] ] . numpy ( ) , predictions [ : [number] ] . numpy ( ) ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_almost_equal ( real_auc_value , computed_auc_value ) [EOL] [EOL] def test_auc_works_without_calling_metric_at_all ( self ) : [EOL] auc = Auc ( ) [EOL] auc . get_metric ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . metrics import BooleanAccuracy [EOL] [EOL] [EOL] class BooleanAccuracyTest ( AllenNlpTestCase ) : [EOL] def test_accuracy_computation ( self ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . Tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] targets = torch . Tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] ] = [number] [EOL] accuracy ( predictions , targets , mask ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] targets [ [number] , [number] ] = [number] [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] accuracy . reset ( ) [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] def test_skips_completely_masked_instances ( self ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . Tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] targets = torch . Tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] [EOL] mask = torch . Tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] accuracy ( predictions , targets , mask ) [EOL] [EOL] [comment] [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] def test_incorrect_gold_labels_shape_catches_exceptions ( self ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . rand ( [ [number] , [number] ] ) [EOL] incorrect_shape_labels = torch . rand ( [ [number] , [number] ] ) [EOL] with pytest . raises ( ValueError ) : [EOL] accuracy ( predictions , incorrect_shape_labels ) [EOL] [EOL] def test_incorrect_mask_shape_catches_exceptions ( self ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . rand ( [ [number] , [number] ] ) [EOL] labels = torch . rand ( [ [number] , [number] ] ) [EOL] incorrect_shape_mask = torch . randint ( [number] , [number] , [ [number] , [number] ] ) [EOL] with pytest . raises ( ValueError ) : [EOL] accuracy ( predictions , labels , incorrect_shape_mask ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
	0
	0
	0
[comment] [EOL] from typing import Any , List , Dict , Union [EOL] import typing [EOL] import spacy [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] from allennlp . predictors import CorefPredictor [EOL] [EOL] [EOL] class TestCorefPredictor ( AllenNlpTestCase ) : [EOL] def test_uses_named_inputs ( self ) : [EOL] inputs = { [string] : [string] [string] } [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] result = predictor . predict_json ( inputs ) [EOL] self . assert_predict_result ( result ) [EOL] [EOL] document = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] result_doc_words = predictor . predict_tokenized ( document ) [EOL] self . assert_predict_result ( result_doc_words ) [EOL] [EOL] @ staticmethod def assert_predict_result ( result ) : [EOL] document = result [ [string] ] [EOL] assert document == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] clusters = result [ [string] ] [EOL] assert isinstance ( clusters , list ) [EOL] for cluster in clusters : [EOL] assert isinstance ( cluster , list ) [EOL] for mention in cluster : [EOL] [comment] [EOL] assert isinstance ( mention [ [number] ] , int ) [EOL] assert isinstance ( mention [ [number] ] , int ) [EOL] [comment] [EOL] assert [number] < mention [ [number] ] <= len ( document ) [EOL] assert [number] < mention [ [number] ] <= len ( document ) [EOL] [EOL] def test_coref_resolved ( self ) : [EOL] [EOL] [docstring] [EOL] [EOL] document = [string] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] result = predictor . coref_resolved ( document ) [EOL] assert isinstance ( result , str ) [EOL] [EOL] def test_replace_corefs ( self ) : [EOL] [EOL] [docstring] [EOL] [EOL] nlp = spacy . load ( [string] ) [EOL] [EOL] inputs = [ [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] expected_clusters = [ [ ] , [ [ [ [number] , [number] ] , [ [number] , [number] ] ] ] , [ [ [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] ] ] , [ [ [ [number] , [number] ] , [ [number] , [number] ] ] ] , [ [ [ [number] , [number] ] , [ [number] , [number] ] ] ] ] [EOL] [EOL] expected_outputs = [ [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] for i , text in enumerate ( inputs ) : [EOL] clusters = expected_clusters [ i ] [EOL] [EOL] if not clusters : [EOL] assert text == inputs [ i ] [EOL] continue [EOL] [EOL] doc = nlp ( text ) [EOL] output = CorefPredictor . replace_corefs ( doc , clusters ) [EOL] assert output == expected_outputs [ i ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[typing.List[typing.List[builtins.int]]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Union[typing.List[typing.Any],typing.List[typing.List[typing.List[builtins.int]]]]$ 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[typing.List[typing.List[builtins.int]]]]]$ 0 0 0 0 0 0 0 $typing.Union[typing.List[typing.Any],typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Union[typing.List[typing.Any],typing.List[typing.List[typing.List[builtins.int]]]]$ 0 0 0 $typing.Any$ 0 $typing.List[builtins.str]$ 0 0 0 0
[comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] [EOL] [EOL] class TestEvent2MindPredictor ( AllenNlpTestCase ) : [EOL] def test_uses_named_inputs ( self ) : [EOL] inputs = { [string] : [string] , } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] result = predictor . predict_json ( inputs ) [EOL] [EOL] token_names = [ [string] , [string] , [string] ] [EOL] [EOL] for token_name in token_names : [EOL] all_predicted_tokens = result . get ( token_name ) [EOL] for predicted_tokens in all_predicted_tokens : [EOL] assert isinstance ( predicted_tokens , list ) [EOL] assert all ( isinstance ( x , str ) for x in predicted_tokens ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any , List , Dict , Union [EOL] import typing [EOL] import math [EOL] [EOL] from pytest import approx [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] [EOL] class TestTextClassifierPredictor ( AllenNlpTestCase ) : [EOL] def test_uses_named_inputs ( self ) : [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] result = predictor . predict_json ( inputs ) [EOL] [EOL] logits = result . get ( [string] ) [EOL] assert logits is not None [EOL] assert isinstance ( logits , list ) [EOL] assert len ( logits ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in logits ) [EOL] [EOL] probs = result . get ( [string] ) [EOL] assert probs is not None [EOL] assert isinstance ( probs , list ) [EOL] assert len ( probs ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in probs ) [EOL] assert all ( x >= [number] for x in probs ) [EOL] assert sum ( probs ) == approx ( [number] ) [EOL] [EOL] label = result . get ( [string] ) [EOL] assert label is not None [EOL] assert label in predictor . _model . vocab . get_token_to_index_vocabulary ( namespace = [string] ) [EOL] [EOL] exps = [ math . exp ( x ) for x in logits ] [EOL] sum_exps = sum ( exps ) [EOL] for e , p in zip ( exps , probs ) : [EOL] assert e / sum_exps == approx ( p ) [EOL] [EOL] def test_batch_prediction ( self ) : [EOL] batch_inputs = [ { [string] : [string] } , { [string] : [string] } , ] [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] results = predictor . predict_batch_json ( batch_inputs ) [EOL] assert len ( results ) == [number] [EOL] [EOL] for result in results : [EOL] logits = result . get ( [string] ) [EOL] assert logits is not None [EOL] assert isinstance ( logits , list ) [EOL] assert len ( logits ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in logits ) [EOL] [EOL] probs = result . get ( [string] ) [EOL] assert probs is not None [EOL] assert isinstance ( probs , list ) [EOL] assert len ( probs ) == [number] [EOL] assert all ( isinstance ( x , float ) for x in probs ) [EOL] assert all ( x >= [number] for x in probs ) [EOL] assert sum ( probs ) == approx ( [number] ) [EOL] [EOL] label = result . get ( [string] ) [EOL] assert label is not None [EOL] assert label in predictor . _model . vocab . get_token_to_index_vocabulary ( namespace = [string] ) [EOL] [EOL] exps = [ math . exp ( x ) for x in logits ] [EOL] sum_exps = sum ( exps ) [EOL] for e , p in zip ( exps , probs ) : [EOL] assert e / sum_exps == approx ( p ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Union[builtins.float,builtins.int]$ 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import json [EOL] import os [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] [EOL] [EOL] class TestNlvrParserPredictor ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . inputs = { [string] : [ [ [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] , [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] , [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] ] , [ [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] , [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] , [ { [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [number] } ] ] ] , [string] : [string] , [string] : [string] } [EOL] [EOL] def test_predictor_with_coverage_parser ( self ) : [EOL] archive_dir = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] archive = load_archive ( os . path . join ( archive_dir , [string] ) ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] result = predictor . predict_json ( self . inputs ) [EOL] assert [string] in result [EOL] assert [string] in result [EOL] [comment] [EOL] [comment] [EOL] assert len ( result [ [string] ] [ [number] ] ) == [number] [comment] [EOL] [EOL] def test_predictor_with_direct_parser ( self ) : [EOL] archive_dir = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] archive = load_archive ( os . path . join ( archive_dir , [string] ) ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] result = predictor . predict_json ( self . inputs ) [EOL] assert [string] in result [EOL] assert [string] in result [EOL] [comment] [EOL] [comment] [EOL] assert len ( result [ [string] ] [ [number] ] ) == [number] [comment] [EOL] [EOL] def test_predictor_with_string_input ( self ) : [EOL] archive_dir = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] archive = load_archive ( os . path . join ( archive_dir , [string] ) ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] self . inputs [ [string] ] = json . dumps ( self . inputs [ [string] ] ) [EOL] result = predictor . predict_json ( self . inputs ) [EOL] assert [string] in result [EOL] assert [string] in result [EOL] [comment] [EOL] [comment] [EOL] assert len ( result [ [string] ] [ [number] ] ) == [number] [comment] [EOL] [EOL] def test_predictor_with_single_world ( self ) : [EOL] archive_dir = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] archive = load_archive ( os . path . join ( archive_dir , [string] ) ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] self . inputs [ [string] ] = self . inputs [ [string] ] [ [number] ] [EOL] del self . inputs [ [string] ] [EOL] result = predictor . predict_json ( self . inputs ) [EOL] assert [string] in result [EOL] assert [string] in result [EOL] [comment] [EOL] [comment] [EOL] assert len ( result [ [string] ] [ [number] ] ) == [number] [comment] [EOL] [EOL] def test_predictor_with_single_world_and_string_input ( self ) : [EOL] archive_dir = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] archive = load_archive ( os . path . join ( archive_dir , [string] ) ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] self . inputs [ [string] ] = json . dumps ( self . inputs [ [string] ] [ [number] ] ) [EOL] del self . inputs [ [string] ] [EOL] result = predictor . predict_json ( self . inputs ) [EOL] assert [string] in result [EOL] assert [string] in result [EOL] [comment] [EOL] [comment] [EOL] assert len ( result [ [string] ] [ [number] ] ) == [number] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . masked_layer_norm import MaskedLayerNorm [EOL] [EOL] class TestMaskedLayerNorm ( AllenNlpTestCase ) : [EOL] def test_masked_layer_norm ( self ) : [EOL] x_n = np . random . rand ( [number] , [number] , [number] ) [EOL] mask_n = np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] x = torch . from_numpy ( x_n ) . float ( ) [EOL] mask = torch . from_numpy ( mask_n ) [EOL] [EOL] layer_norm = MaskedLayerNorm ( [number] , gamma0 = [number] ) [EOL] normed_x = layer_norm ( x , mask ) [EOL] [EOL] N = [number] * [number] [EOL] mean = ( x_n * np . expand_dims ( mask_n , axis = - [number] ) ) . sum ( ) / N [EOL] std = np . sqrt ( ( ( ( x_n - mean ) * np . expand_dims ( mask_n , axis = - [number] ) ) ** [number] ) . sum ( ) / N + [number] ) [EOL] expected = [number] * ( x_n - mean ) / ( std + [number] ) [EOL] [EOL] assert np . allclose ( normed_x . data . numpy ( ) , expected ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.float$ 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from numpy . testing import assert_almost_equal [EOL] import torch [EOL] [EOL] from allennlp . modules import ResidualWithLayerDropout [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestResidualWithLayerDropout ( AllenNlpTestCase ) : [EOL] def test_dropout_works_for_training ( self ) : [EOL] layer_input_tensor = torch . FloatTensor ( [ [ [number] , [number] ] , [ - [number] , - [number] ] ] ) [EOL] layer_output_tensor = torch . FloatTensor ( [ [ [number] , [number] ] , [ [number] , - [number] ] ] ) [EOL] [EOL] [comment] [EOL] residual_with_layer_dropout = ResidualWithLayerDropout ( [number] ) [EOL] residual_with_layer_dropout . train ( ) [EOL] result = residual_with_layer_dropout ( layer_input_tensor , layer_output_tensor ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] ) [EOL] assert_almost_equal ( result , [ [ [number] , [number] ] , [ - [number] , - [number] ] ] ) [EOL] [EOL] result = residual_with_layer_dropout ( layer_input_tensor , layer_output_tensor , [number] , [number] ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] ) [EOL] assert_almost_equal ( result , [ [ [number] , [number] ] , [ - [number] , - [number] ] ] ) [EOL] [EOL] [comment] [EOL] residual_with_layer_dropout = ResidualWithLayerDropout ( [number] ) [EOL] residual_with_layer_dropout . train ( ) [EOL] result = residual_with_layer_dropout ( layer_input_tensor , layer_output_tensor ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] ) [EOL] assert_almost_equal ( result , [ [ [number] + [number] , [number] + [number] ] , [ - [number] + [number] , - [number] - [number] ] ] ) [EOL] [EOL] def test_dropout_works_for_testing ( self ) : [EOL] layer_input_tensor = torch . FloatTensor ( [ [ [number] , [number] ] , [ - [number] , - [number] ] ] ) [EOL] layer_output_tensor = torch . FloatTensor ( [ [ [number] , [number] ] , [ [number] , - [number] ] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] residual_with_layer_dropout = ResidualWithLayerDropout ( [number] ) [EOL] residual_with_layer_dropout . eval ( ) [EOL] result = residual_with_layer_dropout ( layer_input_tensor , layer_output_tensor ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] ) [EOL] assert_almost_equal ( result , [ [ [number] + [number] * [number] , [number] + [number] * [number] ] , [ - [number] + [number] * [number] , - [number] - [number] * [number] ] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import builtins [EOL] import os [EOL] import json [EOL] import warnings [EOL] from typing import List [EOL] [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , category = FutureWarning ) [EOL] import h5py [EOL] import numpy [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . token_indexers . elmo_indexer import ELMoTokenCharactersIndexer [EOL] from allennlp . data . token_indexers . single_id_token_indexer import SingleIdTokenIndexer [EOL] from allennlp . data import Token , Vocabulary , Instance [EOL] from allennlp . data . dataset import Batch [EOL] from allennlp . data . iterators import BasicIterator [EOL] from allennlp . modules . elmo import _ElmoBiLm , Elmo , _ElmoCharacterEncoder [EOL] from allennlp . modules . token_embedders import ElmoTokenEmbedder [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . nn . util import remove_sentence_boundaries [EOL] [EOL] [EOL] class ElmoTestCase ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ElmoTestCase , self ) . setUp ( ) [EOL] self . elmo_fixtures_path = self . FIXTURES_ROOT / [string] [EOL] self . options_file = str ( self . elmo_fixtures_path / [string] ) [EOL] self . weight_file = str ( self . elmo_fixtures_path / [string] ) [EOL] self . sentences_json_file = str ( self . elmo_fixtures_path / [string] ) [EOL] self . sentences_txt_file = str ( self . elmo_fixtures_path / [string] ) [EOL] [EOL] def _load_sentences_embeddings ( self ) : [EOL] [docstring] [EOL] with open ( self . sentences_json_file ) as fin : [EOL] sentences = json . load ( fin ) [EOL] [EOL] [comment] [EOL] expected_lm_embeddings = [ ] [EOL] for k in range ( len ( sentences ) ) : [EOL] embed_fname = os . path . join ( self . elmo_fixtures_path , [string] . format ( k ) ) [EOL] expected_lm_embeddings . append ( [ ] ) [EOL] with h5py . File ( embed_fname , [string] ) as fin : [EOL] for i in range ( [number] ) : [EOL] sent_embeds = fin [ [string] % i ] [ ... ] [EOL] sent_embeds_concat = numpy . concatenate ( ( sent_embeds [ [number] , : , : ] , sent_embeds [ [number] , : , : ] ) , axis = - [number] ) [EOL] expected_lm_embeddings [ - [number] ] . append ( sent_embeds_concat ) [EOL] [EOL] return sentences , expected_lm_embeddings [EOL] [EOL] @ staticmethod def get_vocab_and_both_elmo_indexed_ids ( batch ) : [EOL] instances = [ ] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indexer2 = SingleIdTokenIndexer ( ) [EOL] for sentence in batch : [EOL] tokens = [ Token ( token ) for token in sentence ] [EOL] field = TextField ( tokens , { [string] : indexer , [string] : indexer2 } ) [EOL] instance = Instance ( { [string] : field } ) [EOL] instances . append ( instance ) [EOL] [EOL] dataset = Batch ( instances ) [EOL] vocab = Vocabulary . from_instances ( instances ) [EOL] dataset . index_instances ( vocab ) [EOL] return vocab , dataset . as_tensor_dict ( ) [ [string] ] [EOL] [EOL] [EOL] class TestElmoBiLm ( ElmoTestCase ) : [EOL] def test_elmo_bilm ( self ) : [EOL] [comment] [EOL] sentences , expected_lm_embeddings = self . _load_sentences_embeddings ( ) [EOL] [EOL] [comment] [EOL] elmo_bilm = _ElmoBiLm ( self . options_file , self . weight_file ) [EOL] [EOL] [comment] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] [EOL] [comment] [EOL] instances = [ ] [EOL] for batch in zip ( * sentences ) : [EOL] for sentence in batch : [EOL] tokens = [ Token ( token ) for token in sentence . split ( ) ] [EOL] field = TextField ( tokens , { [string] : indexer } ) [EOL] instance = Instance ( { [string] : field } ) [EOL] instances . append ( instance ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] [comment] [EOL] iterator = BasicIterator ( [number] ) [EOL] iterator . index_with ( vocab ) [EOL] for i , batch in enumerate ( iterator ( instances , num_epochs = [number] , shuffle = False ) ) : [EOL] lm_embeddings = elmo_bilm ( batch [ [string] ] [ [string] ] ) [EOL] top_layer_embeddings , mask = remove_sentence_boundaries ( lm_embeddings [ [string] ] [ [number] ] , lm_embeddings [ [string] ] ) [EOL] [EOL] [comment] [EOL] lengths = mask . data . numpy ( ) . sum ( axis = [number] ) [EOL] batch_sentences = [ sentences [ k ] [ i ] for k in range ( [number] ) ] [EOL] expected_lengths = [ len ( sentence . split ( ) ) for sentence in batch_sentences ] [EOL] self . assertEqual ( lengths . tolist ( ) , expected_lengths ) [EOL] [EOL] [comment] [EOL] expected_top_layer = [ expected_lm_embeddings [ k ] [ i ] for k in range ( [number] ) ] [EOL] for k in range ( [number] ) : [EOL] self . assertTrue ( numpy . allclose ( top_layer_embeddings [ k , : lengths [ k ] , : ] . data . numpy ( ) , expected_top_layer [ k ] , atol = [number] ) ) [EOL] [EOL] def test_elmo_char_cnn_cache_does_not_raise_error_for_uncached_words ( self ) : [EOL] sentences = [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] ] ] [EOL] in_vocab_sentences = [ [ [string] , [string] ] , [ [string] , [string] ] ] [EOL] oov_tensor = self . get_vocab_and_both_elmo_indexed_ids ( sentences ) [ [number] ] [EOL] vocab , in_vocab_tensor = self . get_vocab_and_both_elmo_indexed_ids ( in_vocab_sentences ) [EOL] words_to_cache = list ( vocab . get_token_to_index_vocabulary ( [string] ) . keys ( ) ) [EOL] elmo_bilm = _ElmoBiLm ( self . options_file , self . weight_file , vocab_to_cache = words_to_cache ) [EOL] [EOL] elmo_bilm ( in_vocab_tensor [ [string] ] , in_vocab_tensor [ [string] ] ) [EOL] elmo_bilm ( oov_tensor [ [string] ] , oov_tensor [ [string] ] ) [EOL] [EOL] def test_elmo_bilm_can_cache_char_cnn_embeddings ( self ) : [EOL] sentences = [ [ [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] ] , [ [string] , [string] ] ] [EOL] vocab , tensor = self . get_vocab_and_both_elmo_indexed_ids ( sentences ) [EOL] words_to_cache = list ( vocab . get_token_to_index_vocabulary ( [string] ) . keys ( ) ) [EOL] elmo_bilm = _ElmoBiLm ( self . options_file , self . weight_file ) [EOL] elmo_bilm . eval ( ) [EOL] no_cache = elmo_bilm ( tensor [ [string] ] , tensor [ [string] ] ) [EOL] [EOL] [comment] [EOL] elmo_bilm = _ElmoBiLm ( self . options_file , self . weight_file , vocab_to_cache = words_to_cache ) [EOL] elmo_bilm . eval ( ) [EOL] cached = elmo_bilm ( tensor [ [string] ] , tensor [ [string] ] ) [EOL] [EOL] numpy . testing . assert_array_almost_equal ( no_cache [ [string] ] . data . cpu ( ) . numpy ( ) , cached [ [string] ] . data . cpu ( ) . numpy ( ) ) [EOL] for activation_cached , activation in zip ( cached [ [string] ] , no_cache [ [string] ] ) : [EOL] numpy . testing . assert_array_almost_equal ( activation_cached . data . cpu ( ) . numpy ( ) , activation . data . cpu ( ) . numpy ( ) , decimal = [number] ) [EOL] [EOL] class TestElmo ( ElmoTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( TestElmo , self ) . setUp ( ) [EOL] [EOL] self . elmo = Elmo ( self . options_file , self . weight_file , [number] , dropout = [number] ) [EOL] [EOL] def _sentences_to_ids ( self , sentences ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] [EOL] [comment] [EOL] instances = [ ] [EOL] for sentence in sentences : [EOL] tokens = [ Token ( token ) for token in sentence ] [EOL] field = TextField ( tokens , { [string] : indexer } ) [EOL] instance = Instance ( { [string] : field } ) [EOL] instances . append ( instance ) [EOL] [EOL] dataset = Batch ( instances ) [EOL] vocab = Vocabulary ( ) [EOL] dataset . index_instances ( vocab ) [EOL] return dataset . as_tensor_dict ( ) [ [string] ] [ [string] ] [EOL] [EOL] def test_elmo ( self ) : [EOL] [comment] [EOL] [comment] [EOL] sentences = [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] [EOL] character_ids = self . _sentences_to_ids ( sentences ) [EOL] output = self . elmo ( character_ids ) [EOL] elmo_representations = output [ [string] ] [EOL] mask = output [ [string] ] [EOL] [EOL] assert len ( elmo_representations ) == [number] [EOL] assert list ( elmo_representations [ [number] ] . size ( ) ) == [ [number] , [number] , [number] ] [EOL] assert list ( elmo_representations [ [number] ] . size ( ) ) == [ [number] , [number] , [number] ] [EOL] assert list ( mask . size ( ) ) == [ [number] , [number] ] [EOL] [EOL] def test_elmo_keep_sentence_boundaries ( self ) : [EOL] sentences = [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] elmo = Elmo ( self . options_file , self . weight_file , [number] , dropout = [number] , keep_sentence_boundaries = True ) [EOL] character_ids = self . _sentences_to_ids ( sentences ) [EOL] output = elmo ( character_ids ) [EOL] elmo_representations = output [ [string] ] [EOL] mask = output [ [string] ] [EOL] [EOL] assert len ( elmo_representations ) == [number] [EOL] [comment] [EOL] assert list ( elmo_representations [ [number] ] . size ( ) ) == [ [number] , [number] + [number] , [number] ] [EOL] assert list ( elmo_representations [ [number] ] . size ( ) ) == [ [number] , [number] + [number] , [number] ] [EOL] assert list ( mask . size ( ) ) == [ [number] , [number] + [number] ] [EOL] [EOL] [EOL] def test_elmo_4D_input ( self ) : [EOL] sentences = [ [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , [ [ [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] , [ [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [ [string] ] ] ] [EOL] [EOL] all_character_ids = [ ] [EOL] for batch_sentences in sentences : [EOL] all_character_ids . append ( self . _sentences_to_ids ( batch_sentences ) ) [EOL] [EOL] [comment] [EOL] character_ids = torch . cat ( [ ids . unsqueeze ( [number] ) for ids in all_character_ids ] , dim = [number] ) [EOL] embeddings_4d = self . elmo ( character_ids ) [EOL] [EOL] [comment] [EOL] embeddings_3d = [ ] [EOL] for char_ids in all_character_ids : [EOL] self . elmo . _elmo_lstm . _elmo_lstm . reset_states ( ) [EOL] embeddings_3d . append ( self . elmo ( char_ids ) ) [EOL] [EOL] for k in range ( [number] ) : [EOL] numpy . testing . assert_array_almost_equal ( embeddings_4d [ [string] ] [ [number] ] [ : , k , : , : ] . data . numpy ( ) , embeddings_3d [ k ] [ [string] ] [ [number] ] . data . numpy ( ) ) [EOL] [EOL] def test_elmo_with_module ( self ) : [EOL] [comment] [EOL] sentences = [ [ [string] , [string] , [string] ] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] [EOL] character_ids = self . _sentences_to_ids ( sentences ) [EOL] elmo_bilm = _ElmoBiLm ( self . options_file , self . weight_file ) [EOL] elmo = Elmo ( None , None , [number] , dropout = [number] , module = elmo_bilm ) [EOL] output = elmo ( character_ids ) [EOL] elmo_representations = output [ [string] ] [EOL] [EOL] assert len ( elmo_representations ) == [number] [EOL] for k in range ( [number] ) : [EOL] assert list ( elmo_representations [ k ] . size ( ) ) == [ [number] , [number] , [number] ] [EOL] [EOL] def test_elmo_bilm_can_handle_higher_dimensional_input_with_cache ( self ) : [EOL] sentences = [ [ [string] , [string] , [string] , [string] ] , [ [string] , [string] , [string] ] , [ [string] , [string] ] ] [EOL] vocab , tensor = self . get_vocab_and_both_elmo_indexed_ids ( sentences ) [EOL] words_to_cache = list ( vocab . get_token_to_index_vocabulary ( [string] ) . keys ( ) ) [EOL] elmo_bilm = Elmo ( self . options_file , self . weight_file , [number] , vocab_to_cache = words_to_cache ) [EOL] elmo_bilm . eval ( ) [EOL] [EOL] individual_dim = elmo_bilm ( tensor [ [string] ] , tensor [ [string] ] ) [EOL] elmo_bilm = Elmo ( self . options_file , self . weight_file , [number] , vocab_to_cache = words_to_cache ) [EOL] elmo_bilm . eval ( ) [EOL] [EOL] expanded_word_ids = torch . stack ( [ tensor [ [string] ] for _ in range ( [number] ) ] , dim = [number] ) [EOL] expanded_char_ids = torch . stack ( [ tensor [ [string] ] for _ in range ( [number] ) ] , dim = [number] ) [EOL] expanded_result = elmo_bilm ( expanded_char_ids , expanded_word_ids ) [EOL] split_result = [ x . squeeze ( [number] ) for x in torch . split ( expanded_result [ [string] ] [ [number] ] , [number] , dim = [number] ) ] [EOL] for expanded in split_result : [EOL] numpy . testing . assert_array_almost_equal ( expanded . data . cpu ( ) . numpy ( ) , individual_dim [ [string] ] [ [number] ] . data . cpu ( ) . numpy ( ) ) [EOL] [EOL] [EOL] class TestElmoRequiresGrad ( ElmoTestCase ) : [EOL] def _run_test ( self , requires_grad ) : [EOL] embedder = ElmoTokenEmbedder ( self . options_file , self . weight_file , requires_grad = requires_grad ) [EOL] batch_size = [number] [EOL] seq_len = [number] [EOL] char_ids = torch . from_numpy ( numpy . random . randint ( [number] , [number] , ( batch_size , seq_len , [number] ) ) ) [EOL] embeddings = embedder ( char_ids ) [EOL] loss = embeddings . sum ( ) [EOL] loss . backward ( ) [EOL] [EOL] elmo_grads = [ param . grad for name , param in embedder . named_parameters ( ) if [string] in name ] [EOL] if requires_grad : [EOL] [comment] [EOL] assert all ( [ grad is not None for grad in elmo_grads ] ) [EOL] else : [EOL] [comment] [EOL] assert all ( [ grad is None for grad in elmo_grads ] ) [EOL] [EOL] def test_elmo_requires_grad ( self ) : [EOL] self . _run_test ( True ) [EOL] [EOL] def test_elmo_does_not_require_grad ( self ) : [EOL] self . _run_test ( False ) [EOL] [EOL] [EOL] class TestElmoTokenRepresentation ( ElmoTestCase ) : [EOL] def test_elmo_token_representation ( self ) : [EOL] [comment] [EOL] with open ( os . path . join ( self . elmo_fixtures_path , [string] ) , [string] ) as fin : [EOL] words = fin . read ( ) . strip ( ) . split ( [string] ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] tokens = [ Token ( word ) for word in words ] [EOL] [EOL] indices = indexer . tokens_to_indices ( tokens , vocab , [string] ) [EOL] [comment] [EOL] sentences = [ ] [EOL] for k in range ( [number] ) : [EOL] char_indices = indices [ [string] ] [ ( k * [number] ) : ( ( k + [number] ) * [number] ) ] [EOL] sentences . append ( indexer . pad_token_sequence ( { [string] : char_indices } , desired_num_tokens = { [string] : [number] } , padding_lengths = { } ) [ [string] ] ) [EOL] batch = torch . from_numpy ( numpy . array ( sentences ) ) [EOL] [EOL] elmo_token_embedder = _ElmoCharacterEncoder ( self . options_file , self . weight_file ) [EOL] elmo_token_embedder_output = elmo_token_embedder ( batch ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] actual_embeddings = remove_sentence_boundaries ( elmo_token_embedder_output [ [string] ] , elmo_token_embedder_output [ [string] ] ) [ [number] ] . data . numpy ( ) [EOL] actual_embeddings = actual_embeddings . reshape ( - [number] , actual_embeddings . shape [ - [number] ] ) [EOL] [EOL] embedding_file = os . path . join ( self . elmo_fixtures_path , [string] ) [EOL] with h5py . File ( embedding_file , [string] ) as fin : [EOL] expected_embeddings = fin [ [string] ] [ ... ] [EOL] [EOL] assert numpy . allclose ( actual_embeddings [ : len ( tokens ) ] , expected_embeddings , atol = [number] ) [EOL] [EOL] def test_elmo_token_representation_bos_eos ( self ) : [EOL] [comment] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] [EOL] elmo_token_embedder = _ElmoCharacterEncoder ( self . options_file , self . weight_file ) [EOL] [EOL] for correct_index , token in [ [ [number] , [string] ] , [ [number] , [string] ] ] : [EOL] indices = indexer . tokens_to_indices ( [ Token ( token ) ] , Vocabulary ( ) , [string] ) [EOL] indices = torch . from_numpy ( numpy . array ( indices [ [string] ] ) ) . view ( [number] , [number] , - [number] ) [EOL] embeddings = elmo_token_embedder ( indices ) [ [string] ] [EOL] assert numpy . allclose ( embeddings [ [number] , correct_index , : ] . data . numpy ( ) , embeddings [ [number] , [number] , : ] . data . numpy ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.str]]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
	0
	0
	0
	0
[comment] [EOL] from typing import List , Tuple , Any [EOL] import typing [EOL] import numpy [EOL] from numpy . testing import assert_almost_equal [EOL] import pytest [EOL] import torch [EOL] from torch . nn import LSTM , GRU [EOL] from torch . nn . utils . rnn import pack_padded_sequence , pad_packed_sequence [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2seq_encoders import PytorchSeq2SeqWrapper [EOL] from allennlp . nn . util import sort_batch_by_length , get_lengths_from_binary_sequence_mask [EOL] [EOL] class TestPytorchSeq2SeqWrapper ( AllenNlpTestCase ) : [EOL] def test_get_dimension_is_correct ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] assert encoder . get_output_dim ( ) == [number] [EOL] assert encoder . get_input_dim ( ) == [number] [EOL] lstm = LSTM ( bidirectional = False , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] assert encoder . get_output_dim ( ) == [number] [EOL] assert encoder . get_input_dim ( ) == [number] [EOL] [EOL] def test_forward_works_even_with_empty_sequences ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] [EOL] tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] tensor [ [number] , : , : ] = [number] [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] tensor [ [number] , : , : ] = [number] [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , : ] = [number] [EOL] [EOL] results = encoder ( tensor , mask ) [EOL] [EOL] for i in ( [number] , [number] , [number] ) : [EOL] assert not ( results [ i ] == [number] ) . data . all ( ) [EOL] for i in ( [number] , [number] ) : [EOL] assert ( results [ i ] == [number] ) . data . all ( ) [EOL] [EOL] def test_forward_pulls_out_correct_tensor_without_sequence_lengths ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . FloatTensor ( [ [ [ [number] , [number] ] , [ [number] , [number] ] ] ] ) [EOL] lstm_output = lstm ( input_tensor ) [EOL] encoder_output = encoder ( input_tensor , None ) [EOL] assert_almost_equal ( encoder_output . data . numpy ( ) , lstm_output [ [number] ] . data . numpy ( ) ) [EOL] [EOL] def test_forward_pulls_out_correct_tensor_with_sequence_lengths ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] [EOL] sequence_lengths = get_lengths_from_binary_sequence_mask ( mask ) [EOL] packed_sequence = pack_padded_sequence ( input_tensor , sequence_lengths . data . tolist ( ) , batch_first = True ) [EOL] lstm_output , _ = lstm ( packed_sequence ) [EOL] encoder_output = encoder ( input_tensor , mask ) [EOL] lstm_tensor , _ = pad_packed_sequence ( lstm_output , batch_first = True ) [EOL] assert_almost_equal ( encoder_output . data . numpy ( ) , lstm_tensor . data . numpy ( ) ) [EOL] [EOL] def test_forward_pulls_out_correct_tensor_for_unsorted_batches ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] input_tensor [ [number] , [number] : , : ] = [number] [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] [EOL] sequence_lengths = get_lengths_from_binary_sequence_mask ( mask ) [EOL] sorted_inputs , sorted_sequence_lengths , restoration_indices , _ = sort_batch_by_length ( input_tensor , sequence_lengths ) [EOL] packed_sequence = pack_padded_sequence ( sorted_inputs , sorted_sequence_lengths . data . tolist ( ) , batch_first = True ) [EOL] lstm_output , _ = lstm ( packed_sequence ) [EOL] encoder_output = encoder ( input_tensor , mask ) [EOL] lstm_tensor , _ = pad_packed_sequence ( lstm_output , batch_first = True ) [EOL] assert_almost_equal ( encoder_output . data . numpy ( ) , lstm_tensor . index_select ( [number] , restoration_indices ) . data . numpy ( ) ) [EOL] [EOL] def test_forward_does_not_compress_tensors_padded_to_greater_than_the_max_sequence_length ( self ) : [EOL] [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] input_tensor [ : , [number] , : ] = [number] [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ : , [number] ] = [number] [EOL] [EOL] encoder_output = encoder ( input_tensor , mask ) [EOL] assert encoder_output . size ( [number] ) == [number] [EOL] [EOL] def test_wrapper_raises_if_batch_first_is_false ( self ) : [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] ) [EOL] _ = PytorchSeq2SeqWrapper ( lstm ) [EOL] [EOL] def test_wrapper_works_when_passed_state_with_zero_length_sequences ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] [EOL] [comment] [EOL] initial_states = torch . randn ( [number] , [number] , [number] ) , torch . randn ( [number] , [number] , [number] ) [EOL] [EOL] _ = encoder ( input_tensor , mask , initial_states ) [EOL] [EOL] def test_wrapper_can_call_backward_with_zero_length_sequences ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm ) [EOL] input_tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] mask = torch . ones ( [number] , [number] ) [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [EOL] mask [ [number] , [number] : ] = [number] [comment] [EOL] mask [ [number] , [number] : ] = [number] [EOL] [EOL] output = encoder ( input_tensor , mask ) [EOL] [EOL] output . sum ( ) . backward ( ) [EOL] [EOL] def test_wrapper_stateful ( self ) : [EOL] lstm = LSTM ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( lstm , stateful = True ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] batch_sizes = [ [number] , [number] , [number] ] [EOL] sequence_lengths = [ [number] , [number] , [number] ] [EOL] states = [ ] [EOL] for batch_size , sequence_length in zip ( batch_sizes , sequence_lengths ) : [EOL] tensor = torch . rand ( [ batch_size , sequence_length , [number] ] ) [EOL] mask = torch . ones ( batch_size , sequence_length ) [EOL] mask . data [ [number] , [number] : ] = [number] [EOL] encoder_output = encoder ( tensor , mask ) [EOL] states . append ( encoder . _states ) [comment] [EOL] [EOL] [comment] [EOL] assert_almost_equal ( encoder_output [ [number] , [number] : , : ] . data . numpy ( ) , numpy . zeros ( ( [number] , [number] ) ) ) [EOL] [EOL] for k in range ( [number] ) : [EOL] assert_almost_equal ( states [ - [number] ] [ k ] [ : , - [number] : , : ] . data . numpy ( ) , states [ - [number] ] [ k ] [ : , - [number] : , : ] . data . numpy ( ) ) [EOL] [EOL] def test_wrapper_stateful_single_state_gru ( self ) : [EOL] gru = GRU ( bidirectional = True , num_layers = [number] , input_size = [number] , hidden_size = [number] , batch_first = True ) [EOL] encoder = PytorchSeq2SeqWrapper ( gru , stateful = True ) [EOL] [EOL] batch_sizes = [ [number] , [number] ] [EOL] states = [ ] [EOL] for batch_size in batch_sizes : [EOL] tensor = torch . rand ( [ batch_size , [number] , [number] ] ) [EOL] mask = torch . ones ( batch_size , [number] ) [EOL] mask . data [ [number] , [number] : ] = [number] [EOL] encoder_output = encoder ( tensor , mask ) [EOL] states . append ( encoder . _states ) [comment] [EOL] [EOL] assert_almost_equal ( encoder_output [ [number] , [number] : , : ] . data . numpy ( ) , numpy . zeros ( ( [number] , [number] ) ) ) [EOL] assert_almost_equal ( states [ - [number] ] [ [number] ] [ : , - [number] : , : ] . data . numpy ( ) , states [ - [number] ] [ [number] ] [ : , - [number] : , : ] . data . numpy ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Tuple[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from numpy . testing import assert_almost_equal [EOL] import pytest [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . similarity_functions import MultiHeadedSimilarity [EOL] [EOL] class TestMultiHeadedSimilarityFunction ( AllenNlpTestCase ) : [EOL] def test_weights_are_correct_sizes ( self ) : [EOL] [comment] [EOL] similarity = MultiHeadedSimilarity ( num_heads = [number] , tensor_1_dim = [number] , tensor_1_projected_dim = [number] , tensor_2_dim = [number] , tensor_2_projected_dim = [number] ) [EOL] assert list ( similarity . _tensor_1_projection . size ( ) ) == [ [number] , [number] ] [EOL] assert list ( similarity . _tensor_2_projection . size ( ) ) == [ [number] , [number] ] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] similarity = MultiHeadedSimilarity ( num_heads = [number] , tensor_1_dim = [number] ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] MultiHeadedSimilarity . from_params ( params ) [EOL] [EOL] def test_forward ( self ) : [EOL] [comment] [EOL] similarity = MultiHeadedSimilarity ( num_heads = [number] , tensor_1_dim = [number] ) [EOL] similarity . _tensor_1_projection = Parameter ( torch . eye ( [number] ) ) [EOL] similarity . _tensor_2_projection = Parameter ( torch . eye ( [number] ) ) [EOL] a_vectors = torch . FloatTensor ( [ [ [ [ [number] , [number] , - [number] , - [number] , [number] , [number] ] , [ - [number] , [number] , [number] , - [number] , [number] , [number] ] ] ] ] ) [EOL] b_vectors = torch . FloatTensor ( [ [ [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , - [number] , - [number] , [number] , [number] ] ] ] ] ) [EOL] result = similarity ( a_vectors , b_vectors ) . data . numpy ( ) [EOL] assert result . shape == ( [number] , [number] , [number] , [number] ) [EOL] assert_almost_equal ( result , [ [ [ [ [number] , - [number] , [number] ] , [ [number] , - [number] , [number] ] ] ] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] [EOL] [EOL] from typing import Any [EOL] import typing [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL] [EOL] class GraphParserTest ( ModelTestCase ) : [EOL] [EOL] def setUp ( self ) : [EOL] super ( GraphParserTest , self ) . setUp ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_graph_parser_can_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_model_can_decode ( self ) : [EOL] self . model . eval ( ) [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] decode_output_dict = self . model . decode ( output_dict ) [EOL] [EOL] assert set ( decode_output_dict . keys ( ) ) == set ( [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from flaky import flaky [EOL] import numpy [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . data . dataset_readers import DatasetReader [EOL] from allennlp . data . iterators import DataIterator , BasicIterator [EOL] from allennlp . models import Model [EOL] from allennlp . training import Trainer [EOL] [EOL] class SimpleTaggerTest ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( SimpleTaggerTest , self ) . setUp ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_simple_tagger_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_forward_pass_runs_correctly ( self ) : [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] output_dict = self . model . decode ( output_dict ) [EOL] class_probs = output_dict [ [string] ] [ [number] ] . data . numpy ( ) [EOL] numpy . testing . assert_almost_equal ( numpy . sum ( class_probs , - [number] ) , numpy . array ( [ [number] , [number] , [number] , [number] ] ) ) [EOL] [EOL] def test_forward_on_instances_ignores_loss_key_when_batched ( self ) : [EOL] batch_outputs = self . model . forward_on_instances ( self . dataset . instances ) [EOL] for output in batch_outputs : [EOL] assert not [string] in output . keys ( ) [EOL] [EOL] [comment] [EOL] single_output = self . model . forward_on_instance ( self . dataset . instances [ [number] ] ) [EOL] assert [string] in single_output . keys ( ) [EOL] [EOL] def test_mismatching_dimensions_throws_configuration_error ( self ) : [EOL] params = Params . from_file ( self . param_file ) [EOL] [comment] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] [ [string] ] = [number] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Model . from_params ( vocab = self . vocab , params = params . pop ( [string] ) ) [EOL] [EOL] def test_regularization ( self ) : [EOL] penalty = self . model . get_regularization_penalty ( ) [EOL] assert penalty == [number] [EOL] [EOL] iterator = BasicIterator ( batch_size = [number] ) [EOL] trainer = Trainer ( self . model , None , iterator , self . instances ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] training_batch = next ( iterator ( self . instances , num_epochs = [number] ) ) [EOL] validation_batch = next ( iterator ( self . instances , num_epochs = [number] ) ) [EOL] [EOL] training_loss = trainer . batch_loss ( [ training_batch ] , for_training = True ) . item ( ) [EOL] validation_loss = trainer . batch_loss ( [ validation_batch ] , for_training = False ) . item ( ) [EOL] [EOL] [comment] [EOL] numpy . testing . assert_almost_equal ( training_loss , validation_loss ) [EOL] [EOL] [EOL] class SimpleTaggerSpanF1Test ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( SimpleTaggerSpanF1Test , self ) . setUp ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_simple_tagger_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] [EOL] class SimpleTaggerRegularizationTest ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] [EOL] self . set_up_model ( param_file , self . FIXTURES_ROOT / [string] / [string] ) [EOL] params = Params . from_file ( param_file ) [EOL] self . reader = DatasetReader . from_params ( params [ [string] ] ) [EOL] self . iterator = DataIterator . from_params ( params [ [string] ] ) [EOL] self . trainer = Trainer . from_params ( self . model , self . TEST_DIR , self . iterator , self . dataset , None , params . get ( [string] ) ) [EOL] [EOL] [EOL] def test_regularization ( self ) : [EOL] penalty = self . model . get_regularization_penalty ( ) . data [EOL] assert ( penalty > [number] ) . all ( ) [EOL] [EOL] penalty2 = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for name , parameter in self . model . named_parameters ( ) : [EOL] if name . endswith ( [string] ) : [EOL] weight_penalty = [number] * torch . sum ( torch . pow ( parameter , [number] ) ) [EOL] penalty2 += weight_penalty [EOL] elif name . endswith ( [string] ) : [EOL] bias_penalty = [number] * torch . sum ( torch . abs ( parameter ) ) [EOL] penalty2 += bias_penalty [EOL] [EOL] assert ( penalty == penalty2 . data ) . all ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] training_batch = next ( self . iterator ( self . instances , num_epochs = [number] ) ) [EOL] validation_batch = next ( self . iterator ( self . instances , num_epochs = [number] ) ) [EOL] [EOL] training_loss = self . trainer . batch_loss ( [ training_batch ] , for_training = True ) . data [EOL] validation_loss = self . trainer . batch_loss ( [ validation_batch ] , for_training = False ) . data [EOL] [EOL] [comment] [EOL] assert ( training_loss != validation_loss ) . all ( ) [EOL] [EOL] [comment] [EOL] penalized = validation_loss + penalty [EOL] assert ( training_loss == penalized ) . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
	0
	0
	0
	0
	0
	0
[comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . semparse . contexts . quarel_utils import group_worlds , to_qr_spec_string [EOL] [EOL] [EOL] class QuarelSemanticParserTest ( ModelTestCase ) : [EOL] [EOL] def setUp ( self ) : [EOL] [EOL] super ( QuarelSemanticParserTest , self ) . setUp ( ) [EOL] self . set_up_model ( str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) , str ( self . FIXTURES_ROOT / [string] / [string] ) ) [EOL] [comment] [EOL] self . ignore = { [string] , [string] } [EOL] [EOL] def test_model_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_elmo_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_zeroshot_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_wdp_zeroshot_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_with_tagger_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_entity_bits_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_friction_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_friction_zeroshot_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [comment] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_denotation_only_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] def test_tagger_model_can_train_save_and_load ( self ) : [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] / [string] / [string] [EOL] self . ensure_model_can_train_save_and_load ( param_file , gradients_to_ignore = self . ignore ) [EOL] [EOL] [comment] [EOL] def test_group_worlds ( self ) : [EOL] tags = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] worlds = group_worlds ( tags , tokens ) [EOL] assert worlds [ [string] ] == [ [string] , [string] ] [EOL] assert worlds [ [string] ] == [ [string] , [string] ] [EOL] [EOL] def test_ ( self ) : [EOL] qr_spec = [ { [string] : [number] , [string] : - [number] , [string] : - [number] , [string] : - [number] , [string] : [number] } , { [string] : [number] , [string] : - [number] } ] [EOL] qr_spec_string = to_qr_spec_string ( qr_spec ) [EOL] assert qr_spec_string == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.int]]$ 0 0 0 $typing.Any$ 0 0 0
	0
	0
	0
	0
	0
	0
	0
import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] @ pytest . mark . skip ( [string] ) class TestBasicAllenNlp ( AllenNlpTestCase ) : [EOL] @ classmethod def test_run_as_script ( cls ) : [EOL] [comment] [EOL] import tutorials . tagger . basic_allennlp [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[docstring] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from allennlp . data . tokenizers import Token as _ [EOL] from allennlp . semparse . common . errors import ParsingError , ExecutionError [EOL] from allennlp . semparse . domain_languages . domain_language import ( DomainLanguage , predicate , predicate_with_side_args ) [EOL] from allennlp . semparse . worlds . world import World [EOL] from allennlp . semparse . action_space_walker import ActionSpaceWalker [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List , Any , Tuple , Set [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import numbers [EOL] from collections import defaultdict [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from numbers import Number [EOL] from typing import Dict , List , NamedTuple , Set , Tuple , Any [EOL] import logging [EOL] import re [EOL] [EOL] from allennlp . semparse . domain_languages . domain_language import ( DomainLanguage , PredicateType , predicate ) [EOL] from allennlp . semparse . common . errors import ExecutionError [EOL] from allennlp . semparse . contexts . table_question_knowledge_graph import MONTH_NUMBERS [EOL] from allennlp . semparse . contexts import TableQuestionContext [EOL] from allennlp . semparse . contexts . table_question_context import CellValueType [EOL] from allennlp . semparse . common import Date [EOL] from allennlp . tools import wikitables_evaluator as evaluator [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] class Row ( NamedTuple ) : [EOL] [comment] [EOL] values = ... [EOL] [EOL] [EOL] class Column ( NamedTuple ) : [EOL] name = ... [EOL] [EOL] [EOL] class StringColumn ( Column ) : [EOL] pass [EOL] [EOL] [EOL] class ComparableColumn ( Column ) : [EOL] pass [EOL] [EOL] [EOL] class DateColumn ( ComparableColumn ) : [EOL] pass [EOL] [EOL] [EOL] class NumberColumn ( ComparableColumn ) : [EOL] pass [EOL] [EOL] [EOL] class WikiTablesLanguage ( DomainLanguage ) : [EOL] [comment] [EOL] [docstring] [EOL] def __init__ ( self , table_context ) : [EOL] super ( ) . __init__ ( start_types = { Number , Date , List [ str ] } ) [EOL] self . table_context = table_context [EOL] self . table_data = [ Row ( row ) for row in table_context . table_data ] [EOL] [EOL] column_types = table_context . column_types [EOL] self . _table_has_string_columns = False [EOL] self . _table_has_date_columns = False [EOL] self . _table_has_number_columns = False [EOL] if [string] in column_types : [EOL] self . add_predicate ( [string] , self . filter_in ) [EOL] self . add_predicate ( [string] , self . filter_not_in ) [EOL] self . _table_has_string_columns = True [EOL] if [string] in column_types : [EOL] self . add_predicate ( [string] , self . filter_date_greater ) [EOL] self . add_predicate ( [string] , self . filter_date_greater_equals ) [EOL] self . add_predicate ( [string] , self . filter_date_lesser ) [EOL] self . add_predicate ( [string] , self . filter_date_lesser_equals ) [EOL] self . add_predicate ( [string] , self . filter_date_equals ) [EOL] self . add_predicate ( [string] , self . filter_date_not_equals ) [EOL] self . add_predicate ( [string] , self . max_date ) [EOL] self . add_predicate ( [string] , self . min_date ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . add_constant ( [string] , - [number] , type_ = Number ) [EOL] self . _table_has_date_columns = True [EOL] if [string] in column_types or [string] in column_types : [EOL] self . add_predicate ( [string] , self . filter_number_greater ) [EOL] self . add_predicate ( [string] , self . filter_number_greater_equals ) [EOL] self . add_predicate ( [string] , self . filter_number_lesser ) [EOL] self . add_predicate ( [string] , self . filter_number_lesser_equals ) [EOL] self . add_predicate ( [string] , self . filter_number_equals ) [EOL] self . add_predicate ( [string] , self . filter_number_not_equals ) [EOL] self . add_predicate ( [string] , self . max_number ) [EOL] self . add_predicate ( [string] , self . min_number ) [EOL] self . add_predicate ( [string] , self . average ) [EOL] self . add_predicate ( [string] , self . sum ) [EOL] self . add_predicate ( [string] , self . diff ) [EOL] self . _table_has_number_columns = True [EOL] if [string] in column_types or [string] in column_types or [string] in column_types : [EOL] self . add_predicate ( [string] , self . argmax ) [EOL] self . add_predicate ( [string] , self . argmin ) [EOL] [EOL] self . table_graph = table_context . get_table_knowledge_graph ( ) [EOL] [EOL] [comment] [EOL] question_entities , question_numbers = table_context . get_entities_from_question ( ) [EOL] self . _question_entities = [ entity for entity , _ in question_entities ] [EOL] self . _question_numbers = [ number for number , _ in question_numbers ] [EOL] for entity in self . _question_entities : [EOL] [comment] [EOL] [comment] [EOL] self . add_constant ( entity , entity , type_ = List [ str ] ) [EOL] [EOL] for number in self . _question_numbers : [EOL] self . add_constant ( str ( number ) , float ( number ) , type_ = Number ) [EOL] [EOL] [comment] [EOL] self . _column_productions_for_agenda = { } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for column_name in table_context . column_names : [EOL] column_type = column_name . split ( [string] ) [ [number] ] . replace ( [string] , [string] ) [EOL] column = None [EOL] if column_type == [string] : [EOL] column = StringColumn ( column_name ) [EOL] elif column_type == [string] : [EOL] column = DateColumn ( column_name ) [EOL] self . add_constant ( column_name , column , type_ = ComparableColumn ) [EOL] elif column_type == [string] or column_type == [string] : [EOL] column = NumberColumn ( column_name ) [EOL] self . add_constant ( column_name , column , type_ = ComparableColumn ) [EOL] self . add_constant ( column_name , column , type_ = Column ) [EOL] self . add_constant ( column_name , column ) [EOL] column_type_name = str ( PredicateType . get_type ( type ( column ) ) ) [EOL] self . _column_productions_for_agenda [ column_name ] = f"{ column_type_name } [string] { column_name }" [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . terminal_productions = { } [EOL] for name , types in self . _function_types . items ( ) : [EOL] self . terminal_productions [ name ] = [string] % ( types [ [number] ] , name ) [EOL] [EOL] def get_agenda ( self , conservative = False ) : [EOL] [docstring] [EOL] agenda_items = [ ] [EOL] question_tokens = [ token . text for token in self . table_context . question_tokens ] [EOL] question = [string] . join ( question_tokens ) [EOL] [EOL] added_number_filters = False [EOL] if self . _table_has_number_columns : [EOL] if [string] in question : [EOL] agenda_items . append ( [string] ) [EOL] if [string] in question : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] comparison_triggers = [ [string] , [string] , [string] ] [EOL] if any ( f" [string] { word } [string] " in question for word in comparison_triggers ) : [EOL] agenda_items . append ( [string] ) [EOL] elif any ( f"{ word } [string] " in question for word in comparison_triggers ) : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if agenda_items : [EOL] added_number_filters = True [EOL] for token in question_tokens : [EOL] if token in [ [string] , [string] ] or ( token == [string] and not conservative ) : [EOL] agenda_items . append ( [string] ) [EOL] if token in [ [string] , [string] ] or ( token == [string] and not conservative ) : [EOL] agenda_items . append ( [string] ) [EOL] if token in [ [string] , [string] ] : [EOL] agenda_items . append ( [string] ) [EOL] if token in [ [string] , [string] ] : [EOL] agenda_items . append ( [string] ) [EOL] if token == [string] : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] if self . _table_has_number_columns : [EOL] [comment] [EOL] if token == [string] and not conservative : [EOL] agenda_items . append ( [string] ) [EOL] if token == [string] or [string] in question or [string] in question : [EOL] agenda_items . append ( [string] ) [EOL] if token == [string] : [EOL] agenda_items . append ( [string] ) [EOL] if token in [ [string] , [string] , [string] , [string] ] and [string] not in question : [EOL] [comment] [EOL] [comment] [EOL] if [string] not in question : [EOL] agenda_items . append ( [string] ) [EOL] if token in [ [string] , [string] , [string] , [string] , [string] ] and [string] not in question : [EOL] [comment] [EOL] [comment] [EOL] if [string] not in question : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] if self . _table_has_date_columns : [EOL] if token in MONTH_NUMBERS or ( token . isdigit ( ) and len ( token ) == [number] and int ( token ) < [number] and int ( token ) > [number] ) : [EOL] [comment] [EOL] if not added_number_filters or not conservative : [EOL] if [string] in question_tokens : [EOL] agenda_items . append ( [string] ) [EOL] elif [string] in question_tokens : [EOL] agenda_items . append ( [string] ) [EOL] elif [string] in question_tokens : [EOL] agenda_items . append ( [string] ) [EOL] else : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] if [string] in question and self . _table_has_number_columns : [EOL] agenda_items . append ( [string] ) [EOL] if [string] in question and self . _table_has_number_columns : [EOL] agenda_items . append ( [string] ) [EOL] if [string] in question_tokens and self . _table_has_date_columns : [EOL] if [string] in question_tokens : [EOL] agenda_items . append ( [string] ) [EOL] elif [string] in question_tokens : [EOL] agenda_items . append ( [string] ) [EOL] else : [EOL] agenda_items . append ( [string] ) [EOL] [EOL] [EOL] if [string] in question : [EOL] if [string] not in agenda_items and [string] not in agenda_items : [EOL] [comment] [EOL] [comment] [EOL] agenda_items . append ( [string] ) [EOL] agenda = [ ] [EOL] [comment] [EOL] for agenda_item in set ( agenda_items ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if agenda_item in self . terminal_productions : [EOL] agenda . append ( self . terminal_productions [ agenda_item ] ) [EOL] [EOL] if conservative : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] refined_column_productions = { } [EOL] for column_name , signature in self . _column_productions_for_agenda . items ( ) : [EOL] column_type , name = column_name . split ( [string] ) [EOL] if column_type == [string] : [EOL] if f" [string] { name }" not in self . _column_productions_for_agenda and f" [string] { name }" not in self . _column_productions_for_agenda : [EOL] refined_column_productions [ column_name ] = signature [EOL] [EOL] elif column_type == [string] : [EOL] if f" [string] { name }" not in self . _column_productions_for_agenda and f" [string] { name }" not in self . _column_productions_for_agenda : [EOL] refined_column_productions [ column_name ] = signature [EOL] [EOL] else : [EOL] if f" [string] { name }" not in self . _column_productions_for_agenda and f" [string] { name }" not in self . _column_productions_for_agenda : [EOL] refined_column_productions [ column_name ] = signature [EOL] [comment] [EOL] [comment] [EOL] refined_entities = [ ] [EOL] refined_numbers = [ ] [EOL] for entity in self . _question_entities : [EOL] if entity . replace ( [string] , [string] ) not in self . _question_numbers : [EOL] refined_entities . append ( entity ) [EOL] for number in self . _question_numbers : [EOL] if f" [string] { number }" not in self . _question_entities : [EOL] refined_numbers . append ( number ) [EOL] else : [EOL] refined_column_productions = dict ( self . _column_productions_for_agenda ) [EOL] refined_entities = list ( self . _question_entities ) [EOL] refined_numbers = list ( self . _question_numbers ) [EOL] [EOL] [comment] [EOL] question_with_underscores = [string] . join ( question_tokens ) [EOL] normalized_question = re . sub ( [string] , [string] , question_with_underscores ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] tokens_in_column_names = set ( ) [EOL] for column_name_with_type , signature in refined_column_productions . items ( ) : [EOL] column_name = column_name_with_type . split ( [string] ) [ [number] ] [EOL] [comment] [EOL] if f" [string] { column_name } [string] " in normalized_question : [EOL] agenda . append ( signature ) [EOL] for token in column_name . split ( [string] ) : [EOL] tokens_in_column_names . add ( token ) [EOL] [EOL] [comment] [EOL] for entity in refined_entities : [EOL] if entity . replace ( [string] , [string] ) not in tokens_in_column_names : [EOL] agenda . append ( f" [string] { entity }" ) [EOL] [EOL] for number in refined_numbers : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if f" [string] { number } [string] " in normalized_question : [EOL] agenda . append ( f" [string] { number }" ) [EOL] return agenda [EOL] [EOL] def evaluate_logical_form ( self , logical_form , target_list ) : [EOL] [docstring] [EOL] normalized_target_list = [ TableQuestionContext . normalize_string ( value ) for value in target_list ] [EOL] target_value_list = evaluator . to_value_list ( normalized_target_list ) [EOL] try : [EOL] denotation = self . execute ( logical_form ) [EOL] except ExecutionError : [EOL] logger . warning ( f' [string] { logical_form }' ) [EOL] return False [EOL] if isinstance ( denotation , list ) : [EOL] denotation_list = [ str ( denotation_item ) for denotation_item in denotation ] [EOL] else : [EOL] denotation_list = [ str ( denotation ) ] [EOL] denotation_value_list = evaluator . to_value_list ( denotation_list ) [EOL] return evaluator . check_denotation ( target_value_list , denotation_value_list ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] @ predicate def all_rows ( self ) : [EOL] return self . table_data [EOL] [EOL] @ predicate def select_string ( self , rows , column ) : [EOL] [docstring] [EOL] return [ str ( row . values [ column . name ] ) for row in rows if row . values [ column . name ] is not None ] [EOL] [EOL] @ predicate def select_number ( self , rows , column ) : [EOL] [docstring] [EOL] numbers = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , float ) : [EOL] numbers . append ( cell_value ) [EOL] [EOL] return numbers [ [number] ] if numbers else - [number] [comment] [EOL] [EOL] @ predicate def select_date ( self , rows , column ) : [EOL] [docstring] [EOL] dates = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] dates . append ( cell_value ) [EOL] [EOL] return dates [ [number] ] if dates else Date ( - [number] , - [number] , - [number] ) [comment] [EOL] [EOL] @ predicate def same_as ( self , rows , column ) : [EOL] [docstring] [EOL] cell_value = rows [ [number] ] . values [ column . name ] [EOL] return_list = [ ] [EOL] for table_row in self . table_data : [EOL] if table_row . values [ column . name ] == cell_value : [EOL] return_list . append ( table_row ) [EOL] return return_list [EOL] [EOL] @ predicate def date ( self , year , month , day ) : [EOL] [docstring] [EOL] return Date ( year , month , day ) [comment] [EOL] [EOL] @ predicate def first ( self , rows ) : [EOL] [docstring] [EOL] if not rows : [EOL] logger . warning ( [string] ) [EOL] return [ ] [EOL] return [ rows [ [number] ] ] [EOL] [EOL] @ predicate def last ( self , rows ) : [EOL] [docstring] [EOL] if not rows : [EOL] logger . warning ( [string] ) [EOL] return [ ] [EOL] return [ rows [ - [number] ] ] [EOL] [EOL] @ predicate def previous ( self , rows ) : [EOL] [docstring] [EOL] if not rows : [EOL] return [ ] [EOL] input_row_index = self . _get_row_index ( rows [ [number] ] ) [EOL] if input_row_index > [number] : [EOL] return [ self . table_data [ input_row_index - [number] ] ] [EOL] return [ ] [EOL] [EOL] @ predicate def next ( self , rows ) : [EOL] [docstring] [EOL] if not rows : [EOL] return [ ] [EOL] input_row_index = self . _get_row_index ( rows [ [number] ] ) [EOL] if input_row_index < len ( self . table_data ) - [number] and input_row_index != - [number] : [EOL] return [ self . table_data [ input_row_index + [number] ] ] [EOL] return [ ] [EOL] [EOL] @ predicate def count ( self , rows ) : [EOL] return len ( rows ) [comment] [EOL] [EOL] @ predicate def mode_string ( self , rows , column ) : [EOL] [docstring] [EOL] most_frequent_list = self . _get_most_frequent_values ( rows , column ) [EOL] if not most_frequent_list : [EOL] return [ ] [EOL] if not all ( [ isinstance ( value , str ) for value in most_frequent_list ] ) : [EOL] raise ExecutionError ( f" [string] { most_frequent_list }" ) [EOL] return most_frequent_list [EOL] [EOL] @ predicate def mode_number ( self , rows , column ) : [EOL] [docstring] [EOL] most_frequent_list = self . _get_most_frequent_values ( rows , column ) [EOL] if not most_frequent_list : [EOL] return [number] [comment] [EOL] most_frequent_value = most_frequent_list [ [number] ] [EOL] if not isinstance ( most_frequent_value , Number ) : [EOL] raise ExecutionError ( f" [string] { most_frequent_value }" ) [EOL] return most_frequent_value [EOL] [EOL] @ predicate def mode_date ( self , rows , column ) : [EOL] [docstring] [EOL] most_frequent_list = self . _get_most_frequent_values ( rows , column ) [EOL] if not most_frequent_list : [EOL] return Date ( - [number] , - [number] , - [number] ) [EOL] most_frequent_value = most_frequent_list [ [number] ] [EOL] if not isinstance ( most_frequent_value , Date ) : [EOL] raise ExecutionError ( f" [string] { most_frequent_value }" ) [EOL] return most_frequent_value [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def argmax ( self , rows , column ) : [EOL] [docstring] [EOL] if not rows : [EOL] return [ ] [EOL] value_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] if not value_row_pairs : [EOL] return [ ] [EOL] [comment] [EOL] return [ sorted ( value_row_pairs , key = lambda x : x [ [number] ] , reverse = True ) [ [number] ] [ [number] ] ] [EOL] [EOL] def argmin ( self , rows , column ) : [EOL] [docstring] [EOL] if not rows : [EOL] return [ ] [EOL] value_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] if not value_row_pairs : [EOL] return [ ] [EOL] [comment] [EOL] return [ sorted ( value_row_pairs , key = lambda x : x [ [number] ] ) [ [number] ] [ [number] ] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def filter_number_greater ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value > filter_value ] [comment] [EOL] [EOL] def filter_number_greater_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value >= filter_value ] [comment] [EOL] [EOL] def filter_number_lesser ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value < filter_value ] [comment] [EOL] [EOL] def filter_number_lesser_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value <= filter_value ] [comment] [EOL] [EOL] def filter_number_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value == filter_value ] [comment] [EOL] [EOL] def filter_number_not_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ( row . values [ column . name ] , row ) for row in rows ] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value != filter_value ] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def filter_date_greater ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value > filter_value ] [EOL] [EOL] def filter_date_greater_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value >= filter_value ] [EOL] [EOL] def filter_date_lesser ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value < filter_value ] [EOL] [EOL] def filter_date_lesser_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value <= filter_value ] [EOL] [EOL] def filter_date_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value == filter_value ] [EOL] [EOL] def filter_date_not_equals ( self , rows , column , filter_value ) : [EOL] cell_row_pairs = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , Date ) : [EOL] cell_row_pairs . append ( ( cell_value , row ) ) [EOL] return [ row for cell_value , row in cell_row_pairs if cell_value != filter_value ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def filter_in ( self , rows , column , filter_values ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not filter_values : [EOL] raise ExecutionError ( f" [string] { filter_values }" ) [EOL] if isinstance ( filter_values , str ) : [EOL] filter_value = filter_values [EOL] elif isinstance ( filter_values , list ) : [EOL] filter_value = filter_values [ [number] ] [EOL] else : [EOL] raise ExecutionError ( f" [string] { filter_values }" ) [EOL] [comment] [EOL] filter_value = filter_value . lstrip ( [string] ) [EOL] filtered_rows = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , str ) and filter_value in cell_value : [EOL] filtered_rows . append ( row ) [EOL] return filtered_rows [EOL] [EOL] def filter_not_in ( self , rows , column , filter_values ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not filter_values : [EOL] raise ExecutionError ( f" [string] { filter_values }" ) [EOL] if isinstance ( filter_values , str ) : [EOL] filter_value = filter_values [EOL] elif isinstance ( filter_values , list ) : [EOL] filter_value = filter_values [ [number] ] [EOL] else : [EOL] raise ExecutionError ( f" [string] { filter_values }" ) [EOL] [comment] [EOL] filter_value = filter_value . lstrip ( [string] ) [EOL] filtered_rows = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] if isinstance ( cell_value , str ) and filter_value not in cell_value : [EOL] filtered_rows . append ( row ) [EOL] return filtered_rows [EOL] [EOL] [comment] [EOL] [comment] [EOL] def max_date ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return Date ( - [number] , - [number] , - [number] ) [EOL] if not all ( [ isinstance ( value , Date ) for value in cell_values ] ) : [EOL] raise ExecutionError ( f" [string] { cell_values }" ) [EOL] return max ( cell_values ) [comment] [EOL] [EOL] def min_date ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return Date ( - [number] , - [number] , - [number] ) [EOL] if not all ( [ isinstance ( value , Date ) for value in cell_values ] ) : [EOL] raise ExecutionError ( f" [string] { cell_values }" ) [EOL] return min ( cell_values ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def max_number ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return [number] [comment] [EOL] if not all ( [ isinstance ( value , Number ) for value in cell_values ] ) : [EOL] raise ExecutionError ( f" [string] { cell_values }" ) [EOL] return max ( cell_values ) [comment] [EOL] [EOL] def min_number ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return [number] [comment] [EOL] if not all ( [ isinstance ( value , Number ) for value in cell_values ] ) : [EOL] raise ExecutionError ( f" [string] { cell_values }" ) [EOL] return min ( cell_values ) [comment] [EOL] [EOL] def sum ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return [number] [comment] [EOL] return sum ( cell_values ) [comment] [EOL] [EOL] def average ( self , rows , column ) : [EOL] [docstring] [EOL] cell_values = [ row . values [ column . name ] for row in rows ] [EOL] if not cell_values : [EOL] return [number] [comment] [EOL] return sum ( cell_values ) / len ( cell_values ) [comment] [EOL] [EOL] def diff ( self , first_row , second_row , column ) : [EOL] [docstring] [EOL] if not first_row or not second_row : [EOL] return [number] [comment] [EOL] first_value = first_row [ [number] ] . values [ column . name ] [EOL] second_value = second_row [ [number] ] . values [ column . name ] [EOL] if isinstance ( first_value , float ) and isinstance ( second_value , float ) : [EOL] return first_value - second_value [comment] [EOL] else : [EOL] raise ExecutionError ( f" [string] { column . name }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if not isinstance ( other , WikiTablesLanguage ) : [EOL] return False [EOL] return self . table_data == other . table_data [EOL] [EOL] @ staticmethod def _make_date ( cell_string ) : [EOL] string_parts = cell_string . split ( [string] ) [EOL] year = - [number] [EOL] month = - [number] [EOL] day = - [number] [EOL] for part in string_parts : [EOL] if part . isdigit ( ) : [EOL] if len ( part ) == [number] : [EOL] year = int ( part ) [EOL] else : [EOL] day = int ( part ) [EOL] elif part in MONTH_NUMBERS : [EOL] month = MONTH_NUMBERS [ part ] [EOL] return Date ( year , month , day ) [EOL] [EOL] def _get_row_index ( self , row ) : [EOL] [docstring] [EOL] row_index = - [number] [EOL] for index , table_row in enumerate ( self . table_data ) : [EOL] if table_row . values == row . values : [EOL] row_index = index [EOL] break [EOL] return row_index [EOL] [EOL] def _get_most_frequent_values ( self , rows , column ) : [EOL] value_frequencies = defaultdict ( int ) [EOL] max_frequency = [number] [EOL] most_frequent_list = [ ] [EOL] for row in rows : [EOL] cell_value = row . values [ column . name ] [EOL] value_frequencies [ cell_value ] += [number] [EOL] frequency = value_frequencies [ cell_value ] [EOL] if frequency > max_frequency : [EOL] max_frequency = frequency [EOL] most_frequent_list = [ cell_value ] [EOL] elif frequency == max_frequency : [EOL] most_frequent_list . append ( cell_value ) [EOL] return most_frequent_list [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.semparse.contexts.table_question_context.CellValueType]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0
class ParsingError ( Exception ) : [EOL] [docstring] [EOL] def __init__ ( self , message ) : [EOL] super ( ) . __init__ ( ) [EOL] self . message = message [EOL] [EOL] def __str__ ( self ) : [EOL] return repr ( self . message ) [EOL] [EOL] [EOL] class ExecutionError ( Exception ) : [EOL] [docstring] [EOL] def __init__ ( self , message ) : [EOL] super ( ) . __init__ ( ) [EOL] self . message = message [EOL] [EOL] def __str__ ( self ) : [EOL] return repr ( self . message ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . semparse . common . date import Date [EOL]	0 0 0 0 0 0 0 0 0 0 0
from allennlp . semparse . worlds . wikitables_world import WikiTablesWorld [EOL] from allennlp . semparse . worlds . atis_world import AtisWorld [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Union , List , Literal , Type , Any , Set [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import typing_extensions [EOL] import nltk [EOL] [docstring] [EOL] from typing import List , Dict , Set [EOL] import re [EOL] [EOL] from nltk . sem . logic import Type [EOL] from overrides import overrides [EOL] [EOL] from allennlp . semparse import util as semparse_util [EOL] from allennlp . semparse . contexts . knowledge_graph import KnowledgeGraph [EOL] from allennlp . semparse . type_declarations . quarel_type_declaration import QuarelTypeDeclaration [EOL] from allennlp . semparse . worlds . world import World [EOL] [EOL] class QuarelWorld ( World ) : [EOL] [docstring] [EOL] def __init__ ( self , table_graph , syntax , qr_coeff_sets = None ) : [EOL] [EOL] self . _syntax = syntax [EOL] self . types = QuarelTypeDeclaration ( syntax ) [EOL] super ( ) . __init__ ( global_type_signatures = self . types . name_mapper . type_signatures , global_name_mapping = self . types . name_mapper . name_mapping ) [EOL] self . table_graph = table_graph [EOL] [EOL] [comment] [EOL] self . _entity_index_maps = dict ( ) [EOL] self . _entity_counters = dict ( ) [EOL] [EOL] for entity in table_graph . entities : [EOL] self . _map_name ( entity , keep_mapping = True ) [EOL] [EOL] self . _entity_set = set ( table_graph . entities ) [EOL] [EOL] self . qr_coeff_sets = qr_coeff_sets [EOL] if qr_coeff_sets is None : [EOL] if [string] in syntax : [EOL] self . qr_coeff_sets = [ self . qr_coeff_sets_default [ [number] ] ] [EOL] else : [EOL] self . qr_coeff_sets = self . qr_coeff_sets_default [EOL] [EOL] def is_table_entity ( self , entity_name ) : [EOL] [docstring] [EOL] return entity_name in self . _entity_set [EOL] [EOL] [comment] [EOL] def _entity_index ( self , entity ) : [EOL] entity_type = entity [ [number] ] [EOL] if entity_type not in self . _entity_counters : [EOL] self . _entity_counters [ entity_type ] = [number] [EOL] self . _entity_index_maps [ entity_type ] = dict ( ) [EOL] entity_index_map = self . _entity_index_maps [ entity_type ] [EOL] if entity not in entity_index_map : [EOL] entity_index_map [ entity ] = self . _entity_counters [ entity_type ] [EOL] self . _entity_counters [ entity_type ] += [number] [EOL] return entity_index_map [ entity ] [EOL] [EOL] @ overrides def _map_name ( self , name , keep_mapping = False ) : [EOL] translated_name = name [EOL] if name in self . types . name_mapper . name_mapping : [EOL] translated_name = self . types . name_mapper . name_mapping [ name ] [EOL] elif name in self . local_name_mapping : [EOL] translated_name = self . local_name_mapping [ name ] [EOL] elif name . startswith ( [string] ) : [EOL] translated_name = [string] + str ( [number] + self . _entity_index ( name ) ) [EOL] self . _add_name_mapping ( name , translated_name , self . types . attr_function_type ) [EOL] [EOL] return translated_name [EOL] [EOL] def _get_curried_functions ( self ) : [EOL] return self . types . curried_functions [EOL] [EOL] @ overrides def get_basic_types ( self ) : [EOL] return self . types . basic_types [EOL] [EOL] @ overrides def get_valid_starting_types ( self ) : [EOL] return self . types . starting_types [EOL] [EOL] [comment] [EOL] [comment] [EOL] qr_coeff_sets_default = [ { [string] : [number] , [string] : - [number] , [string] : - [number] , [string] : - [number] , [string] : [number] } , { [string] : [number] , [string] : - [number] } , { [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : - [number] } , { [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : [number] } , { [string] : [number] , [string] : - [number] } , { [string] : [number] , [string] : - [number] , [string] : - [number] , [string] : - [number] } , { [string] : [number] , [string] : [number] } ] [EOL] [EOL] [comment] [EOL] qr_size = { [string] : [number] , [string] : [number] , [string] : - [number] , [string] : - [number] } [EOL] [EOL] def _get_qr_coeff ( self , attr1 , attr2 ) : [EOL] for qset in self . qr_coeff_sets : [EOL] if attr1 in qset and attr2 in qset : [EOL] return qset [ attr1 ] * qset [ attr2 ] [EOL] return [number] [EOL] [EOL] def _check_compatible ( self , setup , answer ) : [EOL] attributes = { setup [ [number] ] , answer [ [number] ] } [EOL] qr_coeff = None [EOL] for qr_coeff_set in self . qr_coeff_sets : [EOL] if not attributes - qr_coeff_set . keys ( ) : [EOL] qr_coeff = qr_coeff_set [EOL] if qr_coeff is None : [EOL] return False [comment] [EOL] [EOL] attribute_dir = qr_coeff [ setup [ [number] ] ] * qr_coeff [ answer [ [number] ] ] [EOL] change_same = [number] if self . qr_size [ setup [ [number] ] ] == self . qr_size [ answer [ [number] ] ] else - [number] [EOL] world_same = [number] if setup [ [number] ] == answer [ [number] ] else - [number] [EOL] return attribute_dir * change_same * world_same == [number] [EOL] [EOL] def _exec_infer ( self , setup , * answers ) : [EOL] answer_index = - [number] [EOL] if len ( answers ) == [number] : [EOL] if self . _check_compatible ( setup , answers [ [number] ] ) : [EOL] return [number] [EOL] else : [EOL] return [number] [EOL] for index , answer in enumerate ( answers ) : [EOL] if self . _check_compatible ( setup , answer ) : [EOL] if answer_index > - [number] : [EOL] [comment] [EOL] answer_index = - [number] [EOL] else : [EOL] answer_index = index [EOL] return answer_index [EOL] [EOL] def _exec_and ( self , expr ) : [EOL] if not expr or expr [ [number] ] != [string] : [EOL] return expr [EOL] args = expr [ [number] : ] [EOL] if len ( args ) == [number] : [EOL] return args [ [number] ] [EOL] if len ( args ) > [number] : [EOL] [comment] [EOL] return None [EOL] if self . _check_compatible ( args [ [number] ] , args [ [number] ] ) : [EOL] [comment] [EOL] return args [ [number] ] [EOL] return None [EOL] [EOL] def execute ( self , lf_raw ) : [EOL] [docstring] [EOL] [comment] [EOL] logical_form = re . sub ( [string] , [string] , lf_raw ) [EOL] parse = semparse_util . lisp_to_nested_expression ( logical_form ) [EOL] if len ( parse ) < [number] : [EOL] return - [number] [EOL] if parse [ [number] ] == [string] : [EOL] args = [ self . _exec_and ( arg ) for arg in parse [ [number] : ] ] [EOL] if None in args : [EOL] return - [number] [EOL] return self . _exec_infer ( * args ) [EOL] return - [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import multiprocessing [EOL] import logging [EOL] import builtins [EOL] import logging [EOL] from typing import List [EOL] [EOL] import sqlite3 [EOL] import multiprocessing [EOL] from multiprocessing import Process [EOL] from allennlp . common . file_utils import cached_path [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] MULTIPROCESSING_LOGGER = multiprocessing . get_logger ( ) [EOL] [EOL] class SqlExecutor : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , database_file ) : [EOL] [comment] [EOL] self . _database_file = cached_path ( database_file ) [EOL] self . _connection = sqlite3 . connect ( self . _database_file ) [EOL] self . _cursor = self . _connection . cursor ( ) [EOL] [EOL] def evaluate_sql_query ( self , predicted_sql_query , sql_query_labels ) : [EOL] [comment] [EOL] [comment] [EOL] MULTIPROCESSING_LOGGER . setLevel ( logging . WARNING ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] process = Process ( target = self . _evaluate_sql_query_subprocess , args = ( predicted_sql_query , sql_query_labels ) ) [EOL] process . start ( ) [EOL] [EOL] [comment] [EOL] process . join ( [number] ) [EOL] denotation_correct = process . exitcode [comment] [EOL] [EOL] if process . is_alive ( ) : [EOL] logger . warning ( [string] ) [EOL] process . terminate ( ) [EOL] process . join ( ) [EOL] [EOL] if denotation_correct is None : [EOL] denotation_correct = [number] [EOL] [EOL] return denotation_correct [EOL] [EOL] def _evaluate_sql_query_subprocess ( self , predicted_query , sql_query_labels ) : [EOL] [docstring] [EOL] [EOL] postprocessed_predicted_query = self . postprocess_query_sqlite ( predicted_query ) [EOL] [EOL] try : [EOL] self . _cursor . execute ( postprocessed_predicted_query ) [EOL] predicted_rows = self . _cursor . fetchall ( ) [EOL] except sqlite3 . Error as error : [EOL] logger . warning ( f' [string] { error }' ) [EOL] exit ( [number] ) [EOL] [EOL] [comment] [EOL] target_rows = None [EOL] for sql_query_label in sql_query_labels : [EOL] postprocessed_sql_query_label = self . postprocess_query_sqlite ( sql_query_label ) [EOL] try : [EOL] self . _cursor . execute ( postprocessed_sql_query_label ) [EOL] target_rows = self . _cursor . fetchall ( ) [EOL] except sqlite3 . Error as error : [EOL] logger . warning ( f' [string] { error }' ) [EOL] if predicted_rows == target_rows : [EOL] exit ( [number] ) [EOL] exit ( [number] ) [EOL] [EOL] @ staticmethod def postprocess_query_sqlite ( query ) : [EOL] [comment] [EOL] [comment] [EOL] query = query . strip ( ) [EOL] if query . startswith ( [string] ) : [EOL] return query [ [number] : query . rfind ( [string] ) ] + [string] [EOL] return query [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0
[docstring] [EOL] from allennlp . semparse . executors . wikitables_sempre_executor import WikiTablesSempreExecutor [EOL] from allennlp . semparse . executors . sql_executor import SqlExecutor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import builtins [EOL] [docstring] [EOL] from allennlp . semparse . type_declarations . type_declaration import ComplexType , NamedBasicType , NameMapper [EOL] [EOL] class QuarelTypeDeclaration : [EOL] def __init__ ( self , syntax ) : [EOL] [EOL] self . name_mapper = NameMapper ( ) [EOL] [EOL] num_type = NamedBasicType ( [string] ) [EOL] attr_type = NamedBasicType ( [string] ) [EOL] rdir_type = NamedBasicType ( [string] ) [EOL] world_type = NamedBasicType ( [string] ) [EOL] var_type = NamedBasicType ( [string] ) [EOL] [EOL] self . basic_types = { num_type , attr_type , rdir_type , world_type , var_type } [EOL] [EOL] if syntax == [string] : [EOL] [comment] [EOL] attr_function_type = ComplexType ( rdir_type , ComplexType ( world_type , attr_type ) ) [EOL] [EOL] and_function_type = ComplexType ( attr_type , ComplexType ( attr_type , attr_type ) ) [EOL] [EOL] [comment] [EOL] infer_function_type = ComplexType ( attr_type , ComplexType ( attr_type , ComplexType ( attr_type , num_type ) ) ) [EOL] self . name_mapper . map_name_with_signature ( [string] , infer_function_type ) [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , and_function_type ) [EOL] [EOL] self . curried_functions = { attr_function_type : [number] , infer_function_type : [number] , and_function_type : [number] } [EOL] elif syntax == [string] or syntax == [string] : [EOL] [comment] [EOL] attr_function_type = ComplexType ( rdir_type , ComplexType ( world_type , attr_type ) ) [EOL] [EOL] and_function_type = ComplexType ( attr_type , ComplexType ( attr_type , attr_type ) ) [EOL] [EOL] [comment] [EOL] infer_function_type = ComplexType ( attr_type , ComplexType ( attr_type , ComplexType ( attr_type , num_type ) ) ) [EOL] self . name_mapper . map_name_with_signature ( [string] , infer_function_type ) [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , and_function_type ) [EOL] [EOL] self . curried_functions = { attr_function_type : [number] , infer_function_type : [number] , and_function_type : [number] } [EOL] [EOL] elif syntax == [string] : [EOL] [comment] [EOL] attr_function_type = ComplexType ( rdir_type , ComplexType ( world_type , attr_type ) ) [EOL] [EOL] and_function_type = ComplexType ( attr_type , ComplexType ( attr_type , attr_type ) ) [EOL] [EOL] [comment] [EOL] infer_function_type = ComplexType ( attr_type , ComplexType ( attr_type , ComplexType ( attr_type , num_type ) ) ) [EOL] self . name_mapper . map_name_with_signature ( [string] , infer_function_type ) [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , attr_function_type ) [EOL] [EOL] [comment] [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , and_function_type ) [EOL] [EOL] self . curried_functions = { attr_function_type : [number] , infer_function_type : [number] , and_function_type : [number] } [EOL] [EOL] else : [EOL] raise Exception ( f" [string] { syntax }" ) [EOL] [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , rdir_type ) [EOL] [EOL] self . name_mapper . map_name_with_signature ( [string] , world_type ) [EOL] self . name_mapper . map_name_with_signature ( [string] , world_type ) [EOL] [EOL] [comment] [EOL] self . world_type = world_type [EOL] self . attr_function_type = attr_function_type [EOL] self . var_type = var_type [EOL] [EOL] self . starting_types = { num_type } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from allennlp . semparse . contexts . table_question_knowledge_graph import TableQuestionKnowledgeGraph [EOL] from allennlp . semparse . contexts . atis_sql_table_context import AtisSqlTableContext [EOL] from allennlp . semparse . contexts . table_question_context import TableQuestionContext [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . nn . activations import Activation [EOL] from allennlp . nn . initializers import Initializer , InitializerApplicator [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] class Regularizer ( Registrable ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __call__ ( self , parameter ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0
from allennlp . state_machines . trainers . decoder_trainer import DecoderTrainer [EOL] from allennlp . state_machines . trainers . expected_risk_minimization import ExpectedRiskMinimization [EOL] from allennlp . state_machines . trainers . maximum_marginal_likelihood import MaximumMarginalLikelihood [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict [EOL] import typing [EOL] import torch [EOL] import allennlp [EOL] import builtins [EOL] from typing import Dict [EOL] [EOL] import torch [EOL] [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] class ChecklistStatelet : [EOL] [docstring] [EOL] def __init__ ( self , terminal_actions , checklist_target , checklist_mask , checklist , terminal_indices_dict = None ) : [EOL] self . terminal_actions = terminal_actions [EOL] self . checklist_target = checklist_target [EOL] self . checklist_mask = checklist_mask [EOL] self . checklist = checklist [EOL] if terminal_indices_dict is not None : [EOL] self . terminal_indices_dict = terminal_indices_dict [EOL] else : [EOL] self . terminal_indices_dict = { } [EOL] for checklist_index , batch_action_index in enumerate ( terminal_actions . detach ( ) . cpu ( ) ) : [EOL] action_index = int ( batch_action_index [ [number] ] ) [EOL] if action_index == - [number] : [EOL] continue [EOL] self . terminal_indices_dict [ action_index ] = checklist_index [EOL] [EOL] def update ( self , action ) : [EOL] [docstring] [EOL] checklist_addition = ( self . terminal_actions == action ) . float ( ) [EOL] new_checklist = self . checklist + checklist_addition [EOL] new_checklist_state = ChecklistStatelet ( terminal_actions = self . terminal_actions , checklist_target = self . checklist_target , checklist_mask = self . checklist_mask , checklist = new_checklist , terminal_indices_dict = self . terminal_indices_dict ) [EOL] return new_checklist_state [EOL] [EOL] def get_balance ( self ) : [EOL] return self . checklist_mask * ( self . checklist_target - self . checklist ) [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( self , other . __class__ ) : [EOL] return all ( [ util . tensors_equal ( self . terminal_actions , other . terminal_actions ) , util . tensors_equal ( self . checklist_target , other . checklist_target ) , util . tensors_equal ( self . checklist_mask , other . checklist_mask ) , util . tensors_equal ( self . checklist , other . checklist ) , self . terminal_indices_dict == other . terminal_indices_dict , ] ) [EOL] return NotImplemented [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 $builtins.int$ 0 0 0 0 0 0 $'ChecklistStatelet'$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.allennlp.state_machines.states.checklist_statelet.ChecklistStatelet$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.allennlp.state_machines.states.checklist_statelet.ChecklistStatelet$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0