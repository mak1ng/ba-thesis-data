[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any , Type , Pattern [EOL] import benchmark [EOL] import typing [EOL] import argparse [EOL] [docstring] [EOL] [EOL] [EOL] import argparse [EOL] import io [EOL] import os [EOL] import re [EOL] import shutil [EOL] import subprocess [EOL] import sys [EOL] import webbrowser [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] import metadata [EOL] [EOL] [EOL] def main ( ) : [EOL] deviceparser = argparse . ArgumentParser ( add_help = False ) [EOL] deviceparser . add_argument ( [string] , [string] , type = int , default = - [number] , help = [string] [string] [string] ) [EOL] [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( dest = [string] , help = [string] ) [EOL] [EOL] version_parser = subparsers . add_parser ( [string] , description = [string] ) [EOL] test_parser = subparsers . add_parser ( [string] , parents = [ deviceparser ] , description = [string] ) [EOL] benchmark_parser = subparsers . add_parser ( [string] , parents = [ deviceparser ] , description = [string] ) [EOL] docs_parser = subparsers . add_parser ( [string] , description = [string] ) [EOL] readme_parser = subparsers . add_parser ( [string] , description = [string] ) [EOL] workflows_parser = subparsers . add_parser ( [string] , description = [string] ) [EOL] should_not_import_parser = subparsers . add_parser ( [string] , description = [string] [string] ) [EOL] [EOL] version_parser . set_defaults ( cmd = version ) [EOL] test_parser . set_defaults ( cmd = test ) [EOL] benchmark_parser . set_defaults ( cmd = benchmark ) [EOL] docs_parser . set_defaults ( cmd = docs ) [EOL] readme_parser . set_defaults ( cmd = readme ) [EOL] workflows_parser . set_defaults ( cmd = workflows ) [EOL] should_not_import_parser . set_defaults ( cmd = should_not_import ) [EOL] [EOL] test_parser . add_argument ( [string] , [string] , default = [string] , help = [string] [string] ) [EOL] test_parser . add_argument ( [string] , [string] , nargs = argparse . REMAINDER , help = [string] ) [EOL] [EOL] benchmark_parser . add_argument ( [string] , [string] , action = [string] , dest = [string] , help = [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , action = [string] , dest = [string] , help = [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , action = [string] , dest = [string] , help = [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , choices = ( [string] , [string] ) , default = [string] , help = [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , choices = ( [string] , [string] , [string] , [string] , [string] ) , default = [string] , help = [string] [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , choices = ( [string] , [string] , [string] , [string] ) , default = [string] , help = [string] [string] [string] [string] [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , choices = ( [string] , [string] , [string] , [string] ) , default = [string] , help = [string] [string] [string] [string] ) [EOL] benchmark_parser . add_argument ( [string] , [string] , action = [string] , help = [string] [string] [string] [string] ) [EOL] [EOL] docs_parser . add_argument ( [string] , [string] , action = [string] , help = [string] ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] [comment] [EOL] if hasattr ( args , [string] ) : [EOL] return args . cmd ( args ) [EOL] else : [EOL] [comment] [EOL] print ( [string] ) [EOL] [EOL] [EOL] _here = os . path . realpath ( os . path . dirname ( __file__ ) ) [EOL] [EOL] [EOL] def _get_device ( ) : [EOL] import torch [EOL] try : [EOL] return [string] + str ( torch . cuda . current_device ( ) ) [EOL] except AssertionError : [EOL] return [string] [EOL] [EOL] [EOL] class _NullContext ( object ) : [EOL] def __enter__ ( self ) : [EOL] pass [EOL] [EOL] def __exit__ ( self , exc_type , exc_val , exc_tb ) : [EOL] pass [EOL] [EOL] [EOL] def version ( args ) : [EOL] print ( metadata . version ) [EOL] [EOL] [EOL] def test ( args ) : [EOL] try : [EOL] import iisignature [comment] [EOL] except ImportError : [EOL] raise ImportError ( [string] [string] ) [EOL] import pytest [EOL] import torch [EOL] with torch . cuda . device ( args . device ) if args . device != - [number] else _NullContext ( ) : [EOL] print ( [string] + _get_device ( ) ) [EOL] pytest_args = [ os . path . join ( _here , [string] , args . test ) ] [EOL] pytest_args . extend ( [ [string] , [string] , [string] ] ) [EOL] if args . args is not None : [EOL] pytest_args . extend ( args . args ) [EOL] return pytest . main ( pytest_args ) == [number] [EOL] [EOL] [EOL] def benchmark ( args ) : [EOL] [docstring] [EOL] try : [EOL] import iisignature [comment] [EOL] except ImportError : [EOL] raise ImportError ( [string] [string] ) [EOL] try : [EOL] import esig [comment] [EOL] except ImportError : [EOL] raise ImportError ( [string] [string] ) [EOL] [EOL] import benchmark . benchmark as bench [EOL] import torch [EOL] [EOL] if args . measure == [string] : [EOL] measure = bench . Measurables . time [EOL] elif args . measure == [string] : [EOL] measure = bench . Measurables . memory [EOL] print ( [string] [string] ) [EOL] else : [EOL] raise RuntimeError [EOL] [EOL] if args . fns == [string] : [EOL] fns = bench . Functions . all_fns [EOL] elif args . fns == [string] : [EOL] fns = bench . Functions . signature_forward_fns [EOL] elif args . fns == [string] : [EOL] fns = bench . Functions . signature_backward_fns [EOL] elif args . fns == [string] : [EOL] fns = bench . Functions . logsignature_forward_fns [EOL] elif args . fns == [string] : [EOL] fns = bench . Functions . logsignature_backward_fns [EOL] else : [EOL] raise RuntimeError [EOL] [EOL] if args . type == [string] : [EOL] type_ = bench . Types . typical [EOL] elif args . type == [string] : [EOL] type_ = bench . Types . depths [EOL] elif args . type == [string] : [EOL] type_ = bench . Types . channels [EOL] elif args . type == [string] : [EOL] type_ = bench . Types . small [EOL] else : [EOL] raise RuntimeError [EOL] [EOL] try : [EOL] runner = bench . BenchmarkRunner ( type_ = type_ , test_esig = args . test_esig , test_iisignature = args . test_iisignature , test_signatory_gpu = args . test_signatory_gpu , measure = measure , fns = fns ) [EOL] if args . output in ( [string] , [string] ) : [EOL] runner . check_graph ( ) [EOL] except bench . InvalidBenchmark as e : [EOL] print ( str ( e ) ) [EOL] else : [EOL] with torch . cuda . device ( args . device ) if args . device != - [number] else _NullContext ( ) : [EOL] print ( [string] + _get_device ( ) ) [EOL] runner . run ( ) [EOL] [EOL] if args . output in ( [string] , [string] ) : [EOL] runner . table ( save = args . save ) [EOL] if args . output in ( [string] , [string] ) : [EOL] runner . graph ( save = args . save ) [EOL] [EOL] return runner [EOL] [EOL] [EOL] def docs ( args = ( ) ) : [EOL] [docstring] [EOL] try : [EOL] import py2annotate [comment] [EOL] except ImportError : [EOL] raise ImportError ( [string] [string] ) [EOL] try : [EOL] shutil . rmtree ( os . path . join ( _here , [string] , [string] ) ) [EOL] except FileNotFoundError : [EOL] pass [EOL] subprocess . Popen ( [string] . format ( os . path . join ( _here , [string] ) , os . path . join ( _here , [string] , [string] ) ) ) . wait ( ) [EOL] if args . open : [EOL] webbrowser . open_new_tab ( [string] . format ( os . path . join ( _here , [string] , [string] , [string] , [string] ) ) ) [EOL] [EOL] [EOL] def workflows ( args = ( ) ) : [EOL] [docstring] [EOL] sys . path . insert ( [number] , os . path . join ( _here , [string] , [string] ) ) [EOL] import from_template [EOL] from_template . main ( ) [EOL] sys . path = sys . path [ [number] : ] [EOL] [EOL] [EOL] def readme ( args = ( ) ) : [EOL] [docstring] [EOL] [EOL] outs = [ ] [EOL] includestr = [string] [EOL] on = [string] [EOL] off = [string] [EOL] insert = [string] [comment] [EOL] reference = re . compile ( [string] ) [EOL] [EOL] def parse_file ( filename ) : [EOL] out_data = [ ] [EOL] with io . open ( filename , [string] , encoding = [string] ) as f : [EOL] data = f . readlines ( ) [EOL] skipping = False [EOL] for line in data : [EOL] stripline = line . strip ( ) [EOL] if stripline . startswith ( on ) : [EOL] skipping = False [EOL] elif stripline . startswith ( off ) : [EOL] skipping = True [EOL] elif skipping : [EOL] pass [EOL] elif reference . match ( stripline ) : [EOL] pass [EOL] else : [EOL] if stripline . startswith ( insert ) : [EOL] indent = line . find ( insert ) [EOL] out_line = line [ : indent ] + line [ indent + len ( insert ) : ] [EOL] elif stripline . startswith ( includestr ) : [EOL] [comment] [EOL] subfilename = stripline [ len ( includestr ) : ] . strip ( ) [ [number] : ] [EOL] out_line = parse_file ( os . path . join ( _here , [string] , subfilename ) ) [EOL] else : [EOL] out_line = line [EOL] if [string] in out_line : [EOL] raise RuntimeError ( [string] ) [EOL] out_line = out_line . replace ( [string] , metadata . version ) [EOL] out_data . append ( out_line ) [EOL] return [string] . join ( out_data ) [EOL] [EOL] def read_from_files ( filenames ) : [EOL] for filename in filenames : [EOL] filename = os . path . join ( _here , filename ) [EOL] outs . append ( parse_file ( filename ) ) [EOL] [EOL] read_from_files ( [ os . path . join ( _here , [string] , [string] ) , os . path . join ( _here , [string] , [string] , [string] , [string] ) , os . path . join ( [string] , [string] , [string] , [string] ) ] ) [EOL] [EOL] outs . append ( [string] [string] [string] ) [EOL] [EOL] outs . append ( [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] [string] ) [EOL] [EOL] read_from_files ( [ os . path . join ( _here , [string] , [string] , [string] , [string] ) ] ) [EOL] [EOL] with io . open ( os . path . join ( _here , [string] ) , [string] , encoding = [string] ) as f : [EOL] f . write ( [string] . join ( outs ) ) [EOL] [EOL] [EOL] def should_not_import ( args = ( ) ) : [EOL] [docstring] [EOL] [EOL] try : [EOL] import signatory [EOL] except ImportError as e : [EOL] [comment] [EOL] return str ( e ) in ( [string] , [string] ) [EOL] else : [EOL] return False [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] result = main ( ) [EOL] if isinstance ( result , bool ) : [EOL] sys . exit ( not result ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Match , Any , Optional [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import io [EOL] import os [EOL] import re [EOL] [comment] [EOL] [EOL] [EOL] project = [string] [EOL] author = [string] [EOL] copyright = [string] . format ( author ) [EOL] author_email = [string] [EOL] url = [string] [EOL] license = [string] [EOL] python_requires = [string] [EOL] keywords = [string] [EOL] classifiers = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] description = [string] [EOL] [EOL] here = os . path . realpath ( os . path . dirname ( __file__ ) ) [EOL] [EOL] [comment] [EOL] with io . open ( os . path . join ( here , [string] , project , [string] ) ) as f : [EOL] meta_match = re . search ( [string] , f . read ( ) , re . M ) [EOL] if meta_match : [EOL] version = meta_match . group ( [number] ) [EOL] else : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] with io . open ( os . path . join ( here , [string] ) , [string] , encoding = [string] ) as f : [EOL] readme = f . read ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Optional[typing.Match[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.Match[builtins.str]]$ 0 0 $typing.Any$ 0 $typing.Optional[typing.Match[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import setuptools [EOL] import sys [EOL] try : [EOL] import torch . utils . cpp_extension as cpp [EOL] except ImportError : [EOL] raise ImportError ( [string] ) [EOL] [EOL] import metadata [EOL] [EOL] extra_compile_args = [ ] [EOL] [EOL] [comment] [EOL] if not sys . platform . startswith ( [string] ) : [comment] [EOL] extra_compile_args . append ( [string] ) [EOL] [EOL] if sys . platform . startswith ( [string] ) : [comment] [EOL] extra_compile_args . append ( [string] ) [EOL] else : [comment] [EOL] extra_compile_args . append ( [string] ) [EOL] [EOL] ext_modules = [ cpp . CppExtension ( name = [string] , sources = [ [string] , [string] , [string] , [string] , [string] , [string] ] , depends = [ [string] , [string] , [string] , [string] , [string] ] , extra_compile_args = extra_compile_args ) ] [EOL] [EOL] [EOL] setuptools . setup ( name = metadata . project , version = metadata . version , author = metadata . author , author_email = metadata . author_email , maintainer = metadata . author , maintainer_email = metadata . author_email , description = metadata . description , long_description = metadata . readme , url = metadata . url , license = metadata . license , keywords = metadata . keywords , classifiers = metadata . classifiers , zip_safe = False , python_requires = metadata . python_requires , packages = [ metadata . project ] , ext_package = metadata . project , package_dir = { [string] : [string] } , ext_modules = ext_modules , cmdclass = { [string] : cpp . BuildExtension } ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Dict , Match , Any , Optional , List , Pattern [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import io [EOL] import os [EOL] import re [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def _substitute ( filename , ** subs ) : [EOL] [docstring] [EOL] [EOL] print ( [string] , filename ) [EOL] [EOL] here = os . path . realpath ( os . path . dirname ( __file__ ) ) [EOL] [EOL] with io . open ( os . path . join ( here , filename + [string] ) , encoding = [string] , mode = [string] ) as f : [EOL] template_lines = f . readlines ( ) [EOL] [EOL] print ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] argument_header_finder = re . compile ( [string] ) [EOL] argument_finder = re . compile ( [string] ) [EOL] found_argument_header = False [EOL] for line in template_lines : [EOL] if argument_header_finder . match ( line ) : [EOL] found_argument_header = True [EOL] continue [EOL] if found_argument_header : [EOL] argument_match = argument_finder . match ( line ) [EOL] if argument_match is None : [EOL] break [comment] [EOL] argument_name = argument_match . group ( [number] ) [EOL] argument_value = argument_match . group ( [number] ) [EOL] print ( [string] , argument_name , [string] , argument_value ) [EOL] if argument_name in subs : [EOL] raise RuntimeError ( [string] . format ( argument_name , filename ) ) [EOL] subs [ argument_name ] = argument_value [EOL] print ( [string] ) [EOL] template = [string] . join ( template_lines ) [EOL] del template_lines [EOL] [EOL] [comment] [EOL] subs_re = { } [EOL] for sub_key , sub_val_raw in subs . items ( ) : [EOL] sub_key_bracket = [string] . format ( sub_key ) [EOL] sub_re = re . compile ( [string] . format ( sub_key_bracket ) , flags = re . MULTILINE ) [EOL] sub_val_split = sub_val_raw . split ( [string] ) [EOL] subs_re [ sub_key_bracket ] = ( sub_re , sub_val_split ) [EOL] [EOL] while True : [EOL] found = False [EOL] for sub_key_bracket , ( sub_re , sub_val_split ) in subs_re . items ( ) : [EOL] searched = sub_re . search ( template ) [EOL] if not searched : [EOL] continue [EOL] found = True [EOL] white_space_amount = searched . end ( ) - searched . start ( ) - len ( sub_key_bracket ) [EOL] white_space = [string] * white_space_amount [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] first_characters = template [ searched . start ( ) : searched . start ( ) + white_space_amount ] [EOL] white_sub_vals = [ first_characters + sub_val_split [ [number] ] ] [EOL] white_sub_vals . extend ( [ white_space + sub_val for sub_val in sub_val_split [ [number] : ] ] ) [EOL] template = template [ : searched . start ( ) ] + [string] . join ( white_sub_vals ) + template [ searched . end ( ) : ] [EOL] if not found : [EOL] break [EOL] [EOL] unsubbed = re . compile ( [string] ) [EOL] search = unsubbed . search ( template ) [EOL] if search : [EOL] raise RuntimeError ( [string] . format ( search . group ( ) , filename ) ) [EOL] [EOL] template = [string] . join ( [ [string] , [string] , [string] , [string] , [string] , [string] , template ] ) [EOL] with io . open ( os . path . join ( here , [string] , [string] , filename ) , encoding = [string] , mode = [string] ) as f : [EOL] f . write ( template ) [EOL] [EOL] print ( [string] , filename ) [EOL] [EOL] [EOL] [comment] [EOL] global_subs = dict ( windows = [string] , linux = [string] , mac = [string] , on = [string] , on_rd = [string] , py27 = [string] , py35 = [string] , py36 = [string] , py37 = [string] , py38 = [string] , py_all = [string] , pytorch12 = [string] , pytorch13 = [string] , pytorch131 = [string] , pytorch14 = [string] , pytorch15 = [string] , pytorch151 = [string] , pytorch16 = [string] , pytorch_all = [string] , strategy = [string] , strategy_single = [string] , strategy_single_all_pytorch = [string] , action_trigger = [string] , _action_os_windows = [string] , _action_os_linux = [string] , _action_os_mac = [string] , _action_os_star = [string] , action_os = [string] , _action_pv_27 = [string] , _action_pv_35 = [string] , _action_pv_36 = [string] , _action_pv_37 = [string] , _action_pv_38 = [string] , _action_pv_star = [string] , action_pv = [string] , if_event = [string] , if_repository_dispatch = [string] , if_ = [string] , checkout_code = [string] , install_python = [string] , setup_windows = [string] , build_windows = [string] , install_local_windows = [string] , install_remote_windows = [string] , test_windows = [string] , terminate_windows = [string] , setup_linux = [string] , build_linux = [string] , install_local_linux = [string] , install_remote_linux = [string] , test_linux = [string] , terminate_linux = [string] , setup_mac = [string] , build_mac = [string] , install_local_mac = [string] , install_remote_mac = [string] , test_mac = [string] , terminate_mac = [string] , upload_windows = [string] , upload_unix = [string] , ) [comment] [EOL] global_subs [ [string] ] = global_subs [ [string] ] = global_subs [ [string] ] [EOL] [EOL] test = False [EOL] if test : [EOL] global_subs [ [string] ] = [string] [EOL] global_subs [ [string] ] = [string] [EOL] else : [EOL] global_subs [ [string] ] = [string] [EOL] global_subs [ [string] ] = [string] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] _substitute ( [string] , ** global_subs ) [EOL] _substitute ( [string] , ** global_subs ) [EOL] _substitute ( [string] , ** global_subs ) [EOL] _substitute ( [string] , ** global_subs ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Dict , Any , Tuple [EOL] import typing [EOL] import os [EOL] import sys [EOL] [EOL] [EOL] sys . path . extend ( [ os . path . abspath ( [string] ) , os . path . abspath ( [string] ) , os . path . abspath ( [string] ) ] ) [comment] [EOL] [EOL] import metadata [EOL] [EOL] [EOL] project = metadata . project . title ( ) [EOL] copyright = metadata . copyright [EOL] author = metadata . author [EOL] version = release = metadata . version [EOL] [EOL] [comment] [EOL] extensions = [ [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] napoleon_use_admonition_for_examples = True [EOL] autodoc_mock_imports = [ [string] . format ( metadata . project ) ] [EOL] autodoc_member_order = [string] [EOL] intersphinx_mapping = { [string] : ( [string] , None ) } [EOL] [EOL] master_doc = [string] [EOL] [EOL] html_theme = [string] [EOL] html_static_path = [ [string] ] [EOL] [EOL] [comment] [EOL] templates_path = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclude_patterns = [ [string] , [string] , [string] ] [EOL] [EOL] [EOL] def setup ( app ) : [EOL] app . add_stylesheet ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Tuple[builtins.str,None]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import matplotlib [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] [EOL] matplotlib . rc ( [string] , usetex = True ) [EOL] matplotlib . rc ( [string] , size = [number] ) [EOL] [EOL] [EOL] def save ( name ) : [EOL] plt . tight_layout ( ) [EOL] plt . savefig ( name ) [EOL] plt . close ( ) [EOL] [EOL] [EOL] def plot ( x , y ) : [EOL] plt . plot ( x , y ) [EOL] plt . ylim ( - [number] , [number] ) [EOL] plt . xlim ( - [number] , [number] * np . pi + [number] ) [EOL] [EOL] [EOL] def scatter ( x , y , letter ) : [EOL] plt . scatter ( x , y ) [EOL] plt . ylim ( - [number] , [number] ) [EOL] plt . xlim ( - [number] , [number] * np . pi + [number] ) [EOL] for i in range ( len ( x ) ) : [EOL] plt . annotate ( [string] . format ( letter , i + [number] , x [ i ] , y [ i ] ) , ( x [ i ] - [number] , y [ i ] - [number] ) ) [EOL] [EOL] [EOL] def noise ( num , eps = [number] ) : [EOL] x = np . linspace ( eps + [number] , [number] * np . pi - eps - [number] , num ) + np . random . uniform ( low = - eps , high = eps , size = num ) [EOL] y = np . sin ( x ) [EOL] return x , y [EOL] [EOL] [EOL] true_x = np . linspace ( [number] , [number] * np . pi , [number] ) [EOL] true_y = np . sin ( true_x ) [EOL] plot ( true_x , true_y ) [EOL] save ( [string] ) [EOL] [EOL] sample1_x , sample1_y = noise ( [number] ) [EOL] plot ( true_x , true_y ) [EOL] scatter ( sample1_x , sample1_y , [string] ) [EOL] save ( [string] ) [EOL] [EOL] sample2_x , sample2_y = noise ( [number] ) [EOL] plot ( true_x , true_y ) [EOL] scatter ( sample2_x , sample2_y , [string] ) [EOL] save ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] [EOL] class Tensor : [EOL] pass [EOL] [EOL] [EOL] class Size : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
class Module : [EOL] pass [EOL]	0 0 0 0 0 0
def relu ( ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0
def once_differentiable ( x ) : [EOL] return x [EOL]	0 0 0 0 0 0 0 0 0 0
from . import function [EOL] [EOL] [EOL] class Function : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0
class ndarray ( object ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any [EOL] import _importlib_modulespec [EOL] import typing [EOL] import argparse [EOL] [docstring] [EOL] [EOL] [EOL] import argparse [EOL] import gc [EOL] import importlib [EOL] import memory_profiler [EOL] import numpy as np [EOL] import time [EOL] import torch [EOL] import sys [EOL] [EOL] [EOL] def main ( ) : [EOL] obj = argparse . Namespace ( size = tuple ( int ( i ) for i in size . split ( [string] ) ) , depth = int ( depth ) ) [EOL] library_module = importlib . import_module ( [string] + library_module_name , __package__ ) [EOL] library_module . setup ( obj ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] def run_wrapper ( ) : [EOL] [comment] [EOL] result = library_module . run ( obj ) [EOL] if result is None : [EOL] raise RuntimeError ( [string] ) [EOL] [comment] [EOL] time . sleep ( [number] ) [EOL] [EOL] [comment] [EOL] try : [EOL] [comment] [EOL] gc . collect ( ) [EOL] baseline = min ( memory_profiler . memory_usage ( proc = - [number] , interval = [number] , timeout = [number] ) ) [EOL] [EOL] run_wrapper ( ) [comment] [EOL] [EOL] gc . collect ( ) [EOL] used = max ( memory_profiler . memory_usage ( ( run_wrapper , ( ) , { } ) ) ) [EOL] amount = used - baseline [EOL] except Exception : [EOL] amount = np . inf [EOL] [EOL] [comment] [EOL] print ( amount ) [EOL] [EOL] [EOL] [comment] [EOL] library_module_name , size , depth , device = sys . argv [ [number] : ] [EOL] device = int ( device ) [EOL] if device == - [number] : [EOL] main ( ) [EOL] else : [EOL] with torch . cuda . device ( device ) : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0
	0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any , Tuple [EOL] import typing [EOL] import benchmark [EOL] [docstring] [EOL] [EOL] [EOL] import collections as co [EOL] import itertools as it [EOL] import numpy as np [EOL] [EOL] [EOL] class namedarray ( object ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , * size ) : [EOL] self . array = np . empty ( size , dtype = object ) [EOL] self . numdims = len ( size ) [EOL] self . dim_lookups = [ co . OrderedDict ( ) for _ in range ( self . numdims ) ] [EOL] [EOL] def __setitem__ ( self , key , value ) : [EOL] if not isinstance ( key , tuple ) : [EOL] raise ValueError [EOL] if len ( key ) != self . numdims : [EOL] raise ValueError [EOL] indices = [ ] [EOL] for elem , lookup in zip ( key , self . dim_lookups ) : [EOL] if isinstance ( elem , slice ) : [EOL] raise ValueError [EOL] try : [EOL] index = lookup [ elem ] [EOL] except KeyError : [EOL] index = lookup [ elem ] = len ( lookup ) [EOL] indices . append ( index ) [EOL] indices = tuple ( indices ) [EOL] self . array [ indices ] = value [EOL] [EOL] def __getitem__ ( self , key ) : [EOL] if not isinstance ( key , tuple ) : [EOL] raise ValueError [EOL] if len ( key ) != self . numdims : [EOL] raise ValueError [EOL] indices = [ ] [EOL] for elem , lookup in zip ( key , self . dim_lookups ) : [EOL] try : [EOL] index = lookup [ elem ] [EOL] except KeyError : [EOL] index = elem [EOL] indices . append ( index ) [EOL] indices = tuple ( indices ) [EOL] return self . array [ indices ] [EOL] [EOL] def __iter__ ( self ) : [EOL] lookups = tuple ( lookup . keys ( ) for lookup in self . dim_lookups ) [EOL] for index in it . product ( * lookups ) : [EOL] yield index , self [ index ] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] class MetaContainer ( type ) : [EOL] def __contains__ ( self , item ) : [EOL] return item in self . __dict__ . values ( ) or any ( item in base for base in self . __bases__ if isinstance ( base , MetaContainer ) ) [EOL] [EOL] [EOL] Container = MetaContainer ( [string] , ( object , ) , { } ) [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $benchmark.helpers.MetaContainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] import _importlib_modulespec [EOL] import argparse [EOL] [docstring] [EOL] [EOL] [EOL] import argparse [EOL] import importlib [EOL] import math [EOL] import sys [EOL] import timeit [EOL] import torch [EOL] [EOL] [EOL] def main ( ) : [EOL] obj = argparse . Namespace ( size = tuple ( int ( i ) for i in size . split ( [string] ) ) , depth = int ( depth ) ) [EOL] library_module = importlib . import_module ( [string] + library_module_name , __package__ ) [EOL] library_module . setup ( obj ) [EOL] [EOL] try : [EOL] result = min ( timeit . Timer ( setup = lambda : library_module . run ( obj ) , stmt = lambda : library_module . run ( obj ) ) . repeat ( repeat = [number] , number = [number] ) ) [EOL] except Exception : [EOL] result = math . inf [EOL] [EOL] [comment] [EOL] print ( result ) [EOL] [EOL] [EOL] [comment] [EOL] library_module_name , size , depth , device = sys . argv [ [number] : ] [EOL] device = int ( device ) [EOL] if device == - [number] : [EOL] main ( ) [EOL] else : [EOL] with torch . cuda . device ( device ) : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def setup ( obj ) : [EOL] pass [EOL] [EOL] [EOL] def run ( obj ) : [EOL] [comment] [EOL] raise Exception [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL]	0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import esig . tosig [EOL] import torch [EOL] [EOL] [EOL] def setup ( obj ) : [EOL] obj . path = torch . rand ( obj . size , dtype = torch . float ) . numpy ( ) [EOL] [EOL] [EOL] def run ( obj ) : [EOL] first_result = esig . tosig . stream2sig ( obj . path [ [number] ] , obj . depth ) [EOL] if not len ( first_result ) : [EOL] raise Exception [EOL] [EOL] result = [ first_result ] [EOL] for batch_elem in obj . path [ [number] : ] : [EOL] result . append ( esig . tosig . stream2sig ( batch_elem , obj . depth ) ) [EOL] return result [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any , Tuple [EOL] import typing [EOL] import signatory [EOL] import torch [EOL] [EOL] [EOL] def setup ( obj ) : [EOL] obj . path = torch . rand ( obj . size , dtype = torch . float , requires_grad = True ) [EOL] shape = obj . size [ - [number] ] , signatory . signature_channels ( obj . size [ - [number] ] , obj . depth ) [EOL] obj . grad = torch . rand ( shape ) [EOL] obj . signature = signatory . signature ( obj . path , obj . depth ) [EOL] [EOL] [EOL] def run ( obj ) : [EOL] obj . signature . backward ( obj . grad , retain_graph = True ) [EOL] return obj . path . grad [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def setup ( obj ) : [EOL] pass [EOL] [EOL] [EOL] def run ( obj ) : [EOL] [comment] [EOL] raise Exception [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Literal , List , Any , Tuple [EOL] import typing [EOL] import typing_extensions [EOL] [docstring] [EOL] [EOL] [EOL] import iisignature [EOL] import gc [EOL] import pytest [EOL] import torch [EOL] from torch import autograd [EOL] import warnings [EOL] import weakref [EOL] [EOL] from helpers import helpers as h [EOL] from helpers import reimplementation as r [EOL] from helpers import validation as v [EOL] [EOL] tests = [ [string] , [string] ] [EOL] depends = [ [string] , [string] ] [EOL] signatory = v . validate_tests ( tests , depends ) [EOL] [EOL] [EOL] class _IisignatureSignatureFunction ( autograd . Function ) : [EOL] @ staticmethod def forward ( ctx , path , depth ) : [EOL] ctx . path = path . detach ( ) . cpu ( ) [EOL] ctx . depth = depth [EOL] ctx . device = path . device [EOL] ctx . dtype = path . dtype [EOL] return torch . tensor ( iisignature . sig ( ctx . path , ctx . depth ) , device = ctx . device , dtype = ctx . dtype ) [EOL] [EOL] @ staticmethod def backward ( ctx , grad ) : [EOL] return torch . tensor ( iisignature . sigbackprop ( grad . cpu ( ) , ctx . path , ctx . depth ) , device = ctx . device , dtype = ctx . dtype ) , None [EOL] [EOL] [EOL] def iisignature_signature ( path , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] [docstring] [EOL] [EOL] def fn ( path , depth ) : [EOL] signature = _IisignatureSignatureFunction . apply ( path , depth ) [EOL] if scalar_term : [EOL] out = torch . ones ( signature . size ( [number] ) , [number] + signature . size ( [number] ) , dtype = signature . dtype , device = signature . device ) [EOL] out [ : , [number] : ] = signature [EOL] signature = out [EOL] if isinstance ( initial , torch . Tensor ) : [EOL] signature = signatory . signature_combine ( initial , signature , path . size ( - [number] ) , depth , inverse , scalar_term ) [EOL] return signature [EOL] [EOL] return r . iisignature_signature_or_logsignature ( fn , path , depth , stream , basepoint , inverse ) [EOL] [EOL] [EOL] def signatory_signature ( class_ , path , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] [docstring] [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] if class_ : [EOL] return signatory . Signature ( depth , stream = stream , inverse = inverse , scalar_term = scalar_term ) ( path , basepoint = basepoint , initial = initial ) [EOL] else : [EOL] return signatory . signature ( path , depth , stream = stream , basepoint = basepoint , inverse = inverse , initial = initial , scalar_term = scalar_term ) [EOL] [EOL] [EOL] def test_forward ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] [comment] [EOL] [comment] [EOL] for path_grad in ( False , True ) : [EOL] for batch_size in ( [number] , [number] , [number] , [number] ) : [EOL] for input_stream in ( [number] , [number] , [number] , [number] , [number] ) : [EOL] for input_channels in ( [number] , [number] , [number] , [number] ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for basepoint in ( False , True , h . without_grad , h . with_grad ) : [EOL] for inverse in ( False , True ) : [EOL] for initial in ( None , h . without_grad , h . with_grad ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_forward ( class_ , device , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] [EOL] def _test_forward ( class_ , device , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad ) [EOL] basepoint = h . get_basepoint ( batch_size , input_channels , device , basepoint ) [EOL] [EOL] expected_exception = ( batch_size < [number] ) or ( input_channels < [number] ) or ( basepoint is False and input_stream < [number] ) or ( input_stream < [number] ) [EOL] try : [EOL] initial = h . get_initial ( batch_size , input_channels , device , depth , initial , scalar_term ) [EOL] signature = signatory_signature ( class_ , path , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] except ValueError : [EOL] if expected_exception : [EOL] return [EOL] else : [EOL] raise [EOL] else : [EOL] assert not expected_exception [EOL] [EOL] _test_shape ( signature , stream , basepoint , batch_size , input_stream , input_channels , depth , scalar_term ) [EOL] h . diff ( signature , iisignature_signature ( path , depth , stream , basepoint , inverse , initial , scalar_term ) ) [EOL] [EOL] if path_grad or ( isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad ) or ( isinstance ( initial , torch . Tensor ) and initial . requires_grad ) : [EOL] ctx = signature . grad_fn [EOL] if stream : [EOL] ctx = ctx . next_functions [ [number] ] [ [number] ] [EOL] assert type ( ctx ) . __name__ in ( [string] , [string] ) [EOL] ref = weakref . ref ( ctx ) [EOL] del ctx [EOL] del signature [EOL] gc . collect ( ) [EOL] assert ref ( ) is None [EOL] else : [EOL] assert signature . grad_fn is None [EOL] [EOL] [EOL] def _test_shape ( signature , stream , basepoint , batch_size , input_stream , input_channels , depth , scalar_term ) : [EOL] [docstring] [EOL] correct_channels = signatory . signature_channels ( input_channels , depth , scalar_term ) [EOL] if stream : [EOL] if isinstance ( basepoint , torch . Tensor ) or basepoint is True : [EOL] correct_shape = ( batch_size , input_stream , correct_channels ) [EOL] else : [EOL] correct_shape = ( batch_size , input_stream - [number] , correct_channels ) [EOL] else : [EOL] correct_shape = ( batch_size , correct_channels ) [EOL] assert signature . shape == correct_shape [EOL] [EOL] [EOL] def test_batch_trick ( ) : [EOL] [docstring] [EOL] if torch . cuda . is_available ( ) : [EOL] device_path_grad = ( [string] , False ) , ( [string] , True ) , ( [string] , True ) [EOL] else : [EOL] device_path_grad = ( ( [string] , True ) , ) [EOL] [EOL] for class_ in ( False , True ) : [EOL] for device , path_grad in device_path_grad : [EOL] for batch_size in ( [number] , [number] , [number] ) : [EOL] for input_stream in ( [number] , [number] ) : [EOL] for input_channels in ( [number] , [number] , [number] ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] stream = False [EOL] for basepoint in ( False , True , h . without_grad , h . with_grad ) : [EOL] for inverse in ( False , True ) : [EOL] for initial in ( None , h . without_grad , h . with_grad ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_batch_trick ( class_ , device , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] [EOL] def _no_batch_trick ( path , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] return [EOL] [EOL] [EOL] def _test_batch_trick ( class_ , device , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] if device == [string] : [EOL] threshold = [number] [EOL] else : [EOL] threshold = torch . get_num_threads ( ) [EOL] if threshold < [number] : [EOL] return [comment] [EOL] if round ( float ( threshold ) / batch_size ) < [number] : [EOL] batch_size = int ( threshold / [number] ) [EOL] [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad ) [EOL] basepoint = h . get_basepoint ( batch_size , input_channels , device , basepoint ) [EOL] initial = h . get_initial ( batch_size , input_channels , device , depth , initial , scalar_term ) [EOL] [EOL] _signature_batch_trick = signatory . signature . __globals__ [ [string] ] [EOL] try : [EOL] [comment] [EOL] signatory . signature . __globals__ [ [string] ] = _no_batch_trick [EOL] signature = signatory_signature ( class_ , path , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] finally : [EOL] signatory . signature . __globals__ [ [string] ] = _signature_batch_trick [EOL] [EOL] batch_trick_signature = signatory . signature . __globals__ [ [string] ] ( path , depth , stream = stream , basepoint = basepoint , inverse = inverse , initial = initial , scalar_term = scalar_term ) [EOL] [EOL] assert batch_trick_signature is not None [comment] [EOL] [EOL] h . diff ( signature , batch_trick_signature ) [EOL] [EOL] can_backward = path_grad or ( isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad ) or ( isinstance ( initial , torch . Tensor ) and initial . requires_grad ) [EOL] try : [EOL] grad = torch . rand_like ( signature ) [EOL] signature . backward ( grad ) [EOL] except RuntimeError : [EOL] assert not can_backward [EOL] return [EOL] else : [EOL] assert can_backward [EOL] [EOL] if path_grad : [EOL] path_grad_ = path . grad . clone ( ) [EOL] path . grad . zero_ ( ) [EOL] if isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad : [EOL] basepoint_grad = basepoint . grad . clone ( ) [EOL] basepoint . grad . zero_ ( ) [EOL] if isinstance ( initial , torch . Tensor ) and initial . requires_grad : [EOL] initial_grad = initial . grad . clone ( ) [EOL] initial . grad . zero_ ( ) [EOL] batch_trick_signature . backward ( grad ) [EOL] if path_grad : [EOL] h . diff ( path_grad_ , path . grad ) [EOL] path . grad . zero_ ( ) [EOL] if isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad : [EOL] h . diff ( basepoint_grad , basepoint . grad ) [EOL] basepoint . grad . zero_ ( ) [EOL] if isinstance ( initial , torch . Tensor ) and initial . requires_grad : [EOL] h . diff ( initial_grad , initial . grad ) [EOL] initial . grad . zero_ ( ) [EOL] [EOL] [EOL] def test_backward ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels , basepoint in h . random_sizes_and_basepoint ( ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for inverse in ( False , True ) : [EOL] for initial in ( None , h . without_grad , h . with_grad ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_backward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] [EOL] def _test_backward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad = True ) [EOL] basepoint = h . get_basepoint ( batch_size , input_channels , device , basepoint ) [EOL] initial = h . get_initial ( batch_size , input_channels , device , depth , initial , scalar_term ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] signature = signatory_signature ( class_ , path , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] grad = torch . rand_like ( signature ) [EOL] signature . backward ( grad ) [EOL] [EOL] path_grad = path . grad . clone ( ) [EOL] path . grad . zero_ ( ) [EOL] if isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad : [EOL] basepoint_grad = basepoint . grad . clone ( ) [EOL] basepoint . grad . zero_ ( ) [EOL] if isinstance ( initial , torch . Tensor ) and initial . requires_grad : [EOL] initial_grad = initial . grad . clone ( ) [EOL] initial . grad . zero_ ( ) [EOL] [EOL] iisignature_signature_result = iisignature_signature ( path , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] iisignature_signature_result . backward ( grad ) [EOL] [EOL] [comment] [EOL] h . diff ( path . grad , path_grad , atol = [number] ) [EOL] if isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad : [EOL] h . diff ( basepoint . grad , basepoint_grad , atol = [number] ) [EOL] if isinstance ( initial , torch . Tensor ) and initial . requires_grad : [EOL] h . diff ( initial . grad , initial_grad , atol = [number] ) [EOL] [EOL] [EOL] def test_no_adjustments ( ) : [EOL] [docstring] [EOL] [EOL] for class_ in ( False , True ) : [EOL] for path_grad in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels , basepoint in h . random_sizes_and_basepoint ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for inverse in ( False , True ) : [EOL] for initial in ( None , h . without_grad , h . with_grad ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_no_adjustments ( class_ , device , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , path_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_no_adjustments ( class_ , device , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , path_grad , scalar_term ) : [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad ) [EOL] basepoint = h . get_basepoint ( batch_size , input_channels , device , basepoint ) [EOL] initial = h . get_initial ( batch_size , input_channels , device , depth , initial , scalar_term ) [EOL] [EOL] path_clone = path . clone ( ) [EOL] if isinstance ( basepoint , torch . Tensor ) : [EOL] basepoint_clone = basepoint . clone ( ) [EOL] if isinstance ( initial , torch . Tensor ) : [EOL] initial_clone = initial . clone ( ) [EOL] [EOL] signature = signatory_signature ( class_ , path , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] can_backward = path_grad or ( isinstance ( basepoint , torch . Tensor ) and basepoint . requires_grad ) or ( isinstance ( initial , torch . Tensor ) and initial . requires_grad ) [EOL] if can_backward : [EOL] grad = torch . rand_like ( signature ) [EOL] [EOL] signature_clone = signature . clone ( ) [EOL] grad_clone = grad . clone ( ) [EOL] [EOL] signature . backward ( grad ) [EOL] else : [EOL] assert signature . grad_fn is None [EOL] [EOL] h . diff ( path , path_clone ) [EOL] if isinstance ( basepoint , torch . Tensor ) : [EOL] h . diff ( basepoint , basepoint_clone ) [EOL] if isinstance ( initial , torch . Tensor ) : [EOL] h . diff ( initial , initial_clone ) [EOL] if can_backward : [EOL] h . diff ( signature , signature_clone ) [EOL] h . diff ( grad , grad_clone ) [EOL] [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_repeat_and_memory_leaks ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for path_grad in ( False , True ) : [EOL] for batch_size , input_stream , input_channels , basepoint in h . random_sizes_and_basepoint ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for inverse in ( False , True ) : [EOL] for initial in ( None , h . without_grad , h . with_grad ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_repeat_and_memory_leaks ( class_ , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) [EOL] [EOL] [EOL] def _test_repeat_and_memory_leaks ( class_ , path_grad , batch_size , input_stream , input_channels , depth , stream , basepoint , inverse , initial , scalar_term ) : [EOL] cpu_path = h . get_path ( batch_size , input_stream , input_channels , device = [string] , path_grad = False ) [EOL] cpu_basepoint = h . get_basepoint ( batch_size , input_channels , device = [string] , basepoint = basepoint ) [EOL] cpu_initial = h . get_initial ( batch_size , input_channels , device = [string] , depth = depth , initial = initial , scalar_term = scalar_term ) [EOL] [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] if class_ : [EOL] signature_instance = signatory . Signature ( depth , stream = stream , inverse = inverse , scalar_term = scalar_term ) [EOL] cpu_signature = signature_instance ( cpu_path , basepoint = cpu_basepoint , initial = cpu_initial ) [EOL] else : [EOL] cpu_signature = signatory . signature ( cpu_path , depth , stream = stream , basepoint = cpu_basepoint , inverse = inverse , initial = cpu_initial , scalar_term = scalar_term ) [EOL] cpu_grad = torch . rand_like ( cpu_signature ) [EOL] [EOL] def one_iteration ( ) : [EOL] gc . collect ( ) [EOL] torch . cuda . synchronize ( ) [EOL] torch . cuda . reset_max_memory_allocated ( ) [EOL] cuda_path = cpu_path . to ( [string] ) [EOL] if path_grad : [EOL] cuda_path . requires_grad_ ( ) [EOL] if isinstance ( cpu_basepoint , torch . Tensor ) : [EOL] cuda_basepoint = cpu_basepoint . cuda ( ) [EOL] if basepoint is h . with_grad : [EOL] cuda_basepoint . requires_grad_ ( ) [EOL] else : [EOL] cuda_basepoint = basepoint [EOL] if isinstance ( cpu_initial , torch . Tensor ) : [EOL] cuda_initial = cpu_initial . cuda ( ) [EOL] if initial is h . with_grad : [EOL] cuda_initial . requires_grad_ ( ) [EOL] else : [EOL] cuda_initial = initial [EOL] [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] if class_ : [EOL] cuda_signature = signature_instance ( cuda_path , basepoint = cuda_basepoint , initial = cuda_initial ) [EOL] else : [EOL] cuda_signature = signatory . signature ( cuda_path , depth , stream = stream , basepoint = cuda_basepoint , inverse = inverse , initial = cuda_initial , scalar_term = scalar_term ) [EOL] [EOL] h . diff ( cuda_signature . cpu ( ) , cpu_signature ) [EOL] [EOL] if path_grad : [EOL] cuda_grad = cpu_grad . cuda ( ) [EOL] cuda_signature . backward ( cuda_grad ) [EOL] torch . cuda . synchronize ( ) [EOL] return torch . cuda . max_memory_allocated ( ) [EOL] [EOL] memory_used = one_iteration ( ) [EOL] [EOL] for repeat in range ( [number] ) : [EOL] assert one_iteration ( ) <= memory_used [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import iisignature [EOL] [EOL] from helpers import helpers as h [EOL] from helpers import validation as v [EOL] [EOL] [EOL] tests = [ [string] , [string] , [string] , [string] ] [EOL] depends = [ ] [EOL] signatory = v . validate_tests ( tests , depends ) [EOL] [EOL] [EOL] def test_lyndon_amount ( ) : [EOL] [docstring] [EOL] for channels in range ( [number] , [number] ) : [EOL] for depth in range ( [number] , [number] ) : [EOL] len_words = len ( signatory . lyndon_words ( channels , depth ) ) [EOL] len_brackets = len ( signatory . lyndon_brackets ( channels , depth ) ) [EOL] print ( [string] + str ( channels ) ) [EOL] print ( [string] + str ( depth ) ) [EOL] assert len_words == len_brackets [EOL] [EOL] [EOL] def _iisignature_convert ( ii_elem ) : [EOL] outstr = [string] [EOL] for character in ii_elem : [EOL] if character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] elif character == [string] : [EOL] outstr += [string] [EOL] else : [EOL] outstr += character [EOL] return outstr [EOL] [EOL] [EOL] def test_lyndon_brackets ( ) : [EOL] [docstring] [EOL] for channels in range ( [number] , [number] ) : [comment] [EOL] for depth in range ( [number] , [number] ) : [EOL] iisignature_brackets = iisignature . basis ( h . iisignature_prepare ( channels , depth ) ) [EOL] signatory_brackets = signatory . lyndon_brackets ( channels , depth ) [EOL] for ii_elem , sig_elem in zip ( iisignature_brackets , signatory_brackets ) : [EOL] print ( [string] + str ( channels ) ) [EOL] print ( [string] + str ( depth ) ) [EOL] assert sig_elem == eval ( _iisignature_convert ( ii_elem ) ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] def test_lyndon_words ( ) : [EOL] [docstring] [EOL] for channels in range ( [number] , [number] ) : [comment] [EOL] for depth in range ( [number] , [number] ) : [EOL] iisignature_brackets = iisignature . basis ( h . iisignature_prepare ( channels , depth ) ) [EOL] signatory_words = signatory . lyndon_words ( channels , depth ) [EOL] for ii_elem , sig_elem in zip ( iisignature_brackets , signatory_words ) : [EOL] ii_elem_new = ii_elem . replace ( [string] , [string] ) . replace ( [string] , [string] ) . replace ( [string] , [string] ) [EOL] ii_elem_new = _iisignature_convert ( ii_elem_new ) [EOL] sig_elem_new = [string] . join ( str ( i ) for i in sig_elem ) [EOL] print ( [string] + str ( channels ) ) [EOL] print ( [string] + str ( depth ) ) [EOL] print ( [string] + str ( ii_elem ) ) [EOL] print ( [string] + str ( sig_elem ) ) [EOL] assert sig_elem_new == ii_elem_new [EOL] [EOL] [EOL] def test_signature_channels ( ) : [EOL] [docstring] [EOL] for channels in range ( [number] , [number] ) : [EOL] for depth in range ( [number] , [number] ) : [EOL] for scalar_term in ( True , False ) : [EOL] result = signatory . signature_channels ( channels , depth , scalar_term ) [EOL] sum_over = sum ( channels ** i for i in range ( [number] , depth + [number] ) ) [EOL] if scalar_term : [EOL] sum_over += [number] [EOL] print ( [string] + str ( channels ) ) [EOL] print ( [string] + str ( depth ) ) [EOL] assert result == sum_over [EOL] [EOL] [EOL] def test_logsignature_channels ( ) : [EOL] [docstring] [EOL] for channels in range ( [number] , [number] ) : [EOL] for depth in range ( [number] , [number] ) : [EOL] result = signatory . logsignature_channels ( channels , depth ) [EOL] from_words = len ( signatory . lyndon_words ( channels , depth ) ) [EOL] print ( [string] + str ( channels ) ) [EOL] print ( [string] + str ( depth ) ) [EOL] assert result == from_words [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import gc [EOL] import iisignature [EOL] import pytest [EOL] import random [EOL] import torch [EOL] from torch import autograd [EOL] import weakref [EOL] [EOL] from helpers import helpers as h [EOL] from helpers import validation as v [EOL] from helpers import reimplementation as r [EOL] [EOL] [EOL] tests = [ [string] , [string] ] [EOL] depends = [ ] [EOL] signatory = v . validate_tests ( tests , depends ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] class _IisignatureSignatureFunction ( autograd . Function ) : [EOL] @ staticmethod def forward ( ctx , path , depth ) : [EOL] ctx . path = path . detach ( ) . cpu ( ) [EOL] ctx . depth = depth [EOL] ctx . device = path . device [EOL] ctx . dtype = path . dtype [EOL] return torch . tensor ( iisignature . sig ( ctx . path , ctx . depth ) , device = ctx . device , dtype = ctx . dtype ) [EOL] [EOL] @ staticmethod def backward ( ctx , grad ) : [EOL] return torch . tensor ( iisignature . sigbackprop ( grad . cpu ( ) , ctx . path , ctx . depth ) , device = ctx . device , dtype = ctx . dtype ) , None [EOL] [EOL] [EOL] def iisignature_signature ( path , depth , stream = False , basepoint = False , inverse = False , scalar_term = False ) : [EOL] [docstring] [EOL] [EOL] def fn ( path , depth ) : [EOL] signature = _IisignatureSignatureFunction . apply ( path , depth ) [EOL] if scalar_term : [EOL] out = torch . ones ( signature . size ( [number] ) , [number] + signature . size ( [number] ) , dtype = signature . dtype , device = signature . device ) [EOL] out [ : , [number] : ] = signature [EOL] signature = out [EOL] return signature [EOL] [EOL] return r . iisignature_signature_or_logsignature ( fn , path , depth , stream , basepoint , inverse ) [EOL] [EOL] [EOL] def test_forward ( ) : [EOL] [docstring] [EOL] for signature_combine , amount in ( ( True , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) ) : [EOL] for signature_grad in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size in ( [number] , [number] , [number] ) : [EOL] input_stream = [number] [EOL] for input_channels in ( [number] , [number] , [number] ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for inverse in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_forward ( signature_combine , signature_grad , amount , device , batch_size , input_stream , input_channels , depth , inverse , scalar_term ) [EOL] [EOL] [EOL] def _test_forward ( signature_combine , signature_grad , amount , device , batch_size , input_stream , input_channels , depth , inverse , scalar_term ) : [EOL] paths = [ ] [EOL] for _ in range ( amount ) : [EOL] paths . append ( torch . rand ( batch_size , input_stream , input_channels , device = device , dtype = torch . double ) ) [EOL] signatures = [ ] [EOL] basepoint = False [EOL] for path in paths : [EOL] signature = iisignature_signature ( path , depth , basepoint = basepoint , inverse = inverse , scalar_term = scalar_term ) [EOL] if signature_grad : [EOL] signature . requires_grad_ ( ) [EOL] signatures . append ( signature ) [EOL] basepoint = path [ : , - [number] ] [EOL] if signature_combine : [EOL] combined_signatures = signatory . signature_combine ( signatures [ [number] ] , signatures [ [number] ] , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] else : [EOL] combined_signatures = signatory . multi_signature_combine ( signatures , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] combined_paths = torch . cat ( paths , dim = [number] ) [EOL] true_combined_signatures = iisignature_signature ( combined_paths , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] h . diff ( combined_signatures , true_combined_signatures ) [EOL] [EOL] if signature_grad : [EOL] ctx = combined_signatures . grad_fn [EOL] assert type ( ctx ) . __name__ == [string] [EOL] ref = weakref . ref ( ctx ) [EOL] del ctx [EOL] del combined_signatures [EOL] gc . collect ( ) [EOL] assert ref ( ) is None [EOL] else : [EOL] assert combined_signatures . grad_fn is None [EOL] [EOL] [EOL] def test_backward ( ) : [EOL] [docstring] [EOL] for signature_combine , amount in ( ( True , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for scalar_term in ( False , True ) : [EOL] inverse = random . choice ( [ False , True ] ) [EOL] _test_backward ( signature_combine , amount , device , batch_size , input_stream , input_channels , depth , inverse , scalar_term ) [EOL] [EOL] [EOL] def _test_backward ( signature_combine , amount , device , batch_size , input_stream , input_channels , depth , inverse , scalar_term ) : [EOL] paths = [ ] [EOL] for _ in range ( amount ) : [EOL] paths . append ( torch . rand ( batch_size , input_stream , input_channels , device = device , dtype = torch . double , requires_grad = True ) ) [EOL] signatures = [ ] [EOL] basepoint = False [EOL] for path in paths : [EOL] signature = iisignature_signature ( path , depth , basepoint = basepoint , inverse = inverse , scalar_term = scalar_term ) [EOL] signatures . append ( signature ) [EOL] basepoint = path [ : , - [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if signature_combine : [EOL] combined_signatures = signatory . signature_combine ( signatures [ [number] ] , signatures [ [number] ] , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] else : [EOL] combined_signatures = signatory . multi_signature_combine ( signatures , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] grad = torch . rand_like ( combined_signatures ) [EOL] combined_signatures . backward ( grad ) [EOL] path_grads = [ path . grad . clone ( ) for path in paths ] [EOL] for path in paths : [EOL] path . grad . zero_ ( ) [EOL] [EOL] true_signature = iisignature_signature ( torch . cat ( paths , dim = [number] ) , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] true_signature . backward ( grad ) [EOL] for path_grad , path in zip ( path_grads , paths ) : [EOL] h . diff ( path_grad , path . grad ) [EOL] [EOL] [EOL] def test_no_adjustments ( ) : [EOL] [docstring] [EOL] for signature_combine , amount in ( ( True , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) ) : [EOL] for signature_grad in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for inverse in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_no_adjustments ( signature_combine , amount , device , batch_size , input_stream , input_channels , depth , inverse , signature_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_no_adjustments ( signature_combine , amount , device , batch_size , input_stream , input_channels , depth , inverse , signature_grad , scalar_term ) : [EOL] paths = [ ] [EOL] for _ in range ( amount ) : [EOL] paths . append ( torch . rand ( batch_size , input_stream , input_channels , device = device , dtype = torch . double ) ) [EOL] [EOL] signatures = [ ] [EOL] signatures_clone = [ ] [EOL] basepoint = False [EOL] for path in paths : [EOL] signature = iisignature_signature ( path , depth , basepoint = basepoint , inverse = inverse , scalar_term = scalar_term ) [EOL] signatures_clone . append ( signature . clone ( ) ) [EOL] if signature_grad : [EOL] signature . requires_grad_ ( ) [EOL] signatures . append ( signature ) [EOL] basepoint = path [ : , - [number] ] [EOL] if signature_combine : [EOL] combined_signatures = signatory . signature_combine ( signatures [ [number] ] , signatures [ [number] ] , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] else : [EOL] combined_signatures = signatory . multi_signature_combine ( signatures , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] [EOL] if signature_grad : [EOL] grad = torch . rand_like ( combined_signatures ) [EOL] grad_clone = grad . clone ( ) [EOL] combined_signatures_clone = combined_signatures . clone ( ) [EOL] combined_signatures . backward ( grad ) [EOL] [EOL] for signature , signature_clone in zip ( signatures , signatures_clone ) : [EOL] h . diff ( signature , signature_clone ) [EOL] if signature_grad : [EOL] h . diff ( grad , grad_clone ) [EOL] h . diff ( combined_signatures , combined_signatures_clone ) [EOL] [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_memory_leaks ( ) : [EOL] [docstring] [EOL] for signature_combine , amount in ( ( True , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) , ( False , [number] ) ) : [EOL] for signature_grad in ( False , True ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for inverse in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_memory_leaks ( signature_combine , amount , batch_size , input_stream , input_channels , depth , inverse , signature_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_memory_leaks ( signature_combine , amount , batch_size , input_stream , input_channels , depth , inverse , signature_grad , scalar_term ) : [EOL] [EOL] def one_iteration ( ) : [EOL] gc . collect ( ) [EOL] torch . cuda . synchronize ( ) [EOL] torch . cuda . reset_max_memory_allocated ( ) [EOL] paths = [ ] [EOL] for _ in range ( amount ) : [EOL] paths . append ( torch . rand ( batch_size , input_stream , input_channels , device = [string] , dtype = torch . double ) ) [EOL] signatures = [ ] [EOL] basepoint = False [EOL] for path in paths : [EOL] signature = iisignature_signature ( path , depth , basepoint = basepoint , inverse = inverse , scalar_term = scalar_term ) [EOL] if signature_grad : [EOL] signature . requires_grad_ ( ) [EOL] signatures . append ( signature ) [EOL] if signature_combine : [EOL] combined_signatures = signatory . signature_combine ( signatures [ [number] ] , signatures [ [number] ] , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] else : [EOL] combined_signatures = signatory . multi_signature_combine ( signatures , input_channels , depth , inverse = inverse , scalar_term = scalar_term ) [EOL] if signature_grad : [EOL] grad = torch . rand_like ( combined_signatures ) [EOL] combined_signatures . backward ( grad ) [EOL] torch . cuda . synchronize ( ) [EOL] return torch . cuda . max_memory_allocated ( ) [EOL] [EOL] memory_used = one_iteration ( ) [EOL] [EOL] for repeat in range ( [number] ) : [EOL] assert one_iteration ( ) <= memory_used [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import gc [EOL] import pytest [EOL] import torch [EOL] import warnings [EOL] import weakref [EOL] [EOL] from helpers import helpers as h [EOL] from helpers import validation as v [EOL] [EOL] [EOL] tests = [ [string] , [string] ] [EOL] depends = [ [string] , [string] ] [EOL] signatory = v . validate_tests ( tests , depends ) [EOL] [EOL] [EOL] def signatory_signature_to_logsignature ( class_ , signature , input_channels , depth , stream , mode , scalar_term ) : [EOL] if class_ : [EOL] return signatory . SignatureToLogsignature ( input_channels , depth , stream = stream , mode = mode , scalar_term = scalar_term ) ( signature ) [EOL] else : [EOL] return signatory . signature_to_logsignature ( signature , input_channels , depth , stream = stream , mode = mode , scalar_term = scalar_term ) [EOL] [EOL] def test_forward ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for mode in h . all_modes : [EOL] for signature_grad in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_forward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_forward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) : [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad = False ) [EOL] signature = signatory . signature ( path , depth , stream = stream , scalar_term = scalar_term ) [EOL] if signature_grad : [EOL] signature . requires_grad_ ( ) [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] logsignature = signatory_signature_to_logsignature ( class_ , signature , input_channels , depth , stream , mode , scalar_term = scalar_term ) [EOL] true_logsignature = signatory . logsignature ( path , depth , stream = stream , mode = mode ) [EOL] h . diff ( logsignature , true_logsignature ) [EOL] [EOL] if signature_grad : [EOL] ctx = logsignature . grad_fn [EOL] if stream : [EOL] ctx = ctx . next_functions [ [number] ] [ [number] ] [EOL] assert type ( ctx ) . __name__ == [string] [EOL] ref = weakref . ref ( ctx ) [EOL] del ctx [EOL] del logsignature [EOL] gc . collect ( ) [EOL] assert ref ( ) is None [EOL] else : [EOL] assert logsignature . grad_fn is None [EOL] [EOL] [EOL] def test_backward_expand_words ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for mode in ( h . expand_mode , h . words_mode ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_backward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , scalar_term ) [EOL] [EOL] [EOL] @ pytest . mark . slow def test_backward_brackets ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for mode in ( h . brackets_mode , ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_backward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , scalar_term ) [EOL] [EOL] [EOL] def _test_backward ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , scalar_term ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad = True ) [EOL] signature = signatory . signature ( path , depth , stream = stream , scalar_term = scalar_term ) [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] logsignature = signatory_signature_to_logsignature ( class_ , signature , input_channels , depth , stream , mode , scalar_term ) [EOL] [EOL] grad = torch . rand_like ( logsignature ) [EOL] logsignature . backward ( grad ) [EOL] [EOL] path_grad = path . grad . clone ( ) [EOL] path . grad . zero_ ( ) [EOL] [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] true_logsignature = signatory . logsignature ( path , depth , stream = stream , mode = mode ) [EOL] true_logsignature . backward ( grad ) [EOL] h . diff ( path . grad , path_grad ) [EOL] [EOL] [EOL] def test_no_adjustments ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for device in h . get_devices ( ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for mode in h . all_modes : [EOL] for signature_grad in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_no_adjustments ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_no_adjustments ( class_ , device , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) : [EOL] path = h . get_path ( batch_size , input_stream , input_channels , device , path_grad = False ) [EOL] signature = signatory . signature ( path , depth , stream = stream , scalar_term = scalar_term ) [EOL] signature_clone = signature . clone ( ) [EOL] if signature_grad : [EOL] signature . requires_grad_ ( ) [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] logsignature = signatory_signature_to_logsignature ( class_ , signature , input_channels , depth , stream , mode , scalar_term ) [EOL] [EOL] if signature_grad : [EOL] grad = torch . rand_like ( logsignature ) [EOL] logsignature_clone = logsignature . clone ( ) [EOL] grad_clone = grad . clone ( ) [EOL] logsignature . backward ( grad ) [EOL] else : [EOL] assert logsignature . grad_fn is None [EOL] [EOL] h . diff ( signature , signature_clone ) [EOL] if signature_grad : [EOL] h . diff ( logsignature , logsignature_clone ) [EOL] h . diff ( grad , grad_clone ) [EOL] [EOL] [EOL] @ pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) def test_repeat_and_memory_leaks ( ) : [EOL] [docstring] [EOL] for class_ in ( False , True ) : [EOL] for batch_size , input_stream , input_channels in h . random_sizes ( ) : [EOL] for depth in ( [number] , [number] , [number] ) : [EOL] for stream in ( False , True ) : [EOL] for mode in h . all_modes : [EOL] for signature_grad in ( False , True ) : [EOL] for scalar_term in ( False , True ) : [EOL] _test_repeat_and_memory_leaks ( class_ , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) [EOL] [EOL] [EOL] def _test_repeat_and_memory_leaks ( class_ , batch_size , input_stream , input_channels , depth , stream , mode , signature_grad , scalar_term ) : [EOL] cpu_path = h . get_path ( batch_size , input_stream , input_channels , device = [string] , path_grad = False ) [EOL] cpu_signature = signatory . signature ( cpu_path , depth , stream = stream , scalar_term = scalar_term ) [EOL] if class_ : [EOL] signature_to_logsignature_instance = signatory . SignatureToLogsignature ( input_channels , depth , stream = stream , mode = mode , scalar_term = scalar_term ) [EOL] cpu_logsignature = signature_to_logsignature_instance ( cpu_signature ) [EOL] else : [EOL] cpu_logsignature = signatory . signature_to_logsignature ( cpu_signature , input_channels , depth , stream = stream , mode = mode , scalar_term = scalar_term ) [EOL] cpu_grad = torch . rand_like ( cpu_logsignature ) [EOL] [EOL] def one_iteration ( ) : [EOL] gc . collect ( ) [EOL] torch . cuda . synchronize ( ) [EOL] torch . cuda . reset_max_memory_allocated ( ) [EOL] cuda_signature = cpu_signature . to ( [string] ) [EOL] if signature_grad : [EOL] cuda_signature . requires_grad_ ( ) [EOL] with warnings . catch_warnings ( ) : [EOL] warnings . filterwarnings ( [string] , message = [string] [string] , category = UserWarning ) [EOL] if class_ : [EOL] cuda_logsignature = signature_to_logsignature_instance ( cuda_signature ) [EOL] else : [EOL] cuda_logsignature = signatory . signature_to_logsignature ( cuda_signature , input_channels , depth , stream = stream , mode = mode , scalar_term = scalar_term ) [EOL] [EOL] h . diff ( cuda_logsignature . cpu ( ) , cpu_logsignature ) [EOL] [EOL] if signature_grad : [EOL] cuda_grad = cpu_grad . cuda ( ) [EOL] cuda_logsignature . backward ( cuda_grad ) [EOL] torch . cuda . synchronize ( ) [EOL] return torch . cuda . max_memory_allocated ( ) [EOL] [EOL] memory_used = one_iteration ( ) [EOL] for repeat in range ( [number] ) : [EOL] [comment] [EOL] [comment] [EOL] assert one_iteration ( ) <= [number] * memory_used [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import os [EOL] import pytest [EOL] import torch [EOL] [EOL] [EOL] _here = os . path . realpath ( os . path . dirname ( __file__ ) ) [EOL] add_to_path = os . path . join ( _here , [string] , [string] ) [EOL] pytestmark = pytest . mark . usefixtures ( [string] ) [EOL] [EOL] [EOL] def test_example1 ( ) : [EOL] import example1 [EOL] _example_tester ( example1 . SigNet ) [EOL] [EOL] [EOL] def test_example2 ( ) : [EOL] import example2 [EOL] _example_tester ( example2 . SigNet2 ) [EOL] [EOL] [EOL] def test_example3 ( ) : [EOL] import example3 [EOL] _example_tester ( example3 . SigNet3 ) [EOL] [EOL] [EOL] def _example_tester ( example_fn ) : [EOL] batch_size = [number] [EOL] in_channels = [number] [EOL] out_dimension = [number] [EOL] sig_depth = [number] [EOL] [EOL] x = torch . rand ( batch_size , [number] , in_channels ) [EOL] signet = example_fn ( in_channels , out_dimension , sig_depth ) [EOL] y = signet ( x ) [EOL] [EOL] assert y . shape == ( batch_size , out_dimension ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any , Iterator [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import copy [EOL] import pytest [EOL] import signatory [EOL] import sys [EOL] [EOL] from helpers import validation as v [EOL] [EOL] [EOL] [comment] [EOL] if not hasattr ( signatory . SignatureToLogSignature , [string] ) : [EOL] raise RuntimeError ( [string] ) [EOL] signatory . SignatureToLogSignature . _lyndon_info_capsule_cache = { } [EOL] [EOL] [EOL] def pytest_addoption ( parser ) : [EOL] parser . addoption ( [string] , action = [string] , dest = [string] , default = False , help = [string] ) [EOL] [EOL] [EOL] def pytest_configure ( config ) : [EOL] config . addinivalue_line ( [string] , [string] ) [EOL] if not config . option . slow : [EOL] if hasattr ( config . option , [string] ) and len ( config . option . markexpr ) > [number] : [EOL] config . option . markexpr = [string] + config . option . markexpr + [string] [EOL] else : [EOL] config . option . markexpr = [string] [EOL] [EOL] [EOL] def pytest_collection_finish ( session ) : [EOL] cycle = v . signatory_functionality_graph . get_cycle ( ) [EOL] if cycle is not None : [EOL] error_pieces = [ [string] ] [EOL] [EOL] itercycle = iter ( reversed ( cycle ) ) [EOL] try : [EOL] while True : [EOL] item = next ( itercycle ) [EOL] error_pieces . append ( str ( item ) ) [EOL] item2 = next ( itercycle ) [EOL] error_pieces . append ( [string] ) [EOL] error_pieces . append ( str ( item2 ) ) [EOL] error_pieces . append ( [string] ) [EOL] except StopIteration : [EOL] pass [EOL] [EOL] raise RuntimeError ( [string] . join ( error_pieces ) ) [EOL] [EOL] unmarked = v . signatory_functionality_graph . get_unmarked ( ) [EOL] if len ( unmarked ) : [EOL] print ( [string] + [string] . join ( unmarked ) ) [EOL] [EOL] if [string] in v . signatory_functionality_graph . vertices ( ) : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def path_hack ( request ) : [EOL] [docstring] [EOL] original_path = copy . copy ( sys . path ) [EOL] add_to_path = getattr ( request . module , [string] ) [EOL] if isinstance ( add_to_path , ( tuple , list ) ) : [EOL] sys . path . extend ( add_to_path ) [EOL] else : [EOL] sys . path . append ( add_to_path ) [EOL] yield [EOL] sys . path = original_path [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] import builtins [EOL] from typing import Literal , Dict , Tuple , Any , List [EOL] import typing [EOL] import typing_extensions [EOL] [docstring] [EOL] [EOL] [EOL] import iisignature [EOL] import pytest [EOL] import signatory [EOL] import torch [EOL] [EOL] with_grad = object ( ) [EOL] without_grad = object ( ) [EOL] [EOL] [EOL] expand_mode = [string] [EOL] words_mode = [string] [EOL] brackets_mode = [string] [EOL] all_modes = ( expand_mode , words_mode , brackets_mode ) [EOL] [EOL] [EOL] def diff ( arg1 , arg2 , atol = [number] ) : [EOL] [docstring] [EOL] if not arg1 . allclose ( arg2 , atol = atol ) : [EOL] diff = arg1 - arg2 [EOL] max_diff = torch . max ( torch . abs ( diff ) ) [EOL] pytest . fail ( [string] . format ( diff = diff , max_diff = max_diff , arg1 = arg1 , arg2 = arg2 ) ) [EOL] [EOL] [EOL] def get_devices ( ) : [EOL] [docstring] [EOL] if torch . cuda . is_available ( ) : [EOL] return [string] , [string] [EOL] else : [EOL] return ( [string] , ) [EOL] [EOL] [EOL] def get_path ( batch_size , input_stream , input_channels , device , path_grad ) : [EOL] [docstring] [EOL] return torch . rand ( batch_size , input_stream , input_channels , device = device , requires_grad = path_grad , dtype = torch . double ) [EOL] [EOL] [EOL] def get_basepoint ( batch_size , input_channels , device , basepoint ) : [EOL] [docstring] [EOL] if basepoint == without_grad : [EOL] return torch . rand ( batch_size , input_channels , device = device , dtype = torch . double ) [EOL] elif basepoint == with_grad : [EOL] return torch . rand ( batch_size , input_channels , device = device , requires_grad = True , dtype = torch . double ) [EOL] else : [EOL] return basepoint [EOL] [EOL] [EOL] def get_initial ( batch_size , input_channels , device , depth , initial , scalar_term ) : [EOL] [docstring] [EOL] if initial in ( without_grad , with_grad ) : [EOL] initial_path = torch . rand ( batch_size , [number] , input_channels , device = device , dtype = torch . double ) [EOL] initial_signature = signatory . signature ( initial_path , depth , scalar_term = scalar_term ) [EOL] if initial == with_grad : [EOL] initial_signature . requires_grad_ ( ) [EOL] return initial_signature [EOL] else : [EOL] return initial [EOL] [EOL] [EOL] def random_sizes ( ) : [EOL] [docstring] [EOL] params = [ ] [EOL] for batch_size in ( [number] , [number] ) : [EOL] for input_stream in ( [number] , ) : [EOL] for input_channels in ( [number] , [number] ) : [EOL] params . append ( ( batch_size , input_stream , input_channels ) ) [EOL] for _ in range ( [number] ) : [EOL] batch_size = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] input_stream = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] input_channels = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] params . append ( ( batch_size , input_stream , input_channels ) ) [EOL] return params [EOL] [EOL] [EOL] def random_sizes_and_basepoint ( ) : [EOL] [docstring] [EOL] params = [ ] [EOL] for batch_size in ( [number] , [number] ) : [EOL] for input_stream in ( [number] , [number] ) : [EOL] for input_channels in ( [number] , [number] ) : [EOL] for basepoint in ( True , without_grad , with_grad ) : [EOL] params . append ( ( batch_size , input_stream , input_channels , basepoint ) ) [EOL] for batch_size in ( [number] , [number] ) : [EOL] for input_stream in ( [number] , ) : [EOL] for input_channels in ( [number] , [number] ) : [EOL] for basepoint in ( False , ) : [EOL] params . append ( ( batch_size , input_stream , input_channels , basepoint ) ) [EOL] for _ in range ( [number] ) : [EOL] for basepoint in ( True , without_grad , with_grad ) : [EOL] batch_size = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] input_stream = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] input_channels = int ( torch . randint ( low = [number] , high = [number] , size = ( [number] , ) ) ) [EOL] params . append ( ( batch_size , input_stream , input_channels , basepoint ) ) [EOL] return params [EOL] [EOL] [EOL] class NullContext ( object ) : [EOL] [docstring] [EOL] def __enter__ ( self ) : [EOL] pass [EOL] [EOL] def __exit__ ( self , exc_type , exc_val , exc_tb ) : [EOL] pass [EOL] [EOL] [EOL] _iisignature_prepare_cache = { } [EOL] [EOL] [EOL] def iisignature_prepare ( channels , depth , method = [string] ) : [EOL] [docstring] [EOL] try : [EOL] return _iisignature_prepare_cache [ ( channels , depth , method ) ] [EOL] except KeyError : [EOL] prepared = iisignature . prepare ( channels , depth , method ) [EOL] _iisignature_prepare_cache [ ( channels , depth , method ) ] = prepared [EOL] return prepared [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] import torch [EOL] [EOL] [EOL] def iisignature_signature_or_logsignature ( fn , path , depth , stream , basepoint , inverse ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] batch_size , input_stream , input_channels = path . shape [EOL] device = path . device [EOL] dtype = path . dtype [EOL] [EOL] [comment] [EOL] [comment] [EOL] iisignature_path_pieces = [ ] [EOL] [comment] [EOL] if isinstance ( basepoint , torch . Tensor ) or basepoint is True : [EOL] if basepoint is True : [EOL] iisignature_basepoint = torch . zeros ( batch_size , [number] , input_channels , device = device , dtype = dtype ) [EOL] else : [EOL] iisignature_basepoint = basepoint . unsqueeze ( [number] ) [EOL] iisignature_path_pieces . append ( iisignature_basepoint ) [EOL] [comment] [EOL] iisignature_path_pieces . append ( path ) [EOL] [EOL] [comment] [EOL] if inverse : [EOL] iisignature_path_pieces_reversed = [ ] [EOL] for tensor in reversed ( iisignature_path_pieces ) : [EOL] iisignature_path_pieces_reversed . append ( tensor . flip ( [number] ) ) [EOL] iisignature_path_pieces = iisignature_path_pieces_reversed [EOL] [EOL] iisignature_path = torch . cat ( iisignature_path_pieces , dim = [number] ) [EOL] [EOL] [comment] [EOL] if stream : [EOL] signature_length = input_stream - [number] [EOL] if isinstance ( basepoint , torch . Tensor ) or basepoint is True : [EOL] signature_length += [number] [EOL] results = [ ] [EOL] if inverse : [EOL] for i in range ( signature_length ) : [EOL] results . append ( fn ( iisignature_path [ : , i : ] , depth ) ) [EOL] result = torch . stack ( results , dim = [number] ) . flip ( [number] ) [EOL] else : [EOL] for i in range ( iisignature_path . size ( [number] ) - signature_length + [number] , iisignature_path . size ( [number] ) + [number] ) : [EOL] results . append ( fn ( iisignature_path [ : , : i ] , depth ) ) [EOL] result = torch . stack ( results , dim = [number] ) [EOL] else : [EOL] result = fn ( iisignature_path , depth ) [EOL] [EOL] return result [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Dict , Any [EOL] import argparse [EOL] import typing [EOL] import test [EOL] [docstring] [EOL] [EOL] [EOL] import argparse [EOL] import collections as co [EOL] import inspect [EOL] import itertools as it [EOL] import os [EOL] import signatory [EOL] [EOL] [EOL] [comment] [EOL] class _Graph ( object ) : [EOL] def __init__ ( self ) : [EOL] super ( _Graph , self ) . __init__ ( ) [EOL] [comment] [EOL] self . _graph = co . defaultdict ( list ) [EOL] self . _marked = set ( ) [EOL] [EOL] def add_directed_edge ( self , start , end , info ) : [EOL] self . _graph [ start ] . append ( ( end , info ) ) [EOL] self . _graph [ end ] [comment] [EOL] [EOL] def vertices ( self ) : [EOL] return set ( self . _graph . keys ( ) ) [EOL] [EOL] def _is_cyclic ( self , vertex , visited , current ) : [EOL] visited [ vertex ] = True [EOL] current [ vertex ] = True [EOL] [EOL] for neighbour , info in self . _graph [ vertex ] : [EOL] if current [ neighbour ] : [EOL] return [ neighbour , info ] , False [EOL] if not visited [ neighbour ] : [EOL] cycle_info = self . _is_cyclic ( neighbour , visited , current ) [EOL] if cycle_info is not None : [EOL] cycle , cycle_is_complete = cycle_info [EOL] if not cycle_is_complete : [EOL] cycle . append ( neighbour ) [EOL] if neighbour == cycle [ [number] ] : [EOL] cycle_is_complete = True [EOL] else : [EOL] cycle . append ( info ) [EOL] return cycle , cycle_is_complete [EOL] [EOL] current [ vertex ] = False [EOL] return [EOL] [EOL] def get_cycle ( self ) : [EOL] master_vertex = object ( ) [EOL] self . _graph [ master_vertex ] = [ ( key , None ) for key in self . _graph . keys ( ) ] [EOL] visited = { key : False for key in self . _graph . keys ( ) } [EOL] current = { key : False for key in self . _graph . keys ( ) } [EOL] cycle_info = self . _is_cyclic ( master_vertex , visited , current ) [EOL] del self . _graph [ master_vertex ] [EOL] if cycle_info is not None : [EOL] cycle , _ = cycle_info [EOL] return cycle [EOL] [EOL] def mark ( self , vertex ) : [EOL] self . _marked . add ( vertex ) [EOL] [EOL] def get_unmarked ( self ) : [EOL] return { key for key in self . _graph . keys ( ) if key not in self . _marked } [EOL] [EOL] [EOL] signatory_functionality_graph = _Graph ( ) [EOL] [EOL] [EOL] def validate_tests ( tests , depends ) : [EOL] [docstring] [EOL] [EOL] if signatory in inspect . currentframe ( ) . f_back . f_globals . values ( ) : [EOL] raise RuntimeError ( [string] [string] ) [EOL] [EOL] for test in tests : [EOL] signatory_functionality_graph . mark ( test ) [EOL] for depend in depends : [EOL] signatory_functionality_graph . add_directed_edge ( test , depend , os . path . basename ( inspect . stack ( ) [ [number] ] [ [number] ] ) ) [EOL] [EOL] signatory_mock = argparse . Namespace ( ) [EOL] for string in it . chain ( tests , depends ) : [EOL] obj = signatory [EOL] obj_mock = signatory_mock [EOL] split_string = string . split ( [string] ) [EOL] last = len ( split_string ) - [number] [EOL] for i , string_elem in enumerate ( split_string ) : [EOL] obj = getattr ( obj , string_elem ) [EOL] obj_mock_new = argparse . Namespace ( ) [EOL] setattr ( obj_mock , string_elem , obj if i == last else obj_mock_new ) [EOL] obj_mock = obj_mock_new [EOL] return signatory_mock [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.object$ 0 0 0 0 0 0 0 0 0 $builtins.object$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.object$ 0 $typing.Dict[typing.Any,builtins.bool]$ 0 $typing.Dict[typing.Any,builtins.bool]$ 0 0 0 0 0 0 0 $builtins.object$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $test.helpers.validation._Graph$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $test.helpers.validation._Graph$ 0 0 0 0 0 0 0 0 0 0 0 0 $test.helpers.validation._Graph$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] [comment] [EOL] import signatory [EOL] import torch [EOL] from torch import nn [EOL] [EOL] [EOL] class SigNet3 ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_dimension , sig_depth ) : [EOL] super ( SigNet3 , self ) . __init__ ( ) [EOL] self . augment1 = signatory . Augment ( in_channels = in_channels , layer_sizes = ( [number] , [number] , [number] ) , kernel_size = [number] , include_original = True , include_time = True ) [EOL] self . signature1 = signatory . Signature ( depth = sig_depth , stream = True ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] sig_channels1 = signatory . signature_channels ( channels = in_channels + [number] , depth = sig_depth ) [EOL] self . augment2 = signatory . Augment ( in_channels = sig_channels1 , layer_sizes = ( [number] , [number] , [number] ) , kernel_size = [number] , include_original = False , include_time = False ) [EOL] self . signature2 = signatory . Signature ( depth = sig_depth , stream = False ) [EOL] [EOL] [comment] [EOL] sig_channels2 = signatory . signature_channels ( channels = [number] , depth = sig_depth ) [EOL] self . linear = torch . nn . Linear ( sig_channels2 , out_dimension ) [EOL] [EOL] def forward ( self , inp ) : [EOL] [comment] [EOL] a = self . augment1 ( inp ) [EOL] if a . size ( [number] ) <= [number] : [EOL] raise RuntimeError ( [string] [string] ) [EOL] [comment] [EOL] b = self . signature1 ( a , basepoint = True ) [EOL] [comment] [EOL] c = self . augment2 ( b ) [EOL] if c . size ( [number] ) <= [number] : [EOL] raise RuntimeError ( [string] [string] ) [EOL] [comment] [EOL] d = self . signature2 ( c , basepoint = True ) [EOL] [comment] [EOL] e = self . linear ( d ) [EOL] [comment] [EOL] return e [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] [EOL] [comment] [EOL] import signatory [EOL] import torch [EOL] from torch import nn [EOL] [EOL] [EOL] class SigNet ( nn . Module ) : [EOL] def __init__ ( self , in_channels , out_dimension , sig_depth ) : [EOL] super ( SigNet , self ) . __init__ ( ) [EOL] self . augment = signatory . Augment ( in_channels = in_channels , layer_sizes = ( ) , kernel_size = [number] , include_original = True , include_time = True ) [EOL] self . signature = signatory . Signature ( depth = sig_depth ) [EOL] [comment] [EOL] sig_channels = signatory . signature_channels ( channels = in_channels + [number] , depth = sig_depth ) [EOL] self . linear = torch . nn . Linear ( sig_channels , out_dimension ) [EOL] [EOL] def forward ( self , inp ) : [EOL] [comment] [EOL] x = self . augment ( inp ) [EOL] if x . size ( [number] ) <= [number] : [EOL] raise RuntimeError ( [string] [string] ) [EOL] [comment] [EOL] [comment] [EOL] y = self . signature ( x , basepoint = True ) [EOL] [comment] [EOL] [comment] [EOL] z = self . linear ( y ) [EOL] [comment] [EOL] return z [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0