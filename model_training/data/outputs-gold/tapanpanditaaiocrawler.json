from typing import List , Dict , Any , Tuple , Iterable , Set , Optional [EOL] import aiocrawler [EOL] import builtins [EOL] import argparse [EOL] import pprint [EOL] import typing [EOL] import bs4 [EOL] import aiohttp [EOL] import asyncio [EOL] import logging [EOL] [docstring] [EOL] import asyncio [EOL] import logging [EOL] from dataclasses import dataclass [EOL] from urllib . parse import urljoin , urlparse [EOL] from typing import Set , Iterable , List , Tuple , Dict , Optional [EOL] [EOL] from aiohttp import ClientSession , ClientResponseError , ClientTimeout [EOL] from aiohttp import ClientConnectionError , ClientPayloadError [EOL] from aiohttp . client_exceptions import TooManyRedirects [EOL] from bs4 import BeautifulSoup [comment] [EOL] from bs4 . element import Tag [comment] [EOL] [EOL] [EOL] logger = logging . getLogger ( [string] ) [EOL] [EOL] [EOL] class InvalidContentTypeError ( Exception ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , response ) : [EOL] self . response = response [EOL] [EOL] [EOL] @ dataclass class TaskQueueMessage : [EOL] url = ... [EOL] depth = ... [EOL] retry_count = ... [EOL] [EOL] [EOL] class AIOCrawler : [EOL] [docstring] [EOL] [EOL] timeout = [number] [EOL] max_redirects = [number] [EOL] valid_content_types = { [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def __init__ ( self , init_url , depth = [number] , concurrency = [number] , max_retries = [number] , user_agent = [string] , ) : [EOL] [docstring] [EOL] self . init_url = init_url [EOL] self . depth = depth [EOL] self . concurrency = concurrency [EOL] self . max_retries = max_retries [EOL] self . user_agent = user_agent [EOL] self . base_url = [string] . format ( urlparse ( self . init_url ) . scheme , urlparse ( self . init_url ) . netloc ) [EOL] self . crawled_urls = set ( ) [EOL] self . results = [ ] [EOL] self . session = None [EOL] self . task_queue = None [EOL] [EOL] async def _make_request ( self , url ) : [EOL] [docstring] [EOL] if not self . session : [EOL] self . session = ClientSession ( ) [EOL] [EOL] logging . debug ( f' [string] { url }' ) [EOL] headers = { [string] : self . user_agent } [EOL] timeout = ClientTimeout ( total = self . timeout ) [EOL] [EOL] async with self . session . get ( url , headers = headers , raise_for_status = True , timeout = timeout , max_redirects = self . max_redirects , ) as response : [EOL] [EOL] if response . content_type not in self . valid_content_types : [EOL] raise InvalidContentTypeError ( response ) [EOL] [EOL] html = await response . text ( ) [EOL] return html [EOL] [EOL] def normalize_urls ( self , urls ) : [EOL] [docstring] [EOL] links = { urljoin ( self . base_url , url [ [string] ] ) for url in urls if urljoin ( self . base_url , url [ [string] ] ) . startswith ( self . base_url ) } [EOL] return links [EOL] [EOL] def find_links ( self , html ) : [EOL] [docstring] [EOL] soup = BeautifulSoup ( html , [string] ) [EOL] links = self . normalize_urls ( soup . select ( [string] ) ) [EOL] return links [EOL] [EOL] def parse ( self , url , links , html ) : [EOL] raise NotImplementedError ( [string] . format ( self . __class__ . __name__ ) ) [EOL] [EOL] async def crawl_page ( self , url ) : [EOL] [docstring] [EOL] html = await self . _make_request ( url ) [EOL] links = self . find_links ( html ) [EOL] return url , links , html [EOL] [EOL] async def retry_task ( self , task ) : [EOL] [docstring] [EOL] if task . retry_count < self . max_retries : [EOL] self . crawled_urls . discard ( task . url ) [EOL] task_message = TaskQueueMessage ( task . url , task . depth , task . retry_count + [number] ) [EOL] await self . task_queue . put ( task_message ) [EOL] else : [EOL] logger . error ( f' [string] { task . url }' ) [EOL] [EOL] async def worker ( self ) : [EOL] [docstring] [EOL] while True : [EOL] [EOL] if not self . task_queue : [EOL] break [EOL] [EOL] task = await self . task_queue . get ( ) [EOL] logger . debug ( f' [string] { task . url } [string] { task . depth }' ) [EOL] [EOL] if ( task . depth >= self . depth ) or ( task . url in self . crawled_urls ) : [EOL] self . task_queue . task_done ( ) [EOL] logger . debug ( [string] ) [EOL] continue [EOL] [EOL] self . crawled_urls . add ( task . url ) [EOL] [EOL] try : [EOL] url , links , html = await self . crawl_page ( task . url ) [EOL] except InvalidContentTypeError as excp : [EOL] logger . error ( f' [string] { task . url }' ) [EOL] except TooManyRedirects as excp : [EOL] logger . error ( f' [string] { task . url }' ) [EOL] except ClientPayloadError as excp : [EOL] logger . error ( f' [string] { task . url }' ) [EOL] except asyncio . TimeoutError as excp : [EOL] logger . error ( f' [string] { task . url } [string] ' ) [EOL] await self . retry_task ( task ) [EOL] except ClientResponseError as excp : [EOL] [EOL] if excp . status > [number] : [EOL] logger . error ( f' [string] { task . url } [string] ' ) [EOL] await self . retry_task ( task ) [EOL] else : [EOL] logger . error ( f' [string] { excp . status } [string] { task . url }' ) [EOL] [EOL] except ClientConnectionError as excp : [EOL] logger . error ( f' [string] { task . url } [string] ' ) [EOL] await self . retry_task ( task ) [EOL] except Exception as excp : [EOL] logger . error ( f' [string] { type ( excp ) } [string] { excp }' ) [EOL] else : [EOL] self . results . append ( self . parse ( url , links , html ) ) [EOL] [EOL] for link in links . difference ( self . crawled_urls ) : [EOL] task_message = TaskQueueMessage ( link , task . depth + [number] , [number] ) [EOL] await self . task_queue . put ( task_message ) [EOL] finally : [EOL] self . task_queue . task_done ( ) [EOL] [EOL] async def crawl ( self ) : [EOL] [docstring] [EOL] self . task_queue = asyncio . Queue ( ) [EOL] task_message = TaskQueueMessage ( self . init_url , [number] , [number] ) [EOL] self . task_queue . put_nowait ( task_message ) [EOL] workers = [ asyncio . create_task ( self . worker ( ) ) for i in range ( self . concurrency ) ] [EOL] [EOL] await self . task_queue . join ( ) [EOL] [EOL] for worker in workers : [EOL] worker . cancel ( ) [EOL] [EOL] if self . session : [EOL] await self . session . close ( ) [EOL] [EOL] async def get_results ( self ) : [EOL] [docstring] [EOL] await self . crawl ( ) [EOL] return self . results [EOL] [EOL] [EOL] class SitemapCrawler ( AIOCrawler ) : [EOL] [docstring] [EOL] [EOL] def parse ( self , url , links , html ) : [EOL] [docstring] [EOL] return url , links [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] import argparse [EOL] import pprint [EOL] [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , action = [string] , dest = [string] , type = str ) [EOL] parser . add_argument ( [string] , action = [string] , dest = [string] , default = [number] , type = int ) [EOL] parser . add_argument ( [string] , action = [string] , dest = [string] , default = [number] , type = int ) [EOL] parser . add_argument ( [string] , action = [string] , dest = [string] , default = [number] , type = int ) [EOL] results = parser . parse_args ( ) [EOL] [EOL] crawler = SitemapCrawler ( results . init_url , results . depth , results . concurrency , results . max_retries ) [EOL] sitemap = asyncio . run ( crawler . get_results ( ) ) [EOL] [EOL] pp = pprint . PrettyPrinter ( ) [EOL] pp . pprint ( sitemap ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 $typing.Optional[aiohttp.ClientSession]$ 0 0 0 0 0 $typing.Optional[asyncio.Queue]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Set[builtins.str]$ 0 0 0 $typing.Iterable[bs4.element.Tag]$ 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[bs4.element.Tag]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 $typing.Set[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Set[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,typing.Set[builtins.str],builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $typing.Set[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 $typing.Optional[asyncio.queues.Queue[typing.Any]]$ 0 0 0 0 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[asyncio.queues.Queue[typing.Any]]$ 0 0 0 $aiocrawler.TaskQueueMessage$ 0 0 $typing.List[asyncio.tasks.Task[None]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[asyncio.queues.Queue[typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.List[asyncio.tasks.Task[None]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,typing.Set[builtins.str]]$ 0 0 0 $builtins.str$ 0 $typing.Set[builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 $aiocrawler.SitemapCrawler$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $aiocrawler.SitemapCrawler$ 0 0 0 0 0 0 0 $pprint.PrettyPrinter$ 0 0 0 0 0 0 0 $pprint.PrettyPrinter$ 0 0 0 $typing.List[typing.Any]$ 0 0
from typing import Set , Any [EOL] import aiocrawler [EOL] import typing [EOL] import pytest [comment] [EOL] import asyncio [EOL] import aiocrawler [EOL] from aiocrawler import ( AIOCrawler , SitemapCrawler , InvalidContentTypeError , TaskQueueMessage , ) [EOL] [EOL] [EOL] @ pytest . fixture def crawler ( ) : [EOL] url = [string] [EOL] crawler = AIOCrawler ( url ) [EOL] return crawler [EOL] [EOL] [EOL] @ pytest . fixture def html ( ) : [EOL] html_string = [string] [EOL] return html_string [EOL] [EOL] [EOL] @ pytest . fixture def create_mock_coroutine ( mocker , monkeypatch ) : [EOL] def _create_mock_patch_coro ( to_patch = None ) : [EOL] mock = mocker . Mock ( ) [EOL] [EOL] async def _coro ( * args , ** kwargs ) : [EOL] return mock ( * args , ** kwargs ) [EOL] [EOL] if to_patch : [EOL] monkeypatch . setattr ( to_patch , _coro ) [EOL] [EOL] return mock , _coro [EOL] [EOL] return _create_mock_patch_coro [EOL] [EOL] [EOL] @ pytest . fixture def mock_make_request ( monkeypatch , html ) : [EOL] async def mock_make_request ( * args , ** kwargs ) : [EOL] return html [EOL] [EOL] monkeypatch . setattr ( AIOCrawler , [string] , mock_make_request ) [EOL] [EOL] [EOL] @ pytest . fixture def mock_make_request_generic ( create_mock_coroutine ) : [EOL] mock_make_request , mock_coroutine = create_mock_coroutine ( to_patch = [string] ) [EOL] return mock_make_request [EOL] [EOL] [EOL] @ pytest . fixture def mock_queue ( mocker , monkeypatch ) : [EOL] queue = mocker . Mock ( ) [EOL] monkeypatch . setattr ( asyncio , [string] , queue ) [EOL] return queue . return_value [EOL] [EOL] [EOL] @ pytest . fixture def mock_put_nowait ( mock_queue , create_mock_coroutine ) : [EOL] mock_put , coro_put = create_mock_coroutine ( ) [EOL] mock_queue . put_nowait = coro_put [EOL] return mock_put [EOL] [EOL] [EOL] @ pytest . fixture def mock_join ( mock_queue , create_mock_coroutine ) : [EOL] mock_join , coro_join = create_mock_coroutine ( ) [EOL] mock_queue . join = coro_join [EOL] return mock_join [EOL] [EOL] [EOL] def test_task_queue_message ( ) : [EOL] url = [string] [EOL] depth = [number] [EOL] retry_count = [number] [EOL] task_message = TaskQueueMessage ( url , depth , retry_count ) [EOL] assert task_message . url == url [EOL] assert task_message . depth == depth [EOL] assert task_message . retry_count == retry_count [EOL] [EOL] [EOL] def test_initial_default_depth ( crawler ) : [EOL] assert crawler . depth == [number] [EOL] [EOL] [EOL] def test_initial_default_concurrency ( crawler ) : [EOL] assert crawler . concurrency == [number] [EOL] [EOL] [EOL] def test_initial_default_max_retries ( crawler ) : [EOL] assert crawler . max_retries == [number] [EOL] [EOL] [EOL] def test_initial_default_user_agent ( crawler ) : [EOL] assert crawler . user_agent == [string] [EOL] [EOL] [EOL] def test_initial_crawled_urls ( crawler ) : [EOL] assert isinstance ( crawler . crawled_urls , set ) [EOL] assert not crawler . crawled_urls [EOL] [EOL] [EOL] def test_initial_results ( crawler ) : [EOL] assert isinstance ( crawler . results , list ) [EOL] assert not crawler . results [EOL] [EOL] [EOL] def test_base_url ( crawler ) : [EOL] assert crawler . base_url == [string] [EOL] [EOL] [EOL] def test_setting_initial_url ( crawler ) : [EOL] url = [string] [EOL] assert crawler . init_url == url [EOL] [EOL] [EOL] def test_setting_depth ( ) : [EOL] depth = [number] [EOL] url = [string] [EOL] crawler = AIOCrawler ( url , depth = depth ) [EOL] assert crawler . depth == depth [EOL] [EOL] [EOL] def test_setting_concurrency ( ) : [EOL] concurrency = [number] [EOL] url = [string] [EOL] crawler = AIOCrawler ( url , concurrency = concurrency ) [EOL] assert crawler . concurrency == concurrency [EOL] [EOL] [EOL] def test_setting_max_retries ( ) : [EOL] max_retries = [number] [EOL] url = [string] [EOL] crawler = AIOCrawler ( url , max_retries = max_retries ) [EOL] assert crawler . max_retries == max_retries [EOL] [EOL] [EOL] def test_setting_user_agent ( ) : [EOL] user_agent = [string] [EOL] url = [string] [EOL] crawler = AIOCrawler ( url , user_agent = user_agent ) [EOL] assert crawler . user_agent == user_agent [EOL] [EOL] [EOL] def test_find_links ( html , crawler ) : [EOL] links = crawler . find_links ( html ) [EOL] assert links == { [string] , [string] } [EOL] [EOL] [EOL] @ pytest . mark . asyncio async def test_crawl_page ( mock_make_request , crawler , html ) : [EOL] url = [string] [EOL] url , links , html = await crawler . crawl_page ( url ) [EOL] assert url == url [EOL] assert links == { [string] , [string] } [EOL] assert html == html [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def test_parse_raises_not_implemented ( crawler , html ) : [EOL] url = [string] [EOL] links = { [string] , [string] } [EOL] [EOL] with pytest . raises ( NotImplementedError ) : [EOL] crawler . parse ( url , links , html ) [EOL] [EOL] [EOL] @ pytest . mark . asyncio async def test_get_results ( crawler , create_mock_coroutine ) : [EOL] mock_crawl , _ = create_mock_coroutine ( to_patch = [string] ) [EOL] results = await crawler . get_results ( ) [EOL] mock_crawl . assert_called_once ( ) [EOL] assert results == [ ] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def test_sitemap_crawler_parse ( html ) : [EOL] crawler = SitemapCrawler ( [string] ) [EOL] url = [string] [EOL] links = { [string] , [string] } [EOL] assert crawler . parse ( url , links , html ) == ( url , links ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0