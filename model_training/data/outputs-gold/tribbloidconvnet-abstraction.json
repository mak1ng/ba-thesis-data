from typing import Any [EOL] import typing [EOL] import builtins [EOL] import matplotlib . pyplot as plt [EOL] [EOL] import mxnet as mx [EOL] [EOL] CTX = mx . gpu ( ) if mx . test_utils . list_gpus ( ) else mx . cpu ( ) [EOL] [EOL] [comment] [EOL] [EOL] MODEL_CHKPNT = [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0
[comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] import matplotlib . pyplot as plt [EOL] import networkx as nx [EOL] from mpl_toolkits . axisartist . axislines import SubplotZero [EOL] [EOL] [EOL] def setCanvas ( ) : [EOL] plt . rcParams [ [string] ] = [ [number] , [number] ] [EOL] plt . rcParams [ [string] ] = [number] [EOL] plt . rcParams [ [string] ] = [number] [EOL] [EOL] [EOL] def drawEdgesAndLabels ( options , g , edgeFactory = lambda g : nx . get_edge_attributes ( g , [string] ) ) : [EOL] _options = { ** options , [string] : edgeFactory ( g ) } [EOL] _labelOptions = { ** _options , [string] : dict ( alpha = [number] , color = [string] ) } [EOL] nx . drawing . draw_networkx_edges ( g , arrows = True , ** _options ) [EOL] nx . drawing . draw_networkx_edge_labels ( g , ** _labelOptions ) [EOL] [EOL] [EOL] def drawGraph ( g , layoutG = None , ** kwargs ) : [EOL] if not layoutG : [EOL] layoutG = g [EOL] [EOL] edges = g . edges . data ( ) [EOL] tails = [ e [ [number] : [number] ] for e in edges if ( [string] in e [ [number] ] ) ] [EOL] tailGraph = g . edge_subgraph ( tails ) [EOL] [EOL] heads = [ e [ [number] : [number] ] for e in edges if ( [string] not in e [ [number] ] ) ] [EOL] headGraph = g . edge_subgraph ( heads ) [EOL] [EOL] defaultOpt = { [string] : nx . drawing . nx_agraph . graphviz_layout ( layoutG , prog = [string] ) , [string] : [number] , [string] : [string] , [string] : [string] } [EOL] sharedOpt = { ** defaultOpt , ** kwargs } [EOL] [EOL] nodeOpt = { ** sharedOpt , [string] : [string] , [string] : [string] , } [EOL] [EOL] arrowOpt = { ** sharedOpt , [string] : [string] , [string] : [number] , [string] : [string] , [string] : [number] } [EOL] [EOL] arrowHeadOpt = { ** arrowOpt } [EOL] [EOL] arrowTailOpt = { ** arrowOpt , [string] : [string] } [EOL] [EOL] with plt . xkcd ( ) : [EOL] fig , ax = plt . subplots ( [number] ) [EOL] fig . patch . set_alpha ( [number] ) [EOL] [EOL] ax . axis ( [string] ) [EOL] [EOL] ax = SubplotZero ( fig , [number] ) [EOL] fig . add_subplot ( ax ) [EOL] plt . xticks ( [ ] ) [EOL] plt . yticks ( [ ] ) [EOL] [EOL] ax . set_ylabel ( [string] ) [EOL] ax . set_xlabel ( [string] ) [EOL] [EOL] for s in [ [string] , [string] ] : [EOL] ax . axis [ s ] . set_visible ( False ) [EOL] [EOL] for s in [ [string] , [string] ] : [EOL] ax . axis [ s ] . set_axisline_style ( [string] ) [EOL] [EOL] ax . patch . set_alpha ( [number] ) [EOL] [EOL] nx . drawing . draw_networkx_nodes ( g , ** nodeOpt ) [EOL] nx . drawing . draw_networkx_labels ( g , ** nodeOpt ) [EOL] [EOL] drawEdgesAndLabels ( arrowHeadOpt , headGraph ) [EOL] drawEdgesAndLabels ( arrowTailOpt , tailGraph ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import logging [EOL] import builtins [EOL] import mxnet [EOL] import logging [EOL] from itertools import tee [EOL] [EOL] import numpy as np [EOL] from mxnet import nd [EOL] from mxnet . gluon import nn [EOL] from mxnet . gluon . data import DataLoader [EOL] from mxnet . gluon . data . vision import transforms , MNIST [EOL] from mxnet . image import copyMakeBorder [EOL] from mxnet . ndarray import NDArray [EOL] [EOL] from const import * [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def shiftY ( img , offset ) : [EOL] [comment] [EOL] xRange = range ( [number] , img . shape [ [number] ] ) [EOL] [EOL] for x in xRange : [EOL] img [ : , x ] = np . roll ( img [ : , x ] . asnumpy ( ) , offset ) [EOL] [EOL] return img [EOL] [EOL] [EOL] def shiftX ( img , offset ) : [EOL] yRange = range ( [number] , img . shape [ [number] ] ) [EOL] [comment] [EOL] [EOL] for y in yRange : [EOL] img [ y ] = np . roll ( img [ y ] . asnumpy ( ) , offset ) [EOL] [EOL] return img [EOL] [EOL] [EOL] def pad ( img , size ) : [EOL] result = copyMakeBorder ( img , size , size , size , size ) [EOL] return result [EOL] [EOL] [EOL] [comment] [EOL] class Augmenter ( object ) : [EOL] [EOL] def __init__ ( self , proto ) : [EOL] self . shape = proto . shape [EOL] ctx = proto . context [EOL] [EOL] h = self . shape [ [number] ] [EOL] w = self . shape [ [number] ] [EOL] [EOL] leftEye = nd . eye ( h , ctx = ctx ) [EOL] rightEye = nd . eye ( w , ctx = ctx ) [EOL] [EOL] leftShifts = nd . zeros ( ( h , h , h ) , ctx = ctx ) [EOL] for i in range ( [number] , h ) : [EOL] leftShifts [ i ] = leftEye [EOL] shiftY ( leftShifts [ i ] , i ) [EOL] [EOL] rightShifts = nd . zeros ( ( w , w , w ) ) [EOL] for i in range ( [number] , w ) : [EOL] rightShifts [ i ] = rightEye [EOL] shiftX ( rightShifts [ i ] , i ) [EOL] [EOL] self . leftShifts = leftShifts [EOL] self . rightShifts = rightShifts [EOL] [EOL] def aug1 ( self , img ) : [EOL] [EOL] result = mx . nd . zeros ( ( self . shape [ [number] ] * self . shape [ [number] ] , self . shape [ [number] ] , self . shape [ [number] ] ) ) [EOL] [EOL] k = [number] [EOL] for i in range ( [number] , self . shape [ [number] ] ) : [EOL] for j in range ( [number] , self . shape [ [number] ] ) : [EOL] result [ k ] = mx . nd . dot ( mx . nd . dot ( self . leftShifts [ i ] , img ) , self . rightShifts [ j ] ) [EOL] k += [number] [EOL] [EOL] return result [EOL] [EOL] def augBatch ( self , imgs , labels ) : [EOL] [EOL] assert imgs . shape . __len__ ( ) == [number] , f" [string] { imgs . shape . __len__ ( ) }" [EOL] assert labels . shape . __len__ ( ) == [number] [EOL] [EOL] def _batchItr ( ) : [EOL] length = imgs . shape [ [number] ] [EOL] for i in range ( [number] , length ) : [EOL] img = imgs [ i ] . squeeze ( ) [EOL] label = labels [ i ] [EOL] [EOL] [comment] [EOL] imgAug = self . aug1 ( img ) [EOL] labelAug = mx . ndarray . ones ( imgAug . shape [ [number] ] , dtype = np . int32 ) * label [ [number] ] [EOL] assert imgAug . shape [ [number] ] == labelAug . shape [ [number] ] [EOL] [comment] [EOL] yield ( imgAug , labelAug ) [EOL] [EOL] batchItr = _batchItr ( ) [EOL] batchItr , batchItr2 = tee ( batchItr ) [EOL] [EOL] imgConcat = mx . nd . concat ( * [ i [ [number] ] for i in batchItr ] , dim = [number] ) [EOL] labelConcat = mx . nd . concat ( * [ i [ [number] ] for i in batchItr2 ] , dim = [number] ) [EOL] [EOL] return imgConcat , labelConcat [EOL] [EOL] def augTuples ( self , tt ) : [EOL] return self . augBatch ( tt [ [number] ] , tt [ [number] ] ) [EOL] [EOL] def augFirstTuple ( self , tt ) : [EOL] return self . augBatch ( tt [ [number] ] [ [ [number] ] ] , tt [ [number] ] [ [ [number] ] ] ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] preprocessor = transforms . Compose ( [ transforms . Resize ( size = ( [number] , [number] ) ) , transforms . ToTensor ( ) , transforms . Normalize ( mean = [number] , std = [number] ) ] ) [EOL] [EOL] [EOL] def getData ( ) : [EOL] [comment] [EOL] trainSet = MNIST ( [string] , train = True ) . transform_first ( preprocessor ) [EOL] trainLoader = DataLoader ( trainSet , batch_size = [number] , shuffle = False , last_batch = [string] ) [EOL] return trainLoader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 $typing.Any$ 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import mxnet [EOL] from mxnet . gluon . nn import Sequential [EOL] from mxnet . io import NDArrayIter [EOL] [EOL] from aug2conv import * [EOL] [EOL] transform = transforms . Compose ( [ transforms . ToTensor ( ) , transforms . Normalize ( mean = [number] , std = [number] ) ] ) [EOL] [EOL] [comment] [EOL] trainSet = MNIST ( [string] , train = True ) . transform_first ( transform ) [EOL] trainLoader = DataLoader ( trainSet , batch_size = [number] , shuffle = True ) [EOL] [EOL] imgs = ... [EOL] imgs , labels = next ( trainLoader . __iter__ ( ) ) [EOL] imgs = imgs . squeeze ( axis = ( [number] , ) ) [EOL] assert imgs . shape . __len__ ( ) == [number] [EOL] img = imgs [ [number] , : , : ] . squeeze ( ) [EOL] [EOL] assert type ( img ) == NDArray [EOL] [EOL] [EOL] def testInPlace ( ) : [EOL] r1 = shiftX ( img . copy ( ) , [number] ) [EOL] shiftX ( img , [number] ) [EOL] [EOL] assert np . array_equal ( r1 . asnumpy ( ) , img . asnumpy ( ) ) [EOL] [EOL] [EOL] def testIter ( ) : [EOL] print ( imgs . shape ) [EOL] itr = NDArrayIter ( imgs ) [EOL] for batch in itr : [EOL] print ( type ( batch ) ) [EOL] print ( len ( batch . data ) ) [EOL] print ( batch . data [ [number] ] . shape ) [EOL] [EOL] [EOL] def testAugBatch ( ) : [EOL] augmenter = Augmenter ( img ) [EOL] r = augmenter . augBatch ( imgs , labels ) [EOL] print ( r [ [number] ] . shape ) [EOL] print ( r [ [number] ] . shape ) [EOL] assert ( r [ [number] ] . shape [ [number] ] == r [ [number] ] . shape [ [number] ] ) [EOL] [EOL] [EOL] def testGetData ( ) : [EOL] p = preprocessor [EOL] from utils import debugSeq [EOL] debugSeq ( p , ( [number] , [number] , [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $MNIST$ 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 $DataLoader$ 0 0 0 $MNIST$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $DataLoader$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import mxnet as mx [EOL] [EOL] from mxnet . ndarray import * [EOL] [EOL] [EOL] def activaton ( x ) : [EOL] return [number] / ( [number] + exp ( - x ) ) [EOL] [EOL] [EOL] mx . random . seed ( [number] ) [EOL] [EOL] features = mx . random . randn ( [number] , [number] ) [EOL] [EOL] weights = mx . random . randn ( * features . shape ) [EOL] [EOL] bias = mx . random . randn ( [number] , [number] ) [EOL] [EOL] output = ( features * weights ) . sum ( ) + bias [EOL] [EOL] y = activaton ( output ) [EOL] print ( y ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import mxnet [EOL] import mxnet as mx [EOL] import mxnet . gluon as gl [EOL] [EOL] from mxnet . gluon . data . vision import transforms , MNIST [EOL] [EOL] toTensor = transforms . ToTensor ( ) [EOL] normalize = transforms . Normalize ( mean = [number] , std = [number] ) [EOL] transform = transforms . Compose ( [ toTensor , normalize ] ) [EOL] [EOL] [comment] [EOL] trainSet = MNIST ( [string] , train = True ) . transform_first ( transform ) [EOL] trainLoader = gl . data . DataLoader ( trainSet , batch_size = [number] , shuffle = True ) [EOL] [EOL] [EOL] class HConv_SO2 ( gl . Block ) : [EOL] [EOL] def forward ( self , * args ) : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $mxnet.gluon.data.vision.MNIST$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $mxnet.gluon.data.vision.MNIST$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] a = [number] + [number] [EOL] b = a + [number]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0
from typing import Any [EOL] import typing [EOL] import mxnet [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] from mxnet . gluon . nn import Dense [EOL] from mxnet . ndarray import NDArray [EOL] import math [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def imshow ( image , ax = None , title = None ) : [EOL] [docstring] [EOL] if ax is None : [EOL] fig , ax = plt . subplots ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] ax . imshow ( image ) [EOL] ax . spines [ [string] ] . set_visible ( False ) [EOL] ax . spines [ [string] ] . set_visible ( False ) [EOL] ax . spines [ [string] ] . set_visible ( False ) [EOL] ax . spines [ [string] ] . set_visible ( False ) [EOL] ax . tick_params ( axis = [string] , length = [number] ) [EOL] ax . set_xticklabels ( [string] ) [EOL] ax . set_yticklabels ( [string] ) [EOL] [EOL] return ax [EOL] [EOL] [EOL] def view_recon ( img , recon ) : [EOL] [docstring] [EOL] [EOL] fig , axes = plt . subplots ( ncols = [number] , sharex = True , sharey = True ) [EOL] axes [ [number] ] . imshow ( img . asnumpy ( ) . squeeze ( ) ) [EOL] axes [ [number] ] . imshow ( recon . asnumpy ( ) . squeeze ( ) ) [EOL] for ax in axes : [EOL] ax . axis ( [string] ) [EOL] ax . set_adjustable ( [string] ) [EOL] [EOL] [EOL] def view_classify ( img , ps , version = [string] ) : [EOL] [docstring] [EOL] ps = ps . asnumpy ( ) . squeeze ( ) [EOL] [EOL] fig , ( ax1 , ax2 ) = plt . subplots ( figsize = ( [number] , [number] ) , ncols = [number] ) [EOL] ax1 . imshow ( img . asnumpy ( ) . squeeze ( ) ) [EOL] ax1 . axis ( [string] ) [EOL] ax2 . barh ( np . arange ( [number] ) , ps ) [EOL] ax2 . set_aspect ( [number] ) [EOL] ax2 . set_yticks ( np . arange ( [number] ) ) [EOL] if version == [string] : [EOL] ax2 . set_yticklabels ( np . arange ( [number] ) ) [EOL] ax2 . set_title ( [string] ) [EOL] ax2 . set_xlim ( [number] , [number] ) [EOL] [EOL] plt . tight_layout ( ) [EOL] [EOL] [EOL] def viewFCWeights ( fc , inShape = ( [number] , [number] ) ) : [EOL] weight = fc . weight . data ( ) [EOL] [EOL] viewWeights ( weight , inShape ) [EOL] [EOL] [EOL] def viewWeights ( w , inShape = ( [number] , [number] ) ) : [EOL] dOut = w . shape [ [number] ] [EOL] dOutSqrt = int ( math . sqrt ( dOut ) ) [EOL] if dOutSqrt ** [number] < dOut : [EOL] dOutSqrt += [number] [EOL] [EOL] fig , axs = plt . subplots ( dOutSqrt , dOutSqrt ) [EOL] ii = [number] [EOL] for x in range ( [number] , dOutSqrt ) : [EOL] for y in range ( [number] , dOutSqrt ) : [EOL] if ii < dOut : [EOL] imshow ( w [ ii ] . reshape ( inShape ) . asnumpy ( ) . squeeze ( ) , ax = axs [ x , y ] ) [EOL] ii += [number] [EOL] [EOL] plt . tight_layout ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import typing [EOL] import mxnet [EOL] from mxnet . gluon . nn import Sequential , HybridSequential [EOL] import mxnet as mx [EOL] [EOL] [EOL] def expand ( v ) : [EOL] c = list ( v ) [EOL] buffer = list ( ) [EOL] [EOL] for i in c : [EOL] if isinstance ( i , Sequential ) : [EOL] buffer . extend ( expand ( i ) ) [EOL] if isinstance ( i , HybridSequential ) : [EOL] buffer . extend ( expand ( i ) ) [EOL] else : [EOL] buffer . append ( i ) [EOL] [EOL] return buffer [EOL] [EOL] [EOL] def debugSeq ( v , shape ) : [EOL] input = mx . ndarray . zeros ( shape ) [EOL] [EOL] children = expand ( v ) [EOL] [EOL] m = input [EOL] print ( f" [string] { m . shape }" ) [EOL] for e in children . __iter__ ( ) : [EOL] m = e ( m ) [EOL] print ( f" [string] { [number] } [string] { type ( e ) } [string] " ) [EOL] print ( f" [string] { m . shape }" ) [EOL] [EOL] return [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import os [EOL] [EOL] import mxnet as mx [EOL] import numpy as np [EOL] from matplotlib import pyplot as plt [EOL] from mxnet import autograd [EOL] from mxnet import gluon [EOL] from mxnet import ndarray as nd [EOL] from mxnet . gluon import nn [EOL] [EOL] [comment] [EOL] context = mx . gpu ( [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] import brine [EOL] anime_train = brine . load_dataset ( [string] ) [EOL] [EOL] [comment] [EOL] type ( anime_train . columns ) [EOL] [EOL] [comment] [EOL] len ( anime_train ) [EOL] [EOL] [comment] [EOL] anime_train . load_image ( anime_train [ [number] ] . image ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] _ , training_folder = anime_train . create_folds ( [ [number] ] , shuffle = True ) [EOL] print ( len ( training_folder ) ) [EOL] [EOL] [comment] [EOL] target_wd = [number] [EOL] target_ht = [number] [EOL] img_list = [ ] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] def transform ( data , target_wd , target_ht ) : [EOL] [comment] [EOL] data = mx . image . imresize ( data , target_wd , target_ht ) [EOL] [comment] [EOL] [comment] [EOL] data = nd . transpose ( data , ( [number] , [number] , [number] ) ) [EOL] [comment] [EOL] data = data . astype ( np . float32 ) / [number] - [number] [EOL] return data . reshape ( ( [number] , ) + data . shape ) [EOL] [EOL] [EOL] [comment] [EOL] def getImageList ( base_path , training_folder ) : [EOL] img_list = [ ] [EOL] for train in training_folder : [EOL] fname = base_path + train . image [EOL] img_arr = mx . image . imread ( fname ) [EOL] img_arr = transform ( img_arr , target_wd , target_ht ) [EOL] img_list . append ( img_arr ) [EOL] return img_list [EOL] [EOL] [EOL] base_path = [string] [EOL] img_list = getImageList ( [string] , training_folder ) [EOL] [EOL] [comment] [EOL] batch_size = [number] [EOL] latent_z_size = [number] [EOL] [comment] [EOL] train_data = mx . io . NDArrayIter ( data = nd . concatenate ( img_list ) , batch_size = batch_size ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def visualize ( img_arr ) : [EOL] plt . imshow ( ( ( img_arr . asnumpy ( ) . transpose ( [number] , [number] , [number] ) + [number] ) * [number] ) . astype ( np . uint8 ) ) [EOL] plt . axis ( [string] ) [EOL] [EOL] [EOL] for i in range ( [number] ) : [EOL] plt . subplot ( [number] , [number] , i + [number] ) [EOL] visualize ( img_list [ i + [number] ] [ [number] ] ) [EOL] plt . show ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] random_z = mx . nd . random_normal ( [number] , [number] , shape = ( [number] , latent_z_size , [number] , [number] ) , ctx = context ) [EOL] netTest = nn . Sequential ( ) [EOL] [EOL] with netTest . name_scope ( ) : [EOL] [comment] [EOL] [comment] [EOL] netTest . add ( nn . Conv2DTranspose ( [number] , [number] , [number] , [number] , use_bias = False ) ) [comment] [EOL] [comment] [EOL] netTest . add ( nn . Conv2DTranspose ( [number] , [number] , [number] , [number] , use_bias = False ) ) [comment] [EOL] netTest . add ( nn . Conv2DTranspose ( [number] , [number] , [number] , [number] , use_bias = False ) ) [EOL] [EOL] netTest . initialize ( mx . init . Normal ( [number] ) , ctx = context ) [EOL] abc = netTest ( random_z ) [EOL] print ( [string] , random_z . shape ) [EOL] print ( [string] , abc . shape ) [EOL] [EOL] [comment] [EOL] epochs = [number] [comment] [EOL] batch_size = [number] [EOL] random_z = [number] [comment] [EOL] lr = [number] [EOL] beta1 = [number] [EOL] [EOL] use_gpu = True [EOL] ctx = mx . gpu ( ) if use_gpu else mx . cpu ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] nc = [number] [comment] [EOL] ngf = [number] [comment] [EOL] netG = nn . Sequential ( ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] with netG . name_scope ( ) : [EOL] [comment] [EOL] netG . add ( nn . Conv2DTranspose ( ngf * [number] , [number] , [number] , [number] ) ) [EOL] netG . add ( nn . BatchNorm ( ) ) [EOL] netG . add ( nn . Activation ( [string] ) ) [EOL] [comment] [EOL] netG . add ( nn . Conv2DTranspose ( ngf * [number] , [number] , [number] , [number] ) ) [EOL] netG . add ( nn . BatchNorm ( ) ) [EOL] netG . add ( nn . Activation ( [string] ) ) [EOL] [comment] [EOL] netG . add ( nn . Conv2DTranspose ( ngf * [number] , [number] , [number] , [number] ) ) [EOL] netG . add ( nn . BatchNorm ( ) ) [EOL] netG . add ( nn . Activation ( [string] ) ) [EOL] [comment] [EOL] netG . add ( nn . Conv2DTranspose ( ngf , [number] , [number] , [number] ) ) [EOL] netG . add ( nn . BatchNorm ( ) ) [EOL] netG . add ( nn . Activation ( [string] ) ) [EOL] [comment] [EOL] netG . add ( nn . Conv2DTranspose ( nc , [number] , [number] , [number] ) ) [EOL] netG . add ( nn . Activation ( [string] ) ) [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] ndf = [number] [EOL] netD = nn . Sequential ( ) [EOL] [comment] [EOL] with netD . name_scope ( ) : [EOL] [comment] [EOL] netD . add ( nn . Conv2D ( ndf , [number] , [number] , [number] ) ) [EOL] netD . add ( nn . LeakyReLU ( [number] ) ) [EOL] [comment] [EOL] netD . add ( nn . Conv2D ( ndf * [number] , [number] , [number] , [number] ) ) [EOL] netD . add ( nn . BatchNorm ( ) ) [EOL] netD . add ( nn . LeakyReLU ( [number] ) ) [EOL] [comment] [EOL] netD . add ( nn . Conv2D ( ndf * [number] , [number] , [number] , [number] ) ) [EOL] netD . add ( nn . BatchNorm ( ) ) [EOL] netD . add ( nn . LeakyReLU ( [number] ) ) [EOL] [comment] [EOL] netD . add ( nn . Conv2D ( ndf * [number] , [number] , [number] , [number] ) ) [EOL] netD . add ( nn . BatchNorm ( ) ) [EOL] netD . add ( nn . LeakyReLU ( [number] ) ) [EOL] [comment] [EOL] netD . add ( nn . Conv2D ( [number] , [number] , [number] , [number] ) ) [EOL] [EOL] [comment] [EOL] loss = gluon . loss . SigmoidBinaryCrossEntropyLoss ( ) [EOL] [comment] [EOL] netG . initialize ( mx . init . Normal ( [number] ) , ctx = ctx ) [EOL] netD . initialize ( mx . init . Normal ( [number] ) , ctx = ctx ) [EOL] train_data . reset ( ) [EOL] [EOL] for batch in train_data : [EOL] sample = batch . data [ [number] ] . as_in_context ( ctx ) [EOL] [comment] [EOL] break [EOL] print ( sample . shape ) [EOL] [EOL] out = netD ( sample ) [EOL] sample_label = nd . zeros ( ( batch_size , ) , ctx = ctx ) [EOL] print ( loss ( out , sample_label ) . shape ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] trainerG = gluon . Trainer ( netG . collect_params ( ) , [string] , { [string] : lr , [string] : beta1 } ) [EOL] trainerD = gluon . Trainer ( netD . collect_params ( ) , [string] , { [string] : lr , [string] : beta1 } ) [EOL] [EOL] [comment] [EOL] import time [EOL] import logging [EOL] [EOL] train_data . reset ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] real_label = nd . ones ( ( batch_size , ) , ctx = ctx ) [EOL] [EOL] [comment] [EOL] fake_label = nd . zeros ( ( batch_size , ) , ctx = ctx ) [EOL] [EOL] logging . basicConfig ( level = logging . DEBUG ) [EOL] [EOL] for epoch in range ( epochs ) : [EOL] tic = time . time ( ) [EOL] btic = time . time ( ) [EOL] train_data . reset ( ) [EOL] iter = [number] [EOL] for batch in train_data : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] data = batch . data [ [number] ] . as_in_context ( ctx ) [EOL] random_z = mx . nd . random_normal ( [number] , [number] , shape = ( batch_size , latent_z_size , [number] , [number] ) , ctx = ctx ) [EOL] [EOL] with autograd . record ( ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] output = netD ( data ) . reshape ( ( - [number] , [number] ) ) [EOL] [comment] [EOL] errD_real = loss ( output , real_label ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] fake = netG ( random_z ) [EOL] [EOL] [comment] [EOL] output = netD ( fake . detach ( ) ) . reshape ( ( - [number] , [number] ) ) [EOL] [EOL] errD_fake = loss ( output , fake_label ) [EOL] [EOL] [comment] [EOL] errD = errD_real + errD_fake [EOL] [EOL] [comment] [EOL] errD . backward ( ) [EOL] [EOL] trainerD . step ( batch . data [ [number] ] . shape [ [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] with autograd . record ( ) : [EOL] fake = netG ( random_z ) [EOL] output = netD ( fake ) . reshape ( ( - [number] , [number] ) ) [EOL] errG = loss ( output , real_label ) [EOL] errG . backward ( ) [EOL] [EOL] trainerG . step ( batch . data [ [number] ] . shape [ [number] ] ) [EOL] [EOL] [comment] [EOL] if iter % [number] == [number] : [EOL] logging . info ( [string] % ( nd . mean ( errD ) . asscalar ( ) , nd . mean ( errG ) . asscalar ( ) , iter , epoch ) ) [EOL] iter = iter + [number] [EOL] [EOL] [comment] [EOL] os . makedirs ( [string] , exist_ok = True ) [EOL] save_GAN = [string] [EOL] netG . save_params ( save_GAN ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] num_image = [number] [EOL] for i in range ( num_image ) : [EOL] [comment] [EOL] random_z = mx . nd . random_normal ( [number] , [number] , shape = ( [number] , latent_z_size , [number] , [number] ) , ctx = ctx ) [EOL] img = netG ( random_z ) [EOL] plt . subplot ( [number] , [number] , i + [number] ) [EOL] visualize ( img [ [number] ] ) [EOL] plt . show ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import mxnet [EOL] import builtins [EOL] import mxnet . gluon as gl [EOL] [EOL] from mxnet import autograd , image [EOL] from mxnet . gluon import nn [EOL] from mxnet . gluon . data import DataLoader [EOL] from mxnet . gluon . data . vision import transforms , MNIST [EOL] from mxnet . gluon . nn import Sequential [EOL] from mxnet . ndarray import NDArray [EOL] [EOL] transform = transforms . Compose ( [ transforms . ToTensor ( ) , transforms . Normalize ( mean = [number] , std = [number] ) ] ) [EOL] [EOL] [comment] [EOL] trainSet = MNIST ( [string] , train = True ) . transform_first ( transform ) [EOL] trainLoader = DataLoader ( trainSet , batch_size = [number] , shuffle = True ) [EOL] [EOL] [comment] [EOL] model = nn . Sequential ( ) [EOL] [comment] [EOL] model . add ( nn . Dense ( [number] , activation = [string] ) , nn . Dense ( [number] , activation = [string] ) , nn . Dense ( [number] ) ) [EOL] model . initialize ( ) [EOL] [EOL] criterion = gl . loss . SoftmaxCrossEntropyLoss ( ) [EOL] optimizer = gl . Trainer ( model . collect_params ( ) , [string] , { [string] : [number] } ) [EOL] [EOL] [EOL] def pad ( img , x , y ) : [EOL] result = image . copyMakeBorder ( img , y , y , x , x ) [EOL] return result [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] epochs = [number] [EOL] for e in range ( epochs ) : [EOL] running_loss = [number] [EOL] for images , labels in trainLoader : [EOL] [comment] [EOL] images = images . reshape ( images . shape [ [number] ] , - [number] ) [EOL] [EOL] [comment] [EOL] [EOL] with autograd . record ( ) : [EOL] output = model . forward ( images ) [EOL] loss = criterion ( output , labels ) [EOL] [EOL] loss . backward ( ) [EOL] optimizer . step ( images . shape [ [number] ] / [number] ) [EOL] [EOL] running_loss += loss . mean ( ) . asscalar ( ) [EOL] else : [EOL] print ( f" [string] { running_loss / len ( trainLoader ) }" ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.data.vision.MNIST$ 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.gluon.nn.Sequential$ 0 0 $mxnet.gluon.data.DataLoader$ 0 0 0 $mxnet.gluon.data.vision.MNIST$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $mxnet.ndarray.NDArray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 $mxnet.gluon.data.DataLoader$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $mxnet.gluon.data.DataLoader$ 0 0 0 0
	0
[comment] [EOL] [comment] [EOL] [EOL] from typing import List , Any [EOL] import typing [EOL] import python [EOL] import os [EOL] from mxnet import gluon , autograd [EOL] from mxnet . gluon import Block , nn [EOL] from mxnet import ndarray as F [EOL] [EOL] from rnn . shared import * [EOL] [EOL] [EOL] class UnRolledRNN_Model ( Block ) : [EOL] [EOL] def __init__ ( self , vocab_size , num_embed , num_hidden , ** kwargs ) : [EOL] super ( UnRolledRNN_Model , self ) . __init__ ( ** kwargs ) [EOL] self . num_embed = num_embed [EOL] self . vocab_size = vocab_size [EOL] [EOL] [comment] [EOL] [comment] [EOL] with self . name_scope ( ) : [EOL] self . encoder = nn . Embedding ( self . vocab_size , self . num_embed ) [EOL] self . dense1 = nn . Dense ( num_hidden , activation = [string] , flatten = True ) [EOL] self . dense2 = nn . Dense ( num_hidden , activation = [string] , flatten = True ) [EOL] self . dense3 = nn . Dense ( vocab_size , flatten = True ) [EOL] [EOL] def forward ( self , inputs ) : [EOL] emd = self . encoder ( inputs ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] character1 = emd [ : , [number] , : ] [EOL] character2 = emd [ : , [number] , : ] [EOL] character3 = emd [ : , [number] , : ] [EOL] c1_hidden = self . dense1 ( character1 ) [comment] [EOL] c2_hidden = self . dense1 ( character2 ) [comment] [EOL] c3_hidden = self . dense1 ( character3 ) [comment] [EOL] c1_hidden_2 = self . dense2 ( c1_hidden ) [comment] [EOL] addition_result = F . add ( c2_hidden , c1_hidden_2 ) [comment] [EOL] addition_hidden = self . dense2 ( addition_result ) [comment] [EOL] addition_result_2 = F . add ( addition_hidden , c3_hidden ) [comment] [EOL] final_output = self . dense3 ( addition_result_2 ) [EOL] return final_output [EOL] [EOL] [EOL] vocab_size = len ( chars ) + [number] [comment] [EOL] num_embed = [number] [EOL] num_hidden = [number] [EOL] [comment] [EOL] simple_model = UnRolledRNN_Model ( vocab_size , num_embed , num_hidden ) [EOL] [comment] [EOL] simple_model . collect_params ( ) . initialize ( mx . init . Xavier ( ) , ctx = context ) [EOL] trainer = gluon . Trainer ( simple_model . collect_params ( ) , [string] ) [EOL] loss = gluon . loss . SoftmaxCrossEntropyLoss ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] os . makedirs ( [string] , exist_ok = True ) [EOL] filename_unrolled_rnn = [string] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] def UnRolledRNNtrain ( train_data , label_data , batch_size = [number] , epochs = [number] ) : [EOL] epochs = epochs [EOL] smoothing_constant = [number] [EOL] for e in range ( epochs ) : [EOL] for ibatch , i in enumerate ( range ( [number] , train_data . shape [ [number] ] - [number] , batch_size ) ) : [EOL] data , target = get_batch ( train_data , label_data , i , batch_size ) [EOL] data = data . as_in_context ( context ) [EOL] target = target . as_in_context ( context ) [EOL] with autograd . record ( ) : [EOL] output = simple_model ( data ) [EOL] L = loss ( output , target ) [EOL] L . backward ( ) [EOL] trainer . step ( data . shape [ [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if ibatch == [number] : [EOL] curr_loss = mx . nd . mean ( L ) . asscalar ( ) [EOL] moving_loss = [number] [EOL] moving_loss = ( curr_loss if ( ( i == [number] ) and ( e == [number] ) ) else [EOL] ( [number] - smoothing_constant ) * moving_loss + (smoothing_constant) * curr_loss ) [EOL] print ( [string] % ( e , curr_loss , moving_loss ) ) [EOL] simple_model . save_parameters ( filename_unrolled_rnn ) [EOL] [EOL] [EOL] [comment] [EOL] epochs = [number] [EOL] UnRolledRNNtrain ( simple_train_data , simple_label_data , batch_size , epochs ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] simple_model . load_parameters ( filename_unrolled_rnn , ctx = context ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] def evaluate ( input_string ) : [EOL] idx = [ char_indices [ c ] for c in input_string ] [EOL] sample_input = mx . nd . array ( [ [ idx [ [number] ] , idx [ [number] ] , idx [ [number] ] ] ] , ctx = context ) [EOL] output = simple_model ( sample_input ) [EOL] index = mx . nd . argmax ( output , axis = [number] ) [EOL] return index . asnumpy ( ) [ [number] ] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] begin_char = [string] [EOL] answer = evaluate ( begin_char ) [EOL] print ( [string] , indices_char [ answer ] ) [EOL] [EOL] [comment] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $python.rnn.RNN.UnRolledRNN_Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] import mxnet as mx [EOL] import numpy as np [EOL] [EOL] [comment] [EOL] [comment] [EOL] context = mx . gpu ( [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] with open ( [string] , errors = [string] ) as f : [EOL] text = f . read ( ) [EOL] print ( len ( text ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] chars = sorted ( list ( set ( text ) ) ) [EOL] vocab_size = len ( chars ) + [number] [EOL] print ( [string] , vocab_size ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] chars . insert ( [number] , [string] ) [EOL] [EOL] [comment] [EOL] [string] . join ( chars [ [number] : - [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] char_indices = dict ( ( c , i ) for i , c in enumerate ( chars ) ) [EOL] [comment] [EOL] indices_char = dict ( ( i , c ) for i , c in enumerate ( chars ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] idx = [ char_indices [ c ] for c in text ] [EOL] [EOL] [comment] [EOL] print ( len ( idx ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [string] . join ( indices_char [ i ] for i in idx [ : [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] cs = [number] [EOL] c1_dat = [ idx [ i ] for i in range ( [number] , len ( idx ) - [number] - cs , cs ) ] [EOL] c2_dat = [ idx [ i + [number] ] for i in range ( [number] , len ( idx ) - [number] - cs , cs ) ] [EOL] c3_dat = [ idx [ i + [number] ] for i in range ( [number] , len ( idx ) - [number] - cs , cs ) ] [EOL] [comment] [EOL] c4_dat = [ idx [ i + [number] ] for i in range ( [number] , len ( idx ) - [number] - cs , cs ) ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] x1 = np . stack ( c1_dat [ : - [number] ] ) [EOL] x2 = np . stack ( c2_dat [ : - [number] ] ) [EOL] x3 = np . stack ( c3_dat [ : - [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] y = np . stack ( c4_dat [ : - [number] ] ) [EOL] [EOL] [comment] [EOL] col_concat = np . array ( [ x1 , x2 , x3 ] ) [EOL] t_col_concat = col_concat . T [EOL] print ( t_col_concat . shape ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] x1_nd = mx . nd . array ( x1 ) [EOL] x2_nd = mx . nd . array ( x2 ) [EOL] x3_nd = mx . nd . array ( x3 ) [EOL] sample_input = mx . nd . array ( [ [ x1 [ [number] ] , x2 [ [number] ] , x3 [ [number] ] ] , [ x1 [ [number] ] , x2 [ [number] ] , x3 [ [number] ] ] ] ) [EOL] [EOL] simple_train_data = mx . nd . array ( t_col_concat ) [EOL] simple_label_data = mx . nd . array ( y ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] batch_size = [number] [EOL] [EOL] [EOL] def get_batch ( source , label_data , i , batch_size = [number] ) : [EOL] bb_size = min ( batch_size , source . shape [ [number] ] - [number] - i ) [EOL] data = source [ i : i + bb_size ] [EOL] target = label_data [ i : i + bb_size ] [EOL] [comment] [EOL] return data , target . reshape ( ( - [number] , ) ) [EOL] [EOL] [EOL] [comment] [EOL] test_bat , test_target = get_batch ( simple_train_data , simple_label_data , [number] , batch_size ) [EOL] print ( test_bat . shape ) [EOL] print ( test_target . shape ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Dict[builtins.int,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] import mxnet as mx [EOL] from mxnet import autograd [EOL] [EOL] mx . test_utils . set_default_context ( mx . context . gpu ( ) ) [EOL] [EOL] a = mx . nd . zeros ( ( [number] , [number] ) ) [EOL] b = mx . nd . zeros ( ( [number] , [number] ) ) [EOL] [EOL] c = a * b [EOL] c *= [number] [EOL] print ( c ) [EOL] [EOL] [comment] [EOL] a = mx . nd . zeros ( ( [number] , [number] ) , ctx = mx . context . cpu ( ) ) [EOL] b = mx . nd . zeros ( ( [number] , [number] ) ) [EOL] [EOL] c = a * b [EOL] c *= [number] [EOL] print ( c ) [EOL] [EOL] [comment] [EOL] with autograd . record ( ) : [EOL] [EOL] x = mx . random . randn ( [number] , [number] ) [EOL] x . attach_grad ( ) [EOL] [EOL] y = x ** [number] [EOL] y . attach_grad ( ) [EOL] [EOL] z = y . mean ( ) [EOL] print ( f"{ x } [string] { y }" ) [EOL] [EOL] print ( f"{ type ( x ) } [string] { type ( y ) }" ) [EOL] [EOL] print ( f"{ x . grad } [string] { y . grad }" ) [EOL] [EOL] z . backward ( ) [EOL] [EOL] print ( f"{ x . grad } [string] { y . grad }" ) [EOL] print ( f"{ x / [number] }" ) [EOL] [EOL] [comment] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] from typing import Any , List , Dict [EOL] import typing [EOL] import matplotlib . pyplot as plot [EOL] import networkx as nx [EOL] [EOL] g = nx . DiGraph ( directed = True ) [EOL] [EOL] g . add_node ( [string] ) [EOL] [EOL] for i in range ( [number] ) : [EOL] g . add_node ( [string] % i ) [EOL] g . add_node ( [string] % i ) [EOL] g . add_node ( [string] % i ) [EOL] [EOL] g . add_edge ( [string] , [string] % i ) [EOL] g . add_edge ( [string] % i , [string] % i ) [EOL] g . add_edge ( [string] % i , [string] % i ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] plot . title ( [string] ) [EOL] pos = nx . drawing . nx_agraph . graphviz_layout ( g , prog = [string] ) [EOL] nx . draw ( g , pos , with_labels = True , arrows = True ) [EOL] [EOL] plot . show ( ) [EOL] [EOL] [comment] [EOL] import matplotlib . pyplot as plot [EOL] import networkx as nx [EOL] [EOL] g = nx . DiGraph ( directed = True ) [EOL] [EOL] n = [string] [EOL] n2 = [string] [EOL] f = [string] [EOL] f2 = [string] [EOL] o = [string] [EOL] c = [string] [EOL] fl = [string] [EOL] [EOL] g . add_nodes_from ( [ n ] ) [EOL] [EOL] g . add_nodes_from ( [ f , fl , o , c ] ) [EOL] [EOL] g . add_nodes_from ( [ n2 , f2 ] ) [EOL] [EOL] g . add_edge ( n , f , middle = True ) [EOL] g . add_edge ( f , n2 ) [EOL] g . add_edge ( f , o , middle = True ) [EOL] g . add_edge ( o , f2 ) [EOL] g . add_edge ( f , fl , middle = True ) [EOL] g . add_edge ( fl , n2 ) [EOL] g . add_edge ( n , c , middle = True ) [EOL] g . add_edge ( c , f2 ) [EOL] [EOL] nodeOpt = { [string] : nx . drawing . nx_agraph . graphviz_layout ( g , prog = [string] ) , [string] : [string] , [string] : [string] } [EOL] arrowHeadOpt = { ** nodeOpt , [string] : [number] , [string] : [string] , [string] : [number] , } [EOL] arrowTailOpt = { ** arrowHeadOpt , [string] : [string] , } [EOL] [EOL] with plot . xkcd ( ) : [EOL] fig , ax = plot . subplots ( [number] ) [EOL] ax . set_ylabel ( [string] ) [EOL] ax . set_xlabel ( [string] ) [EOL] [EOL] [EOL] [comment] [EOL] nx . drawing . draw_networkx_labels ( g , arrows = True , ** nodeOpt ) [EOL] [EOL] edges = g . edges . data ( ) [EOL] attrs = nx . get_edge_attributes ( g , [string] ) [EOL] [EOL] heads = [ e for e in edges if ( [string] in e [ [number] ] ) ] [EOL] nx . drawing . draw_networkx_edges ( g , edgelist = heads , arrows = True , ** arrowHeadOpt ) [EOL] [EOL] tails = [ e for e in edges if ( [string] not in e [ [number] ] ) ] [EOL] nx . drawing . draw_networkx_edges ( g , edgelist = tails , arrows = True , ** arrowTailOpt ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] plot . show ( ) [EOL] [EOL] [EOL] [EOL] [comment] [EOL] from mpl_toolkits . axisartist . axislines import SubplotZero [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] [EOL] if [number] : [EOL] fig = plt . figure ( [number] ) [EOL] ax = SubplotZero ( fig , [number] ) [EOL] print ( type ( ax ) ) [EOL] fig . add_subplot ( ax ) [EOL] [EOL] for direction in [ [string] , [string] ] : [EOL] [comment] [EOL] ax . axis [ direction ] . set_axisline_style ( [string] ) [EOL] [EOL] [comment] [EOL] ax . axis [ direction ] . set_visible ( True ) [EOL] [EOL] for direction in [ [string] , [string] , [string] , [string] ] : [EOL] [comment] [EOL] ax . axis [ direction ] . set_visible ( False ) [EOL] [EOL] x = np . linspace ( - [number] , [number] , [number] ) [EOL] ax . plot ( x , np . sin ( x * np . pi ) ) [EOL] [EOL] plt . show ( ) [EOL] [EOL] [EOL] [EOL] [comment] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] import pygraphviz as pgv [EOL] [EOL] G = pgv . AGraph ( ) [EOL] G . add_node ( [string] ) [EOL] G . add_edge ( [string] , [string] ) [EOL] G [EOL] [comment] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] left = np . zeros ( ( [number] , [number] ) ) [EOL] right = np . zeros ( ( [number] , [number] ) ) [EOL] [EOL] np . matmul ( left , right ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0