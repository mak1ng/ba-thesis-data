import builtins [EOL] from typing import Any , List [EOL] import typing [EOL] import paramiko [EOL] import paramiko [EOL] import fnmatch [EOL] import os [EOL] [EOL] [docstring] [EOL] ssh_client = paramiko . SSHClient ( ) [EOL] ssh_client . set_missing_host_key_policy ( paramiko . AutoAddPolicy ( ) ) [EOL] ssh_client . connect ( hostname = [string] , username = [string] , password = [string] ) [EOL] sftp = ssh_client . open_sftp ( ) [EOL] [EOL] [docstring] [EOL] cmd = [string] [EOL] stdin , stdout , stderr = ssh_client . exec_command ( cmd ) [EOL] [docstring] [EOL] [EOL] def sftp_walk ( remotepath ) : [EOL] path = remotepath [EOL] folders = [ ] [EOL] files = [ ] [EOL] for f in sftp . listdir_attr ( remotepath ) : [EOL] if os . stat . S_ISDIR ( file . st_mode ) : [EOL] folders . append ( f . filename ) [EOL] else : [EOL] if f . filename . endswith ( [string] ) : [EOL] files . append ( f . filename ) [EOL] print ( files ) [EOL] for folder in folders : [EOL] new_path = os . path . join ( remotepath , folder ) [EOL] sftp_walk ( new_path ) [EOL] [EOL] [EOL] sftp_walk ( [string] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple , List [EOL] import builtins [EOL] import typing [EOL] import os [EOL] from typing import List , Tuple [EOL] import utils . gtzan_genres as gtzan [EOL] [EOL] from sklearn . model_selection import train_test_split [EOL] [EOL] [EOL] def split_data_sklearn ( path , test_size ) : [EOL] ids = [ song . rstrip ( ) for song in open ( path ) ] [EOL] [EOL] train_input = list ( map ( lambda song_id : gtzan . genres [ song_id . split ( [string] ) [ [number] ] . split ( [string] ) [ [number] ] ] , ids ) ) [EOL] x_train , x_test , y_train , y_test = train_test_split ( ids , train_input , test_size = test_size ) [EOL] [EOL] return x_train , y_train , x_test , y_test [EOL] [EOL] [EOL] def split_data ( path , train_ratio ) : [EOL] ids = os . listdir ( path ) [EOL] train_index = int ( len ( ids ) * train_ratio ) [EOL] train_x = ids [ : train_index ] [EOL] test_x = ids [ train_index : ] [EOL] train_y = list ( map ( lambda id : id . split ( [string] ) [ [number] ] , train_x ) ) [EOL] test_y = list ( map ( lambda id : id . split ( [string] ) [ [number] ] , test_x ) ) [EOL] [EOL] return train_x , train_y , test_x , test_y	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.List[builtins.str],typing.List[builtins.str],typing.List[builtins.str],typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.List[builtins.str],typing.List[builtins.str],typing.List[builtins.str],typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy , scipy , matplotlib . pyplot as plt [EOL] import librosa , librosa . display [EOL] [EOL] x , sr = librosa . load ( [string] ) [EOL] [EOL] plt . figure ( figsize = ( [number] , [number] ) ) [EOL] librosa . display . waveplot ( x , sr , alpha = [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] X = scipy . fft ( x [ : [number] ] ) [EOL] X_mag = numpy . absolute ( X ) [comment] [EOL] f = numpy . linspace ( [number] , sr , [number] ) [comment] [EOL] plt . figure ( figsize = ( [number] , [number] ) ) [EOL] plt . plot ( f [ : [number] ] , X_mag [ : [number] ] ) [comment] [EOL] plt . suptitle ( [string] ) [EOL] plt . ylabel ( [string] ) [EOL] plt . xlabel ( [string] ) [EOL] plt . ylim ( [number] ** - [number] , [number] ** [number] ) [EOL] plt . yscale ( [string] ) [EOL] [EOL] [EOL] plt . show ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import numpy as np [EOL] import keras [EOL] from sklearn . utils import shuffle [EOL] [EOL] [EOL] class DataGenerator ( keras . utils . Sequence ) : [EOL] [EOL] def __init__ ( self , transform , ids , labels , batch_size , dim , n_channels , n_classes , shuffle = True ) : [EOL] self . transform_data = transform [EOL] self . ids = ids [EOL] self . labels = labels [EOL] self . batch_size = batch_size [EOL] self . dim = dim [EOL] self . n_channels = n_channels [EOL] self . n_classes = n_classes [EOL] self . shuffle = shuffle [EOL] self . on_epoch_end ( ) [EOL] [EOL] def __len__ ( self ) : [EOL] [docstring] [EOL] return int ( np . floor ( len ( self . ids ) / self . batch_size ) ) [EOL] [EOL] def __getitem__ ( self , index ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] x_indexes = range ( index * self . batch_size , ( index + [number] ) * self . batch_size ) [EOL] y_indexes = range ( index * self . batch_size , ( index + [number] ) * self . batch_size ) [EOL] [EOL] [EOL] [comment] [EOL] ids_temp = [ self . ids [ k ] for k in x_indexes ] [EOL] labels_temp = [ self . labels [ k ] for k in y_indexes ] [EOL] [EOL] [comment] [EOL] x , y = self . __data_generation ( ids_temp , labels_temp ) [EOL] [EOL] return x , y [EOL] [EOL] def on_epoch_end ( self ) : [EOL] [docstring] [EOL] ids_tmp = self . ids [EOL] labels_tmp = self . labels [EOL] [EOL] if self . shuffle : [EOL] self . ids , self . labels = shuffle ( ids_tmp , labels_tmp , random_state = [number] ) [EOL] [EOL] def __data_generation ( self , ids_temp , labels_temp ) : [EOL] [docstring] [EOL] [comment] [EOL] x , y = self . transform_data ( ids_temp , labels_temp , self . batch_size ) [EOL] return x , y [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import sys [EOL] import numpy as np [EOL] from evaluator import plot_confusion_matrix2 [EOL] [EOL] predictions = np . load ( sys . argv [ [number] ] ) [EOL] truths = np . load ( sys . argv [ [number] ] ) [EOL] labels = [ label . rstrip ( ) for label in open ( [string] ) ] [EOL] [EOL] confusion_matrix = plot_confusion_matrix2 ( predictions , truths , labels , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[builtins.str]$ 0 0 0 0
from typing import Dict , Any , List [EOL] import typing [EOL] import sqlite3 [EOL] import os [EOL] import sys [EOL] import sqlite3 [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] def database ( ) : [EOL] db_file = [string] [EOL] [EOL] if not os . path . isfile ( db_file ) : [EOL] print ( [string] % db_file ) [EOL] sys . exit ( [number] ) [EOL] return db_file [EOL] [EOL] [EOL] def fetch_tag_from_song ( tid ) : [EOL] db_file = database ( ) [EOL] conn = sqlite3 . connect ( db_file ) [EOL] [EOL] sql = [string] [string] % tid [EOL] res = conn . execute ( sql ) [EOL] data = res . fetchall ( ) [EOL] [EOL] conn . close ( ) [EOL] return data [EOL] [EOL] [EOL] def fetch_tag_from_song_above_threshold ( tid , threshold ) : [EOL] db_file = database ( ) [EOL] conn = sqlite3 . connect ( db_file ) [EOL] [EOL] sql = [string] [string] % ( tid , threshold ) [EOL] res = conn . execute ( sql ) [EOL] data = res . fetchall ( ) [EOL] [EOL] conn . close ( ) [EOL] return data [EOL] [EOL] [EOL] def fetch_tags_from_songs ( tids ) : [EOL] tags = { } [EOL] db_file = database ( ) [EOL] conn = sqlite3 . connect ( db_file ) [EOL] [EOL] str_representation = [string] . join ( tids ) [EOL] sql = [string] [string] % str_representation [EOL] res = conn . execute ( sql ) [EOL] data = res . fetchall ( ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] for line in data : [EOL] if line [ [number] ] in tags : [EOL] tags [ line [ [number] ] ] . append ( [ line [ [number] ] , line [ [number] ] ] ) [EOL] else : [EOL] tags [ line [ [number] ] ] = [ [ line [ [number] ] , line [ [number] ] ] ] [EOL] [EOL] return tags [EOL] [EOL] [EOL] def fetch_tags_from_songs_above_treshold ( tids , threshold ) : [EOL] tags = { } [EOL] db_file = database ( ) [EOL] conn = sqlite3 . connect ( db_file ) [EOL] [EOL] str_representation = [string] . join ( tids ) [EOL] sql = [string] [string] % ( str_representation , threshold ) [EOL] res = conn . execute ( sql ) [EOL] data = res . fetchall ( ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] for line in data : [EOL] if line [ [number] ] in tags : [EOL] tags [ line [ [number] ] ] . append ( line [ [number] ] ) [EOL] else : [EOL] tags [ line [ [number] ] ] = [ line [ [number] ] ] [EOL] [EOL] return tags [EOL] [EOL] [EOL] def fetch_all_songs ( ) : [EOL] songs = [ ] [EOL] [EOL] db_file = database ( ) [EOL] conn = sqlite3 . connect ( db_file ) [EOL] [EOL] sql = [string] [EOL] res = conn . execute ( sql ) [EOL] data = res . fetchall ( ) [EOL] [EOL] for song in data : [EOL] songs . append ( song [ [number] ] ) [EOL] [EOL] return songs [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import numpy as np [EOL] import itertools [EOL] [EOL] from src . models . max_average_net import MaxAverageNet [EOL] from src . utils . utils import load_multigpu_checkpoint_weights [EOL] [EOL] [EOL] def predict ( model ) : [EOL] x_test = [ song . rstrip ( ) for song in open ( [string] ) ] [EOL] sample_length = [number] [EOL] num_segments = [number] [EOL] [EOL] x_test_temp = np . zeros ( ( num_segments , sample_length , [number] ) ) [EOL] x_pred = np . zeros ( ( len ( x_test ) , [number] ) ) [EOL] [EOL] for i , song_id in enumerate ( x_test ) : [EOL] song = np . load ( [string] % ( [string] , song_id ) ) [ [string] ] [EOL] [EOL] for segment in range ( [number] , num_segments ) : [EOL] x_test_temp [ segment ] = song [ segment * sample_length : segment * sample_length + sample_length ] . reshape ( ( - [number] , [number] ) ) [EOL] [EOL] x_pred [ i ] = np . mean ( model . predict ( x_test_temp ) , axis = [number] ) [EOL] [EOL] return x_pred [EOL] [EOL] [docstring]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import sqlite3 [EOL] import os [EOL] import sys [EOL] import sqlite3 [EOL] [EOL] def readDatabase ( dbpath ) : [EOL] if not os . path . isfile ( dbpath ) : [EOL] print ( [string] % dbpath ) [EOL] sys . exit ( [number] ) [EOL] return dbpath [EOL] [EOL] def deleteNonMusicRelatedTags ( dbfile ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] [EOL] conn = sqlite3 . connect ( dbfile ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] conn . commit ( ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] def deleteEverythingButTop50Tags ( dbfile ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] [EOL] conn = sqlite3 . connect ( dbfile ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] sql = [string] [EOL] [comment] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] rows = cursor . fetchall ( ) [EOL] [EOL] for row in rows : [EOL] print ( row ) [EOL] [EOL] conn . commit ( ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] def deleteZeroVals ( dbfile ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] [EOL] conn = sqlite3 . connect ( dbfile ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] [EOL] print ( str ( this_function_name + [string] + str ( cursor . rowcount ) + [string] ) ) [EOL] [EOL] conn . commit ( ) [EOL] conn . close ( ) [EOL] [EOL] def vacuum ( dbfile ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] [EOL] conn = sqlite3 . connect ( dbfile ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] ) ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] def createTable ( dbfile ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] conn = sqlite3 . connect ( dbfile ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] print ( str ( this_function_name + [string] ) ) [EOL] [EOL] conn . close ( ) [EOL] [EOL] def joinMetaData ( lastfm , metadata ) : [EOL] this_function_name = sys . _getframe ( ) . f_code . co_name [EOL] [EOL] conn = sqlite3 . connect ( lastfm ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] sql = [string] + metadata + [string] [EOL] cursor . execute ( sql ) [EOL] sql = [string] [EOL] cursor . execute ( sql ) [EOL] [EOL] print ( str ( this_function_name + [string] ) ) [EOL] [EOL] conn . commit ( ) [EOL] conn . close ( ) [EOL] [EOL] [comment] [EOL] lastfm = readDatabase ( [string] ) [EOL] metadata = [string] [EOL] [EOL] deleteZeroVals ( lastfm ) [EOL] deleteNonMusicRelatedTags ( lastfm ) [EOL] deleteEverythingButTop50Tags ( lastfm ) [EOL] vacuum ( lastfm ) [EOL] createTable ( lastfm ) [EOL] joinMetaData ( lastfm , metadata )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0
from typing import Any , List [EOL] import typing [EOL] import io [EOL] import numpy as np [EOL] from iterstrat . ml_stratifiers import MultilabelStratifiedShuffleSplit [EOL] import time [EOL] [EOL] import evaluator as evaluator [EOL] from utils . utils import get_data [EOL] [EOL] learning_rates = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] [EOL] def run_experiment ( build_model , args ) : [EOL] x_train , y_train , x_valid , y_valid , x_test , y_test = get_data ( args ) [EOL] [EOL] output = open ( [string] , [string] ) [EOL] [EOL] [docstring] [EOL] base_model = build_model ( ) [EOL] [EOL] start = time . time ( ) [EOL] output . write ( [string] % ( base_model . model_name , start ) ) [EOL] print ( [string] ) [EOL] lr = learning_rates [ [number] ] [EOL] weight_name = [string] % ( base_model . model_name , lr ) [EOL] model = base_model . train ( x_train , y_train , x_valid , y_valid , epoch_size = [number] , lr = lr , weight_name = weight_name ) [EOL] [EOL] print ( [string] ) [EOL] model . load_weights ( weight_name ) [EOL] x_pred = evaluator . predict ( base_model , model , x_test ) [EOL] [EOL] [docstring] [EOL] np . save ( [string] % ( base_model . model_name , args . d , lr ) , x_pred ) [EOL] [EOL] test_result = evaluator . mean_roc_auc ( x_pred , y_test ) [EOL] print ( [string] % test_result ) [EOL] output . write ( [string] % ( lr , test_result , time . time ( ) ) ) [EOL] [EOL] [docstring] [EOL] for lr_index in range ( [number] , len ( learning_rates ) ) : [EOL] lr = learning_rates [ lr_index ] [EOL] [EOL] base_model = build_model ( ) [EOL] [EOL] print ( [string] % lr ) [EOL] weight_name = [string] % ( base_model . model_name , lr ) [EOL] model = base_model . retrain ( x_train , y_train , x_valid , y_valid , epoch_size = [number] , lr = lr , lr_prev = learning_rates [ lr_index - [number] ] , weight_name = weight_name ) [EOL] [EOL] print ( [string] ) [EOL] model . load_weights ( weight_name ) [EOL] x_pred = evaluator . predict ( base_model , model , x_test ) [EOL] [EOL] [docstring] [EOL] np . save ( [string] % ( base_model . model_name , args . d , lr ) , x_pred ) [EOL] [EOL] test_result = evaluator . mean_roc_auc ( x_pred , y_test ) [EOL] print ( [string] % test_result ) [EOL] output . write ( [string] % ( lr , test_result , time . time ( ) ) ) [EOL] [EOL] end = time . time ( ) [EOL] output . write ( [string] % ( base_model . model_name , end ) ) [EOL] [EOL] output . close ( ) [EOL] [EOL] [EOL] def run_cross_experiment ( build_model , args ) : [EOL] base_path = [string] [EOL] [EOL] for i in range ( [number] , [number] ) : [EOL] train_ids = [ song . rstrip ( ) for song in open ( base_path % ( i , [string] ) ) ] [EOL] Y_train = np . load ( base_path % ( i , [string] ) ) [EOL] x_test = [ song . rstrip ( ) for song in open ( base_path % ( i , [string] ) ) ] [EOL] y_test = np . load ( ( base_path % ( i , [string] ) ) ) [EOL] [EOL] [comment] [EOL] mskf = MultilabelStratifiedShuffleSplit ( n_splits = [number] , test_size = [number] , random_state = [number] ) [EOL] for train_idx , valid_idx in mskf . split ( train_ids , Y_train ) : [EOL] x_train = [ ] [EOL] y_train = Y_train [ train_idx ] [EOL] for idx in train_idx : [EOL] x_train . append ( train_ids [ idx ] ) [EOL] [EOL] x_valid = [ ] [EOL] y_valid = Y_train [ valid_idx ] [EOL] for idx in valid_idx : [EOL] x_valid . append ( train_ids [ idx ] ) [EOL] [EOL] output = open ( [string] , [string] ) [EOL] [EOL] [docstring] [EOL] base_model = build_model ( ) [EOL] [EOL] start = time . time ( ) [EOL] output . write ( [string] % ( i , base_model . model_name , start ) ) [EOL] [EOL] print ( [string] ) [EOL] lr = learning_rates [ [number] ] [EOL] weight_name = [string] % ( i , base_model . model_name , lr ) [EOL] model = base_model . train ( x_train , y_train , x_valid , y_valid , epoch_size = [number] , lr = lr , weight_name = weight_name ) [EOL] [EOL] print ( [string] ) [EOL] model . load_weights ( weight_name ) [EOL] x_pred = evaluator . predict ( base_model , model , x_test ) [EOL] [EOL] [docstring] [EOL] np . save ( [string] % ( i , base_model . model_name , args . d , lr ) , x_pred ) [EOL] [EOL] test_result = evaluator . mean_roc_auc ( x_pred , y_test ) [EOL] print ( [string] % test_result ) [EOL] output . write ( [string] % ( lr , test_result , time . time ( ) ) ) [EOL] [EOL] [docstring] [EOL] for lr_index in range ( [number] , len ( learning_rates ) ) : [EOL] lr = learning_rates [ lr_index ] [EOL] [EOL] base_model = build_model ( ) [EOL] [EOL] print ( [string] % lr ) [EOL] weight_name = [string] % ( i , base_model . model_name , lr ) [EOL] model = base_model . retrain ( x_train , y_train , x_valid , y_valid , epoch_size = [number] , lr = lr , lr_prev = learning_rates [ lr_index - [number] ] , weight_name = weight_name ) [EOL] [EOL] print ( [string] ) [EOL] model . load_weights ( weight_name ) [EOL] x_pred = evaluator . predict ( base_model , model , x_test ) [EOL] [EOL] [docstring] [EOL] np . save ( [string] % ( i , base_model . model_name , args . d , lr ) , x_pred ) [EOL] [EOL] test_result = evaluator . mean_roc_auc ( x_pred , y_test ) [EOL] print ( [string] % test_result ) [EOL] output . write ( [string] % ( lr , test_result , time . time ( ) ) ) [EOL] [EOL] end = time . time ( ) [EOL] output . write ( [string] % ( base_model . model_name , end ) ) [EOL] [EOL] output . close ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import evaluator [EOL] [EOL] _predictions = np . load ( [string] ) [EOL] _truths = np . load ( [string] ) . astype ( int ) [EOL] auc_score = evaluator . individual_roc_auc ( _predictions , _truths ) [EOL] [EOL] for tag in auc_score : [EOL] print ( tag ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import itertools [EOL] from typing import List [EOL] [EOL] from sklearn . metrics import roc_auc_score [EOL] import numpy as np [EOL] import pandas as pd [EOL] import seaborn as sn [EOL] import matplotlib . pyplot as plt [EOL] from scipy . sparse import csr_matrix [EOL] from scipy . sparse . csgraph import reverse_cuthill_mckee [EOL] [EOL] from utils import utils [EOL] [EOL] [EOL] [docstring] [EOL] [EOL] [EOL] def predict ( base_model , model , x_test ) : [EOL] [EOL] sample_length = base_model . dimension [ [number] ] [EOL] num_segments = utils . calculate_num_segments ( sample_length ) [EOL] [EOL] x_test_temp = np . zeros ( ( num_segments , sample_length , [number] ) ) [EOL] x_pred = np . zeros ( ( len ( x_test ) , base_model . n_labels ) ) [EOL] [EOL] for i , song_id in enumerate ( x_test ) : [EOL] song = np . load ( base_model . path % ( base_model . dataset , song_id ) ) [ [string] ] [EOL] [EOL] for segment in range ( [number] , num_segments ) : [EOL] x_test_temp [ segment ] = song [ segment * sample_length : segment * sample_length + sample_length ] . reshape ( ( - [number] , [number] ) ) [EOL] [EOL] x_pred [ i ] = np . mean ( model . predict ( x_test_temp ) , axis = [number] ) [EOL] [EOL] return x_pred [EOL] [EOL] [EOL] def make_confusion_matrix ( predictions , truths ) : [EOL] n_labels = len ( truths [ [number] ] ) [EOL] n_predictions = len ( predictions ) [EOL] cm = np . zeros ( ( n_labels , n_labels ) ) [EOL] for index in range ( n_predictions ) : [EOL] prediction = predictions [ index ] [EOL] truth = truths [ index ] [EOL] norm_prediction = prediction / sum ( prediction ) [EOL] norm_truth = truth / sum ( truth ) [EOL] pred_matrix = np . repeat ( np . array ( [ norm_prediction ] ) , n_labels , axis = [number] ) [EOL] truth_matrix = np . repeat ( norm_truth . reshape ( ( - [number] , [number] ) ) , n_labels , axis = [number] ) [EOL] cm_temp = pred_matrix * truth_matrix [EOL] cm = np . add ( cm_temp , cm ) [EOL] norm_cm = cm / cm . sum ( axis = [number] , keepdims = [number] ) [EOL] return norm_cm [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def mean_roc_auc ( predictions , truths ) : [EOL] num_predictions = len ( predictions ) [EOL] n_labels = len ( truths [ [number] ] ) [EOL] auc = np . zeros ( n_labels ) [EOL] label_truths = np . zeros ( ( n_labels , num_predictions ) ) [EOL] label_predictions = np . zeros ( ( n_labels , num_predictions ) ) [EOL] for label_index in range ( n_labels ) : [EOL] for index in range ( num_predictions ) : [EOL] label_predictions [ label_index , index ] = predictions [ index , label_index ] [EOL] label_truths [ label_index , index ] = truths [ index , label_index ] [EOL] for i in range ( n_labels ) : [EOL] truths = label_truths [ i ] [EOL] predictions = label_predictions [ i ] [EOL] auc [ i ] = roc_auc_score ( truths , predictions ) [EOL] return np . mean ( auc ) [EOL] [EOL] [EOL] def individual_roc_auc ( predictions , truths ) : [EOL] num_predictions = len ( predictions ) [EOL] n_labels = len ( truths [ [number] ] ) [EOL] auc = np . zeros ( n_labels ) [EOL] label_truths = np . zeros ( ( n_labels , num_predictions ) ) [EOL] label_predictions = np . zeros ( ( n_labels , num_predictions ) ) [EOL] for label_index in range ( n_labels ) : [EOL] for index in range ( num_predictions ) : [EOL] label_predictions [ label_index , index ] = predictions [ index , label_index ] [EOL] label_truths [ label_index , index ] = truths [ index , label_index ] [EOL] for i in range ( n_labels ) : [EOL] truths = label_truths [ i ] [EOL] predictions = label_predictions [ i ] [EOL] auc [ i ] = roc_auc_score ( truths , predictions ) [EOL] return auc [EOL] [EOL] [EOL] def plot_confusion_matrix ( predictions , truths , target_names , title = [string] , cmap = None , normalize = True ) : [EOL] cm = make_confusion_matrix ( truths , predictions ) [EOL] accuracy = np . trace ( cm ) / float ( np . sum ( cm ) ) [EOL] misclass = [number] - accuracy [EOL] if cmap is None : [EOL] cmap = plt . get_cmap ( [string] ) [EOL] plt . imshow ( cm , interpolation = [string] , cmap = cmap ) [EOL] plt . title ( title ) [EOL] plt . colorbar ( ) [EOL] if target_names is not None : [EOL] tick_marks = np . arange ( len ( target_names ) ) [EOL] plt . xticks ( tick_marks , target_names , rotation = [number] ) [EOL] plt . yticks ( tick_marks , target_names ) [EOL] if normalize : [EOL] cm = cm . astype ( [string] ) / cm . sum ( axis = [number] ) [ : , np . newaxis ] [EOL] thresh = cm . max ( ) / [number] if normalize else cm . max ( ) / [number] [EOL] for i , j in itertools . product ( range ( cm . shape [ [number] ] ) , range ( cm . shape [ [number] ] ) ) : [EOL] if normalize : [EOL] plt . text ( j , i , [string] . format ( cm [ i , j ] ) , horizontalalignment = [string] , color = [string] if cm [ i , j ] > thresh else [string] ) [EOL] else : [EOL] plt . text ( j , i , [string] . format ( cm [ i , j ] ) , horizontalalignment = [string] , color = [string] if cm [ i , j ] > thresh else [string] ) [EOL] plt . tight_layout ( ) [EOL] plt . ylabel ( [string] ) [EOL] plt . xlabel ( [string] . format ( accuracy , misclass ) ) [EOL] plt . savefig ( [string] , bbox_inches = [string] ) [EOL] [EOL] [comment] [EOL] def plot_confusion_matrix2 ( predictions , truths , labels , group = [number] ) : [EOL] genre_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] instrumental_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] gender_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] vocal_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] other_ids = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] cm = make_confusion_matrix ( truths , predictions ) [EOL] [EOL] if group == [number] : [EOL] labels = np . take ( labels , genre_ids ) [EOL] cm = np . take ( cm [ genre_ids ] , genre_ids , [number] ) [EOL] if group == [number] : [EOL] labels = np . take ( labels , instrumental_ids ) [EOL] cm = np . take ( cm [ instrumental_ids ] , instrumental_ids , [number] ) [EOL] if group == [number] : [EOL] labels = np . take ( labels , gender_ids ) [EOL] cm = np . take ( cm [ gender_ids ] , gender_ids , [number] ) [EOL] if group == [number] : [EOL] labels = np . take ( labels , vocal_ids ) [EOL] cm = np . take ( cm [ vocal_ids ] , vocal_ids , [number] ) [EOL] if group == [number] : [EOL] labels = np . take ( labels , other_ids ) [EOL] cm = np . take ( cm [ other_ids ] , other_ids , [number] ) [EOL] if group == [number] : [EOL] all_ids = genre_ids + instrumental_ids + gender_ids + vocal_ids + other_ids [EOL] labels = np . take ( labels , all_ids ) [EOL] cm = np . take ( cm [ all_ids ] , all_ids , [number] ) [EOL] print ( labels ) [EOL] df_cm = pd . DataFrame ( cm , index = [ i for i in labels ] , columns = [ i for i in labels ] ) [EOL] [comment] [EOL] sn . set ( font_scale = [number] ) [EOL] [comment] [EOL] sn . heatmap ( df_cm ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] plt . show ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] train = np . load ( [string] ) [EOL] valid = np . load ( [string] ) [EOL] test = np . load ( [string] ) [EOL] [EOL] for i in range ( [number] , [number] ) : [EOL] print ( np . sum ( train [ ... , i ] ) ) [EOL] [EOL] print ( [string] ) [EOL] [EOL] for i in range ( [number] , [number] ) : [EOL] print ( np . sum ( valid [ ... , i ] ) ) [EOL] [EOL] print ( [string] ) [EOL] [EOL] for i in range ( [number] , [number] ) : [EOL] print ( np . sum ( test [ : , i ] ) ) [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import io [EOL] import numpy as np [EOL] [EOL] [comment] [EOL] def check ( data ) : [EOL] print ( [string] % data ) [EOL] file_path = [string] % data [EOL] file = open ( file_path , [string] ) [EOL] lines = file . readlines ( ) [EOL] [EOL] for line in lines : [EOL] path = line . strip ( ) [EOL] fullPath = [string] % path [EOL] nparr = np . load ( fullPath ) [ [string] ] [EOL] sum = np . sum ( nparr ) [EOL] isNaN = np . isnan ( sum ) [EOL] isInf = np . isinf ( sum ) [EOL] if isNaN : [EOL] print ( [string] + path ) [EOL] if isInf : [EOL] print ( [string] + path ) [EOL] file . close ( ) [EOL] [EOL] check ( [string] ) [EOL] check ( [string] ) [EOL] check ( [string] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Type , Any , List [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from keras . models import Sequential [EOL] from keras . layers import Dense , Activation , Convolution1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] [EOL] from keras . layers . merge import add [EOL] from keras import backend as K [EOL] from keras . regularizers import l2 [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] from models . base_model import BaseModel [EOL] [EOL] [EOL] class ResNet ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def _shortcut ( self , input , residual ) : [EOL] channel = [number] [EOL] step = [number] [EOL] input_shape = K . int_shape ( input ) [EOL] residual_shape = K . int_shape ( residual ) [EOL] stride = int ( round ( input_shape [ step ] / residual_shape [ step ] , [number] ) ) [EOL] equal_channels = input_shape [ channel ] == residual_shape [ channel ] [EOL] [EOL] shortcut = input [EOL] [comment] [EOL] if stride > [number] or not equal_channels : [EOL] shortcut = Conv1D ( residual_shape [ channel ] , kernel_size = [number] , strides = stride , padding = [string] , kernel_initializer = [string] , kernel_regularizer = l2 ( [number] ) ) ( input ) [EOL] [EOL] return add ( [ shortcut , residual ] ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] [EOL] pool_input = Input ( shape = self . input_shape ) [EOL] [EOL] conv0 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] [comment] [EOL] conv1 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ0 ) [EOL] bn1 = BatchNormalization ( name = [string] ) ( conv1 ) [EOL] activ1 = Activation ( activ , name = [string] ) ( bn1 ) [EOL] [EOL] conv2 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ1 ) [EOL] bn2 = BatchNormalization ( name = [string] ) ( conv2 ) [EOL] activ2 = Activation ( activ , name = [string] ) ( bn2 ) [EOL] [EOL] res1 = self . _shortcut ( activ0 , activ2 ) [EOL] [EOL] [comment] [EOL] conv3 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( res1 ) [EOL] bn3 = BatchNormalization ( name = [string] ) ( conv3 ) [EOL] activ3 = Activation ( activ , name = [string] ) ( bn3 ) [EOL] [EOL] conv4 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ3 ) [EOL] bn4 = BatchNormalization ( name = [string] ) ( conv4 ) [EOL] activ4 = Activation ( activ , name = [string] ) ( bn4 ) [EOL] [EOL] res2 = self . _shortcut ( res1 , activ4 ) [EOL] [EOL] [comment] [EOL] conv5 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( res2 ) [EOL] bn5 = BatchNormalization ( name = [string] ) ( conv5 ) [EOL] activ5 = Activation ( activ , name = [string] ) ( bn5 ) [EOL] [EOL] conv6 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ5 ) [EOL] bn6 = BatchNormalization ( name = [string] ) ( conv6 ) [EOL] activ6 = Activation ( activ , name = [string] ) ( bn6 ) [EOL] [EOL] res3 = self . _shortcut ( res2 , activ6 ) [EOL] [EOL] [comment] [EOL] conv7 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( res3 ) [EOL] bn7 = BatchNormalization ( name = [string] ) ( conv7 ) [EOL] activ7 = Activation ( activ , name = [string] ) ( bn7 ) [EOL] [EOL] conv8 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ7 ) [EOL] bn8 = BatchNormalization ( name = [string] ) ( conv8 ) [EOL] activ8 = Activation ( activ , name = [string] ) ( bn8 ) [EOL] [EOL] res4 = self . _shortcut ( res3 , activ8 ) [EOL] [EOL] Flattened = Flatten ( ) ( res4 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( input = pool_input , output = output ) [EOL] [EOL] return model [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0
from typing import Type , Any , List [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from keras . models import Sequential [EOL] from keras . layers import Dense , Activation , Convolution1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] [EOL] from keras . layers . merge import add [EOL] from keras import backend as K [EOL] from keras . regularizers import l2 [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] from models . base_model import BaseModel [EOL] [EOL] [EOL] class SampleCNNDeepResNet ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def _shortcut ( self , input , residual ) : [EOL] channel = [number] [EOL] step = [number] [EOL] input_shape = K . int_shape ( input ) [EOL] residual_shape = K . int_shape ( residual ) [EOL] stride = int ( round ( input_shape [ step ] / residual_shape [ step ] , [number] ) ) [EOL] equal_channels = input_shape [ channel ] == residual_shape [ channel ] [EOL] [EOL] shortcut = input [EOL] [comment] [EOL] if stride > [number] or not equal_channels : [EOL] shortcut = Conv1D ( residual_shape [ channel ] , kernel_size = [number] , strides = stride , padding = [string] , kernel_initializer = [string] , kernel_regularizer = l2 ( [number] ) ) ( input ) [EOL] [EOL] return add ( [ shortcut , residual ] ) [EOL] [EOL] def generate_stacked_convolutions ( self , input_layer , filters ) : [EOL] activation = [string] [EOL] init = [string] [EOL] output_layer = input_layer [EOL] for i in range ( [number] ) : [EOL] conv = Conv1D ( filters , [number] , padding = [string] , kernel_initializer = init ) ( output_layer ) [EOL] bn = BatchNormalization ( ) ( conv ) [EOL] activ = Activation ( activation ) ( bn ) [EOL] output_layer = activ [EOL] return self . _shortcut ( input_layer , output_layer ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] [EOL] pool_input = Input ( shape = self . input_shape ) [EOL] [EOL] conv0 = Conv1D ( [number] , [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] deep1 = self . generate_stacked_convolutions ( activ0 , [number] ) [EOL] MP1 = MaxPooling1D ( pool_size = [number] ) ( deep1 ) [EOL] [EOL] deep2 = self . generate_stacked_convolutions ( MP1 , [number] ) [EOL] MP2 = MaxPooling1D ( pool_size = [number] ) ( deep2 ) [EOL] [EOL] deep3 = self . generate_stacked_convolutions ( MP2 , [number] ) [EOL] MP3 = MaxPooling1D ( pool_size = [number] ) ( deep3 ) [EOL] [EOL] deep4 = self . generate_stacked_convolutions ( MP3 , [number] ) [EOL] MP4 = MaxPooling1D ( pool_size = [number] ) ( deep4 ) [EOL] [EOL] deep5 = self . generate_stacked_convolutions ( MP4 , [number] ) [EOL] MP5 = MaxPooling1D ( pool_size = [number] ) ( deep5 ) [EOL] [EOL] deep6 = self . generate_stacked_convolutions ( MP5 , [number] ) [EOL] MP6 = MaxPooling1D ( pool_size = [number] ) ( deep6 ) [EOL] [EOL] deep7 = self . generate_stacked_convolutions ( MP6 , [number] ) [EOL] MP7 = MaxPooling1D ( pool_size = [number] ) ( deep7 ) [EOL] [EOL] deep8 = self . generate_stacked_convolutions ( MP7 , [number] ) [EOL] MP8 = MaxPooling1D ( pool_size = [number] ) ( deep8 ) [EOL] [EOL] deep9 = self . generate_stacked_convolutions ( MP8 , [number] ) [EOL] MP9 = MaxPooling1D ( pool_size = [number] ) ( deep9 ) [EOL] [EOL] deep10 = self . generate_stacked_convolutions ( MP9 , [number] ) [EOL] dropout1 = Dropout ( [number] ) ( deep10 ) [EOL] [EOL] Flattened = Flatten ( ) ( dropout1 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( input = pool_input , output = output ) [EOL] [EOL] return model [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0
from typing import Type , Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from models . base_model import BaseModel [EOL] [EOL] from keras . layers import Dense , Activation , Convolution1D , AveragePooling1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] from keras . layers import concatenate [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] [EOL] from utils . MixedMaxAvgPooling1D import MixedMaxAvgPooling1D [EOL] [EOL] [EOL] class MixedRegion ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL] [EOL] def max_average_pooling ( self , input_layer , filters , pool_size = [number] ) : [EOL] max_pooling = MaxPooling1D ( pool_size ) ( input_layer ) [EOL] average_pooling = AveragePooling1D ( pool_size ) ( input_layer ) [EOL] alpha = Conv1D ( filters , kernel_size = pool_size , activation = [string] ) [EOL] [EOL] return concatenate ( [ max_pooling , average_pooling ] ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] [EOL] pool_input = Input ( shape = ( self . input_dim , [number] ) ) [EOL] [EOL] conv0 = Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] conv1 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ0 ) [EOL] bn1 = BatchNormalization ( ) ( conv1 ) [EOL] activ1 = Activation ( activ ) ( bn1 ) [EOL] input_1 = self . input_dim / [number] [EOL] max_average1 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_1 , pool_size = [number] ) ( activ1 ) [EOL] [EOL] conv2 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average1 ) [EOL] bn2 = BatchNormalization ( ) ( conv2 ) [EOL] activ2 = Activation ( activ ) ( bn2 ) [EOL] input_2 = input_1 / [number] [EOL] max_average2 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_2 , pool_size = [number] ) ( activ2 ) [EOL] [EOL] conv3 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average2 ) [EOL] bn3 = BatchNormalization ( ) ( conv3 ) [EOL] activ3 = Activation ( activ ) ( bn3 ) [EOL] input_3 = input_2 / [number] [EOL] max_average3 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_3 , pool_size = [number] ) ( activ3 ) [EOL] [EOL] conv4 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average3 ) [EOL] bn4 = BatchNormalization ( ) ( conv4 ) [EOL] activ4 = Activation ( activ ) ( bn4 ) [EOL] input_4 = input_3 / [number] [EOL] max_average4 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_4 , pool_size = [number] ) ( activ4 ) [EOL] [EOL] conv5 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average4 ) [EOL] bn5 = BatchNormalization ( ) ( conv5 ) [EOL] activ5 = Activation ( activ ) ( bn5 ) [EOL] input_5 = input_4 / [number] [EOL] max_average5 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_5 , pool_size = [number] ) ( activ5 ) [EOL] [EOL] conv6 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average5 ) [EOL] bn6 = BatchNormalization ( ) ( conv6 ) [EOL] activ6 = Activation ( activ ) ( bn6 ) [EOL] input_6 = input_5 / [number] [EOL] max_average6 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_6 , pool_size = [number] ) ( activ6 ) [EOL] [EOL] conv7 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average6 ) [EOL] bn7 = BatchNormalization ( ) ( conv7 ) [EOL] activ7 = Activation ( activ ) ( bn7 ) [EOL] input_7 = input_6 / [number] [EOL] max_average7 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_7 , pool_size = [number] ) ( activ7 ) [EOL] [EOL] conv8 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average7 ) [EOL] bn8 = BatchNormalization ( ) ( conv8 ) [EOL] activ8 = Activation ( activ ) ( bn8 ) [EOL] input_8 = input_7 / [number] [EOL] max_average8 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_8 , pool_size = [number] ) ( activ8 ) [EOL] [EOL] conv9 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average8 ) [EOL] bn9 = BatchNormalization ( ) ( conv9 ) [EOL] activ9 = Activation ( activ ) ( bn9 ) [EOL] input_9 = input_8 / [number] [EOL] max_average9 = MixedMaxAvgPooling1D ( name = [string] , method = [string] , alpha = None , input_dim = input_9 , pool_size = [number] ) ( activ9 ) [EOL] [EOL] conv10 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average9 ) [EOL] bn10 = BatchNormalization ( ) ( conv10 ) [EOL] activ10 = Activation ( activ ) ( bn10 ) [EOL] dropout1 = Dropout ( [number] ) ( activ10 ) [EOL] [EOL] Flattened = Flatten ( ) ( dropout1 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( inputs = pool_input , outputs = output ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
from typing import Type , Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from models . base_model import BaseModel [EOL] [EOL] from keras . layers import Dense , Activation , Convolution1D , AveragePooling1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] from keras . layers import concatenate [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] [EOL] [EOL] class SampleCNNMaxAverage ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL] [EOL] def max_average_pooling ( self , input_layer , pool_size = [number] ) : [EOL] max_pooling = MaxPooling1D ( pool_size ) ( input_layer ) [EOL] average_pooling = AveragePooling1D ( pool_size ) ( input_layer ) [EOL] return concatenate ( [ max_pooling , average_pooling ] ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] [EOL] pool_input = Input ( shape = ( self . input_dim , [number] ) ) [EOL] [EOL] conv0 = Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] conv1 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ0 ) [EOL] bn1 = BatchNormalization ( ) ( conv1 ) [EOL] activ1 = Activation ( activ ) ( bn1 ) [EOL] MP1 = MaxPooling1D ( pool_length = [number] ) ( activ1 ) [EOL] [EOL] conv2 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP1 ) [EOL] bn2 = BatchNormalization ( ) ( conv2 ) [EOL] activ2 = Activation ( activ ) ( bn2 ) [EOL] MP2 = MaxPooling1D ( pool_length = [number] ) ( activ2 ) [EOL] [EOL] conv3 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP2 ) [EOL] bn3 = BatchNormalization ( ) ( conv3 ) [EOL] activ3 = Activation ( activ ) ( bn3 ) [EOL] MP3 = MaxPooling1D ( pool_length = [number] ) ( activ3 ) [EOL] [EOL] conv4 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP3 ) [EOL] bn4 = BatchNormalization ( ) ( conv4 ) [EOL] activ4 = Activation ( activ ) ( bn4 ) [EOL] MP4 = MaxPooling1D ( pool_length = [number] ) ( activ4 ) [EOL] [EOL] conv5 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP4 ) [EOL] bn5 = BatchNormalization ( ) ( conv5 ) [EOL] activ5 = Activation ( activ ) ( bn5 ) [EOL] MP5 = MaxPooling1D ( pool_length = [number] ) ( activ5 ) [EOL] [EOL] conv6 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP5 ) [EOL] bn6 = BatchNormalization ( ) ( conv6 ) [EOL] activ6 = Activation ( activ ) ( bn6 ) [EOL] MP6 = MaxPooling1D ( pool_length = [number] ) ( activ6 ) [EOL] [EOL] conv7 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP6 ) [EOL] bn7 = BatchNormalization ( ) ( conv7 ) [EOL] activ7 = Activation ( activ ) ( bn7 ) [EOL] MP7 = MaxPooling1D ( pool_length = [number] ) ( activ7 ) [EOL] [EOL] conv8 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP7 ) [EOL] bn8 = BatchNormalization ( ) ( conv8 ) [EOL] activ8 = Activation ( activ ) ( bn8 ) [EOL] MP8 = MaxPooling1D ( pool_length = [number] ) ( activ8 ) [EOL] [EOL] conv9 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( MP8 ) [EOL] bn9 = BatchNormalization ( ) ( conv9 ) [EOL] activ9 = Activation ( activ ) ( bn9 ) [EOL] max_average = self . max_average_pooling ( activ9 ) [EOL] [EOL] conv10 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average ) [EOL] bn10 = BatchNormalization ( ) ( conv10 ) [EOL] activ10 = Activation ( activ ) ( bn10 ) [EOL] dropout1 = Dropout ( [number] ) ( activ10 ) [EOL] [EOL] Flattened = Flatten ( ) ( dropout1 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( input = pool_input , output = output ) [EOL] [EOL] return model	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$
from typing import Type , Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from models . base_model import BaseModel [EOL] [EOL] from keras . layers import Dense , Activation , Convolution1D , AveragePooling1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] from keras . layers import concatenate [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] [EOL] from utils . MixedMaxAvgPooling1D import MixedMaxAvgPooling1D [EOL] [EOL] [EOL] class MaxAverageNet ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL] [EOL] def max_average_pooling ( self , input_layer , pool_size = [number] ) : [EOL] max_pooling = MaxPooling1D ( pool_size ) ( input_layer ) [EOL] average_pooling = AveragePooling1D ( pool_size ) ( input_layer ) [EOL] return concatenate ( [ max_pooling , average_pooling ] ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] [EOL] pool_input = Input ( shape = ( self . input_dim , [number] ) ) [EOL] [EOL] conv0 = Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] conv1 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ0 ) [EOL] bn1 = BatchNormalization ( ) ( conv1 ) [EOL] activ1 = Activation ( activ ) ( bn1 ) [EOL] input_dim1 = self . input_dim / [number] [EOL] max_average1 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim1 , pool_size = [number] ) ( activ1 ) [EOL] [EOL] conv2 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average1 ) [EOL] bn2 = BatchNormalization ( ) ( conv2 ) [EOL] activ2 = Activation ( activ ) ( bn2 ) [EOL] input_dim2 = input_dim1 / [number] [EOL] max_average2 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim2 , pool_size = [number] ) ( activ2 ) [EOL] [EOL] conv3 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average2 ) [EOL] bn3 = BatchNormalization ( ) ( conv3 ) [EOL] activ3 = Activation ( activ ) ( bn3 ) [EOL] input_dim3 = input_dim2 / [number] [EOL] max_average3 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim3 , pool_size = [number] ) ( activ3 ) [EOL] [EOL] conv4 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average3 ) [EOL] bn4 = BatchNormalization ( ) ( conv4 ) [EOL] activ4 = Activation ( activ ) ( bn4 ) [EOL] input_dim4 = input_dim3 / [number] [EOL] max_average4 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim4 , pool_size = [number] ) ( activ4 ) [EOL] [EOL] conv5 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average4 ) [EOL] bn5 = BatchNormalization ( ) ( conv5 ) [EOL] activ5 = Activation ( activ ) ( bn5 ) [EOL] input_dim5 = input_dim4 / [number] [EOL] max_average5 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim5 , pool_size = [number] ) ( activ5 ) [EOL] [EOL] conv6 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average5 ) [EOL] bn6 = BatchNormalization ( ) ( conv6 ) [EOL] activ6 = Activation ( activ ) ( bn6 ) [EOL] input_dim6 = input_dim5 / [number] [EOL] max_average6 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim6 , pool_size = [number] ) ( activ6 ) [EOL] [EOL] conv7 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average6 ) [EOL] bn7 = BatchNormalization ( ) ( conv7 ) [EOL] activ7 = Activation ( activ ) ( bn7 ) [EOL] input_dim7 = input_dim6 / [number] [EOL] max_average7 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim7 , pool_size = [number] ) ( activ7 ) [EOL] [EOL] conv8 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average7 ) [EOL] bn8 = BatchNormalization ( ) ( conv8 ) [EOL] activ8 = Activation ( activ ) ( bn8 ) [EOL] input_dim8 = input_dim7 / [number] [EOL] max_average8 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim8 , pool_size = [number] ) ( activ8 ) [EOL] [EOL] conv9 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average8 ) [EOL] bn9 = BatchNormalization ( ) ( conv9 ) [EOL] activ9 = Activation ( activ ) ( bn9 ) [EOL] input_dim9 = input_dim8 / [number] [EOL] max_average9 = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , input_dim = input_dim9 , pool_size = [number] ) ( activ9 ) [EOL] [EOL] conv10 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average9 ) [EOL] bn10 = BatchNormalization ( ) ( conv10 ) [EOL] activ10 = Activation ( activ ) ( bn10 ) [EOL] dropout1 = Dropout ( [number] ) ( activ10 ) [EOL] [EOL] Flattened = Flatten ( ) ( dropout1 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( inputs = pool_input , outputs = output ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
from typing import Type , Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from models . base_model import BaseModel [EOL] [EOL] from keras . layers import Dense , Activation , Convolution1D , AveragePooling1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] from keras . layers import concatenate [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] [EOL] from utils . mixed_pooling import mixed_pooling [EOL] [EOL] from utils . MixedMaxAvgPooling1D import MixedMaxAvgPooling1D [EOL] [EOL] [EOL] class MixedNet ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL] [EOL] def max_average_pooling ( self , input_layer , pool_size = [number] ) : [EOL] max_pooling = MaxPooling1D ( pool_size ) ( input_layer ) [EOL] average_pooling = AveragePooling1D ( pool_size ) ( input_layer ) [EOL] return concatenate ( [ max_pooling , average_pooling ] ) [EOL] [EOL] def build_model ( self ) : [EOL] activ = [string] [EOL] init = [string] [EOL] max_average = MixedMaxAvgPooling1D ( name = [string] , alpha = None , method = [string] , pool_size = [number] ) [EOL] [EOL] pool_input = Input ( shape = ( self . input_dim , [number] ) ) [EOL] [EOL] conv0 = Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( pool_input ) [EOL] bn0 = BatchNormalization ( name = [string] ) ( conv0 ) [EOL] activ0 = Activation ( activ , name = [string] ) ( bn0 ) [EOL] [EOL] conv1 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init , name = [string] ) ( activ0 ) [EOL] bn1 = BatchNormalization ( ) ( conv1 ) [EOL] activ1 = Activation ( activ ) ( bn1 ) [EOL] max_average1 = max_average ( activ1 ) [EOL] [EOL] conv2 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average1 ) [EOL] bn2 = BatchNormalization ( ) ( conv2 ) [EOL] activ2 = Activation ( activ ) ( bn2 ) [EOL] max_average2 = max_average ( activ2 ) [EOL] [EOL] conv3 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average2 ) [EOL] bn3 = BatchNormalization ( ) ( conv3 ) [EOL] activ3 = Activation ( activ ) ( bn3 ) [EOL] max_average3 = max_average ( activ3 ) [EOL] [EOL] conv4 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average3 ) [EOL] bn4 = BatchNormalization ( ) ( conv4 ) [EOL] activ4 = Activation ( activ ) ( bn4 ) [EOL] max_average4 = max_average ( activ4 ) [EOL] [EOL] conv5 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average4 ) [EOL] bn5 = BatchNormalization ( ) ( conv5 ) [EOL] activ5 = Activation ( activ ) ( bn5 ) [EOL] max_average5 = max_average ( activ5 ) [EOL] [EOL] conv6 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average5 ) [EOL] bn6 = BatchNormalization ( ) ( conv6 ) [EOL] activ6 = Activation ( activ ) ( bn6 ) [EOL] max_average6 = max_average ( activ6 ) [EOL] [EOL] conv7 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average6 ) [EOL] bn7 = BatchNormalization ( ) ( conv7 ) [EOL] activ7 = Activation ( activ ) ( bn7 ) [EOL] max_average7 = max_average ( activ7 ) [EOL] [EOL] conv8 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average7 ) [EOL] bn8 = BatchNormalization ( ) ( conv8 ) [EOL] activ8 = Activation ( activ ) ( bn8 ) [EOL] max_average8 = max_average ( activ8 ) [EOL] [EOL] conv9 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average8 ) [EOL] bn9 = BatchNormalization ( ) ( conv9 ) [EOL] activ9 = Activation ( activ ) ( bn9 ) [EOL] max_average9 = max_average ( activ9 ) [EOL] [EOL] conv10 = Conv1D ( [number] , [number] , padding = [string] , kernel_initializer = init ) ( max_average9 ) [EOL] bn10 = BatchNormalization ( ) ( conv10 ) [EOL] activ10 = Activation ( activ ) ( bn10 ) [EOL] dropout1 = Dropout ( [number] ) ( activ10 ) [EOL] [EOL] Flattened = Flatten ( ) ( dropout1 ) [EOL] [EOL] output = Dense ( self . n_labels , activation = [string] ) ( Flattened ) [EOL] model = Model ( inputs = pool_input , outputs = output ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] from keras . models import Sequential [EOL] from keras . layers import Dense , Activation , Convolution1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] [EOL] def build_model ( ) : [EOL] model = Sequential ( ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , activation = [string] , input_shape = ( [number] , [number] ) ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Flatten ( ) ) [EOL] model . add ( Dense ( [number] , activation = [string] ) ) [EOL] [EOL] return model	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Type , Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] import src [EOL] from typing import List , Tuple [EOL] import numpy as np [EOL] from keras import Input , Model [EOL] [EOL] from models . base_model import BaseModel [EOL] [EOL] from keras . models import Sequential [EOL] from keras . layers import Dense , Activation , Convolution1D [EOL] from keras . layers import Conv1D [EOL] from keras . layers import MaxPooling1D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] from keras . layers import BatchNormalization [EOL] [EOL] from keras . layers . merge import add [EOL] from keras import backend as K [EOL] [EOL] from utils . utils import calculate_num_segments [EOL] [EOL] [EOL] class SampleCNN39 ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] input_dim = [number] * [number] ** [number] [EOL] overlap = [number] [EOL] [EOL] def transform_data ( self , ids_temp , labels_temp , batch_size ) : [EOL] num_segments = calculate_num_segments ( self . input_dim ) [EOL] new_batch_size = batch_size * num_segments [EOL] [EOL] [comment] [EOL] x = np . empty ( ( new_batch_size , * self . dimension , self . n_channels ) , dtype = [string] ) [EOL] y = np . empty ( ( new_batch_size , len ( labels_temp [ [number] ] ) ) ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , song_id in enumerate ( ids_temp ) : [EOL] song = np . load ( [string] % ( self . dataset , song_id ) ) [EOL] [EOL] song_temp = None [EOL] try : [EOL] song_temp = song [ [string] ] [EOL] except : [EOL] print ( song_id ) [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( song_temp , num_segments ) [EOL] [EOL] for sub_song in sub_signals : [EOL] sub_song = sub_song . reshape ( ( - [number] , [number] ) ) [EOL] x [ count , ] = sub_song [EOL] y [ count ] = labels_temp [ i ] [EOL] [EOL] count += [number] [EOL] [EOL] return x , y [EOL] [EOL] def split_song ( self , song , num_segments ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = self . input_dim [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i * chunk : i * chunk + chunk ] for i in range ( [number] , num_segments ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song ) [EOL] [EOL] def build_model ( self ) : [EOL] model = Sequential ( ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , kernel_size = [number] , strides = [number] , padding = [string] , activation = [string] , input_shape = self . input_shape ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( MaxPooling1D ( pool_size = [number] , strides = [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv1D ( [number] , [number] , strides = [number] , padding = [string] , activation = [string] ) ) [EOL] model . add ( BatchNormalization ( ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Flatten ( ) ) [EOL] model . add ( Dense ( self . n_labels , activation = [string] ) ) [EOL] [EOL] return model	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$
from typing import Any , Tuple , List [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] from typing import List , Tuple [EOL] [EOL] from models . base_model import BaseModel [EOL] import utils . gtzan_genres as gtzan [EOL] [EOL] import numpy as np [EOL] import librosa [EOL] [EOL] from keras . models import Sequential [EOL] from keras . layers import Dense [EOL] from keras . layers import Conv2D [EOL] from keras . layers import MaxPooling2D [EOL] from keras . layers import Dropout [EOL] from keras . layers import Flatten [EOL] [EOL] [EOL] class Basic2DCNN ( BaseModel ) : [EOL] [EOL] model_name = [string] [EOL] [EOL] def transform_data ( self , ids_temp , batch_size ) : [EOL] [EOL] batch_size = [number] * batch_size [EOL] [EOL] [comment] [EOL] X = np . empty ( ( batch_size , * self . dimension , self . n_channels ) ) [EOL] y = np . empty ( batch_size , dtype = int ) [EOL] [EOL] count = [number] [EOL] [comment] [EOL] for i , id in enumerate ( ids_temp ) : [EOL] x = np . load ( [string] + id ) [EOL] genre = id . split ( [string] ) [ [number] ] [EOL] [EOL] [comment] [EOL] sub_signals = self . split_song ( x ) [EOL] [EOL] for song in sub_signals : [EOL] [EOL] [comment] [EOL] mel_spectogram = librosa . feature . melspectrogram ( song , n_fft = [number] , hop_length = [number] ) [ : , : , np . newaxis ] [EOL] mel_spectogram = np . array ( list ( mel_spectogram ) ) [EOL] [EOL] X [ count , ] = mel_spectogram [EOL] y [ count ] = gtzan . genres [ genre ] [EOL] [EOL] count += [number] [EOL] [EOL] return X , y [EOL] [EOL] def build_model ( self ) : [EOL] model = Sequential ( ) [EOL] [EOL] [comment] [EOL] model . add ( Conv2D ( [number] , kernel_size = ( [number] , [number] ) , strides = ( [number] , [number] ) , activation = [string] , input_shape = self . input_shape ) ) [EOL] model . add ( MaxPooling2D ( pool_size = ( [number] , [number] ) , strides = ( [number] , [number] ) ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv2D ( [number] , ( [number] , [number] ) , strides = ( [number] , [number] ) , activation = [string] ) ) [EOL] model . add ( MaxPooling2D ( pool_size = ( [number] , [number] ) , strides = ( [number] , [number] ) ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv2D ( [number] , ( [number] , [number] ) , strides = ( [number] , [number] ) , activation = [string] ) ) [EOL] model . add ( MaxPooling2D ( pool_size = ( [number] , [number] ) , strides = ( [number] , [number] ) ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv2D ( [number] , ( [number] , [number] ) , strides = ( [number] , [number] ) , activation = [string] ) ) [EOL] model . add ( MaxPooling2D ( pool_size = ( [number] , [number] ) , strides = ( [number] , [number] ) ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Conv2D ( [number] , ( [number] , [number] ) , strides = ( [number] , [number] ) , activation = [string] ) ) [EOL] model . add ( MaxPooling2D ( pool_size = ( [number] , [number] ) , strides = ( [number] , [number] ) ) ) [EOL] model . add ( Dropout ( [number] ) ) [EOL] [EOL] [comment] [EOL] model . add ( Flatten ( ) ) [EOL] model . add ( Dense ( self . n_labels , activation = [string] ) ) [EOL] [EOL] return model [EOL] [EOL] def split_song ( self , song , window = [number] , overlap = [number] ) : [EOL] [comment] [EOL] temp_song = [ ] [EOL] [EOL] [comment] [EOL] x_shape = song . shape [ [number] ] [EOL] chunk = int ( x_shape * window ) [EOL] offset = int ( chunk * ( [number] - overlap ) ) [EOL] [EOL] [comment] [EOL] splitted_song = [ song [ i : i + chunk ] for i in range ( [number] , x_shape - chunk + offset , offset ) ] [EOL] for sub_song in splitted_song : [EOL] if len ( sub_song ) == chunk : [EOL] temp_song . append ( sub_song ) [EOL] [EOL] return np . array ( temp_song )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0
from typing import Any [EOL] import typing [EOL] from __future__ import absolute_import [EOL] from __future__ import print_function [EOL] [EOL] import keras [EOL] from keras import backend as K [EOL] [EOL] [EOL] class LossLearningRateScheduler ( keras . callbacks . History ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , base_lr , lookback_epochs , spike_epochs = None , spike_multiple = [number] , decay_threshold = [number] , decay_multiple = [number] , loss_type = [string] ) : [EOL] [EOL] super ( LossLearningRateScheduler , self ) . __init__ ( ) [EOL] [EOL] self . base_lr = base_lr [EOL] self . lookback_epochs = lookback_epochs [EOL] self . spike_epochs = spike_epochs [EOL] self . spike_multiple = spike_multiple [EOL] self . decay_threshold = decay_threshold [EOL] self . decay_multiple = decay_multiple [EOL] self . loss_type = loss_type [EOL] [EOL] def on_epoch_begin ( self , epoch , logs = None ) : [EOL] [EOL] if len ( self . epoch ) > self . lookback_epochs : [EOL] [EOL] current_lr = K . get_value ( self . model . optimizer . lr ) [EOL] [EOL] target_loss = self . history [ self . loss_type ] [EOL] [EOL] last_loss = target_loss [ - [number] ] [EOL] [EOL] change_lr = False [EOL] for i in range ( [number] , self . lookback_epochs ) : [EOL] if last_loss > target_loss [ - i ] : [EOL] change_lr = True [EOL] break [EOL] last_loss = target_loss [ - i ] [EOL] [EOL] if change_lr : [EOL] [EOL] print ( [string] . join ( ( [string] , str ( current_lr ) , [string] , str ( current_lr * self . decay_multiple ) ) ) ) [EOL] K . set_value ( self . model . optimizer . lr , current_lr * self . decay_multiple ) [EOL] current_lr = current_lr * self . decay_multiple [EOL] [EOL] else : [EOL] [EOL] print ( [string] . join ( ( [string] , str ( current_lr ) ) ) ) [EOL] [EOL] if self . spike_epochs is not None and len ( self . epoch ) in self . spike_epochs : [EOL] print ( [string] . join ( ( [string] , str ( current_lr ) , [string] , str ( current_lr * self . spike_multiple ) ) ) ) [EOL] K . set_value ( self . model . optimizer . lr , current_lr * self . spike_multiple ) [EOL] [EOL] else : [EOL] [EOL] print ( [string] . join ( ( [string] , str ( self . base_lr ) ) ) ) [EOL] K . set_value ( self . model . optimizer . lr , self . base_lr ) [EOL] [EOL] return K . get_value ( self . model . optimizer . lr ) [EOL] [EOL] [EOL] def main ( ) : [EOL] return [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import numpy as np [EOL] import sqllite_repository as sql [EOL] import utils . msd_tags as msd [EOL] [EOL] [EOL] def convert_tags_to_npy ( ids , npy_path ) : [EOL] y = np . empty ( ( len ( ids ) , len ( msd . TAGS ) ) , dtype = bool ) [EOL] tid_tag = sql . fetch_tags_from_songs_above_treshold ( ids , [number] ) [EOL] if len ( tid_tag ) != len ( ids ) : [EOL] print ( [string] % npy_path ) [EOL] return [EOL] for i , song in enumerate ( ids ) : [EOL] tags = [ ] [EOL] for tag in msd . TAGS : [EOL] if tag in tid_tag [ song ] : [EOL] tags . append ( True ) [EOL] else : [EOL] tags . append ( False ) [EOL] y [ i ] = tags [EOL] np . savez_compressed ( npy_path , y ) [EOL] [EOL] [EOL] train_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( [string] ) ] [EOL] valid_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( [string] ) ] [EOL] test_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( [string] ) ] [EOL] [EOL] convert_tags_to_npy ( train_ids , [string] ) [EOL] convert_tags_to_npy ( valid_ids , [string] ) [EOL] convert_tags_to_npy ( test_ids , [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0
from typing import Any [EOL] import typing [EOL] import tensorflow as tf [EOL] from keras . layers import MaxPooling1D , AveragePooling1D , add , K , RepeatVector [EOL] [EOL] [EOL] [EOL] [comment] [EOL] def mixed_pooling ( inputs , alpha , size = [number] ) : [EOL] [docstring] [EOL] if alpha == - [number] : [EOL] alpha = tf . Variable ( initial_value = [number] , trainable = True ) [EOL] x1 = MaxPooling1D ( pool_size = size , strides = [number] ) ( inputs ) [EOL] x2 = AveragePooling1D ( pool_size = size , strides = [number] ) ( inputs ) [EOL] outputs = add ( [ tf . multiply ( x1 , alpha ) , x2 ] ) [EOL] [comment] [EOL] return [ alpha , outputs ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Union , Any , Literal [EOL] import typing_extensions [EOL] import typing [EOL] import tensorflow as tf [EOL] from keras . engine import InputSpec [EOL] from keras . engine . topology import Layer [EOL] from keras . initializers import RandomUniform [EOL] from keras . layers import MaxPooling1D , AveragePooling1D , multiply , add , conv_utils [EOL] [EOL] [EOL] class MixedMaxAvgPooling1D ( Layer ) : [EOL] [EOL] def __init__ ( self , name , alpha , method = [string] , input_dim = None , pool_size = [number] , strides = None , padding = [string] , data_format = [string] , ** kwargs ) : [EOL] if strides is None : [EOL] strides = pool_size [EOL] self . pool_size = conv_utils . normalize_tuple ( pool_size , [number] , [string] ) [EOL] self . strides = conv_utils . normalize_tuple ( strides , [number] , [string] ) [EOL] self . padding = conv_utils . normalize_padding ( padding ) [EOL] self . data_format = data_format [EOL] self . input_spec = InputSpec ( ndim = [number] ) [EOL] self . alpha = alpha [EOL] self . method = method [EOL] self . name = name [EOL] self . input_dim = input_dim [EOL] super ( MixedMaxAvgPooling1D , self ) . __init__ ( ** kwargs ) [EOL] [EOL] def build ( self , input_shape ) : [EOL] if self . alpha is None : [EOL] if [string] not in self . method : [EOL] self . alpha = self . add_weight ( name = self . name , shape = ( [number] , ) , initializer = RandomUniform ( minval = [number] , maxval = [number] ) , constraint = lambda t : tf . clip_by_value ( t , [number] , [number] ) ) [EOL] else : [EOL] self . alpha = self . add_weight ( name = self . name , shape = ( int ( self . input_dim / [number] ) , [number] ) , initializer = RandomUniform ( minval = [number] , maxval = [number] ) , constraint = lambda t : tf . clip_by_value ( t , [number] , [number] ) ) [EOL] super ( MixedMaxAvgPooling1D , self ) . build ( input_shape ) [comment] [EOL] [EOL] def call ( self , inputs ) : [EOL] dummy_axis = [number] if self . data_format == [string] else [number] [EOL] max = MaxPooling1D ( pool_size = [number] , strides = [number] ) ( inputs ) [EOL] avg = AveragePooling1D ( pool_size = [number] , strides = [number] ) ( inputs ) [EOL] outputs = add ( [ tf . multiply ( max , self . alpha ) , tf . multiply ( avg , [number] - self . alpha ) ] ) [EOL] return outputs [EOL] [EOL] def compute_output_shape ( self , input_shape ) : [EOL] if self . data_format == [string] : [EOL] steps = input_shape [ [number] ] [EOL] features = input_shape [ [number] ] [EOL] else : [EOL] steps = input_shape [ [number] ] [EOL] features = input_shape [ [number] ] [EOL] length = conv_utils . conv_output_length ( steps , self . pool_size [ [number] ] , self . padding , self . strides [ [number] ] ) [EOL] if self . data_format == [string] : [EOL] return ( input_shape [ [number] ] , features , length ) [EOL] else : [EOL] return ( input_shape [ [number] ] , length , features ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Dict [EOL] import builtins [EOL] import typing [EOL] from typing import Dict [EOL] [EOL] genres = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] }	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] from keras . callbacks import Callback [EOL] from keras import backend as K [EOL] [EOL] [EOL] class LearningRateTracker ( Callback ) : [EOL] [EOL] def on_epoch_end ( self , epoch , logs = { } ) : [EOL] optimizer = self . model . optimizer [EOL] [EOL] [comment] [EOL] lr = K . eval ( K . variable ( ( optimizer . lr * ( [number] / ( [number] + optimizer . decay * K . cast ( optimizer . iterations , dtype = [string] ) ) ) ) ) ) [EOL] print ( [string] % ( epoch + [number] , lr ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0
from typing import Any , List [EOL] import typing [EOL] import os [EOL] from queue import Queue [EOL] from sys import stderr [EOL] [EOL] import h5py [EOL] import numpy as np [EOL] [EOL] import train_test_divider as train_test_divider [EOL] [EOL] def make_path ( * paths ) : [EOL] path = os . path . join ( * [ str ( path ) for path in paths ] ) [EOL] path = os . path . realpath ( path ) [EOL] return path [EOL] [EOL] [EOL] def calculate_num_segments ( sample_length ) : [EOL] return [number] // sample_length [EOL] [EOL] [EOL] def train_generator ( train_list , y_train_init , batch_size , song_batch , sample_length , num_tags , dataset , path ) : [EOL] i = [number] [EOL] j = [number] [EOL] [EOL] batch_size = batch_size [EOL] train_length = len ( train_list ) [EOL] subset_size = int ( train_length / song_batch ) [EOL] [comment] [EOL] [comment] [EOL] [EOL] num_segments = calculate_num_segments ( sample_length ) [EOL] [EOL] while [number] : [EOL] [comment] [EOL] x_train_sub = np . zeros ( ( subset_size * num_segments , sample_length , [number] ) ) [EOL] y_train_sub = np . zeros ( ( subset_size , num_tags ) ) [EOL] [EOL] for subset_size_index in range ( [number] , subset_size ) : [EOL] [docstring] [EOL] [EOL] [docstring] [EOL] try : [EOL] [comment] [EOL] tmp = np . load ( path % ( dataset , train_list [ subset_size_index * song_batch + i ] ) ) [ [string] ] [EOL] except : [EOL] break [EOL] [EOL] for num_segments_index in range ( [number] , num_segments ) : [EOL] x_train_sub [ num_segments * subset_size_index + num_segments_index , : , [number] ] = tmp [ num_segments_index * sample_length : num_segments_index * sample_length + sample_length ] [EOL] [EOL] y_train_sub [ subset_size_index ] = y_train_init [ subset_size_index * song_batch + i , : ] [EOL] [EOL] [comment] [EOL] y_train_sub = np . repeat ( y_train_sub , num_segments , axis = [number] ) [EOL] [comment] [EOL] [EOL] [comment] [EOL] tmp_train = np . arange ( num_segments * subset_size ) [EOL] np . random . shuffle ( tmp_train ) [EOL] x_train_sub = x_train_sub [ tmp_train ] [EOL] y_train_sub = y_train_sub [ tmp_train ] [EOL] [EOL] [comment] [EOL] x_train_sub_batch = np . zeros ( ( batch_size , sample_length , [number] ) ) [EOL] y_train_sub_batch = np . zeros ( ( batch_size , num_tags ) ) [EOL] [EOL] for iter2 in range ( [number] , int ( subset_size * num_segments / batch_size ) ) : [EOL] [EOL] [comment] [EOL] for batch_index in range ( [number] , batch_size ) : [EOL] x_train_sub_batch [ batch_index ] = x_train_sub [ int ( batch_index * subset_size * num_segments / batch_size ) + j , : ] [EOL] y_train_sub_batch [ batch_index ] = y_train_sub [ int ( batch_index * subset_size * num_segments / batch_size ) + j , : ] [EOL] [EOL] [docstring] [EOL] [EOL] j = j + [number] [EOL] yield ( x_train_sub_batch , y_train_sub_batch ) [EOL] [EOL] if j == int ( subset_size * num_segments / batch_size ) : [EOL] j = [number] [EOL] i = i + [number] [EOL] if i == song_batch : [EOL] i = [number] [EOL] [EOL] def get_data ( args ) : [EOL] [docstring] [EOL] x_train , y_train , x_valid , y_valid , x_test , y_test = None , None , None , None , None , None [EOL] [EOL] if args . d == [string] : [EOL] validation_size = [number] [EOL] [EOL] x_train , y_train , x_test , y_test = train_test_divider . split_data_sklearn ( [string] , [number] ) [EOL] [EOL] num_train = len ( x_train ) [EOL] x_valid = x_train [ : int ( num_train * validation_size ) ] [EOL] y_valid = y_train [ : int ( num_train * validation_size ) ] [EOL] x_train = x_train [ int ( num_train * validation_size ) : ] [EOL] y_train = y_train [ int ( num_train * validation_size ) : ] [EOL] [EOL] elif args . d == [string] : [EOL] base_path = [string] [EOL] x_train = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_train = np . load ( base_path + [string] ) [ [string] ] [EOL] [EOL] [comment] [EOL] error_idx = x_train . index ( [string] ) [EOL] del x_train [ error_idx ] [EOL] y_train = np . delete ( y_train , [ error_idx ] , [number] ) [EOL] [EOL] x_valid = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_valid = np . load ( base_path + [string] ) [ [string] ] [EOL] [EOL] x_test = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_test = np . load ( base_path + [string] ) [ [string] ] [EOL] [EOL] elif args . d == [string] : [EOL] base_path = [string] [EOL] x_train = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_train = np . load ( base_path + [string] ) [EOL] [EOL] x_valid = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_valid = np . load ( base_path + [string] ) [EOL] [EOL] x_test = [ song . rstrip ( ) for song in open ( base_path + [string] ) ] [EOL] y_test = np . load ( base_path + [string] ) [EOL] [EOL] return x_train , y_train , x_valid , y_valid , x_test , y_test [EOL] [EOL] [EOL] def load_multigpu_checkpoint_weights ( model , h5py_file ) : [EOL] [docstring] [EOL] [EOL] print ( [string] ) [EOL] with h5py . File ( h5py_file , [string] ) as file : [EOL] model_name = None [EOL] for key in file . keys ( ) : [EOL] if [string] in key : [EOL] model_name = key [EOL] [EOL] [comment] [EOL] weight_file = file [ model_name ] [EOL] [EOL] for layer in model . layers : [EOL] [EOL] try : [EOL] layer_weights = weight_file [ layer . name ] [EOL] [EOL] except : [EOL] [comment] [EOL] continue [EOL] [EOL] try : [EOL] weights = [ ] [EOL] [comment] [EOL] for term in layer_weights : [EOL] if isinstance ( layer_weights [ term ] , h5py . Dataset ) : [EOL] [comment] [EOL] weights . insert ( [number] , np . array ( layer_weights [ term ] ) ) [EOL] [EOL] [comment] [EOL] layer . set_weights ( weights ) [EOL] [EOL] except Exception as e : [EOL] print ( [string] , layer . name , file = stderr ) [EOL] [EOL] [EOL] def check_weights ( build_model , file ) : [EOL] load_multigpu_checkpoint_weights ( build_model , file ) [EOL] weights = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights2 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights3 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights4 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights5 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights6 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights7 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights8 = build_model . layers [ [number] ] . get_weights ( ) [EOL] weights9 = build_model . layers [ [number] ] . get_weights ( ) [EOL] [EOL] print ( [string] % ( weights , weights2 , weights3 , weights4 , weights5 , weights6 , weights7 , weights8 , weights9 ) ) [EOL] [EOL] [EOL] def check_weight ( build_model , file ) : [EOL] load_multigpu_checkpoint_weights ( build_model , file ) [EOL] weights = build_model . layers [ [number] ] . get_weights ( ) [EOL] [EOL] print ( [string] % (weights) ) [EOL] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import src . sqllite_repository as sql [EOL] [EOL] [comment] [EOL] [EOL] base_path = [string] [EOL] [EOL] loaded_train_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] loaded_valid_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] loaded_test_ids = [ song . split ( [string] ) [ - [number] ] . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] [EOL] train_ids = [ song . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] valid_ids = [ song . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] test_ids = [ song . rstrip ( ) for song in open ( base_path % [string] ) ] [EOL] [EOL] count = [number] [EOL] [EOL] print ( [string] ) [EOL] diff_train = list ( set ( train_ids ) - set ( loaded_train_ids ) ) [EOL] for id in diff_train : [EOL] count += [number] [EOL] print ( id ) [EOL] print ( [string] ) [EOL] [EOL] print ( [string] ) [EOL] diff_valid = list ( set ( valid_ids ) - set ( loaded_valid_ids ) ) [EOL] for id in diff_valid : [EOL] count += [number] [EOL] print ( id ) [EOL] print ( [string] ) [EOL] [EOL] print ( [string] ) [EOL] diff_test = list ( set ( test_ids ) - set ( loaded_test_ids ) ) [EOL] for id in diff_test : [EOL] count += [number] [EOL] print ( id ) [EOL] print ( [string] ) [EOL] [EOL] print ( count ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] from typing import Any , List [EOL] import typing [EOL] import io [EOL] import os [EOL] [EOL] import numpy as np [EOL] import librosa [EOL] [EOL] path = [string] [EOL] feature_path = [string] [EOL] dataset = [string] [EOL] max_length = [number] [EOL] frequency = [number] [EOL] dir_name = [string] [EOL] [EOL] error_logs = open ( [string] , [string] ) [EOL] [EOL] [EOL] def convert_files ( ) : [EOL] count = [number] [EOL] [EOL] print ( [string] ) [EOL] train_ids = [ song . rstrip ( ) for song in open ( [string] % dataset ) ] [EOL] [comment] [EOL] convert_list ( train_ids , [string] % dataset , count ) [EOL] [EOL] print ( [string] ) [EOL] valid_ids = [ song . rstrip ( ) for song in open ( [string] % dataset ) ] [EOL] convert_list ( valid_ids , [string] % dataset , count ) [EOL] [EOL] print ( [string] ) [EOL] test_ids = [ song . rstrip ( ) for song in open ( [string] % dataset ) ] [EOL] convert_list ( test_ids , [string] % dataset , count ) [EOL] [EOL] error_logs . close ( ) [EOL] [EOL] [EOL] def convert_list ( list_ids , file_name , count ) : [EOL] file_ids = open ( file_name , [string] ) [EOL] for root , dirs , files in os . walk ( path ) : [EOL] for file in files : [EOL] splitted_file = file . split ( [string] ) [EOL] if splitted_file [ [number] ] in list_ids : [EOL] file_name = os . path . join ( root , file ) [EOL] if count % [number] == [number] : dir_name = [string] % ( count , count + [number] ) [EOL] [EOL] if file . endswith ( [string] ) : [EOL] count += [number] [EOL] save_name = feature_path + str ( dir_name ) + [string] + file . replace ( [string] , [string] ) [EOL] if not os . path . exists ( os . path . dirname ( save_name ) ) : [EOL] os . makedirs ( os . path . dirname ( save_name ) ) [EOL] [EOL] if os . path . isfile ( save_name ) == [number] : [EOL] continue [EOL] try : [EOL] y , sr = librosa . load ( file_name , sr = frequency ) [EOL] except : [EOL] error_logs . write ( [string] % save_name ) [EOL] print ( [string] % save_name ) [EOL] continue [EOL] [EOL] y = y . astype ( np . float32 ) [EOL] [EOL] if len ( y ) > max_length : [EOL] y = y [ [number] : max_length ] [EOL] [EOL] file_ids . write ( [string] % save_name ) [EOL] np . savez_compressed ( save_name , y ) [EOL] [EOL] file_ids . close ( ) [EOL] [EOL] [EOL] convert_files ( ) [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import os [EOL] [EOL] import numpy as np [EOL] import librosa [EOL] [EOL] def convert_files ( path , feature_path , max_length ) : [EOL] count = [number] [EOL] for root , dirs , files in os . walk ( path ) : [EOL] for file in files : [EOL] file_name = os . path . join ( root , file ) [EOL] dirname = int ( count / [number] ) [EOL] count = count + [number] [EOL] if file . endswith ( [string] ) : [EOL] save_name = feature_path + str ( dirname ) + [string] + file . replace ( [string] , [string] ) [EOL] [EOL] if not os . path . exists ( os . path . dirname ( save_name ) ) : [EOL] print ( os . path . dirname ( save_name ) ) [EOL] os . makedirs ( os . path . dirname ( save_name ) ) [EOL] [EOL] if os . path . isfile ( save_name ) == [number] : [EOL] print ( save_name + [string] ) [EOL] continue [EOL] print ( file_name ) [EOL] try : [EOL] y , sr = librosa . load ( file_name ) [EOL] except : [EOL] continue [EOL] y = y . astype ( np . float32 ) [EOL] [EOL] if len ( y ) > max_length : [EOL] y = y [ [number] : max_length ] [EOL] [EOL] print ( len ( y ) , save_name ) [EOL] np . savez_compressed ( save_name , y ) [EOL] [EOL] [EOL] convert_files ( [string] , [string] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0