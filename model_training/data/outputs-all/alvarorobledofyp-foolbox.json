[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Tuple , Dict , List , Any [EOL] import typing [EOL] import os [EOL] import sys [EOL] from unittest . mock import Mock [EOL] sys . path . insert ( [number] , os . path . abspath ( [string] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] sys . modules [ [string] ] = Mock ( ) [EOL] [EOL] import foolbox [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] extensions = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] [comment] [EOL] numpydoc_show_class_members = False [EOL] [EOL] [comment] [EOL] templates_path = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] source_suffix = [string] [EOL] [EOL] [comment] [EOL] master_doc = [string] [EOL] [EOL] [comment] [EOL] project = [string] [EOL] copyright = [string] [EOL] author = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] version = foolbox . __version__ [EOL] [comment] [EOL] release = foolbox . __version__ [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] language = None [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclude_patterns = [ [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] pygments_style = [string] [EOL] [EOL] [comment] [EOL] todo_include_todos = False [EOL] [EOL] [EOL] [comment] [EOL] def linkcode_resolve ( domain , info ) : [EOL] def find_source ( ) : [EOL] [comment] [EOL] [comment] [EOL] obj = sys . modules [ info [ [string] ] ] [EOL] for part in info [ [string] ] . split ( [string] ) : [EOL] obj = getattr ( obj , part ) [EOL] import inspect [EOL] import os [EOL] fn = inspect . getsourcefile ( obj ) [EOL] fn = os . path . relpath ( fn , start = os . path . dirname ( foolbox . __file__ ) ) [EOL] source , lineno = inspect . getsourcelines ( obj ) [EOL] return fn , lineno , lineno + len ( source ) - [number] [EOL] [EOL] if domain != [string] or not info [ [string] ] : [EOL] return None [EOL] try : [EOL] filename = [string] % find_source ( ) [EOL] except Exception : [EOL] filename = info [ [string] ] . replace ( [string] , [string] ) + [string] [EOL] tag = [string] [EOL] url = [string] [EOL] return url % ( tag , filename ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if os . environ . get ( [string] ) != [string] : [EOL] try : [EOL] import sphinx_rtd_theme [EOL] except ImportError : [EOL] pass [comment] [EOL] else : [EOL] html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] [EOL] html_theme = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_static_path = [ [string] ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] htmlhelp_basename = [string] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] latex_elements = { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] latex_documents = [ ( master_doc , [string] , [string] , [string] , [string] ) , ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] man_pages = [ ( master_doc , [string] , [string] , [ author ] , [number] ) ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] texinfo_documents = [ ( master_doc , [string] , [string] , author , [string] , [string] , [string] ) , ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,typing.List[builtins.str],builtins.int]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str,builtins.str]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0
from typing import Type , Any [EOL] import typing [EOL] import foolbox [EOL] [docstring] [EOL] from __future__ import division [EOL] import sys [EOL] import abc [EOL] abstractmethod = abc . abstractmethod [EOL] [EOL] if sys . version_info >= ( [number] , [number] ) : [EOL] ABC = abc . ABC [EOL] else : [comment] [EOL] ABC = abc . ABCMeta ( [string] , ( ) , { } ) [EOL] [EOL] import functools [EOL] import numpy as np [EOL] from numbers import Number [EOL] [EOL] [EOL] @ functools . total_ordering class Distance ( ABC ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , reference = None , other = None , bounds = None , value = None ) : [EOL] [EOL] if value is not None : [EOL] [comment] [EOL] assert isinstance ( value , Number ) [EOL] assert reference is None [EOL] assert other is None [EOL] assert bounds is None [EOL] self . reference = None [EOL] self . other = None [EOL] self . _bounds = None [EOL] self . _value = value [EOL] self . _gradient = None [EOL] else : [EOL] [comment] [EOL] self . reference = reference [EOL] self . other = other [EOL] self . _bounds = bounds [EOL] self . _value , self . _gradient = self . _calculate ( ) [EOL] [EOL] assert self . _value is not None [EOL] [EOL] @ property def value ( self ) : [EOL] return self . _value [EOL] [EOL] @ property def gradient ( self ) : [EOL] return self . _gradient [EOL] [EOL] @ abstractmethod def _calculate ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def name ( self ) : [EOL] return self . __class__ . __name__ [EOL] [EOL] def __str__ ( self ) : [EOL] return [string] . format ( self . name ( ) , self . _value ) [EOL] [EOL] def __repr__ ( self ) : [EOL] return self . __str__ ( ) [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if other . __class__ != self . __class__ : [EOL] raise TypeError ( [string] ) [comment] [EOL] return self . value == other . value [EOL] [EOL] def __lt__ ( self , other ) : [EOL] if other . __class__ != self . __class__ : [EOL] raise TypeError ( [string] ) [comment] [EOL] return self . value < other . value [EOL] [EOL] [EOL] class MeanSquaredDistance ( Distance ) : [EOL] [docstring] [EOL] [EOL] def _calculate ( self ) : [EOL] min_ , max_ = self . _bounds [EOL] n = self . reference . size [EOL] f = n * ( max_ - min_ ) ** [number] [EOL] [EOL] diff = self . other - self . reference [EOL] value = np . vdot ( diff , diff ) / f [EOL] [EOL] [comment] [EOL] self . _g_diff = diff [EOL] self . _g_f = f [EOL] gradient = None [EOL] return value , gradient [EOL] [EOL] @ property def gradient ( self ) : [EOL] if self . _gradient is None : [EOL] self . _gradient = self . _g_diff / ( self . _g_f / [number] ) [EOL] return self . _gradient [EOL] [EOL] def __str__ ( self ) : [EOL] return [string] . format ( self . _value ) [EOL] [EOL] [EOL] MSE = MeanSquaredDistance [EOL] [EOL] [EOL] class MeanAbsoluteDistance ( Distance ) : [EOL] [docstring] [EOL] [EOL] def _calculate ( self ) : [EOL] min_ , max_ = self . _bounds [EOL] diff = ( self . other - self . reference ) / ( max_ - min_ ) [EOL] value = np . mean ( np . abs ( diff ) ) . astype ( np . float64 ) [EOL] n = self . reference . size [EOL] gradient = [number] / n * np . sign ( diff ) / ( max_ - min_ ) [EOL] return value , gradient [EOL] [EOL] def __str__ ( self ) : [EOL] return [string] . format ( self . _value ) [EOL] [EOL] [EOL] MAE = MeanAbsoluteDistance [EOL] [EOL] [EOL] class Linfinity ( Distance ) : [EOL] [docstring] [EOL] [EOL] def _calculate ( self ) : [EOL] min_ , max_ = self . _bounds [EOL] diff = ( self . other - self . reference ) / ( max_ - min_ ) [EOL] value = np . max ( np . abs ( diff ) ) . astype ( np . float64 ) [EOL] gradient = None [EOL] return value , gradient [EOL] [EOL] @ property def gradient ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] def __str__ ( self ) : [EOL] return [string] . format ( self . _value ) [EOL] [EOL] [EOL] Linf = Linfinity [EOL] [EOL] [EOL] class L0 ( Distance ) : [EOL] [docstring] [EOL] [EOL] def _calculate ( self ) : [EOL] diff = self . other - self . reference [EOL] value = np . sum ( diff != [number] ) [EOL] gradient = None [EOL] return value , gradient [EOL] [EOL] @ property def gradient ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] def __str__ ( self ) : [EOL] return [string] . format ( self . _value ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[foolbox.distances.MeanSquaredDistance]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[foolbox.distances.MeanAbsoluteDistance]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[foolbox.distances.Linfinity]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import random [EOL] import random [EOL] import numpy as np [EOL] [EOL] rng = random . Random ( ) [EOL] nprng = np . random . RandomState ( ) [EOL] [EOL] [EOL] def set_seeds ( seed ) : [EOL] [docstring] [EOL] rng . seed ( seed ) [EOL] nprng . seed ( seed ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $random.Random$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $random.Random$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import foolbox [EOL] import numpy as np [EOL] [EOL] from foolbox import Adversarial [EOL] from foolbox . distances import MSE [EOL] import foolbox [EOL] [EOL] import sys [EOL] if sys . version_info > ( [number] , [number] ) : [EOL] from unittest . mock import Mock [EOL] else : [EOL] [comment] [EOL] from mock import Mock [EOL] [EOL] [EOL] [comment] [EOL] def test_adversarial ( model , criterion , image , label ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] adversarial = Adversarial ( model , criterion , image , label , verbose = False ) [EOL] [EOL] assert not adversarial . predictions ( image ) [ [number] ] [EOL] [EOL] assert adversarial . image is None [EOL] assert adversarial . output is None [EOL] assert adversarial . adversarial_class is None [EOL] assert adversarial . distance == MSE ( value = np . inf ) [EOL] assert adversarial . original_image is image [EOL] assert adversarial . original_class == label [EOL] assert adversarial . target_class ( ) is None [EOL] assert adversarial . normalized_distance ( image ) == MSE ( value = [number] ) [EOL] assert adversarial . normalized_distance ( image ) . value == [number] [EOL] [EOL] np . random . seed ( [number] ) [EOL] perturbation = np . random . uniform ( - [number] , [number] , size = image . shape ) [EOL] perturbed = np . clip ( image + perturbation , [number] , [number] ) . astype ( np . float32 ) [EOL] d1 = adversarial . normalized_distance ( perturbed ) . value [EOL] assert d1 != [number] [EOL] [EOL] assert adversarial . original_image . dtype == np . float32 [EOL] [EOL] adversarial . set_distance_dtype ( np . float32 ) [EOL] assert adversarial . normalized_distance ( perturbed ) . value == d1 [EOL] [EOL] adversarial . set_distance_dtype ( np . float64 ) [EOL] assert adversarial . normalized_distance ( perturbed ) . value != d1 [EOL] [EOL] adversarial . reset_distance_dtype ( ) [EOL] assert adversarial . normalized_distance ( perturbed ) . value == d1 [EOL] [EOL] true_label = label [EOL] label = [number] [comment] [EOL] adversarial = Adversarial ( model , criterion , image , label , verbose = True ) [EOL] [EOL] assert adversarial . image is not None [EOL] assert adversarial . output is not None [EOL] assert adversarial . adversarial_class == true_label [EOL] assert adversarial . adversarial_class == np . argmax ( adversarial . output ) [EOL] assert adversarial . distance == MSE ( value = [number] ) [EOL] assert adversarial . original_image is image [EOL] assert adversarial . original_class == label [EOL] assert adversarial . target_class ( ) is None [EOL] assert adversarial . normalized_distance ( image ) == MSE ( value = [number] ) [EOL] assert adversarial . normalized_distance ( image ) . value == [number] [EOL] [EOL] predictions , is_adversarial = adversarial . predictions ( image ) [EOL] first_predictions = predictions [EOL] assert is_adversarial [EOL] [EOL] predictions , is_adversarial , _ , _ = adversarial . predictions ( image , return_details = True ) [comment] [EOL] first_predictions = predictions [EOL] assert is_adversarial [EOL] [EOL] predictions , is_adversarial = adversarial . batch_predictions ( image [ np . newaxis ] ) [comment] [EOL] assert ( predictions == first_predictions [ np . newaxis ] ) . all ( ) [EOL] assert np . all ( is_adversarial == np . array ( [ True ] ) ) [EOL] [EOL] predictions , is_adversarial , index = adversarial . batch_predictions ( image [ np . newaxis ] , greedy = True ) [comment] [EOL] assert ( predictions == first_predictions [ np . newaxis ] ) . all ( ) [EOL] assert is_adversarial [EOL] assert index == [number] [EOL] [EOL] predictions , is_adversarial , index , _ , _ = adversarial . batch_predictions ( image [ np . newaxis ] , greedy = True , return_details = True ) [comment] [EOL] assert ( predictions == first_predictions [ np . newaxis ] ) . all ( ) [EOL] assert is_adversarial [EOL] assert index == [number] [EOL] [EOL] predictions , gradient , is_adversarial = adversarial . predictions_and_gradient ( image , label ) [comment] [EOL] assert ( predictions == first_predictions ) . all ( ) [EOL] assert gradient . shape == image . shape [EOL] assert is_adversarial [EOL] [EOL] predictions , gradient , is_adversarial , _ , _ = adversarial . predictions_and_gradient ( image , label , return_details = True ) [comment] [EOL] assert ( predictions == first_predictions ) . all ( ) [EOL] assert gradient . shape == image . shape [EOL] assert is_adversarial [EOL] [EOL] predictions , gradient , is_adversarial = adversarial . predictions_and_gradient ( ) [comment] [EOL] assert ( predictions == first_predictions ) . all ( ) [EOL] assert gradient . shape == image . shape [EOL] assert is_adversarial [EOL] [EOL] gradient_pre = np . ones_like ( predictions ) * [number] [EOL] gradient = adversarial . backward ( gradient_pre , image ) [EOL] gradient2 = adversarial . backward ( gradient_pre ) [EOL] assert gradient . shape == image . shape [EOL] assert ( gradient == gradient2 ) . all ( ) [EOL] [EOL] gradient = adversarial . gradient ( ) [EOL] assert gradient . shape == image . shape [EOL] assert is_adversarial [EOL] [EOL] assert adversarial . num_classes ( ) == [number] [EOL] [EOL] assert adversarial . has_gradient ( ) [EOL] [EOL] assert adversarial . channel_axis ( batch = True ) == [number] [EOL] assert adversarial . channel_axis ( batch = False ) == [number] [EOL] [EOL] [comment] [EOL] criterion . is_adversarial = Mock ( return_value = False ) [EOL] adversarial = Adversarial ( model , criterion , image , label ) [EOL] predictions , is_adversarial , index = adversarial . batch_predictions ( image [ np . newaxis ] , greedy = True ) [comment] [EOL] assert ( predictions == first_predictions [ np . newaxis ] ) . all ( ) [EOL] assert not is_adversarial [EOL] assert index is None [EOL] [EOL] [comment] [EOL] del model . predictions_and_gradient [EOL] [EOL] assert not adversarial . has_gradient ( ) [EOL] [EOL] [EOL] def test_inplace ( bn_model , bn_adversarial , bn_label ) : [EOL] class TestAttack ( foolbox . attacks . Attack ) : [EOL] @ foolbox . attacks . base . call_decorator def __call__ ( self , input_or_adv , label , unpack ) : [EOL] a = input_or_adv [EOL] x = np . zeros_like ( a . original_image ) [EOL] a . predictions ( x ) [EOL] x [ : ] = a . original_image [EOL] [EOL] assert bn_adversarial . image is None [EOL] assert np . argmax ( bn_model . predictions ( bn_adversarial . original_image ) ) == bn_label [comment] [EOL] attack = TestAttack ( ) [EOL] attack ( bn_adversarial ) [EOL] assert bn_adversarial . image is not None [EOL] assert bn_adversarial . distance . value > [number] [EOL] assert np . argmax ( bn_model . predictions ( bn_adversarial . original_image ) ) == bn_label [comment] [EOL] assert np . argmax ( bn_model . predictions ( bn_adversarial . image ) ) != bn_label [EOL] assert not ( bn_adversarial . image == bn_adversarial . original_image ) . all ( ) [EOL] assert ( bn_adversarial . distance . reference == bn_adversarial . original_image ) . all ( ) [comment] [EOL] assert ( bn_adversarial . distance . other == bn_adversarial . image ) . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import foolbox [EOL] import pytest [EOL] import numpy as np [EOL] from foolbox import distances [EOL] from pytest import approx [EOL] [EOL] [EOL] def test_abstract_distance ( ) : [EOL] with pytest . raises ( TypeError ) : [EOL] distances . Distance ( ) [EOL] [EOL] [EOL] def test_base_distance ( ) : [EOL] [EOL] class TestDistance ( distances . Distance ) : [EOL] [EOL] def _calculate ( self ) : [EOL] return [number] , [number] [EOL] [EOL] distance = TestDistance ( None , None , bounds = ( [number] , [number] ) ) [EOL] assert distance . name ( ) == [string] [EOL] assert distance . value == [number] [EOL] assert distance . gradient == [number] [EOL] assert [string] in str ( distance ) [EOL] assert [string] in str ( distance ) [EOL] assert distance == distance [EOL] assert not distance < distance [EOL] assert not distance > distance [EOL] assert distance <= distance [EOL] assert distance >= distance [EOL] [EOL] with pytest . raises ( TypeError ) : [EOL] distance < [number] [EOL] [EOL] with pytest . raises ( TypeError ) : [EOL] distance == [number] [EOL] [EOL] [EOL] def test_mse ( ) : [EOL] assert distances . MSE == distances . MeanSquaredDistance [EOL] [EOL] [EOL] def test_mae ( ) : [EOL] assert distances . MAE == distances . MeanAbsoluteDistance [EOL] [EOL] [EOL] def test_linf ( ) : [EOL] assert distances . Linf == distances . Linfinity [EOL] [EOL] [EOL] def test_mean_squared_distance ( ) : [EOL] d = distances . MeanSquaredDistance ( np . array ( [ [number] , [number] ] ) , np . array ( [ [number] , [number] ] ) , bounds = ( [number] , [number] ) ) [EOL] assert d . value == [number] / [number] [EOL] assert ( d . gradient == np . array ( [ [number] , [number] ] ) ) . all ( ) [EOL] [EOL] [EOL] def test_mean_absolute_distance ( ) : [EOL] d = distances . MeanAbsoluteDistance ( np . array ( [ [number] , [number] ] ) , np . array ( [ [number] , [number] ] ) , bounds = ( [number] , [number] ) ) [EOL] assert d . value == approx ( [number] ) [EOL] assert ( d . gradient == np . array ( [ [number] , [number] ] ) ) . all ( ) [EOL] [EOL] [EOL] def test_linfinity ( ) : [EOL] d = distances . Linfinity ( np . array ( [ [number] , [number] ] ) , np . array ( [ [number] , [number] ] ) , bounds = ( [number] , [number] ) ) [EOL] assert d . value == approx ( [number] ) [EOL] with pytest . raises ( NotImplementedError ) : [EOL] d . gradient [EOL] [EOL] [EOL] def test_l0 ( ) : [EOL] d = distances . L0 ( np . array ( [ [number] , [number] ] ) , np . array ( [ [number] , [number] ] ) , bounds = ( [number] , [number] ) ) [EOL] assert d . value == approx ( [number] ) [EOL] with pytest . raises ( NotImplementedError ) : [EOL] d . gradient [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ distances . MeanSquaredDistance , distances . MeanAbsoluteDistance , distances . Linfinity , distances . L0 , ] ) def test_str_repr ( Distance ) : [EOL] [docstring] [EOL] reference = np . zeros ( ( [number] , [number] ) ) [EOL] other = np . ones ( ( [number] , [number] ) ) [EOL] d = Distance ( reference , other , bounds = ( [number] , [number] ) ) [EOL] assert isinstance ( str ( d ) , str ) [EOL] if [string] in str ( d ) : [EOL] assert [string] in str ( d ) [EOL] assert [string] in repr ( d ) [EOL] else : [EOL] assert [string] in str ( d ) [EOL] assert [string] in repr ( d ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Literal [EOL] import typing [EOL] import typing_extensions [EOL] import pytest [EOL] import numpy as np [EOL] import torch [EOL] [EOL] from foolbox . models import PyTorchModel [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_pytorch_model ( num_classes ) : [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = torch . mean ( x , [number] ) [EOL] x = torch . mean ( x , [number] ) [EOL] logits = x [EOL] return logits [EOL] [EOL] model = Net ( ) [EOL] model = PyTorchModel ( model , bounds = bounds , num_classes = num_classes ) [EOL] [EOL] test_images = np . random . rand ( [number] , channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_label = [number] [EOL] [EOL] assert model . batch_predictions ( test_images ) . shape == ( [number] , num_classes ) [EOL] [EOL] test_logits = model . predictions ( test_images [ [number] ] ) [EOL] assert test_logits . shape == ( num_classes , ) [EOL] [EOL] test_gradient = model . gradient ( test_images [ [number] ] , test_label ) [EOL] assert test_gradient . shape == test_images [ [number] ] . shape [EOL] [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_logits ) [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_gradient ) [EOL] [EOL] assert model . num_classes ( ) == num_classes [EOL] [EOL] [EOL] def test_pytorch_model_preprocessing ( ) : [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] num_classes = [number] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = torch . mean ( x , [number] ) [EOL] x = torch . mean ( x , [number] ) [EOL] logits = x [EOL] return logits [EOL] [EOL] model = Net ( ) [EOL] preprocessing = ( np . arange ( num_classes ) [ : , None , None ] , np . random . uniform ( size = ( channels , [number] , [number] ) ) + [number] ) [EOL] [EOL] model1 = PyTorchModel ( model , bounds = bounds , num_classes = num_classes ) [EOL] [EOL] model2 = PyTorchModel ( model , bounds = bounds , num_classes = num_classes , preprocessing = preprocessing ) [EOL] [EOL] model3 = PyTorchModel ( model , bounds = bounds , num_classes = num_classes ) [EOL] [EOL] np . random . seed ( [number] ) [EOL] test_images = np . random . rand ( [number] , channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_images_copy = test_images . copy ( ) [EOL] [EOL] p1 = model1 . batch_predictions ( test_images ) [EOL] p2 = model2 . batch_predictions ( test_images ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert np . all ( test_images == test_images_copy ) [EOL] [EOL] p3 = model3 . batch_predictions ( test_images ) [EOL] [EOL] assert p1 . shape == p2 . shape == p3 . shape == ( [number] , num_classes ) [EOL] [EOL] np . testing . assert_array_almost_equal ( p1 - p1 . max ( ) , p3 - p3 . max ( ) , decimal = [number] ) [EOL] [EOL] [EOL] def test_pytorch_model_gradient ( ) : [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] num_classes = [number] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = torch . mean ( x , [number] ) [EOL] x = torch . mean ( x , [number] ) [EOL] logits = x [EOL] return logits [EOL] [EOL] model = Net ( ) [EOL] preprocessing = ( np . arange ( num_classes ) [ : , None , None ] , np . random . uniform ( size = ( channels , [number] , [number] ) ) + [number] ) [EOL] [EOL] model = PyTorchModel ( model , bounds = bounds , num_classes = num_classes , preprocessing = preprocessing ) [EOL] [EOL] epsilon = [number] [EOL] [EOL] np . random . seed ( [number] ) [EOL] test_image = np . random . rand ( channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_label = [number] [EOL] [EOL] _ , g1 = model . predictions_and_gradient ( test_image , test_label ) [EOL] [EOL] l1 = model . _loss_fn ( test_image - epsilon / [number] * g1 , test_label ) [EOL] l2 = model . _loss_fn ( test_image + epsilon / [number] * g1 , test_label ) [EOL] [EOL] assert [number] * ( l2 - l1 ) > [number] [EOL] [EOL] [comment] [EOL] np . testing . assert_array_almost_equal ( [number] * ( l2 - l1 ) , [number] * epsilon * np . linalg . norm ( g1 ) ** [number] , decimal = [number] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_pytorch_backward ( num_classes ) : [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = torch . mean ( x , [number] ) [EOL] x = torch . mean ( x , [number] ) [EOL] logits = x [EOL] return logits [EOL] [EOL] model = Net ( ) [EOL] model = PyTorchModel ( model , bounds = bounds , num_classes = num_classes ) [EOL] [EOL] test_image = np . random . rand ( channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_grad_pre = np . random . rand ( num_classes ) . astype ( np . float32 ) [EOL] [EOL] test_grad = model . backward ( test_grad_pre , test_image ) [EOL] assert test_grad . shape == test_image . shape [EOL] [EOL] manual_grad = np . repeat ( np . repeat ( ( test_grad_pre / [number] ) . reshape ( ( - [number] , [number] , [number] ) ) , [number] , axis = [number] ) , [number] , axis = [number] ) [EOL] [EOL] np . testing . assert_almost_equal ( test_grad , manual_grad ) [EOL] [EOL] [EOL] def test_pytorch_model_preprocessing_shape_change ( ) : [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] num_classes = [number] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] class Net ( nn . Module ) : [EOL] [EOL] def __init__ ( self ) : [EOL] super ( Net , self ) . __init__ ( ) [EOL] [EOL] def forward ( self , x ) : [EOL] x = torch . mean ( x , [number] ) [EOL] x = torch . mean ( x , [number] ) [EOL] logits = x [EOL] return logits [EOL] [EOL] model = Net ( ) [EOL] [EOL] model1 = PyTorchModel ( model , bounds = bounds , num_classes = num_classes ) [EOL] [EOL] def preprocessing2 ( x ) : [EOL] if x . ndim == [number] : [EOL] x = np . transpose ( x , axes = ( [number] , [number] , [number] ) ) [EOL] elif x . ndim == [number] : [EOL] x = np . transpose ( x , axes = ( [number] , [number] , [number] , [number] ) ) [EOL] [EOL] def grad ( dmdp ) : [EOL] assert dmdp . ndim == [number] [EOL] dmdx = np . transpose ( dmdp , axes = ( [number] , [number] , [number] ) ) [EOL] return dmdx [EOL] [EOL] return x , grad [EOL] [EOL] model2 = PyTorchModel ( model , bounds = bounds , num_classes = num_classes , preprocessing = preprocessing2 ) [EOL] [EOL] np . random . seed ( [number] ) [EOL] test_images_nhwc = np . random . rand ( [number] , [number] , [number] , channels ) . astype ( np . float32 ) [EOL] test_images_nchw = np . transpose ( test_images_nhwc , ( [number] , [number] , [number] , [number] ) ) [EOL] [EOL] p1 = model1 . batch_predictions ( test_images_nchw ) [EOL] p2 = model2 . batch_predictions ( test_images_nhwc ) [EOL] [EOL] assert np . all ( p1 == p2 ) [EOL] [EOL] p1 = model1 . predictions ( test_images_nchw [ [number] ] ) [EOL] p2 = model2 . predictions ( test_images_nhwc [ [number] ] ) [EOL] [EOL] assert np . all ( p1 == p2 ) [EOL] [EOL] g1 = model1 . gradient ( test_images_nchw [ [number] ] , [number] ) [EOL] assert g1 . ndim == [number] [EOL] g1 = np . transpose ( g1 , ( [number] , [number] , [number] ) ) [EOL] g2 = model2 . gradient ( test_images_nhwc [ [number] ] , [number] ) [EOL] [EOL] np . testing . assert_array_almost_equal ( g1 , g2 ) [EOL] [EOL] [EOL] def test_pytorch_device ( bn_model_pytorch ) : [EOL] m = bn_model_pytorch [EOL] model1 = PyTorchModel ( m . _model , bounds = m . bounds ( ) , num_classes = m . num_classes ( ) , device = [string] ) [EOL] model2 = PyTorchModel ( m . _model , bounds = m . bounds ( ) , num_classes = m . num_classes ( ) , device = torch . device ( [string] ) ) [EOL] assert model1 . device == model2 . device [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] from foolbox . attacks import SinglePixelAttack as Attack [EOL] [EOL] [EOL] def test_attack ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_gl ( gl_bn_adversarial ) : [EOL] adv = gl_bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , List [EOL] import typing [EOL] from foolbox import zoo [EOL] import numpy as np [EOL] import foolbox [EOL] import sys [EOL] import pytest [EOL] from foolbox . zoo . model_loader import ModelLoader [EOL] from os . path import join , dirname [EOL] [EOL] [EOL] @ pytest . fixture ( autouse = True ) def unload_foolbox_model_module ( ) : [EOL] [comment] [EOL] [comment] [EOL] module_names = [ [string] , [string] ] [EOL] for module_name in module_names : [EOL] if module_name in sys . modules : [EOL] del sys . modules [ module_name ] [EOL] [EOL] [EOL] test_data = [ ( join ( [string] , dirname ( __file__ ) , [string] ) , ( [number] , [number] , [number] ) ) ] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , test_data ) def test_loading_model ( url , dim ) : [EOL] [comment] [EOL] model = zoo . get_model ( url ) [EOL] [EOL] [comment] [EOL] x = np . zeros ( dim , dtype = np . float32 ) [EOL] x [ : ] = np . random . randn ( * x . shape ) [EOL] [EOL] [comment] [EOL] logits = model . predictions ( x ) [EOL] probabilities = foolbox . utils . softmax ( logits ) [EOL] predicted_class = np . argmax ( logits ) [EOL] [EOL] [comment] [EOL] assert predicted_class >= [number] [EOL] assert np . sum ( probabilities ) >= [number] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def test_non_default_module_throws_error ( ) : [EOL] with pytest . raises ( RuntimeError ) : [EOL] ModelLoader . get ( key = [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Tuple[builtins.int,builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Tuple[builtins.int,builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import foolbox [EOL] import pytest [EOL] import numpy as np [EOL] [EOL] from foolbox import models [EOL] [EOL] [EOL] def test_abstract_model ( ) : [EOL] with pytest . raises ( TypeError ) : [EOL] models . Model ( ) [EOL] [EOL] [EOL] def test_abstract_differentiable_model ( ) : [EOL] with pytest . raises ( TypeError ) : [EOL] models . DifferentiableModel ( ) [EOL] [EOL] [EOL] def test_base_model ( ) : [EOL] [EOL] class TestModel ( models . Model ) : [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] pass [EOL] [EOL] def num_classes ( self ) : [EOL] return [number] [EOL] [EOL] model = TestModel ( bounds = ( [number] , [number] ) , channel_axis = [number] ) [EOL] assert model . bounds ( ) == ( [number] , [number] ) [EOL] assert model . channel_axis ( ) == [number] [EOL] with model : [EOL] assert model . num_classes ( ) == [number] [EOL] [EOL] [EOL] def test_differentiable_base_model ( ) : [EOL] [EOL] class TestModel ( models . DifferentiableModel ) : [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] pass [EOL] [EOL] def num_classes ( self ) : [EOL] return [number] [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] return [string] , [string] [EOL] [EOL] def backward ( self , gradient , image ) : [EOL] return image [EOL] [EOL] model = TestModel ( bounds = ( [number] , [number] ) , channel_axis = [number] ) [EOL] [EOL] image = np . ones ( ( [number] , [number] , [number] ) , dtype = np . float32 ) [EOL] label = [number] [EOL] assert model . gradient ( image , label ) == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import foolbox [EOL] import numpy as np [EOL] [EOL] from foolbox . attacks import SaliencyMapAttack as Attack [EOL] [EOL] [EOL] def test_attack ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_random_targets ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , num_random_targets = [number] ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_targeted_attack ( bn_targeted_adversarial ) : [EOL] adv = bn_targeted_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_targeted_attack_slow ( bn_targeted_adversarial ) : [EOL] adv = bn_targeted_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , fast = False ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_targeted_attack_max ( bn_targeted_adversarial ) : [EOL] adv = bn_targeted_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , max_perturbations_per_pixel = [number] ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Literal [EOL] import typing [EOL] import typing_extensions [EOL] import pytest [EOL] import mxnet as mx [EOL] import numpy as np [EOL] [EOL] from foolbox . models import MXNetModel [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_model ( num_classes ) : [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] def mean_brightness_net ( images ) : [EOL] logits = mx . symbol . mean ( images , axis = ( [number] , [number] ) ) [EOL] return logits [EOL] [EOL] images = mx . symbol . Variable ( [string] ) [EOL] logits = mean_brightness_net ( images ) [EOL] [EOL] model = MXNetModel ( images , logits , { } , ctx = mx . cpu ( ) , num_classes = num_classes , bounds = bounds , channel_axis = [number] ) [EOL] [EOL] test_images = np . random . rand ( [number] , channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_label = [number] [EOL] [EOL] [comment] [EOL] assert model . batch_predictions ( test_images ) . shape == ( [number] , num_classes ) [EOL] [EOL] test_logits = model . predictions ( test_images [ [number] ] ) [EOL] assert test_logits . shape == ( num_classes , ) [EOL] [EOL] test_gradient = model . gradient ( test_images [ [number] ] , test_label ) [EOL] assert test_gradient . shape == test_images [ [number] ] . shape [EOL] [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_logits ) [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_gradient ) [EOL] [EOL] assert model . num_classes ( ) == num_classes [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_model_gradient ( num_classes ) : [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] def mean_brightness_net ( images ) : [EOL] logits = mx . symbol . mean ( images , axis = ( [number] , [number] ) ) [EOL] return logits [EOL] [EOL] images = mx . symbol . Variable ( [string] ) [EOL] logits = mean_brightness_net ( images ) [EOL] [EOL] preprocessing = ( np . arange ( num_classes ) [ : , None , None ] , np . random . uniform ( size = ( channels , [number] , [number] ) ) + [number] ) [EOL] [EOL] model = MXNetModel ( images , logits , { } , ctx = mx . cpu ( ) , num_classes = num_classes , bounds = bounds , preprocessing = preprocessing , channel_axis = [number] ) [EOL] [EOL] test_images = np . random . rand ( [number] , channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_image = test_images [ [number] ] [EOL] test_label = [number] [EOL] [EOL] epsilon = [number] [EOL] _ , g1 = model . predictions_and_gradient ( test_image , test_label ) [EOL] l1 = model . _loss_fn ( test_image - epsilon / [number] * g1 , test_label ) [EOL] l2 = model . _loss_fn ( test_image + epsilon / [number] * g1 , test_label ) [EOL] [EOL] assert [number] * ( l2 - l1 ) > [number] [EOL] [EOL] [comment] [EOL] np . testing . assert_array_almost_equal ( [number] * ( l2 - l1 ) , [number] * epsilon * np . linalg . norm ( g1 ) ** [number] , decimal = [number] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [number] , [number] ] ) def test_model_backward ( num_classes ) : [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes [EOL] [EOL] def mean_brightness_net ( images ) : [EOL] logits = mx . symbol . mean ( images , axis = ( [number] , [number] ) ) [EOL] return logits [EOL] [EOL] images = mx . symbol . Variable ( [string] ) [EOL] logits = mean_brightness_net ( images ) [EOL] [EOL] model = MXNetModel ( images , logits , { } , ctx = mx . cpu ( ) , num_classes = num_classes , bounds = bounds , channel_axis = [number] ) [EOL] [EOL] test_image = np . random . rand ( channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_grad_pre = np . random . rand ( num_classes ) . astype ( np . float32 ) [EOL] [EOL] test_grad = model . backward ( test_grad_pre , test_image ) [EOL] assert test_grad . shape == test_image . shape [EOL] [EOL] manual_grad = np . repeat ( np . repeat ( ( test_grad_pre / [number] ) . reshape ( ( - [number] , [number] , [number] ) ) , [number] , axis = [number] ) , [number] , axis = [number] ) [EOL] [EOL] np . testing . assert_almost_equal ( test_grad , manual_grad ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import pytest [EOL] [EOL] from foolbox . attacks import BoundaryAttack [EOL] from foolbox . attacks import DeepFoolAttack [EOL] from foolbox . attacks import BlendedUniformNoiseAttack [EOL] [EOL] [EOL] def test_attack ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] attack ( adv , iterations = [number] , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_non_verbose ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] attack ( adv , iterations = [number] , verbose = False ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_continue ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack1 = BlendedUniformNoiseAttack ( ) [EOL] attack1 ( adv ) [EOL] d1 = adv . distance . value [EOL] attack2 = BoundaryAttack ( ) [EOL] attack2 ( adv , iterations = [number] , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] assert adv . distance . value < d1 [EOL] [EOL] [EOL] def test_attack_parameters ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] o = adv . original_image [EOL] np . random . seed ( [number] ) [EOL] starting_point = np . random . uniform ( [number] , [number] , size = o . shape ) . astype ( o . dtype ) [EOL] attack ( adv , iterations = [number] , starting_point = starting_point , log_every_n_steps = [number] , tune_batch_size = False , threaded_rnd = False , threaded_gen = False , alternative_generator = True , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_parameters2 ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] attack ( adv , iterations = [number] , alternative_generator = True , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] @ pytest . mark . filterwarnings ( [string] ) def test_attack_parameters3 ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] o = adv . original_image [EOL] np . random . seed ( [number] ) [EOL] starting_point = np . random . uniform ( [number] , [number] , size = o . shape ) . astype ( o . dtype ) [EOL] attack ( adv , iterations = [number] , starting_point = starting_point , log_every_n_steps = [number] , tune_batch_size = [number] , threaded_rnd = False , threaded_gen = False , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_gl ( gl_bn_adversarial ) : [EOL] adv = gl_bn_adversarial [EOL] attack = BoundaryAttack ( ) [EOL] attack ( adv , iterations = [number] , verbose = True ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_impossible ( bn_impossible ) : [EOL] adv = bn_impossible [EOL] attack = BoundaryAttack ( ) [EOL] attack ( adv , iterations = [number] , verbose = True ) [EOL] assert adv . image is None [EOL] assert adv . distance . value == np . inf [EOL] [EOL] [EOL] @ pytest . mark . filterwarnings ( [string] ) def test_attack_convergence ( bn_adversarial ) : [EOL] adv = bn_adversarial [EOL] attack1 = DeepFoolAttack ( ) [EOL] attack1 ( adv ) [EOL] attack2 = BoundaryAttack ( ) [EOL] attack2 ( adv , iterations = [number] , verbose = True ) [EOL] [comment] [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import foolbox [EOL] import pytest [EOL] import numpy as np [EOL] from foolbox import criteria [EOL] [EOL] [EOL] def test_abstract_criterion ( ) : [EOL] with pytest . raises ( TypeError ) : [EOL] criteria . Criterion ( ) [EOL] [EOL] [EOL] def test_base_criterion ( ) : [EOL] [EOL] class TestCriterion ( criteria . Criterion ) : [EOL] [EOL] def is_adversarial ( self , predictions , label ) : [EOL] return False [EOL] [EOL] criterion = TestCriterion ( ) [EOL] assert criterion . name ( ) == [string] [EOL] [EOL] [EOL] def test_combined_criteria ( ) : [EOL] c1 = criteria . Misclassification ( ) [EOL] c2 = criteria . OriginalClassProbability ( [number] ) [EOL] c3 = c1 & c2 [EOL] [EOL] probabilities = np . array ( [ [number] , [number] , [number] , [number] ] ) [EOL] predictions = np . log ( probabilities ) [EOL] [EOL] for i in range ( len ( predictions ) ) : [EOL] b1 = c1 . is_adversarial ( predictions , i ) [EOL] b2 = c2 . is_adversarial ( predictions , i ) [EOL] b3 = c3 . is_adversarial ( predictions , i ) [EOL] [EOL] assert ( b1 and b2 ) == b3 [EOL] [EOL] assert c1 . name ( ) == [string] [EOL] assert c2 . name ( ) == [string] [EOL] assert c3 . name ( ) == c2 . name ( ) + [string] + c1 . name ( ) [EOL] [EOL] [EOL] def test_misclassfication ( ) : [EOL] c = criteria . Misclassification ( ) [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] ] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] [EOL] [EOL] def test_misclassification_names ( ) : [EOL] c = criteria . Misclassification ( ) [EOL] c1 = criteria . TopKMisclassification ( k = [number] ) [EOL] c5 = criteria . TopKMisclassification ( k = [number] ) [EOL] assert c . name ( ) == c1 . name ( ) [EOL] assert c1 . name ( ) != c5 . name ( ) [EOL] c22 = criteria . TopKMisclassification ( k = [number] ) [EOL] assert [string] in c22 . name ( ) [EOL] [EOL] [EOL] def test_top_k_misclassfication ( ) : [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] ] ) [EOL] [EOL] c = criteria . TopKMisclassification ( k = [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] [EOL] c = criteria . TopKMisclassification ( k = [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] [EOL] [EOL] def test_target_class ( ) : [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] ] ) [EOL] [EOL] c = criteria . TargetClass ( [number] ) [EOL] for i in range ( len ( predictions ) ) : [EOL] assert not c . is_adversarial ( predictions , i ) [EOL] [EOL] assert c . name ( ) == [string] [EOL] [EOL] c = criteria . TargetClass ( [number] ) [EOL] for i in range ( len ( predictions ) ) : [EOL] assert c . is_adversarial ( predictions , i ) [EOL] [EOL] assert c . name ( ) == [string] [EOL] [EOL] [EOL] def test_original_class_probability ( ) : [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] c = criteria . OriginalClassProbability ( [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] assert c . is_adversarial ( predictions , [number] ) [EOL] [EOL] assert [string] in c . name ( ) [EOL] [EOL] [EOL] def test_target_class_probability ( ) : [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] for t in [ [number] , [number] , [number] , [number] ] : [EOL] c = criteria . TargetClassProbability ( [number] , p = [number] ) [EOL] for i in range ( len ( predictions ) ) : [EOL] assert not c . is_adversarial ( predictions , i ) [EOL] [EOL] c = criteria . TargetClassProbability ( [number] , p = [number] ) [EOL] for i in range ( len ( predictions ) ) : [EOL] assert c . is_adversarial ( predictions , i ) [EOL] [EOL] assert [string] in c . name ( ) [EOL] assert [string] in c . name ( ) [EOL] [EOL] [EOL] def test_confident_misclassification ( ) : [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] , [number] ] ) [comment] [EOL] [EOL] for p in [ [number] , [number] , [number] ] : [EOL] c = criteria . ConfidentMisclassification ( p = p ) [EOL] for i in [ [number] , [number] , [number] , [number] ] : [EOL] assert c . is_adversarial ( predictions , i ) [EOL] assert not c . is_adversarial ( predictions , [number] ) [EOL] [EOL] predictions = np . array ( [ [number] , [number] , [number] , [number] , [number] ] ) [comment] [EOL] [EOL] for p in [ [number] , [number] , [number] ] : [EOL] c = criteria . ConfidentMisclassification ( p = p ) [EOL] for i in range ( [number] ) : [EOL] expect = i < [number] and p <= [number] [EOL] assert c . is_adversarial ( predictions , i ) == expect [EOL] [EOL] c = criteria . ConfidentMisclassification ( p = [number] ) [EOL] assert [string] in c . name ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Tuple , Any [EOL] import typing [EOL] import pytest [EOL] import numpy as np [EOL] [EOL] from foolbox . models . base import _create_preprocessing_fn [EOL] [EOL] params = [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( np . array ( [ [number] , [number] , [number] ] , dtype = np . float64 ) , np . array ( [ [number] , [number] , [number] ] , dtype = np . float64 ) ) , ] [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , params ) def test_preprocessing ( params , image ) : [EOL] image_copy = image . copy ( ) [EOL] preprocessing = _create_preprocessing_fn ( params ) [EOL] preprocessed , backward = preprocessing ( image ) [EOL] assert image . shape == preprocessed . shape [EOL] assert image . dtype == preprocessed . dtype [EOL] assert np . allclose ( ( image - params [ [number] ] ) / params [ [number] ] , preprocessed ) [EOL] assert np . all ( image == image_copy ) [EOL] assert callable ( backward ) [EOL] dmdp = image [EOL] dmdx = backward ( dmdp ) [EOL] assert np . all ( image == image_copy ) [EOL] assert image . shape == dmdx . shape [EOL] assert image . dtype == dmdx . dtype [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] [EOL] from foolbox import set_seeds [EOL] from foolbox . attacks import LocalSearchAttack as Attack [EOL] [EOL] [EOL] def test_attack ( bn_adversarial ) : [EOL] set_seeds ( [number] ) [EOL] adv = bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , d = [number] , t = [number] ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_attack_gl ( gl_bn_adversarial ) : [EOL] set_seeds ( [number] ) [EOL] adv = gl_bn_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , d = [number] , t = [number] ) [EOL] assert adv . image is not None [EOL] assert adv . distance . value < np . inf [EOL] [EOL] [EOL] def test_targeted_attack ( bn_targeted_adversarial ) : [EOL] set_seeds ( [number] ) [EOL] adv = bn_targeted_adversarial [EOL] attack = Attack ( ) [EOL] attack ( adv , d = [number] ) [EOL] assert adv . image is None [EOL] assert adv . distance . value == np . inf [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Tuple , Any , Literal [EOL] import typing [EOL] import typing_extensions [EOL] import pytest [EOL] import numpy as np [EOL] [EOL] from foolbox . models import CaffeModel [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [number] , [number] ) , ( [number] , [number] ) ] , indirect = [ [string] ] ) def test_caffe_model ( bn_model_caffe , num_classes ) : [EOL] model = bn_model_caffe [EOL] test_images = np . random . rand ( [number] , num_classes , [number] , [number] ) . astype ( np . float32 ) [EOL] test_label = [number] [EOL] [EOL] assert model . batch_predictions ( test_images ) . shape == ( [number] , num_classes ) [EOL] [EOL] test_logits = model . predictions ( test_images [ [number] ] ) [EOL] assert test_logits . shape == ( num_classes , ) [EOL] [EOL] test_gradient = model . gradient ( test_images [ [number] ] , test_label ) [EOL] assert test_gradient . shape == test_images [ [number] ] . shape [EOL] [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_logits ) [EOL] np . testing . assert_almost_equal ( model . predictions_and_gradient ( test_images [ [number] ] , test_label ) [ [number] ] , test_gradient ) [EOL] [EOL] assert model . num_classes ( ) == num_classes [EOL] [EOL] [EOL] def test_caffe_model_gradient ( tmpdir ) : [EOL] import caffe [EOL] from caffe import layers as L [EOL] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes = [number] [EOL] [EOL] net_spec = caffe . NetSpec ( ) [EOL] net_spec . data = L . Input ( name = [string] , shape = dict ( dim = [ [number] , num_classes , [number] , [number] ] ) ) [EOL] net_spec . reduce_1 = L . Reduction ( net_spec . data , reduction_param = { [string] : [number] , [string] : [number] } ) [EOL] net_spec . output = L . Reduction ( net_spec . reduce_1 , reduction_param = { [string] : [number] , [string] : [number] } ) [EOL] net_spec . label = L . Input ( name = [string] , shape = dict ( dim = [ [number] ] ) ) [EOL] net_spec . loss = L . SoftmaxWithLoss ( net_spec . output , net_spec . label ) [EOL] wf = tmpdir . mkdir ( [string] ) . join ( [string] . format ( num_classes ) ) [EOL] wf . write ( [string] + str ( net_spec . to_proto ( ) ) ) [EOL] preprocessing = ( np . arange ( num_classes ) [ : , None , None ] , np . random . uniform ( size = ( channels , [number] , [number] ) ) + [number] ) [EOL] net = caffe . Net ( str ( wf ) , caffe . TEST ) [EOL] model = CaffeModel ( net , bounds = bounds , preprocessing = preprocessing ) [EOL] [EOL] epsilon = [number] [EOL] [EOL] np . random . seed ( [number] ) [EOL] test_image = np . random . rand ( channels , [number] , [number] ) . astype ( np . float32 ) [EOL] test_label = [number] [EOL] [EOL] _ , g1 = model . predictions_and_gradient ( test_image , test_label ) [EOL] [EOL] l1 = model . _loss_fn ( test_image - epsilon / [number] * g1 , test_label ) [EOL] l2 = model . _loss_fn ( test_image + epsilon / [number] * g1 , test_label ) [EOL] assert [number] * ( l2 - l1 ) > [number] [EOL] [EOL] [comment] [EOL] np . testing . assert_array_almost_equal ( [number] * ( l2 - l1 ) , [number] * epsilon * np . linalg . norm ( g1 ) ** [number] , decimal = [number] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [number] , [number] ) , ( [number] , [number] ) ] , indirect = [ [string] ] ) def test_caffe_backward ( bn_model_caffe , num_classes ) : [EOL] model = bn_model_caffe [EOL] test_image = np . random . rand ( num_classes , [number] , [number] ) . astype ( np . float32 ) [EOL] test_grad_pre = np . random . rand ( num_classes ) . astype ( np . float32 ) [EOL] [EOL] test_grad = model . backward ( test_grad_pre , test_image ) [EOL] assert test_grad . shape == test_image . shape [EOL] [EOL] manual_grad = np . repeat ( np . repeat ( ( test_grad_pre / [number] ) . reshape ( ( - [number] , [number] , [number] ) ) , [number] , axis = [number] ) , [number] , axis = [number] ) [EOL] [EOL] np . testing . assert_almost_equal ( test_grad , manual_grad ) [EOL] [EOL] [EOL] def test_caffe_model_preprocessing_shape_change ( tmpdir ) : [EOL] import caffe [EOL] from caffe import layers as L [EOL] [EOL] bounds = ( [number] , [number] ) [EOL] channels = num_classes = [number] [EOL] [EOL] net_spec = caffe . NetSpec ( ) [EOL] net_spec . data = L . Input ( name = [string] , shape = dict ( dim = [ [number] , num_classes , [number] , [number] ] ) ) [EOL] net_spec . reduce_1 = L . Reduction ( net_spec . data , reduction_param = { [string] : [number] , [string] : [number] } ) [EOL] net_spec . output = L . Reduction ( net_spec . reduce_1 , reduction_param = { [string] : [number] , [string] : [number] } ) [EOL] net_spec . label = L . Input ( name = [string] , shape = dict ( dim = [ [number] ] ) ) [EOL] net_spec . loss = L . SoftmaxWithLoss ( net_spec . output , net_spec . label ) [EOL] wf = tmpdir . mkdir ( [string] ) . join ( [string] . format ( num_classes ) ) [EOL] wf . write ( [string] + str ( net_spec . to_proto ( ) ) ) [EOL] net = caffe . Net ( str ( wf ) , caffe . TEST ) [EOL] model1 = CaffeModel ( net , bounds = bounds ) [EOL] [EOL] def preprocessing2 ( x ) : [EOL] if x . ndim == [number] : [EOL] x = np . transpose ( x , axes = ( [number] , [number] , [number] ) ) [EOL] elif x . ndim == [number] : [EOL] x = np . transpose ( x , axes = ( [number] , [number] , [number] , [number] ) ) [EOL] [EOL] def grad ( dmdp ) : [EOL] assert dmdp . ndim == [number] [EOL] dmdx = np . transpose ( dmdp , axes = ( [number] , [number] , [number] ) ) [EOL] return dmdx [EOL] [EOL] return x , grad [EOL] [EOL] model2 = CaffeModel ( net , bounds = bounds , preprocessing = preprocessing2 ) [EOL] [EOL] np . random . seed ( [number] ) [EOL] test_images_nhwc = np . random . rand ( [number] , [number] , [number] , channels ) . astype ( np . float32 ) [EOL] test_images_nchw = np . transpose ( test_images_nhwc , ( [number] , [number] , [number] , [number] ) ) [EOL] [EOL] p1 = model1 . batch_predictions ( test_images_nchw ) [EOL] p2 = model2 . batch_predictions ( test_images_nhwc ) [EOL] [EOL] assert np . all ( p1 == p2 ) [EOL] [EOL] p1 = model1 . predictions ( test_images_nchw [ [number] ] ) [EOL] p2 = model2 . predictions ( test_images_nhwc [ [number] ] ) [EOL] [EOL] assert np . all ( p1 == p2 ) [EOL] [EOL] g1 = model1 . gradient ( test_images_nchw [ [number] ] , [number] ) [EOL] assert g1 . ndim == [number] [EOL] g1 = np . transpose ( g1 , ( [number] , [number] , [number] ) ) [EOL] g2 = model2 . gradient ( test_images_nhwc [ [number] ] , [number] ) [EOL] [EOL] np . testing . assert_array_almost_equal ( g1 , g2 ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from . zoo import get_model [comment] [EOL] from . weights_fetcher import fetch_weights [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] from . git_cloner import clone [EOL] from . model_loader import ModelLoader [EOL] [EOL] [EOL] def get_model ( url ) : [EOL] [docstring] [EOL] repo_path = clone ( url ) [EOL] loader = ModelLoader . get ( ) [EOL] model = loader . load ( repo_path ) [EOL] [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] import warnings [EOL] [EOL] from . base import DifferentiableModel [EOL] [EOL] [EOL] class PyTorchModel ( DifferentiableModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model , bounds , num_classes , channel_axis = [number] , device = None , preprocessing = ( [number] , [number] ) ) : [EOL] [EOL] [comment] [EOL] import torch [EOL] [EOL] super ( PyTorchModel , self ) . __init__ ( bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] self . _num_classes = num_classes [EOL] [EOL] if device is None : [EOL] self . device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] elif isinstance ( device , str ) : [EOL] self . device = torch . device ( device ) [EOL] else : [EOL] self . device = device [EOL] self . _model = model . to ( self . device ) [EOL] [EOL] if model . training : [EOL] warnings . warn ( [string] [string] [string] ) [EOL] [EOL] def _old_pytorch ( self ) : [EOL] [comment] [EOL] import torch [EOL] version = torch . __version__ . split ( [string] ) [ : [number] ] [EOL] pre04 = int ( version [ [number] ] ) == [number] and int ( version [ [number] ] ) < [number] [EOL] return pre04 [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] [comment] [EOL] import torch [EOL] if self . _old_pytorch ( ) : [comment] [EOL] from torch . autograd import Variable [EOL] [EOL] images , _ = self . _process_input ( images ) [EOL] n = len ( images ) [EOL] images = torch . from_numpy ( images ) . to ( self . device ) [EOL] [EOL] if self . _old_pytorch ( ) : [comment] [EOL] images = Variable ( images , volatile = True ) [EOL] predictions = self . _model ( images ) [EOL] predictions = predictions . data [EOL] else : [EOL] predictions = self . _model ( images ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] predictions = predictions . to ( [string] ) [EOL] if not self . _old_pytorch ( ) : [EOL] predictions = predictions . detach ( ) [EOL] predictions = predictions . numpy ( ) [EOL] assert predictions . ndim == [number] [EOL] assert predictions . shape == ( n , self . num_classes ( ) ) [EOL] return predictions [EOL] [EOL] def num_classes ( self ) : [EOL] return self . _num_classes [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] [comment] [EOL] import torch [EOL] import torch . nn as nn [EOL] if self . _old_pytorch ( ) : [comment] [EOL] from torch . autograd import Variable [EOL] [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] target = np . array ( [ label ] ) [EOL] target = torch . from_numpy ( target ) . long ( ) . to ( self . device ) [EOL] [EOL] images = image [ np . newaxis ] [EOL] images = torch . from_numpy ( images ) . to ( self . device ) [EOL] [EOL] if self . _old_pytorch ( ) : [comment] [EOL] target = Variable ( target ) [EOL] images = Variable ( images , requires_grad = True ) [EOL] else : [EOL] images . requires_grad_ ( ) [EOL] [EOL] predictions = self . _model ( images ) [EOL] ce = nn . CrossEntropyLoss ( ) [EOL] loss = ce ( predictions , target ) [EOL] loss . backward ( ) [EOL] grad = images . grad [EOL] [EOL] if self . _old_pytorch ( ) : [comment] [EOL] predictions = predictions . data [EOL] predictions = predictions . to ( [string] ) [EOL] [EOL] if not self . _old_pytorch ( ) : [EOL] predictions = predictions . detach ( ) [EOL] predictions = predictions . numpy ( ) [EOL] predictions = np . squeeze ( predictions , axis = [number] ) [EOL] assert predictions . ndim == [number] [EOL] assert predictions . shape == ( self . num_classes ( ) , ) [EOL] [EOL] if self . _old_pytorch ( ) : [comment] [EOL] grad = grad . data [EOL] grad = grad . to ( [string] ) [EOL] if not self . _old_pytorch ( ) : [EOL] grad = grad . detach ( ) [EOL] grad = grad . numpy ( ) [EOL] grad = np . squeeze ( grad , axis = [number] ) [EOL] grad = self . _process_gradient ( dpdx , grad ) [EOL] assert grad . shape == input_shape [EOL] [EOL] return predictions , grad [EOL] [EOL] def _loss_fn ( self , image , label ) : [EOL] [comment] [EOL] import torch [EOL] import torch . nn as nn [EOL] if self . _old_pytorch ( ) : [comment] [EOL] from torch . autograd import Variable [EOL] [EOL] image , _ = self . _process_input ( image ) [EOL] target = np . array ( [ label ] ) [EOL] target = torch . from_numpy ( target ) . long ( ) . to ( self . device ) [EOL] if self . _old_pytorch ( ) : [comment] [EOL] target = Variable ( target ) [EOL] [EOL] images = torch . from_numpy ( image [ None ] ) . to ( self . device ) [EOL] if self . _old_pytorch ( ) : [comment] [EOL] images = Variable ( images , volatile = True ) [EOL] predictions = self . _model ( images ) [EOL] ce = nn . CrossEntropyLoss ( ) [EOL] loss = ce ( predictions , target ) [EOL] if self . _old_pytorch ( ) : [comment] [EOL] loss = loss . data [EOL] loss = loss . to ( [string] ) [EOL] loss = loss . numpy ( ) [EOL] return loss [EOL] [EOL] def backward ( self , gradient , image ) : [EOL] [comment] [EOL] import torch [EOL] if self . _old_pytorch ( ) : [comment] [EOL] from torch . autograd import Variable [EOL] [EOL] assert gradient . ndim == [number] [EOL] [EOL] gradient = torch . from_numpy ( gradient ) . to ( self . device ) [EOL] if self . _old_pytorch ( ) : [comment] [EOL] gradient = Variable ( gradient ) [EOL] [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] images = image [ np . newaxis ] [EOL] images = torch . from_numpy ( images ) . to ( self . device ) [EOL] if self . _old_pytorch ( ) : [comment] [EOL] images = Variable ( images , requires_grad = True ) [EOL] else : [EOL] images . requires_grad_ ( ) [EOL] predictions = self . _model ( images ) [EOL] [EOL] predictions = predictions [ [number] ] [EOL] [EOL] assert gradient . dim ( ) == [number] [EOL] assert predictions . dim ( ) == [number] [EOL] assert gradient . size ( ) == predictions . size ( ) [EOL] [EOL] loss = torch . dot ( predictions , gradient ) [EOL] loss . backward ( ) [EOL] [comment] [EOL] [EOL] grad = images . grad [EOL] [EOL] if self . _old_pytorch ( ) : [comment] [EOL] grad = grad . data [EOL] grad = grad . to ( [string] ) [EOL] if not self . _old_pytorch ( ) : [EOL] grad = grad . detach ( ) [EOL] grad = grad . numpy ( ) [EOL] grad = np . squeeze ( grad , axis = [number] ) [EOL] grad = self . _process_gradient ( dpdx , grad ) [EOL] assert grad . shape == input_shape [EOL] [EOL] return grad [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] [EOL] from . base import DifferentiableModel [EOL] [EOL] [EOL] class LasagneModel ( DifferentiableModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_layer , logits_layer , bounds , channel_axis = [number] , preprocessing = ( [number] , [number] ) ) : [EOL] [EOL] super ( LasagneModel , self ) . __init__ ( bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] [comment] [EOL] import theano as th [EOL] import theano . tensor as T [EOL] import lasagne [EOL] [EOL] images = input_layer . input_var [EOL] labels = T . ivector ( [string] ) [EOL] bw_gradient_pre = T . fmatrix ( [string] ) [EOL] [EOL] shape = lasagne . layers . get_output_shape ( logits_layer ) [EOL] _ , num_classes = shape [EOL] self . _num_classes = num_classes [EOL] [EOL] logits = lasagne . layers . get_output ( logits_layer ) [EOL] [EOL] probs = T . nnet . nnet . softmax ( logits ) [EOL] [EOL] loss = lasagne . objectives . categorical_crossentropy ( probs , labels ) [EOL] gradient = th . gradient . grad ( loss [ [number] ] , images ) [EOL] [EOL] bw_loss = ( logits * bw_gradient_pre ) . sum ( ) [EOL] bw_gradient = th . gradient . grad ( bw_loss , images ) [EOL] [EOL] self . _batch_prediction_fn = th . function ( [ images ] , logits ) [EOL] self . _predictions_and_gradient_fn = th . function ( [ images , labels ] , [ logits , gradient ] ) [EOL] self . _gradient_fn = th . function ( [ images , labels ] , gradient ) [EOL] self . _loss_fn = th . function ( [ images , labels ] , loss ) [EOL] self . _bw_gradient_fn = th . function ( [ bw_gradient_pre , images ] , bw_gradient ) [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] images , _ = self . _process_input ( images ) [EOL] predictions = self . _batch_prediction_fn ( images ) [EOL] assert predictions . shape == ( images . shape [ [number] ] , self . num_classes ( ) ) [EOL] return predictions [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] label = np . array ( label , dtype = np . int32 ) [EOL] predictions , gradient = self . _predictions_and_gradient_fn ( image [ np . newaxis ] , label [ np . newaxis ] ) [EOL] predictions = np . squeeze ( predictions , axis = [number] ) [EOL] gradient = np . squeeze ( gradient , axis = [number] ) [EOL] gradient = gradient . astype ( image . dtype , copy = False ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] assert predictions . shape == ( self . num_classes ( ) , ) [EOL] assert gradient . shape == input_shape [EOL] assert gradient . dtype == image . dtype [EOL] return predictions , gradient [EOL] [EOL] def gradient ( self , image , label ) : [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] label = np . array ( label , dtype = np . int32 ) [EOL] gradient = self . _gradient_fn ( image [ np . newaxis ] , label [ np . newaxis ] ) [EOL] gradient = np . squeeze ( gradient , axis = [number] ) [EOL] gradient = gradient . astype ( image . dtype , copy = False ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] assert gradient . shape == input_shape [EOL] assert gradient . dtype == image . dtype [EOL] return gradient [EOL] [EOL] def num_classes ( self ) : [EOL] return self . _num_classes [EOL] [EOL] def backward ( self , gradient , image ) : [EOL] assert gradient . ndim == [number] [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] gradient = self . _bw_gradient_fn ( gradient [ np . newaxis ] , image [ np . newaxis ] ) [EOL] gradient = np . squeeze ( gradient , axis = [number] ) [EOL] gradient = gradient . astype ( image . dtype , copy = False ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] assert gradient . shape == input_shape [EOL] assert gradient . dtype == image . dtype [EOL] return gradient [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] from __future__ import absolute_import [EOL] [EOL] from . base import DifferentiableModel [EOL] [EOL] import numpy as np [EOL] [EOL] [EOL] class MXNetGluonModel ( DifferentiableModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , block , bounds , num_classes , ctx = None , channel_axis = [number] , preprocessing = ( [number] , [number] ) ) : [EOL] import mxnet as mx [EOL] self . _num_classes = num_classes [EOL] [EOL] if ctx is None : [EOL] ctx = mx . cpu ( ) [EOL] [EOL] super ( MXNetGluonModel , self ) . __init__ ( bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] self . _device = ctx [EOL] self . _block = block [EOL] [EOL] def num_classes ( self ) : [EOL] return self . _num_classes [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] import mxnet as mx [EOL] images , _ = self . _process_input ( images ) [EOL] data_array = mx . nd . array ( images , ctx = self . _device ) [EOL] data_array . attach_grad ( ) [EOL] with mx . autograd . record ( train_mode = False ) : [EOL] L = self . _block ( data_array ) [EOL] return L . asnumpy ( ) [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] import mxnet as mx [EOL] image , dpdx = self . _process_input ( image ) [EOL] label = mx . nd . array ( [ label ] ) [EOL] data_array = mx . nd . array ( image [ np . newaxis ] , ctx = self . _device ) [EOL] data_array . attach_grad ( ) [EOL] with mx . autograd . record ( train_mode = False ) : [EOL] logits = self . _block ( data_array ) [EOL] loss = mx . nd . softmax_cross_entropy ( logits , label ) [EOL] loss . backward ( ) [EOL] predictions = np . squeeze ( logits . asnumpy ( ) , axis = [number] ) [EOL] gradient = np . squeeze ( data_array . grad . asnumpy ( ) , axis = [number] ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] return predictions , gradient [EOL] [EOL] def _loss_fn ( self , image , label ) : [EOL] import mxnet as mx [EOL] image , _ = self . _process_input ( image ) [EOL] label = mx . nd . array ( [ label ] ) [EOL] data_array = mx . nd . array ( image [ np . newaxis ] , ctx = self . _device ) [EOL] data_array . attach_grad ( ) [EOL] with mx . autograd . record ( train_mode = False ) : [EOL] logits = self . _block ( data_array ) [EOL] loss = mx . nd . softmax_cross_entropy ( logits , label ) [EOL] loss . backward ( ) [EOL] return loss . asnumpy ( ) [EOL] [EOL] def backward ( self , gradient , image ) : [comment] [EOL] [comment] [EOL] [comment] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict [EOL] import typing [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] [EOL] from . base import DifferentiableModel [EOL] [EOL] [EOL] class MXNetModel ( DifferentiableModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data , logits , args , ctx , num_classes , bounds , channel_axis = [number] , aux_states = None , preprocessing = ( [number] , [number] ) ) : [EOL] [EOL] super ( MXNetModel , self ) . __init__ ( bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] import mxnet as mx [EOL] [EOL] self . _num_classes = num_classes [EOL] [EOL] self . _device = ctx [EOL] [EOL] self . _data_sym = data [EOL] self . _batch_logits_sym = logits [EOL] [EOL] label = mx . symbol . Variable ( [string] ) [EOL] self . _label_sym = label [EOL] [EOL] [comment] [EOL] log_softmax = mx . sym . log_softmax ( logits ) [EOL] [EOL] loss = mx . sym . sum ( mx . sym . one_hot ( indices = label , depth = num_classes ) * log_softmax ) [EOL] [EOL] [comment] [EOL] self . _loss_sym = loss [EOL] [EOL] self . _args_map = args . copy ( ) [EOL] self . _aux_map = aux_states . copy ( ) if aux_states is not None else None [EOL] [EOL] [comment] [EOL] for k in self . _args_map . keys ( ) : [EOL] self . _args_map [ k ] = self . _args_map [ k ] . as_in_context ( ctx ) [comment] [EOL] [EOL] if aux_states is not None : [EOL] for k in self . _aux_map . keys ( ) : [comment] [EOL] self . _aux_map [ k ] = self . _aux_map [ k ] . as_in_context ( ctx ) [comment] [EOL] [EOL] def num_classes ( self ) : [EOL] return self . _num_classes [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] import mxnet as mx [EOL] images , _ = self . _process_input ( images ) [EOL] data_array = mx . nd . array ( images , ctx = self . _device ) [EOL] self . _args_map [ self . _data_sym . name ] = data_array [EOL] model = self . _batch_logits_sym . bind ( ctx = self . _device , args = self . _args_map , grad_req = [string] , aux_states = self . _aux_map ) [EOL] model . forward ( is_train = False ) [EOL] logits_array = model . outputs [ [number] ] [EOL] logits = logits_array . asnumpy ( ) [EOL] return logits [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] import mxnet as mx [EOL] label = np . asarray ( label ) [EOL] image , dpdx = self . _process_input ( image ) [EOL] data_array = mx . nd . array ( image [ np . newaxis ] , ctx = self . _device ) [EOL] label_array = mx . nd . array ( label [ np . newaxis ] , ctx = self . _device ) [EOL] self . _args_map [ self . _data_sym . name ] = data_array [EOL] self . _args_map [ self . _label_sym . name ] = label_array [EOL] [EOL] grad_array = mx . nd . zeros ( image [ np . newaxis ] . shape , ctx = self . _device ) [EOL] grad_map = { self . _data_sym . name : grad_array } [EOL] [EOL] logits_loss = mx . sym . Group ( [ self . _batch_logits_sym , self . _loss_sym ] ) [EOL] model = logits_loss . bind ( ctx = self . _device , args = self . _args_map , args_grad = grad_map , grad_req = [string] , aux_states = self . _aux_map ) [EOL] model . forward ( is_train = False ) [EOL] logits_array = model . outputs [ [number] ] [EOL] model . backward ( [ mx . nd . zeros ( logits_array . shape ) , mx . nd . array ( np . array ( [ [number] ] ) ) ] ) [EOL] logits = logits_array . asnumpy ( ) [EOL] gradient = grad_array . asnumpy ( ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] return np . squeeze ( logits , axis = [number] ) , np . squeeze ( gradient , axis = [number] ) [EOL] [EOL] def _loss_fn ( self , image , label ) : [EOL] import mxnet as mx [EOL] image , _ = self . _process_input ( image ) [EOL] data_array = mx . nd . array ( image [ np . newaxis ] , ctx = self . _device ) [EOL] label_array = mx . nd . array ( np . array ( [ label ] ) , ctx = self . _device ) [EOL] self . _args_map [ self . _data_sym . name ] = data_array [EOL] self . _args_map [ self . _label_sym . name ] = label_array [EOL] model = self . _loss_sym . bind ( ctx = self . _device , args = self . _args_map , grad_req = [string] , aux_states = self . _aux_map ) [EOL] model . forward ( is_train = False ) [EOL] loss_array = model . outputs [ [number] ] [EOL] loss = loss_array . asnumpy ( ) [ [number] ] [EOL] return loss [EOL] [EOL] def backward ( self , gradient , image ) : [EOL] import mxnet as mx [EOL] [EOL] assert gradient . ndim == [number] [EOL] [EOL] image , dpdx = self . _process_input ( image ) [EOL] data_array = mx . nd . array ( image [ np . newaxis ] , ctx = self . _device ) [EOL] self . _args_map [ self . _data_sym . name ] = data_array [EOL] [EOL] grad_array = mx . nd . zeros ( image [ np . newaxis ] . shape , ctx = self . _device ) [EOL] grad_map = { self . _data_sym . name : grad_array } [EOL] [EOL] logits = self . _batch_logits_sym . bind ( ctx = self . _device , args = self . _args_map , args_grad = grad_map , grad_req = [string] , aux_states = self . _aux_map ) [EOL] [EOL] logits . forward ( is_train = False ) [EOL] [EOL] gradient_pre_array = mx . nd . array ( gradient [ np . newaxis ] , ctx = self . _device ) [EOL] logits . backward ( gradient_pre_array ) [EOL] [EOL] gradient = grad_array . asnumpy ( ) [EOL] gradient = np . squeeze ( gradient , axis = [number] ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] return gradient [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[unknown,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0
from typing import Any [EOL] import typing [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] import logging [EOL] [EOL] from . base import DifferentiableModel [EOL] [EOL] [EOL] class TensorFlowModel ( DifferentiableModel ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , images , logits , bounds , channel_axis = [number] , preprocessing = ( [number] , [number] ) ) : [EOL] [EOL] super ( TensorFlowModel , self ) . __init__ ( bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] [comment] [EOL] import tensorflow as tf [EOL] [EOL] session = tf . get_default_session ( ) [EOL] if session is None : [EOL] logging . warning ( [string] [string] ) [EOL] session = tf . Session ( graph = images . graph ) [EOL] self . _created_session = True [EOL] else : [EOL] self . _created_session = False [EOL] assert session . graph == images . graph , [string] [EOL] [EOL] with session . graph . as_default ( ) : [EOL] self . _session = session [EOL] self . _images = images [EOL] self . _batch_logits = logits [EOL] self . _logits = tf . squeeze ( logits , axis = [number] ) [EOL] self . _label = tf . placeholder ( tf . int64 , ( ) , name = [string] ) [EOL] [EOL] loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = self . _label [ tf . newaxis ] , logits = self . _logits [ tf . newaxis ] ) [EOL] self . _loss = tf . squeeze ( loss , axis = [number] ) [EOL] gradients = tf . gradients ( loss , images ) [EOL] assert len ( gradients ) == [number] [EOL] if gradients [ [number] ] is None : [EOL] gradients [ [number] ] = tf . zeros_like ( images ) [EOL] self . _gradient = tf . squeeze ( gradients [ [number] ] , axis = [number] ) [EOL] [EOL] self . _bw_gradient_pre = tf . placeholder ( tf . float32 , self . _logits . shape ) [comment] [EOL] bw_loss = tf . reduce_sum ( self . _logits * self . _bw_gradient_pre ) [EOL] bw_gradients = tf . gradients ( bw_loss , images ) [EOL] assert len ( bw_gradients ) == [number] [EOL] if bw_gradients [ [number] ] is None : [EOL] bw_gradients [ [number] ] = tf . zeros_like ( images ) [EOL] self . _bw_gradient = tf . squeeze ( bw_gradients [ [number] ] , axis = [number] ) [EOL] [EOL] @ classmethod def from_keras ( cls , model , bounds , input_shape = None , channel_axis = [number] , preprocessing = ( [number] , [number] ) ) : [EOL] [docstring] [EOL] import tensorflow as tf [EOL] if input_shape is None : [EOL] try : [EOL] input_shape = model . input_shape [ [number] : ] [EOL] except AttributeError : [EOL] raise ValueError ( [string] [string] ) [EOL] with tf . keras . backend . get_session ( ) . as_default ( ) : [EOL] inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) [EOL] logits = model ( inputs ) [EOL] return cls ( inputs , logits , bounds = bounds , channel_axis = channel_axis , preprocessing = preprocessing ) [EOL] [EOL] def __exit__ ( self , exc_type , exc_value , traceback ) : [EOL] if self . _created_session : [EOL] self . _session . close ( ) [EOL] return None [EOL] [EOL] @ property def session ( self ) : [EOL] return self . _session [EOL] [EOL] def num_classes ( self ) : [EOL] _ , n = self . _batch_logits . get_shape ( ) . as_list ( ) [EOL] return n [EOL] [EOL] def batch_predictions ( self , images ) : [EOL] images , _ = self . _process_input ( images ) [EOL] predictions = self . _session . run ( self . _batch_logits , feed_dict = { self . _images : images } ) [EOL] return predictions [EOL] [EOL] def predictions_and_gradient ( self , image , label ) : [EOL] image , dpdx = self . _process_input ( image ) [EOL] predictions , gradient = self . _session . run ( [ self . _logits , self . _gradient ] , feed_dict = { self . _images : image [ np . newaxis ] , self . _label : label } ) [EOL] gradient = self . _process_gradient ( dpdx , gradient ) [EOL] return predictions , gradient [EOL] [EOL] def gradient ( self , image , label ) : [EOL] image , dpdx = self . _process_input ( image ) [EOL] g = self . _session . run ( self . _gradient , feed_dict = { self . _images : image [ np . newaxis ] , self . _label : label } ) [EOL] g = self . _process_gradient ( dpdx , g ) [EOL] return g [EOL] [EOL] def _loss_fn ( self , image , label ) : [EOL] image , dpdx = self . _process_input ( image ) [EOL] loss = self . _session . run ( self . _loss , feed_dict = { self . _images : image [ np . newaxis ] , self . _label : label } ) [EOL] return loss [EOL] [EOL] def backward ( self , gradient , image ) : [EOL] assert gradient . ndim == [number] [EOL] input_shape = image . shape [EOL] image , dpdx = self . _process_input ( image ) [EOL] g = self . _session . run ( self . _bw_gradient , feed_dict = { self . _images : image [ np . newaxis ] , self . _bw_gradient_pre : gradient } ) [EOL] g = self . _process_gradient ( dpdx , g ) [EOL] assert g . shape == input_shape [EOL] return g [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0