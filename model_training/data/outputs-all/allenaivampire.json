from typing import Dict [EOL] import typing [EOL] DATASETS = { [string] : { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] } } [EOL]	0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import typing [EOL] from environments . random_search import RandomSearch [EOL] from environments . datasets import DATASETS [EOL] import os [EOL] [EOL] [EOL] CLASSIFIER = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : [number] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ . get ( [string] , None ) , [string] : [number] , [string] : [ [string] ] , [string] : [ [string] , [string] ] , [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : os . environ . get ( [string] , None ) , [string] : os . environ . get ( [string] , None ) , [string] : [number] , [string] : [number] , [string] : [number] , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_subset ( [string] , [string] , [string] , [string] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : RandomSearch . random_choice ( [string] , [string] , [string] ) , [string] : RandomSearch . random_choice ( [number] , [number] ) , } [EOL] [EOL] VAMPIRE = { [string] : os . environ . get ( [string] , [number] ) , [string] : [string] , [string] : None , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : os . environ [ [string] ] + [string] , [string] : [number] , [string] : [string] , [string] : [string] , [string] : [number] , [string] : [string] , [string] : [number] , [string] : RandomSearch . random_integer ( [number] , [number] ) , [string] : [number] , [string] : [number] , [string] : True , [string] : [number] , [string] : [number] , [string] : os . environ . get ( [string] , [number] ) , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [string] } [EOL] [EOL] [EOL] [EOL] ENVIRONMENTS = { [string] : VAMPIRE , [string] : CLASSIFIER , } [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0
from environments . environments import ENVIRONMENTS [EOL] from environments . random_search import RandomSearch	0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Callable , Dict , Any [EOL] import typing [EOL] import os [EOL] from typing import Any , Dict [EOL] [EOL] import numpy as np [EOL] [EOL] [EOL] class RandomSearch : [EOL] [EOL] @ staticmethod def random_choice ( * args ) : [EOL] choices = [ ] [EOL] for arg in args : [EOL] choices . append ( arg ) [EOL] return lambda : np . random . choice ( choices ) [EOL] [EOL] @ staticmethod def random_integer ( low , high ) : [EOL] return lambda : int ( np . random . randint ( low , high ) ) [EOL] [EOL] @ staticmethod def random_loguniform ( low , high ) : [EOL] return lambda : str ( np . exp ( np . random . uniform ( np . log ( low ) , np . log ( high ) ) ) ) [EOL] [EOL] @ staticmethod def random_subset ( * args ) : [EOL] choices = [ ] [EOL] for arg in args : [EOL] choices . append ( arg ) [EOL] func = lambda : np . random . choice ( choices , np . random . randint ( [number] , len ( choices ) + [number] ) , replace = False ) [EOL] return func [EOL] [EOL] @ staticmethod def random_pair ( * args ) : [EOL] choices = [ ] [EOL] for arg in args : [EOL] choices . append ( arg ) [EOL] func = lambda : np . random . choice ( choices , [number] , replace = False ) [EOL] return func [EOL] [EOL] @ staticmethod def random_uniform ( low , high ) : [EOL] return lambda : np . random . uniform ( low , high ) [EOL] [EOL] [EOL] class HyperparameterSearch : [EOL] [EOL] def __init__ ( self , ** kwargs ) : [EOL] self . search_space = { } [EOL] self . lambda_ = lambda : [number] [EOL] for key , val in kwargs . items ( ) : [EOL] self . search_space [ key ] = val [EOL] [EOL] def parse ( self , val ) : [EOL] if isinstance ( val , type ( self . lambda_ ) ) and val . __name__ == self . lambda_ . __name__ : [EOL] val = val ( ) [EOL] if isinstance ( val , ( int , np . int ) ) : [EOL] return int ( val ) [EOL] elif isinstance ( val , ( float , np . float ) ) : [EOL] return float ( val ) [EOL] elif isinstance ( val , ( np . ndarray , list ) ) : [EOL] return [string] . join ( val ) [EOL] else : [EOL] return val [EOL] elif isinstance ( val , ( int , np . int ) ) : [EOL] return int ( val ) [EOL] elif isinstance ( val , ( float , np . float ) ) : [EOL] return float ( val ) [EOL] elif isinstance ( val , ( np . ndarray , list ) ) : [EOL] return [string] . join ( val ) [EOL] elif val is None : [EOL] return None [EOL] else : [EOL] return val [EOL] [EOL] [EOL] def sample ( self ) : [EOL] res = { } [EOL] for key , val in self . search_space . items ( ) : [EOL] res [ key ] = self . parse ( val ) [EOL] return res [EOL] [EOL] def update_environment ( self , sample ) : [EOL] for key , val in sample . items ( ) : [EOL] os . environ [ key ] = str ( val ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Dict$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [EOL] from typing import List [EOL] import argparse [EOL] import typing [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import sys [EOL] from subprocess import CalledProcessError , run [EOL] [EOL] [EOL] def main ( arguments ) : [EOL] try : [EOL] print ( [string] + str ( arguments ) ) [EOL] if [string] in args : [EOL] print ( [string] , flush = True ) [EOL] run ( [string] , shell = True , check = True ) [EOL] [EOL] if [string] in arguments : [EOL] print ( [string] , flush = True ) [EOL] run ( [string] , shell = True , check = True ) [EOL] print ( [string] ) [EOL] [EOL] if [string] in arguments : [EOL] print ( [string] , flush = True ) [EOL] run ( [string] , shell = True , check = True ) [EOL] print ( [string] ) [EOL] [EOL] if [string] in arguments : [EOL] print ( [string] , flush = True ) [EOL] run ( [string] , shell = True , check = True ) [EOL] print ( [string] ) [EOL] [EOL] except CalledProcessError : [EOL] [comment] [EOL] sys . exit ( [number] ) [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] checks = [ [string] , [string] , [string] , [string] ] [EOL] [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = str , required = False , nargs = [string] , choices = checks ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] if args . checks : [EOL] run_checks = args . checks [EOL] else : [EOL] run_checks = checks [EOL] [EOL] main ( run_checks ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $typing.List[builtins.str]$ 0 0 $typing.List[builtins.str]$ 0 $argparse.Namespace$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0
from typing import List , Dict , Any [EOL] import argparse [EOL] import environments [EOL] import typing [EOL] import argparse [EOL] import json [EOL] import os [EOL] import random [EOL] import shutil [EOL] import subprocess [EOL] import tempfile [EOL] from typing import Any , Dict [EOL] [EOL] from allennlp . common . params import Params [EOL] [EOL] from environments import ENVIRONMENTS [EOL] from environments . random_search import HyperparameterSearch [EOL] [EOL] random_int = random . randint ( [number] , [number] ** [number] ) [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( ) [comment] [EOL] parser . add_argument ( [string] , [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = str , help = [string] , required = True ) [EOL] parser . add_argument ( [string] , [string] , type = str , help = [string] , required = True ) [EOL] parser . add_argument ( [string] , [string] , type = str , help = [string] , required = True ) [EOL] parser . add_argument ( [string] , [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = str , required = False , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = str , required = False , help = [string] ) [EOL] [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] env = ENVIRONMENTS [ args . environment . upper ( ) ] [EOL] [EOL] [EOL] space = HyperparameterSearch ( ** env ) [EOL] [EOL] sample = space . sample ( ) [EOL] [EOL] for key , val in sample . items ( ) : [EOL] os . environ [ key ] = str ( val ) [EOL] [EOL] if args . device : [EOL] os . environ [ [string] ] = args . device [EOL] [EOL] if args . seed : [EOL] os . environ [ [string] ] = args . seed [EOL] [EOL] [EOL] allennlp_command = [ [string] , [string] , [string] , [string] , args . config , [string] , args . serialization_dir ] [EOL] [EOL] if args . seed : [EOL] allennlp_command [ - [number] ] = allennlp_command [ - [number] ] + [string] + args . seed [EOL] [EOL] if args . recover : [EOL] def append_seed_to_config ( seed , serialization_dir ) : [EOL] seed = str ( seed ) [EOL] seed_dict = { [string] : seed , [string] : seed , [string] : seed } [EOL] config_path = os . path . join ( serialization_dir , [string] ) [EOL] with open ( config_path , [string] ) as f : [EOL] config_dict = json . load ( f ) [EOL] seed_dict . update ( config_dict ) [EOL] f . seek ( [number] ) [EOL] json . dump ( seed_dict , f , indent = [number] ) [EOL] [EOL] append_seed_to_config ( seed = args . seed , serialization_dir = allennlp_command [ - [number] ] ) [EOL] [EOL] allennlp_command . append ( [string] ) [EOL] [EOL] if os . path . exists ( allennlp_command [ - [number] ] ) and args . override : [EOL] print ( f" [string] { allennlp_command [ - [number] ] }" ) [EOL] shutil . rmtree ( allennlp_command [ - [number] ] ) [EOL] [EOL] [EOL] subprocess . run ( [string] . join ( allennlp_command ) , shell = True , check = True ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import List , Dict , Any [EOL] import argparse [EOL] import builtins [EOL] import io [EOL] import typing [EOL] import argparse [EOL] import json [EOL] import os [EOL] from typing import List [EOL] [EOL] import nltk [EOL] import numpy as np [EOL] import pandas as pd [EOL] import spacy [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from scipy import sparse [EOL] from sklearn . feature_extraction . text import CountVectorizer , TfidfVectorizer [EOL] from spacy . tokenizer import Tokenizer [EOL] from tqdm import tqdm [EOL] [EOL] from vampire . common . util import read_text , save_sparse , write_to_json [EOL] [EOL] [EOL] def load_data ( data_path , tokenize = False , tokenizer_type = [string] ) : [EOL] if tokenizer_type == [string] : [EOL] tokenizer = SpacyWordSplitter ( ) [EOL] elif tokenizer_type == [string] : [EOL] nlp = spacy . load ( [string] ) [EOL] tokenizer = Tokenizer ( nlp . vocab ) [EOL] tokenized_examples = [ ] [EOL] with tqdm ( open ( data_path , [string] ) , desc = f" [string] { data_path }" ) as f : [EOL] for line in f : [EOL] if data_path . endswith ( [string] ) or data_path . endswith ( [string] ) : [EOL] example = json . loads ( line ) [EOL] else : [EOL] example = { [string] : line } [EOL] if tokenize : [EOL] if tokenizer_type == [string] : [EOL] tokens = list ( map ( str , tokenizer . split_words ( example [ [string] ] ) ) ) [EOL] elif tokenizer_type == [string] : [EOL] tokens = list ( map ( str , tokenizer ( example [ [string] ] ) ) ) [EOL] text = [string] . join ( tokens ) [EOL] else : [EOL] text = example [ [string] ] [EOL] tokenized_examples . append ( text ) [EOL] return tokenized_examples [EOL] [EOL] def main ( ) : [EOL] parser = argparse . ArgumentParser ( formatter_class = argparse . ArgumentDefaultsHelpFormatter ) [EOL] parser . add_argument ( [string] , type = str , required = True , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , required = True , help = [string] ) [EOL] parser . add_argument ( [string] , [string] , type = str , required = True , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = int , required = False , default = [number] , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , default = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , required = False , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , default = [string] , help = [string] ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] if not os . path . isdir ( args . serialization_dir ) : [EOL] os . mkdir ( args . serialization_dir ) [EOL] [EOL] vocabulary_dir = os . path . join ( args . serialization_dir , [string] ) [EOL] [EOL] if not os . path . isdir ( vocabulary_dir ) : [EOL] os . mkdir ( vocabulary_dir ) [EOL] [EOL] tokenized_train_examples = load_data ( args . train_path , args . tokenize , args . tokenizer_type ) [EOL] tokenized_dev_examples = load_data ( args . dev_path , args . tokenize , args . tokenizer_type ) [EOL] [EOL] print ( [string] ) [EOL] if args . tfidf : [EOL] count_vectorizer = TfidfVectorizer ( stop_words = [string] , max_features = args . vocab_size , token_pattern = [string] ) [EOL] else : [EOL] count_vectorizer = CountVectorizer ( stop_words = [string] , max_features = args . vocab_size , token_pattern = [string] ) [EOL] [EOL] text = tokenized_train_examples + tokenized_dev_examples [EOL] [EOL] count_vectorizer . fit ( tqdm ( text ) ) [EOL] [EOL] vectorized_train_examples = count_vectorizer . transform ( tqdm ( tokenized_train_examples ) ) [EOL] vectorized_dev_examples = count_vectorizer . transform ( tqdm ( tokenized_dev_examples ) ) [EOL] [EOL] if args . tfidf : [EOL] reference_vectorizer = TfidfVectorizer ( stop_words = [string] , token_pattern = [string] ) [EOL] else : [EOL] reference_vectorizer = CountVectorizer ( stop_words = [string] , token_pattern = [string] ) [EOL] if not args . reference_corpus_path : [EOL] print ( [string] ) [EOL] reference_matrix = reference_vectorizer . fit_transform ( tqdm ( tokenized_dev_examples ) ) [EOL] else : [EOL] print ( f" [string] { args . reference_corpus_path } [string] " ) [EOL] reference_examples = load_data ( args . reference_corpus_path , args . tokenize_reference , args . reference_tokenizer_type ) [EOL] print ( [string] ) [EOL] reference_matrix = reference_vectorizer . fit_transform ( tqdm ( reference_examples ) ) [EOL] [EOL] reference_vocabulary = reference_vectorizer . get_feature_names ( ) [EOL] [EOL] [comment] [EOL] vectorized_train_examples = sparse . hstack ( ( np . array ( [ [number] ] * len ( tokenized_train_examples ) ) [ : , None ] , vectorized_train_examples ) ) [EOL] vectorized_dev_examples = sparse . hstack ( ( np . array ( [ [number] ] * len ( tokenized_dev_examples ) ) [ : , None ] , vectorized_dev_examples ) ) [EOL] master = sparse . vstack ( [ vectorized_train_examples , vectorized_dev_examples ] ) [EOL] [EOL] [comment] [EOL] print ( [string] ) [EOL] bgfreq = dict ( zip ( count_vectorizer . get_feature_names ( ) , ( np . array ( master . sum ( [number] ) ) / args . vocab_size ) . squeeze ( ) ) ) [EOL] [EOL] print ( [string] ) [EOL] save_sparse ( vectorized_train_examples , os . path . join ( args . serialization_dir , [string] ) ) [EOL] save_sparse ( vectorized_dev_examples , os . path . join ( args . serialization_dir , [string] ) ) [EOL] if not os . path . isdir ( os . path . join ( args . serialization_dir , [string] ) ) : [EOL] os . mkdir ( os . path . join ( args . serialization_dir , [string] ) ) [EOL] save_sparse ( reference_matrix , os . path . join ( args . serialization_dir , [string] , [string] ) ) [EOL] write_to_json ( reference_vocabulary , os . path . join ( args . serialization_dir , [string] , [string] ) ) [EOL] write_to_json ( bgfreq , os . path . join ( args . serialization_dir , [string] ) ) [EOL] [EOL] write_list_to_file ( [ [string] ] + count_vectorizer . get_feature_names ( ) , os . path . join ( vocabulary_dir , [string] ) ) [EOL] write_list_to_file ( [ [string] , [string] , [string] ] , os . path . join ( vocabulary_dir , [string] ) ) [EOL] [EOL] def write_list_to_file ( ls , save_path ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] out_file = open ( save_path , [string] ) [EOL] for example in ls : [EOL] out_file . write ( example ) [EOL] out_file . write ( [string] ) [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pandas as pd [EOL] import seaborn as sns [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] [EOL] sns . set ( font_scale = [number] , style = [string] ) [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] if __name__ == [string] : [EOL] import matplotlib . gridspec as gridspec [EOL] fig , ax = plt . subplots ( [number] , [number] ) [EOL] [EOL] ax1 = ax [ [number] , [number] ] [EOL] ax2 = ax [ [number] , [number] ] [EOL] ax3 = ax [ [number] , [number] ] [EOL] ax4 = ax [ [number] , [number] ] [EOL] [EOL] df = pd . read_json ( [string] , lines = True ) [EOL] sns . regplot ( df [ [string] ] , df [ [string] ] , ax = ax1 , color = [string] ) [EOL] ax1 . set_xlabel ( [string] ) [EOL] ax1 . set_ylabel ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] ax1 . xaxis . set_ticks ( [ [number] , [number] ] ) [EOL] ax1 . set_ylim ( [ [number] , [number] ] ) [EOL] ax1 . yaxis . set_ticks ( [ [number] , [number] , [number] ] ) [EOL] [EOL] [EOL] ax1 . text ( - [number] , [number] , [string] , transform = ax1 . transAxes , fontsize = [number] , fontweight = [string] , va = [string] , ha = [string] ) [EOL] [EOL] df = pd . read_json ( [string] , lines = True ) [EOL] sns . regplot ( df [ [string] ] , df [ [string] ] , ax = ax2 ) [EOL] ax2 . set_xlabel ( [string] ) [EOL] ax2 . set_ylabel ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] ax2 . set_ylim ( [ [number] , [number] ] ) [EOL] ax2 . xaxis . set_ticks ( [ [number] , [number] ] ) [EOL] ax2 . yaxis . set_ticks ( [ [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] ax2 . text ( - [number] , [number] , [string] , transform = ax2 . transAxes , fontsize = [number] , fontweight = [string] , va = [string] , ha = [string] ) [EOL] [EOL] [EOL] df = pd . read_json ( [string] , lines = True ) [EOL] [EOL] sns . regplot ( df [ [string] ] , df [ [string] ] , ax = ax3 ) [EOL] ax3 . set_xlabel ( [string] ) [EOL] ax3 . set_ylabel ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] ax3 . text ( - [number] , [number] , [string] , transform = ax3 . transAxes , fontsize = [number] , fontweight = [string] , va = [string] , ha = [string] ) [EOL] [EOL] df = pd . read_json ( [string] , lines = True ) [EOL] df1 = pd . read_json ( [string] , lines = True ) [EOL] master = pd . concat ( [ df , df1 ] , [number] ) [EOL] master [ [string] ] = master [ [string] ] . fillna ( [string] ) [EOL] sns . boxplot ( master [ [string] ] , master [ [string] ] , ax = ax4 , order = [ [string] , [string] ] ) [EOL] ax4 . set_xticklabels ( [ [string] , [string] ] ) [EOL] ax4 . set_xlabel ( [string] ) [EOL] ax4 . set_ylabel ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] ax4 . set_ylim ( [ [number] , [number] ] ) [EOL] ax4 . yaxis . set_ticks ( [ [number] , [number] ] ) [EOL] ax4 . text ( - [number] , [number] , [string] , transform = ax4 . transAxes , fontsize = [number] , fontweight = [string] , va = [string] , ha = [string] ) [EOL] [EOL] plt . tight_layout ( ) [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] [EOL] plt . savefig ( [string] , dpi = [number] ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pandas as pd [EOL] import seaborn as sns [EOL] import matplotlib . pyplot as plt [EOL] import numpy as np [EOL] [EOL] sns . set ( font_scale = [number] , style = [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] fig , ax = plt . subplots ( [number] , [number] , sharex = True , figsize = ( [number] , [number] ) ) [EOL] df = pd . read_csv ( [string] ) [EOL] df1 = pd . read_csv ( [string] ) [EOL] sns . lineplot ( df1 [ [string] ] , df1 [ [string] ] , ax = ax [ [number] ] ) [EOL] sns . lineplot ( df [ [string] ] , df [ [string] ] , ax = ax [ [number] ] ) [EOL] ax [ [number] ] . set_xlabel ( [string] ) [EOL] ax [ [number] ] . set_ylabel ( [string] ) [EOL] ax [ [number] ] . set_ylabel ( [string] ) [EOL] plt . tight_layout ( ) [EOL] plt . savefig ( [string] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from vampire . data . dataset_readers import SemiSupervisedTextClassificationJsonReader [EOL]	0 0 0 0 0 0 0 0 0
from vampire . data . dataset_readers . semisupervised_text_classification_json import ( SemiSupervisedTextClassificationJsonReader ) [EOL] from vampire . data . dataset_readers . vampire_reader import VampireReader [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Iterator , Dict , Any [EOL] import allennlp [EOL] import builtins [EOL] import logging [EOL] import io [EOL] import typing [EOL] import json [EOL] import logging [EOL] from io import TextIOWrapper [EOL] from typing import Dict [EOL] import numpy as np [EOL] from overrides import overrides [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers import TextClassificationJsonReader [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer , TokenIndexer [EOL] from allennlp . data . tokenizers import Tokenizer , WordTokenizer [EOL] from allennlp . data . tokenizers . sentence_splitter import SpacySentenceSplitter [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . fields import LabelField , TextField , Field [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class SemiSupervisedTextClassificationJsonReader ( TextClassificationJsonReader ) : [EOL] [docstring] [EOL] def __init__ ( self , token_indexers = None , tokenizer = None , max_sequence_length = None , ignore_labels = False , sample = None , skip_label_indexing = False , lazy = False ) : [EOL] super ( ) . __init__ ( lazy = lazy , token_indexers = token_indexers , tokenizer = tokenizer , max_sequence_length = max_sequence_length , skip_label_indexing = skip_label_indexing ) [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _sample = sample [EOL] self . _max_sequence_length = max_sequence_length [EOL] self . _ignore_labels = ignore_labels [EOL] self . _skip_label_indexing = skip_label_indexing [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] if self . _segment_sentences : [EOL] self . _sentence_segmenter = SpacySentenceSplitter ( ) [EOL] [EOL] @ staticmethod def _reservoir_sampling ( file_ , sample ) : [EOL] [docstring] [EOL] [comment] [EOL] file_iterator = iter ( file_ ) [EOL] [EOL] try : [EOL] [comment] [EOL] result = [ next ( file_iterator ) for _ in range ( sample ) ] [EOL] except StopIteration : [EOL] raise ConfigurationError ( f" [string] { sample } [string] " ) [EOL] [EOL] [comment] [EOL] for index , item in enumerate ( file_iterator , start = sample ) : [EOL] sample_index = np . random . randint ( [number] , index ) [EOL] if sample_index < sample : [EOL] result [ sample_index ] = item [EOL] [EOL] for line in result : [EOL] yield line [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] with open ( cached_path ( file_path ) , [string] ) as data_file : [EOL] if self . _sample is not None : [EOL] data_file = self . _reservoir_sampling ( data_file , self . _sample ) [EOL] for line in data_file : [EOL] items = json . loads ( line ) [EOL] text = items [ [string] ] [EOL] if self . _ignore_labels : [EOL] instance = self . text_to_instance ( text = text , label = None ) [EOL] else : [EOL] label = str ( items . get ( [string] ) ) [EOL] instance = self . text_to_instance ( text = text , label = label ) [EOL] if instance is not None and instance . fields [ [string] ] . tokens : [EOL] yield instance [EOL] [EOL] @ overrides def text_to_instance ( self , text , label = None ) : [comment] [EOL] [docstring] [EOL] [comment] [EOL] fields = { } [EOL] tokens = self . _tokenizer . tokenize ( text ) [EOL] if self . _max_sequence_length is not None : [EOL] tokens = self . _truncate ( tokens ) [EOL] fields [ [string] ] = TextField ( tokens , self . _token_indexers ) [EOL] if label is not None : [EOL] fields [ [string] ] = LabelField ( label , skip_indexing = self . _skip_label_indexing ) [EOL] return Instance ( fields ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $io.TextIOWrapper$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Iterator[builtins.str]$ 0 0 0 $io.TextIOWrapper$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Iterator[builtins.str]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[builtins.str]$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0
from typing import Dict , Any [EOL] import logging [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import logging [EOL] from typing import Dict [EOL] [EOL] import numpy as np [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import ArrayField , Field [EOL] from allennlp . data . instance import Instance [EOL] from overrides import overrides [EOL] [EOL] from vampire . common . util import load_sparse [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class VampireReader ( DatasetReader ) : [EOL] [docstring] [EOL] def __init__ ( self , lazy = False , sample = None , min_sequence_length = [number] ) : [EOL] super ( ) . __init__ ( lazy = lazy ) [EOL] self . _sample = sample [EOL] self . _min_sequence_length = min_sequence_length [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] [comment] [EOL] mat = load_sparse ( file_path ) [EOL] [comment] [EOL] mat = mat . tolil ( ) [EOL] [EOL] [comment] [EOL] if self . _sample : [EOL] indices = np . random . choice ( range ( mat . shape [ [number] ] ) , self . _sample ) [EOL] else : [EOL] indices = range ( mat . shape [ [number] ] ) [EOL] [EOL] for index in indices : [EOL] instance = self . text_to_instance ( vec = mat [ index ] . toarray ( ) . squeeze ( ) ) [EOL] if instance is not None and mat [ index ] . toarray ( ) . sum ( ) > self . _min_sequence_length : [EOL] yield instance [EOL] [EOL] @ overrides def text_to_instance ( self , vec = None ) : [comment] [EOL] [docstring] [EOL] [comment] [EOL] fields = { } [EOL] fields [ [string] ] = ArrayField ( vec ) [EOL] return Instance ( fields ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.range$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.fields.Field]$ 0 0
from typing import Dict , Any [EOL] import allennlp [EOL] import torch [EOL] import builtins [EOL] import vampire [EOL] import typing [EOL] from typing import Dict [EOL] [EOL] import torch [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . modules import TextFieldEmbedder [EOL] from allennlp . nn import InitializerApplicator [EOL] from allennlp . nn . util import get_text_field_mask [EOL] from allennlp . training . metrics import CategoricalAccuracy [EOL] [EOL] from vampire . modules . encoder import Encoder [EOL] [EOL] [EOL] @ Model . register ( [string] ) class Classifier ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab , input_embedder , encoder = None , dropout = None , initializer = InitializerApplicator ( ) ) : [EOL] [docstring] [EOL] super ( ) . __init__ ( vocab ) [EOL] self . _input_embedder = input_embedder [EOL] if dropout : [EOL] self . _dropout = torch . nn . Dropout ( dropout ) [EOL] else : [EOL] self . _dropout = None [EOL] self . _encoder = encoder [EOL] self . _num_labels = vocab . get_vocab_size ( namespace = [string] ) [EOL] if self . _encoder : [EOL] self . _clf_input_dim = self . _encoder . get_output_dim ( ) [EOL] else : [EOL] self . _clf_input_dim = self . _input_embedder . get_output_dim ( ) [EOL] self . _classification_layer = torch . nn . Linear ( self . _clf_input_dim , self . _num_labels ) [EOL] self . _accuracy = CategoricalAccuracy ( ) [EOL] self . _loss = torch . nn . CrossEntropyLoss ( ) [EOL] initializer ( self ) [EOL] [EOL] def forward ( self , tokens , label = None ) : [EOL] [comment] [EOL] [docstring] [EOL] embedded_text = self . _input_embedder ( tokens ) [EOL] mask = get_text_field_mask ( tokens ) . float ( ) [EOL] [EOL] if self . _encoder : [EOL] embedded_text = self . _encoder ( embedded_text = embedded_text , mask = mask ) [EOL] [EOL] if self . _dropout : [EOL] embedded_text = self . _dropout ( embedded_text ) [EOL] [EOL] logits = self . _classification_layer ( embedded_text ) [EOL] probs = torch . nn . functional . softmax ( logits , dim = - [number] ) [EOL] [EOL] output_dict = { [string] : logits , [string] : probs } [EOL] [EOL] if label is not None : [EOL] loss = self . _loss ( logits , label . long ( ) . view ( - [number] ) ) [EOL] output_dict [ [string] ] = loss [EOL] self . _accuracy ( logits , label ) [EOL] [EOL] return output_dict [EOL] [EOL] def get_metrics ( self , reset = False ) : [EOL] metrics = { [string] : self . _accuracy . get_metric ( reset ) } [EOL] return metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 $typing.Dict[builtins.str,torch.LongTensor]$ 0 $torch.IntTensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,torch.LongTensor]$ 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,torch.LongTensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $torch.IntTensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $torch.IntTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $torch.IntTensor$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0
from vampire . models . classifier import Classifier [EOL] from vampire . models . vampire import VAMPIRE [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Union , Any , List , Tuple , Dict , Optional [EOL] import allennlp [EOL] import torch [EOL] import builtins [EOL] import vampire [EOL] import logging [EOL] import typing [EOL] import logging [EOL] import os [EOL] from functools import partial [EOL] from itertools import combinations [EOL] from operator import is_not [EOL] from typing import Dict , List , Optional , Tuple , Union [EOL] [EOL] import numpy as np [EOL] import torch [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . models . model import Model [EOL] from allennlp . modules import TokenEmbedder [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] from allennlp . training . metrics import Average [EOL] from overrides import overrides [EOL] from scipy import sparse [EOL] from tabulate import tabulate [EOL] [EOL] from vampire . common . util import ( compute_background_log_frequency , load_sparse , read_json ) [EOL] from vampire . modules import VAE [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Model . register ( [string] ) class VAMPIRE ( Model ) : [EOL] [docstring] [EOL] def __init__ ( self , vocab , bow_embedder , vae , kl_weight_annealing = [string] , linear_scaling = [number] , sigmoid_weight_1 = [number] , sigmoid_weight_2 = [number] , reference_counts = None , reference_vocabulary = None , background_data_path = None , update_background_freq = False , track_topics = True , track_npmi = True , initializer = InitializerApplicator ( ) , regularizer = None ) : [EOL] super ( ) . __init__ ( vocab , regularizer ) [EOL] [EOL] self . metrics = { [string] : Average ( ) , [string] : Average ( ) } [EOL] [EOL] self . vocab = vocab [EOL] self . vae = vae [EOL] self . track_topics = track_topics [EOL] self . track_npmi = track_npmi [EOL] self . vocab_namespace = [string] [EOL] self . _update_background_freq = update_background_freq [EOL] self . _background_freq = self . initialize_bg_from_file ( file_ = background_data_path ) [EOL] self . _ref_counts = reference_counts [EOL] [EOL] self . _npmi_updated = False [EOL] [EOL] if reference_vocabulary is not None : [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] self . _ref_vocab = read_json ( cached_path ( reference_vocabulary ) ) [EOL] self . _ref_vocab_index = dict ( zip ( self . _ref_vocab , range ( len ( self . _ref_vocab ) ) ) ) [EOL] logger . info ( [string] ) [EOL] self . _ref_count_mat = load_sparse ( cached_path ( self . _ref_counts ) ) [EOL] logger . info ( [string] ) [EOL] self . _ref_doc_counts = ( self . _ref_count_mat > [number] ) . astype ( float ) [EOL] self . _ref_interaction = ( self . _ref_doc_counts ) . T . dot ( self . _ref_doc_counts ) [EOL] self . _ref_doc_sum = np . array ( self . _ref_doc_counts . sum ( [number] ) . tolist ( ) [ [number] ] ) [EOL] logger . info ( [string] ) [EOL] ( self . _npmi_numerator , self . _npmi_denominator ) = self . generate_npmi_vals ( self . _ref_interaction , self . _ref_doc_sum ) [EOL] self . n_docs = self . _ref_count_mat . shape [ [number] ] [EOL] [EOL] vampire_vocab_size = self . vocab . get_vocab_size ( self . vocab_namespace ) [EOL] self . _bag_of_words_embedder = bow_embedder [EOL] [EOL] self . _kl_weight_annealing = kl_weight_annealing [EOL] [EOL] self . _linear_scaling = float ( linear_scaling ) [EOL] self . _sigmoid_weight_1 = float ( sigmoid_weight_1 ) [EOL] self . _sigmoid_weight_2 = float ( sigmoid_weight_2 ) [EOL] if kl_weight_annealing == [string] : [EOL] self . _kld_weight = min ( [number] , [number] / self . _linear_scaling ) [EOL] elif kl_weight_annealing == [string] : [EOL] self . _kld_weight = float ( [number] / ( [number] + np . exp ( - self . _sigmoid_weight_1 * ( [number] - self . _sigmoid_weight_2 ) ) ) ) [EOL] elif kl_weight_annealing == [string] : [EOL] self . _kld_weight = [number] [EOL] else : [EOL] raise ConfigurationError ( [string] . format ( kl_weight_annealing ) ) [EOL] [EOL] [comment] [EOL] self . bow_bn = torch . nn . BatchNorm1d ( vampire_vocab_size , eps = [number] , momentum = [number] , affine = True ) [EOL] self . bow_bn . weight . data . copy_ ( torch . ones ( vampire_vocab_size , dtype = torch . float64 ) ) [EOL] self . bow_bn . weight . requires_grad = False [EOL] [EOL] [comment] [EOL] self . _metric_epoch_tracker = [number] [EOL] self . _kl_epoch_tracker = [number] [EOL] self . _cur_epoch = [number] [EOL] self . _cur_npmi = [number] [EOL] self . batch_num = [number] [EOL] [EOL] initializer ( self ) [EOL] [EOL] def initialize_bg_from_file ( self , file_ = None ) : [EOL] [docstring] [EOL] background_freq = compute_background_log_frequency ( self . vocab , self . vocab_namespace , file_ ) [EOL] return torch . nn . Parameter ( background_freq , requires_grad = self . _update_background_freq ) [EOL] [EOL] @ staticmethod def bow_reconstruction_loss ( reconstructed_bow , target_bow ) : [EOL] [docstring] [EOL] log_reconstructed_bow = torch . nn . functional . log_softmax ( reconstructed_bow + [number] , dim = - [number] ) [EOL] reconstruction_loss = torch . sum ( target_bow * log_reconstructed_bow , dim = - [number] ) [EOL] return reconstruction_loss [EOL] [EOL] def update_kld_weight ( self , epoch_num ) : [EOL] [docstring] [EOL] if not epoch_num : [EOL] self . _kld_weight = [number] [EOL] else : [EOL] _epoch_num = epoch_num [ [number] ] [EOL] if _epoch_num != self . _kl_epoch_tracker : [EOL] print ( self . _kld_weight ) [EOL] self . _kl_epoch_tracker = _epoch_num [EOL] self . _cur_epoch += [number] [EOL] if self . _kl_weight_annealing == [string] : [EOL] self . _kld_weight = min ( [number] , self . _cur_epoch / self . _linear_scaling ) [EOL] elif self . _kl_weight_annealing == [string] : [EOL] exp = np . exp ( - self . _sigmoid_weight_1 * ( self . _cur_epoch - self . _sigmoid_weight_2 ) ) [EOL] self . _kld_weight = float ( [number] / ( [number] + exp ) ) [EOL] elif self . _kl_weight_annealing == [string] : [EOL] self . _kld_weight = [number] [EOL] else : [EOL] raise ConfigurationError ( [string] . format ( self . _kl_weight_annealing ) ) [EOL] [EOL] def update_topics ( self , epoch_num ) : [EOL] [docstring] [EOL] [EOL] if epoch_num and epoch_num [ [number] ] != self . _metric_epoch_tracker : [EOL] [EOL] [comment] [EOL] if self . track_topics : [EOL] topic_table = tabulate ( self . extract_topics ( self . vae . get_beta ( ) ) , headers = [ [string] , [string] ] ) [EOL] topic_dir = os . path . join ( os . path . dirname ( self . vocab . serialization_dir ) , [string] ) [EOL] if not os . path . exists ( topic_dir ) : [EOL] os . mkdir ( topic_dir ) [EOL] ser_dir = os . path . dirname ( self . vocab . serialization_dir ) [EOL] [EOL] [comment] [EOL] topic_filepath = os . path . join ( ser_dir , [string] , [string] . format ( self . _metric_epoch_tracker ) ) [EOL] with open ( topic_filepath , [string] ) as file_ : [EOL] file_ . write ( topic_table ) [EOL] [EOL] self . _metric_epoch_tracker = epoch_num [ [number] ] [EOL] [EOL] def update_npmi ( self ) : [EOL] [docstring] [EOL] [EOL] if self . track_npmi and self . _ref_vocab and not self . training and not self . _npmi_updated : [EOL] topics = self . extract_topics ( self . vae . get_beta ( ) ) [EOL] self . _cur_npmi = self . compute_npmi ( topics [ [number] : ] ) [EOL] self . _npmi_updated = True [EOL] elif self . training : [EOL] self . _npmi_updated = False [EOL] [EOL] [EOL] def extract_topics ( self , weights , k = [number] ) : [EOL] [docstring] [EOL] [EOL] words = list ( range ( weights . size ( [number] ) ) ) [EOL] words = [ self . vocab . get_token_from_index ( i , self . vocab_namespace ) for i in words ] [EOL] [EOL] topics = [ ] [EOL] [EOL] word_strengths = list ( zip ( words , self . _background_freq . tolist ( ) ) ) [EOL] sorted_by_strength = sorted ( word_strengths , key = lambda x : x [ [number] ] , reverse = True ) [EOL] background = [ x [ [number] ] for x in sorted_by_strength ] [ : k ] [EOL] topics . append ( ( [string] , background ) ) [EOL] [EOL] for i , topic in enumerate ( weights ) : [EOL] word_strengths = list ( zip ( words , topic . tolist ( ) ) ) [EOL] sorted_by_strength = sorted ( word_strengths , key = lambda x : x [ [number] ] , reverse = True ) [EOL] top_k = [ x [ [number] ] for x in sorted_by_strength ] [ : k ] [EOL] topics . append ( ( str ( i ) , top_k ) ) [EOL] [EOL] return topics [EOL] [EOL] @ staticmethod def generate_npmi_vals ( interactions , document_sums ) : [EOL] [docstring] [EOL] interaction_rows , interaction_cols = interactions . nonzero ( ) [EOL] logger . info ( [string] ) [EOL] doc_sums = sparse . csr_matrix ( ( np . log10 ( document_sums [ interaction_rows ] ) + np . log10 ( document_sums [ interaction_cols ] ) , ( interaction_rows , interaction_cols ) ) , shape = interactions . shape ) [EOL] logger . info ( [string] ) [EOL] interactions . data = np . log10 ( interactions . data ) [EOL] numerator = interactions - doc_sums [EOL] logger . info ( [string] ) [EOL] denominator = interactions [EOL] return numerator , denominator [EOL] [EOL] def compute_npmi ( self , topics , num_words = [number] ) : [EOL] [docstring] [EOL] topics_idx = [ [ self . _ref_vocab_index . get ( word ) for word in topic [ [number] ] [ : num_words ] ] for topic in topics ] [EOL] rows = [ ] [EOL] cols = [ ] [EOL] res_rows = [ ] [EOL] res_cols = [ ] [EOL] max_seq_len = max ( [ len ( topic ) for topic in topics_idx ] ) [EOL] [EOL] for index , topic in enumerate ( topics_idx ) : [EOL] topic = list ( filter ( partial ( is_not , None ) , topic ) ) [EOL] if len ( topic ) > [number] : [EOL] _rows , _cols = zip ( * combinations ( topic , [number] ) ) [EOL] res_rows . extend ( [ index ] * len ( _rows ) ) [EOL] res_cols . extend ( range ( len ( _rows ) ) ) [EOL] rows . extend ( _rows ) [EOL] cols . extend ( _cols ) [EOL] [EOL] npmi_data = ( ( np . log10 ( self . n_docs ) + self . _npmi_numerator [ rows , cols ] ) / ( np . log10 ( self . n_docs ) - self . _npmi_denominator [ rows , cols ] ) ) [EOL] npmi_data [ npmi_data == [number] ] = [number] [EOL] npmi_shape = ( len ( topics ) , len ( list ( combinations ( range ( max_seq_len ) , [number] ) ) ) ) [EOL] npmi = sparse . csr_matrix ( ( npmi_data . tolist ( ) [ [number] ] , ( res_rows , res_cols ) ) , shape = npmi_shape ) [EOL] return npmi . mean ( ) [EOL] [EOL] def freeze_weights ( self ) : [EOL] [docstring] [EOL] model_parameters = dict ( self . vae . named_parameters ( ) ) [EOL] for item in model_parameters : [EOL] model_parameters [ item ] . requires_grad = False [EOL] [EOL] @ overrides def forward ( self , tokens , epoch_num = None ) : [EOL] [docstring] [EOL] [comment] [EOL] self . device = self . vae . get_beta ( ) . device [comment] [EOL] [EOL] output_dict = { } [EOL] [EOL] self . update_npmi ( ) [EOL] self . update_topics ( epoch_num ) [EOL] [EOL] if not self . training : [EOL] self . _kld_weight = [number] [comment] [EOL] else : [EOL] self . update_kld_weight ( epoch_num ) [EOL] [EOL] [comment] [EOL] if isinstance ( tokens , dict ) : [EOL] embedded_tokens = ( self . _bag_of_words_embedder ( tokens [ [string] ] ) . to ( device = self . device ) ) [EOL] else : [EOL] embedded_tokens = tokens [EOL] [EOL] [comment] [EOL] variational_output = self . vae ( embedded_tokens ) [EOL] [EOL] [comment] [EOL] reconstructed_bow = variational_output [ [string] ] + self . _background_freq [EOL] [EOL] [comment] [EOL] [comment] [EOL] reconstructed_bow = self . bow_bn ( reconstructed_bow ) [EOL] [EOL] [comment] [EOL] reconstruction_loss = self . bow_reconstruction_loss ( reconstructed_bow , embedded_tokens ) [EOL] [EOL] [comment] [EOL] negative_kl_divergence = variational_output [ [string] ] [EOL] [EOL] [comment] [EOL] elbo = negative_kl_divergence * self . _kld_weight + reconstruction_loss [EOL] [EOL] loss = - torch . mean ( elbo ) [EOL] [EOL] output_dict [ [string] ] = loss [EOL] [EOL] output_dict [ [string] ] = variational_output [ [string] ] [EOL] [EOL] [comment] [EOL] self . metrics [ [string] ] ( - torch . mean ( negative_kl_divergence ) ) [EOL] self . metrics [ [string] ] ( - torch . mean ( reconstruction_loss ) ) [EOL] [EOL] [comment] [EOL] self . batch_num += [number] [EOL] [EOL] self . metrics [ [string] ] = self . _cur_npmi [EOL] [EOL] return output_dict [EOL] [EOL] @ overrides def get_metrics ( self , reset = False ) : [EOL] output = { } [EOL] for metric_name , metric in self . metrics . items ( ) : [EOL] if isinstance ( metric , float ) : [EOL] output [ metric_name ] = metric [EOL] else : [EOL] output [ metric_name ] = float ( metric . get_metric ( reset ) ) [EOL] return output [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $vampire.modules.vae.vae.VAE$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $allennlp.nn.InitializerApplicator$ 0 0 0 0 0 $typing.Optional[allennlp.nn.RegularizerApplicator]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[allennlp.nn.RegularizerApplicator]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.modules.vae.vae.VAE$ 0 $vampire.modules.vae.vae.VAE$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.nn.InitializerApplicator$ 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 $typing.Optional[typing.List[builtins.int]]$ 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 $typing.Optional[typing.List[builtins.int]]$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $typing.List[typing.Tuple[builtins.str,typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.List[builtins.int]]]$ 0 0 0 $torch.Tensor$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,typing.Any]]$ 0 0 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing.Dict[builtins.str,torch.IntTensor],torch.IntTensor]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Union[typing.Dict[builtins.str,torch.IntTensor],torch.IntTensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing.Dict[builtins.str,torch.IntTensor],torch.IntTensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing.Dict[builtins.str,torch.IntTensor],torch.IntTensor]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0
from vampire . modules . encoder import * [EOL] from vampire . modules . pretrained_vae import PretrainedVAE [EOL] from vampire . modules . token_embedders . vampire_token_embedder import VampireTokenEmbedder [EOL] from vampire . modules . vae import LogisticNormal [EOL] from vampire . modules . vae import VAE [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . common import Registrable [EOL] from allennlp . modules import FeedForward , Seq2SeqEncoder , Seq2VecEncoder [EOL] from allennlp . nn . util import ( get_final_encoder_states , masked_max , masked_mean , masked_log_softmax ) [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] class Encoder ( Registrable , torch . nn . Module ) : [EOL] [docstring] [EOL] default_implementation = [string] [EOL] [EOL] def __init__ ( self , architecture ) : [EOL] super ( Encoder , self ) . __init__ ( ) [EOL] self . _architecture = architecture [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _architecture . get_output_dim ( ) [EOL] [EOL] def forward ( self , ** kwargs ) : [EOL] raise NotImplementedError [EOL] [EOL] @ Encoder . register ( [string] ) class MLP ( Encoder ) : [EOL] [EOL] def __init__ ( self , architecture ) : [EOL] super ( MLP , self ) . __init__ ( architecture ) [EOL] self . _architecture = architecture [EOL] [EOL] @ overrides def forward ( self , ** kwargs ) : [EOL] return self . _architecture ( kwargs [ [string] ] ) [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class MaxPoolEncoder ( Seq2VecEncoder ) : [EOL] def __init__ ( self , embedding_dim ) : [EOL] super ( MaxPoolEncoder , self ) . __init__ ( ) [EOL] self . _embedding_dim = embedding_dim [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] def forward ( self , tokens , mask ) : [comment] [EOL] broadcast_mask = mask . unsqueeze ( - [number] ) . float ( ) [EOL] one_minus_mask = ( [number] - broadcast_mask ) . byte ( ) [EOL] replaced = tokens . masked_fill ( one_minus_mask , - [number] ) [EOL] max_value , _ = replaced . max ( dim = [number] , keepdim = False ) [EOL] return max_value [EOL] [EOL] @ Encoder . register ( [string] ) class Seq2Vec ( Encoder ) : [EOL] [EOL] def __init__ ( self , architecture ) : [EOL] super ( Seq2Vec , self ) . __init__ ( architecture ) [EOL] self . _architecture = architecture [EOL] [EOL] @ overrides def forward ( self , ** kwargs ) : [EOL] return self . _architecture ( kwargs [ [string] ] , kwargs [ [string] ] ) [EOL] [EOL] [EOL] @ Encoder . register ( [string] ) class Seq2Seq ( Encoder ) : [EOL] [EOL] def __init__ ( self , architecture , aggregations ) : [EOL] super ( Seq2Seq , self ) . __init__ ( architecture ) [EOL] self . _architecture = architecture [EOL] self . _aggregations = aggregations [EOL] if [string] in self . _aggregations : [EOL] self . _attention_layer = torch . nn . Linear ( self . _architecture . get_output_dim ( ) , [number] ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _architecture . get_output_dim ( ) * len ( self . _aggregations ) [EOL] [EOL] @ overrides def forward ( self , ** kwargs ) : [EOL] mask = kwargs [ [string] ] [EOL] embedded_text = kwargs [ [string] ] [EOL] encoded_output = self . _architecture ( embedded_text , mask ) [EOL] encoded_repr = [ ] [EOL] for aggregation in self . _aggregations : [EOL] if aggregation == [string] : [EOL] broadcast_mask = mask . unsqueeze ( - [number] ) . float ( ) [EOL] context_vectors = encoded_output * broadcast_mask [EOL] encoded_text = masked_mean ( context_vectors , broadcast_mask , dim = [number] , keepdim = False ) [EOL] elif aggregation == [string] : [EOL] broadcast_mask = mask . unsqueeze ( - [number] ) . float ( ) [EOL] context_vectors = encoded_output * broadcast_mask [EOL] encoded_text = masked_max ( context_vectors , broadcast_mask , dim = [number] ) [EOL] elif aggregation == [string] : [EOL] is_bi = self . _architecture . is_bidirectional ( ) [EOL] encoded_text = get_final_encoder_states ( encoded_output , mask , is_bi ) [EOL] elif aggregation == [string] : [EOL] alpha = self . _attention_layer ( encoded_output ) [EOL] alpha = masked_log_softmax ( alpha , mask . unsqueeze ( - [number] ) , dim = [number] ) . exp ( ) [EOL] encoded_text = alpha * encoded_output [EOL] encoded_text = encoded_text . sum ( dim = [number] ) [EOL] else : [EOL] raise ConfigurationError ( f"{ aggregation } [string] " ) [EOL] encoded_repr . append ( encoded_text ) [EOL] [EOL] encoded_repr = torch . cat ( encoded_repr , [number] ) [EOL] return encoded_repr [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from vampire . modules . token_embedders . vampire_token_embedder import VampireTokenEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0
from typing import List , Any [EOL] import allennlp [EOL] import torch [EOL] import builtins [EOL] import vampire [EOL] import typing [EOL] from typing import List [EOL] [EOL] import torch [EOL] from allennlp . common import Params [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] from vampire . modules . pretrained_vae import PretrainedVAE [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class VampireTokenEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] def __init__ ( self , model_archive , device , background_frequency , scalar_mix = None , dropout = None , requires_grad = False , projection_dim = None , expand_dim = False ) : [EOL] super ( VampireTokenEmbedder , self ) . __init__ ( ) [EOL] [EOL] self . _vae = PretrainedVAE ( model_archive , device , background_frequency , requires_grad , scalar_mix , dropout ) [EOL] self . _expand_dim = expand_dim [EOL] self . _layers = None [EOL] if projection_dim : [EOL] self . _projection = torch . nn . Linear ( self . _vae . get_output_dim ( ) , projection_dim ) [EOL] self . output_dim = projection_dim [EOL] else : [EOL] self . _projection = None [EOL] self . output_dim = self . _vae . get_output_dim ( ) [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] def forward ( self , inputs ) : [EOL] [docstring] [EOL] vae_output = self . _vae ( inputs ) [EOL] embedded = vae_output [ [string] ] [EOL] self . _layers = vae_output [ [string] ] [EOL] if self . _expand_dim : [EOL] embedded = ( embedded . unsqueeze ( [number] ) . expand ( inputs . shape [ [number] ] , inputs . shape [ [number] ] , - [number] ) . permute ( [number] , [number] , [number] ) . contiguous ( ) ) [EOL] if self . _projection : [EOL] projection = self . _projection [EOL] for _ in range ( embedded . dim ( ) - [number] ) : [EOL] projection = TimeDistributed ( projection ) [EOL] embedded = projection ( embedded ) [EOL] return embedded [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , vocab , params ) : [comment] [EOL] [comment] [EOL] params . add_file_to_archive ( [string] ) [EOL] model_archive = params . pop ( [string] ) [EOL] device = params . pop_int ( [string] ) [EOL] background_frequency = params . pop ( [string] ) [EOL] requires_grad = params . pop ( [string] , False ) [EOL] scalar_mix = params . pop ( [string] , None ) [EOL] dropout = params . pop_float ( [string] , None ) [EOL] expand_dim = params . pop_float ( [string] , False ) [EOL] projection_dim = params . pop_int ( [string] , None ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( expand_dim = expand_dim , scalar_mix = scalar_mix , background_frequency = background_frequency , device = device , model_archive = model_archive , dropout = dropout , requires_grad = requires_grad , projection_dim = projection_dim ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $'VampireTokenEmbedder'$ 0 0 0 $allennlp.data.Vocabulary$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
import torch [EOL] import torch [EOL] from allennlp . models import Model [EOL] [EOL] [EOL] class VAE ( Model ) : [EOL] [EOL] def __init__ ( self , vocab ) : [EOL] super ( VAE , self ) . __init__ ( vocab ) [EOL] [EOL] def estimate_params ( self , input_repr ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def compute_negative_kld ( self , params ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def generate_latent_code ( self , input_repr ) : [comment] [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_beta ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def encode ( self , input_vector ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0
from vampire . modules . vae . vae import VAE [EOL] from vampire . modules . vae . logistic_normal import LogisticNormal [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . data . dataset import Batch [EOL] [EOL] from vampire . common . testing . test_case import VAETestCase [EOL] from vampire . models import classifier [EOL] from vampire . modules . token_embedders import VampireTokenEmbedder [EOL] [EOL] [EOL] class TestClassifiers ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [EOL] def test_seq2seq_clf_with_vae_token_embedder_can_train_save_and_load ( self ) : [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_seq2seq_clf_with_vae_token_embedder_batch_predictions_are_consistent ( self ) : [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_seq2vec_clf_with_vae_token_embedder_can_train_save_and_load ( self ) : [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] [EOL] def test_seq2seq_clf_with_vae_token_embedder_forward_pass_runs_correctly ( self ) : [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] dataset = Batch ( self . instances ) [EOL] dataset . index_instances ( self . vocab ) [EOL] training_tensors = dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] assert output_dict [ [string] ] . shape == ( [number] , [number] ) [EOL] assert output_dict [ [string] ] . shape == ( [number] , [number] ) [EOL] assert output_dict [ [string] ] [EOL] [EOL] def test_seq2vec_clf_with_vae_token_embedder_forward_pass_runs_correctly ( self ) : [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] dataset = Batch ( self . instances ) [EOL] dataset . index_instances ( self . vocab ) [EOL] training_tensors = dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] assert output_dict [ [string] ] . shape == ( [number] , [number] ) [EOL] assert output_dict [ [string] ] . shape == ( [number] , [number] ) [EOL] assert output_dict [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0
[comment] [EOL] from typing import List , Tuple , Dict , Any [EOL] import typing [EOL] import numpy as np [EOL] from allennlp . commands . train import train_model_from_file [EOL] from allennlp . common . testing import ModelTestCase [EOL] [EOL] from vampire . common . allennlp_bridge import ExtendedVocabulary [EOL] from vampire . common . testing . test_case import VAETestCase [EOL] from vampire . data . dataset_readers import VampireReader [EOL] from vampire . models import VAMPIRE [EOL] [EOL] [EOL] class TestVampire ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( TestVampire , self ) . setUp ( ) [EOL] self . set_up_model ( VAETestCase . FIXTURES_ROOT / [string] / [string] , VAETestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def test_model_can_train_save_and_load_unsupervised ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_npmi_computed_correctly ( self ) : [EOL] save_dir = self . TEST_DIR / [string] [EOL] model = train_model_from_file ( self . param_file , save_dir , overrides = [string] ) [EOL] [EOL] topics = [ ( [number] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) , ( [number] , [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) ] [EOL] npmi = model . compute_npmi ( topics , num_words = [number] ) [EOL] [EOL] ref_vocab = model . _ref_vocab [EOL] ref_counts = model . _ref_count_mat [EOL] [EOL] vocab_index = dict ( zip ( ref_vocab , range ( len ( ref_vocab ) ) ) ) [EOL] n_docs , _ = ref_counts . shape [EOL] [EOL] npmi_means = [ ] [EOL] for topic in topics : [EOL] words = topic [ [number] ] [EOL] npmi_vals = [ ] [EOL] for word_i , word1 in enumerate ( words [ : [number] ] ) : [EOL] if word1 in vocab_index : [EOL] index1 = vocab_index [ word1 ] [EOL] else : [EOL] index1 = None [EOL] for word2 in words [ word_i + [number] : [number] ] : [EOL] if word2 in vocab_index : [EOL] index2 = vocab_index [ word2 ] [EOL] else : [EOL] index2 = None [EOL] if index1 is None or index2 is None : [EOL] _npmi = [number] [EOL] else : [EOL] col1 = np . array ( ref_counts [ : , index1 ] . todense ( ) > [number] , dtype = int ) [EOL] col2 = np . array ( ref_counts [ : , index2 ] . todense ( ) > [number] , dtype = int ) [EOL] sum1 = col1 . sum ( ) [EOL] sum2 = col2 . sum ( ) [EOL] interaction = np . sum ( col1 * col2 ) [EOL] if interaction == [number] : [EOL] assert model . _npmi_numerator [ index1 , index2 ] == [number] and model . _npmi_denominator [ index1 , index2 ] == [number] [EOL] _npmi = [number] [EOL] else : [EOL] assert model . _ref_interaction [ index1 , index2 ] == np . log10 ( interaction ) [EOL] assert model . _ref_doc_sum [ index1 ] == sum1 [EOL] assert model . _ref_doc_sum [ index2 ] == sum2 [EOL] expected_numerator = np . log10 ( n_docs ) + np . log10 ( interaction ) - np . log10 ( sum1 ) - np . log10 ( sum2 ) [EOL] numerator = np . log10 ( model . n_docs ) + model . _npmi_numerator [ index1 , index2 ] [EOL] assert np . isclose ( expected_numerator , numerator ) [EOL] expected_denominator = np . log10 ( n_docs ) - np . log10 ( interaction ) [EOL] denominator = np . log10 ( model . n_docs ) - model . _npmi_denominator [ index1 , index2 ] [EOL] assert np . isclose ( expected_denominator , denominator ) [EOL] _npmi = expected_numerator / expected_denominator [EOL] npmi_vals . append ( _npmi ) [EOL] npmi_means . append ( np . mean ( npmi_vals ) ) [EOL] assert np . isclose ( npmi , np . mean ( npmi_means ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[builtins.int,typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Dict[typing.Any,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,typing.List[builtins.str]]]$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,builtins.int]$ 0 0 $None$ 0 $typing.Dict[typing.Any,builtins.int]$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,builtins.int]$ 0 0 $None$ 0 $typing.Dict[typing.Any,builtins.int]$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 $None$ 0 0 0 $None$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 $None$ 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 $None$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 $None$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $None$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $None$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 $None$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $None$ 0 $None$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0
[comment] [EOL] from typing import List , Union , Dict , Any [EOL] import typing [EOL] import vampire [EOL] import pytest [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from vampire . common . testing import VAETestCase [EOL] from allennlp . common . util import ensure_list , prepare_environment [EOL] [EOL] from vampire . data . dataset_readers import SemiSupervisedTextClassificationJsonReader [EOL] [EOL] [EOL] class TestSemiSupervisedTextClassificationJsonReader ( VAETestCase ) : [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_read_from_file ( self ) : [EOL] reader = SemiSupervisedTextClassificationJsonReader ( ) [EOL] ag_path = self . FIXTURES_ROOT / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL] [EOL] def test_read_from_file_and_truncates_properly ( self ) : [EOL] [EOL] reader = SemiSupervisedTextClassificationJsonReader ( max_sequence_length = [number] ) [EOL] ag_path = self . FIXTURES_ROOT / [string] / [string] [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] [EOL] instance1 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance2 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] instance3 = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance1 [ [string] ] [EOL] assert fields [ [string] ] . label == instance1 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance2 [ [string] ] [EOL] assert fields [ [string] ] . label == instance2 [ [string] ] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance3 [ [string] ] [EOL] assert fields [ [string] ] . label == instance3 [ [string] ] [EOL] [EOL] def test_samples_properly ( self ) : [EOL] reader = SemiSupervisedTextClassificationJsonReader ( sample = [number] , max_sequence_length = [number] ) [EOL] ag_path = self . FIXTURES_ROOT / [string] / [string] [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] prepare_environment ( params ) [EOL] instances = reader . read ( ag_path ) [EOL] instances = ensure_list ( instances ) [EOL] instance = { [string] : [ [string] , [string] , [string] , [string] , [string] ] , [string] : [string] } [EOL] assert len ( instances ) == [number] [EOL] fields = instances [ [number] ] . fields [EOL] assert [ t . text for t in fields [ [string] ] . tokens ] == instance [ [string] ] [EOL] assert fields [ [string] ] . label == instance [ [string] ] [EOL] [EOL] def test_sampling_fails_when_sample_size_larger_than_file_size ( self ) : [EOL] reader = SemiSupervisedTextClassificationJsonReader ( sample = [number] , max_sequence_length = [number] ) [EOL] ag_path = self . FIXTURES_ROOT / [string] / [string] [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] prepare_environment ( params ) [EOL] self . assertRaises ( ConfigurationError , reader . read , ag_path ) [EOL] [EOL] def test_samples_according_to_seed_properly ( self ) : [EOL] [EOL] reader1 = SemiSupervisedTextClassificationJsonReader ( sample = [number] , max_sequence_length = [number] ) [EOL] reader2 = SemiSupervisedTextClassificationJsonReader ( sample = [number] , max_sequence_length = [number] ) [EOL] reader3 = SemiSupervisedTextClassificationJsonReader ( sample = [number] , max_sequence_length = [number] ) [EOL] [EOL] imdb_path = self . FIXTURES_ROOT / [string] / [string] [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] prepare_environment ( params ) [EOL] instances1 = reader1 . read ( imdb_path ) [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] prepare_environment ( params ) [EOL] instances2 = reader2 . read ( imdb_path ) [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] prepare_environment ( params ) [EOL] instances3 = reader3 . read ( imdb_path ) [EOL] fields1 = [ i . fields for i in instances1 ] [EOL] fields2 = [ i . fields for i in instances2 ] [EOL] fields3 = [ i . fields for i in instances3 ] [EOL] tokens1 = [ f [ [string] ] . tokens for f in fields1 ] [EOL] tokens2 = [ f [ [string] ] . tokens for f in fields2 ] [EOL] tokens3 = [ f [ [string] ] . tokens for f in fields3 ] [EOL] text1 = [ [ t . text for t in doc ] for doc in tokens1 ] [EOL] text2 = [ [ t . text for t in doc ] for doc in tokens2 ] [EOL] text3 = [ [ t . text for t in doc ] for doc in tokens3 ] [EOL] assert text1 != text2 [EOL] assert text1 == text3 [EOL] [EOL] def test_ignores_label_properly ( self ) : [EOL] [EOL] imdb_labeled_path = self . FIXTURES_ROOT / [string] / [string] [EOL] reader = SemiSupervisedTextClassificationJsonReader ( ignore_labels = True ) [EOL] instances = reader . read ( imdb_labeled_path ) [EOL] instances = ensure_list ( instances ) [EOL] fields = [ i . fields for i in instances ] [EOL] labels = [ f . get ( [string] ) for f in fields ] [EOL] assert labels == [ None ] * [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.List[typing.list]$ 0 $typing.List[typing.list]$ 0 0 $typing.List[typing.list]$ 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $vampire.data.dataset_readers.semisupervised_text_classification_json.SemiSupervisedTextClassificationJsonReader$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.list$ 0 0 0 0 $typing.list$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0
[comment] [EOL] from typing import List , Any [EOL] import typing [EOL] import vampire [EOL] import torch [EOL] import numpy as np [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . data . dataset import Batch [EOL] from vampire . common . testing . test_case import VAETestCase [EOL] from vampire . modules . token_embedders import VampireTokenEmbedder [EOL] [EOL] [EOL] class TestVampireTokenEmbedder ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] [EOL] def test_forward_works_with_encoder_output_and_projection ( self ) : [EOL] params = Params ( { [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : - [number] , [string] : [number] } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = VampireTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] assert embedding_layer . get_output_dim ( ) == [number] [EOL] input_tensor = torch . LongTensor ( [ word1 , word2 ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] ) [EOL] [EOL] def test_forward_encoder_output_with_expansion_works ( self ) : [EOL] params = Params ( { [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : - [number] , [string] : True , [string] : [number] } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = VampireTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] input_tensor = torch . LongTensor ( [ word1 , word2 ] ) [EOL] expected_vectors = embedding_layer . _vae ( input_tensor ) [ [string] ] . detach ( ) . data . numpy ( ) [EOL] embedded = embedding_layer ( input_tensor ) . detach ( ) . data . numpy ( ) [EOL] for row in range ( input_tensor . shape [ [number] ] ) : [EOL] for col in range ( input_tensor . shape [ [number] ] ) : [EOL] np . testing . assert_allclose ( embedded [ row , col , : ] , expected_vectors [ row , : ] ) [EOL] [EOL] def test_projection_works_with_encoder_weight_representations ( self ) : [EOL] params = Params ( { [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : - [number] , [string] : [number] , [string] : True } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = VampireTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] assert embedding_layer . get_output_dim ( ) == [number] [EOL] input_tensor = torch . LongTensor ( [ word1 , word2 ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] , [number] ) [EOL] [EOL] def test_forward_works_with_encoder_weight_and_projection ( self ) : [EOL] params = Params ( { [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : - [number] , [string] : [number] , [string] : True } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = VampireTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] assert embedding_layer . get_output_dim ( ) == [number] [EOL] input_tensor = torch . LongTensor ( [ word1 , word2 ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] , [number] ) [EOL] [EOL] def test_forward_works_with_encoder_output_expand_and_projection ( self ) : [EOL] params = Params ( { [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : VAETestCase . FIXTURES_ROOT / [string] / [string] , [string] : - [number] , [string] : [number] , [string] : True } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = VampireTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] assert embedding_layer . get_output_dim ( ) == [number] [EOL] input_tensor = torch . LongTensor ( [ word1 , word2 ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] , [number] )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 $vampire.modules.token_embedders.vampire_token_embedder.VampireTokenEmbedder$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from vampire . common . testing . test_case import VAETestCase [EOL] from vampire . common . allennlp_bridge import ExtendedVocabulary , VocabularyWithPretrainedVAE [EOL] from vampire . common . util import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List , Dict , Any [EOL] import allennlp [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] import codecs [EOL] import json [EOL] import os [EOL] import pickle [EOL] from typing import Any , Dict , List [EOL] [EOL] import numpy as np [EOL] import torch [EOL] from allennlp . data import Vocabulary [EOL] from scipy import sparse [EOL] [EOL] [EOL] def compute_background_log_frequency ( vocab , vocab_namespace , precomputed_bg_file = None ) : [EOL] [docstring] [EOL] [comment] [EOL] log_term_frequency = torch . FloatTensor ( vocab . get_vocab_size ( vocab_namespace ) ) [EOL] if precomputed_bg_file is not None : [EOL] with open ( precomputed_bg_file , [string] ) as file_ : [EOL] precomputed_bg = json . load ( file_ ) [EOL] else : [EOL] precomputed_bg = vocab . _retained_counter . get ( vocab_namespace ) [comment] [EOL] if precomputed_bg is None : [EOL] return log_term_frequency [EOL] for i in range ( vocab . get_vocab_size ( vocab_namespace ) ) : [EOL] token = vocab . get_token_from_index ( i , vocab_namespace ) [EOL] if token in ( [string] , [string] , [string] , [string] ) or token not in precomputed_bg : [EOL] log_term_frequency [ i ] = [number] [EOL] elif token in precomputed_bg : [EOL] if precomputed_bg [ token ] == [number] : [EOL] log_term_frequency [ i ] = [number] [EOL] else : [EOL] log_term_frequency [ i ] = precomputed_bg [ token ] [EOL] log_term_frequency = torch . log ( log_term_frequency ) [EOL] return log_term_frequency [EOL] [EOL] [EOL] def log_standard_categorical ( logits ) : [EOL] [docstring] [EOL] [comment] [EOL] prior = torch . softmax ( torch . ones_like ( logits ) , dim = [number] ) [EOL] prior . requires_grad = False [EOL] [EOL] cross_entropy = - torch . sum ( logits * torch . log ( prior + [number] ) , dim = [number] ) [EOL] [EOL] return cross_entropy [EOL] [EOL] [EOL] def separate_labeled_unlabeled_instances ( text , classifier_text , label , metadata ) : [EOL] [docstring] [EOL] labeled_instances = { } [EOL] unlabeled_instances = { } [EOL] is_labeled = [ int ( md [ [string] ] ) for md in metadata ] [EOL] [EOL] is_labeled = np . array ( is_labeled ) [EOL] [comment] [EOL] labeled_indices = ( is_labeled != [number] ) . nonzero ( ) [comment] [EOL] labeled_instances [ [string] ] = text [ labeled_indices ] [EOL] labeled_instances [ [string] ] = classifier_text [ labeled_indices ] [EOL] labeled_instances [ [string] ] = label [ labeled_indices ] [EOL] [EOL] unlabeled_indices = ( is_labeled == [number] ) . nonzero ( ) [comment] [EOL] unlabeled_instances [ [string] ] = text [ unlabeled_indices ] [EOL] unlabeled_instances [ [string] ] = classifier_text [ unlabeled_indices ] [EOL] [EOL] return labeled_instances , unlabeled_instances [EOL] [EOL] [EOL] def schedule ( batch_num , anneal_type = [string] ) : [EOL] [docstring] [EOL] if anneal_type == [string] : [EOL] return min ( [number] , batch_num / [number] ) [EOL] elif anneal_type == [string] : [EOL] return float ( [number] / ( [number] + np . exp ( - [number] * ( batch_num - [number] ) ) ) ) [EOL] elif anneal_type == [string] : [EOL] return [number] [EOL] elif anneal_type == [string] : [EOL] return float ( [number] / ( [number] + np . exp ( [number] * ( batch_num - [number] ) ) ) ) [EOL] else : [EOL] return [number] [EOL] [EOL] [EOL] def makedirs ( directory ) : [EOL] if not os . path . exists ( directory ) : [EOL] os . makedirs ( directory ) [EOL] [EOL] [EOL] def write_to_json ( data , output_filename , indent = [number] , sort_keys = True ) : [EOL] with codecs . open ( output_filename , [string] , encoding = [string] ) as output_file : [EOL] json . dump ( data , output_file , indent = indent , sort_keys = sort_keys ) [EOL] [EOL] [EOL] def read_json ( input_filename ) : [EOL] with codecs . open ( input_filename , [string] , encoding = [string] ) as input_file : [EOL] data = json . load ( input_file , encoding = [string] ) [EOL] return data [EOL] [EOL] [EOL] def read_jsonlist ( input_filename ) : [EOL] data = [ ] [EOL] with codecs . open ( input_filename , [string] , encoding = [string] ) as input_file : [EOL] for line in input_file : [EOL] data . append ( json . loads ( line , encoding = [string] ) ) [EOL] return data [EOL] [EOL] [EOL] def write_jsonlist ( list_of_json_objects , output_filename , sort_keys = True ) : [EOL] with codecs . open ( output_filename , [string] , encoding = [string] ) as output_file : [EOL] for obj in list_of_json_objects : [EOL] output_file . write ( json . dumps ( obj , sort_keys = sort_keys ) + [string] ) [EOL] [EOL] [EOL] def pickle_data ( data , output_filename ) : [EOL] with open ( output_filename , [string] ) as outfile : [EOL] pickle . dump ( data , outfile , pickle . HIGHEST_PROTOCOL ) [EOL] [EOL] [EOL] def unpickle_data ( input_filename ) : [EOL] with open ( input_filename , [string] ) as infile : [EOL] data = pickle . load ( infile ) [EOL] return data [EOL] [EOL] [EOL] def read_text ( input_filename ) : [EOL] with codecs . open ( input_filename , [string] , encoding = [string] ) as input_file : [EOL] lines = [ x . strip ( ) for x in input_file . readlines ( ) ] [EOL] return lines [EOL] [EOL] [EOL] def write_list_to_text ( lines , output_filename , add_newlines = True , add_final_newline = False ) : [EOL] if add_newlines : [EOL] lines = [string] . join ( lines ) [EOL] if add_final_newline : [EOL] lines += [string] [EOL] else : [EOL] lines = [string] . join ( lines ) [EOL] if add_final_newline : [EOL] lines [ - [number] ] += [string] [EOL] [EOL] with codecs . open ( output_filename , [string] , encoding = [string] ) as output_file : [EOL] output_file . writelines ( lines ) [EOL] [EOL] [EOL] def save_sparse ( sparse_matrix , output_filename ) : [EOL] assert sparse . issparse ( sparse_matrix ) [EOL] if sparse . isspmatrix_coo ( sparse_matrix ) : [EOL] coo = sparse_matrix [EOL] else : [EOL] coo = sparse_matrix . tocoo ( ) [EOL] row = coo . row [EOL] col = coo . col [EOL] data = coo . data [EOL] shape = coo . shape [EOL] np . savez ( output_filename , row = row , col = col , data = data , shape = shape ) [EOL] [EOL] [EOL] def load_sparse ( input_filename ) : [EOL] npy = np . load ( input_filename ) [EOL] coo_matrix = sparse . coo_matrix ( ( npy [ [string] ] , ( npy [ [string] ] , npy [ [string] ] ) ) , shape = npy [ [string] ] ) [EOL] return coo_matrix . tocsc ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from vampire . common . testing . test_case import VAETestCase [EOL]	0 0 0 0 0 0 0 0 0 0 0