from typing import Union , Dict , Literal , Any [EOL] import pathlib [EOL] import typing [EOL] import typing_extensions [EOL] import logging [EOL] import os [EOL] import tempfile [EOL] from pathlib import Path [EOL] [EOL] import pytest [EOL] [EOL] [comment] [EOL] TEST_CACHE_DIR = Path ( os . getenv ( [string] , Path ( __file__ ) . parent ) ) [EOL] TEST_CACHE_PATH = TEST_CACHE_DIR / [string] [EOL] TEST_CACHE_PATH . mkdir ( exist_ok = True , parents = True ) [EOL] [EOL] [comment] [EOL] PERSIST_DIR = TEST_CACHE_PATH / [string] [EOL] PERSIST_DIR . mkdir ( exist_ok = True , parents = True ) [EOL] [EOL] [EOL] def pytest_addoption ( parser ) : [EOL] parser . addoption ( [string] , action = [string] , help = [string] [string] [string] [string] f"{ PERSIST_DIR } [string] " [string] , ) [EOL] parser . addoption ( [string] , action = [string] , help = [string] [string] , ) [EOL] parser . addoption ( [string] , default = [string] , help = [string] [string] , ) [EOL] parser . addoption ( [string] , default = logging . WARN , help = [string] [string] , ) [EOL] parser . addoption ( [string] , action = [string] , help = [string] [string] , ) [EOL] [EOL] [EOL] def make_temp_dir ( ) : [EOL] [comment] [EOL] [comment] [EOL] with tempfile . TemporaryDirectory ( dir = TEST_CACHE_PATH ) as d : [EOL] old_val = os . getenv ( [string] , [string] ) [EOL] os . environ [ [string] ] = d [EOL] [EOL] yield Path ( d ) [EOL] [EOL] os . environ [ [string] ] = old_val [EOL] [EOL] [EOL] @ pytest . yield_fixture ( scope = [string] ) def tmp_gobbli_dir ( ) : [EOL] [docstring] [EOL] for temp_dir in make_temp_dir ( ) : [EOL] yield temp_dir [EOL] [EOL] [EOL] @ pytest . yield_fixture ( scope = [string] ) def gobbli_dir ( request ) : [EOL] [docstring] [EOL] if request . config . getoption ( [string] ) : [EOL] old_val = os . getenv ( [string] , [string] ) [EOL] os . environ [ [string] ] = str ( PERSIST_DIR ) [EOL] yield PERSIST_DIR [EOL] os . environ [ [string] ] = old_val [EOL] else : [EOL] for temp_dir in make_temp_dir ( ) : [EOL] yield temp_dir [EOL] [EOL] [EOL] @ pytest . fixture def model_gpu_config ( request ) : [EOL] [docstring] [EOL] gpu_config = { } [EOL] if request . config . getoption ( [string] ) : [EOL] gpu_config [ [string] ] = True [EOL] gpu_config [ [string] ] = request . config . getoption ( [string] ) [EOL] [EOL] return gpu_config [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import json [EOL] import os [EOL] [EOL] from setuptools import find_packages , setup [EOL] [EOL] containing_dir = os . path . split ( __file__ ) [ [number] ] [EOL] [EOL] with open ( os . path . join ( containing_dir , [string] ) , [string] ) as f : [EOL] meta = json . load ( f ) [EOL] [EOL] readme_path = os . path . join ( containing_dir , [string] ) [EOL] [EOL] with open ( readme_path , [string] ) as f : [EOL] long_description = f . read ( ) [EOL] [EOL] setup ( name = meta [ [string] ] , author = meta [ [string] ] , maintainer = meta [ [string] ] , version = meta [ [string] ] , packages = find_packages ( exclude = [ [string] ] ) , license = [string] , description = meta [ [string] ] , long_description = long_description , long_description_content_type = [string] , include_package_data = True , url = meta [ [string] ] , install_requires = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] , extras_require = { [string] : [ [string] , [string] , [string] ] , [string] : [ [string] ] , [string] : [ [string] , [string] , [string] , [string] , [string] , ] , } , python_requires = [string] , entry_points = { [string] : [ [string] ] } , classifiers = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] from typing import Dict , Any , Tuple , List [EOL] import pathlib [EOL] import typing [EOL] import datetime as dt [EOL] import json [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] import os [EOL] import sys [EOL] from pathlib import Path [EOL] [EOL] import gobbli [EOL] [EOL] file_loc = os . path . split ( __file__ ) [ [number] ] [EOL] sys . path . insert ( [number] , os . path . join ( os . path . dirname ( file_loc ) , [string] ) ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] with open ( os . path . join ( file_loc , [string] , [string] ) , [string] ) as f : [EOL] meta = json . load ( f ) [EOL] [EOL] project = meta [ [string] ] [EOL] author = meta [ [string] ] [EOL] copyright = f"{ dt . date . today ( ) . year } [string] { author }" [EOL] [EOL] [comment] [EOL] version = meta [ [string] ] [EOL] release = version [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] extensions = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] autodoc_default_options = { [string] : None , [string] : None , [string] : None , } [EOL] [EOL] autoclass_content = [string] [EOL] autosummary_generate = True [EOL] [EOL] intersphinx_mapping = { [string] : ( [string] , None ) , [string] : ( [string] , None ) , [string] : ( [string] , None ) , } [EOL] [EOL] [comment] [EOL] templates_path = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclude_patterns = [ [string] , [string] , [string] ] [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_theme = [string] [EOL] html_theme_options = { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , } [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_static_path = [ [string] ] [EOL] [EOL] html_sidebars = { [string] : [ [string] , [string] , [string] , [string] ] } [EOL] [EOL] html_favicon = os . path . join ( [string] , [string] ) [EOL] html_title = f" [string] { version } [string] " [EOL] [EOL] [comment] [EOL] def run_apidoc ( _ ) : [EOL] from sphinx . ext . apidoc import main [EOL] [EOL] [comment] [EOL] base_dir = Path ( __file__ ) . parent . parent . resolve ( ) [EOL] [EOL] output_path = base_dir / [string] / [string] [EOL] main ( [ [string] , [string] , [string] , str ( output_path ) , str ( base_dir / project ) , str ( base_dir / project / [string] / [string] / [string] ) , ] ) [EOL] [EOL] [EOL] def setup ( app ) : [EOL] app . connect ( [string] , run_apidoc ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Dict[builtins.str,typing.Tuple[builtins.str,None]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Tuple , List [EOL] import builtins [EOL] import typing [EOL] import pandas [EOL] import pathlib [EOL] import json [EOL] from typing import List , Tuple [EOL] [EOL] import pandas as pd [EOL] [EOL] from gobbli . dataset . base import BaseDataset [EOL] from gobbli . util import download_archive [EOL] [EOL] [EOL] class MovieSummaryDataset ( BaseDataset ) : [EOL] [docstring] [EOL] [EOL] PLOT_SUMMARIES_FILE = [string] [EOL] METADATA_FILE = [string] [EOL] TRAIN_PCT = [number] [EOL] [EOL] def _build ( self ) : [EOL] data_dir = self . data_dir ( ) [EOL] data_dir . mkdir ( exist_ok = True , parents = True ) [EOL] [EOL] download_archive ( [string] , data_dir ) [EOL] [EOL] @ staticmethod def _make_multilabels ( genres ) : [EOL] return [ list ( json . loads ( g ) . values ( ) ) for g in genres ] [EOL] [EOL] def _is_built ( self ) : [EOL] data_dir = self . data_dir ( ) [EOL] return ( data_dir / MovieSummaryDataset . PLOT_SUMMARIES_FILE ) . exists ( ) and ( data_dir / MovieSummaryDataset . METADATA_FILE ) . exists ( ) [EOL] [EOL] def _get_source_df_split ( self ) : [EOL] if not hasattr ( self , [string] ) : [EOL] data_dir = self . data_dir ( ) [EOL] plot_df = pd . read_csv ( data_dir / MovieSummaryDataset . PLOT_SUMMARIES_FILE , delimiter = [string] , index_col = [number] , header = None , names = [ [string] , [string] ] , ) [EOL] [EOL] meta_df = pd . read_csv ( data_dir / MovieSummaryDataset . METADATA_FILE , delimiter = [string] , index_col = [number] , header = None , names = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] , ) [EOL] [EOL] self . _source_df = plot_df . join ( meta_df , how = [string] ) [ [ [string] , [string] ] ] . sort_index ( ) [EOL] [EOL] return ( self . _source_df , int ( len ( self . _source_df ) * MovieSummaryDataset . TRAIN_PCT ) , ) [EOL] [EOL] def X_train ( self ) : [EOL] source_df , split_ndx = self . _get_source_df_split ( ) [EOL] return source_df [ [string] ] . tolist ( ) [ : split_ndx ] [EOL] [EOL] def y_train ( self ) : [EOL] source_df , split_ndx = self . _get_source_df_split ( ) [EOL] return MovieSummaryDataset . _make_multilabels ( source_df [ [string] ] [ : split_ndx ] ) [EOL] [EOL] def X_test ( self ) : [EOL] source_df , split_ndx = self . _get_source_df_split ( ) [EOL] return source_df [ [string] ] . tolist ( ) [ split_ndx : ] [EOL] [EOL] def y_test ( self ) : [EOL] source_df , split_ndx = self . _get_source_df_split ( ) [EOL] return MovieSummaryDataset . _make_multilabels ( source_df [ [string] ] [ split_ndx : ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 $pandas.Series$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pandas.Series$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[pandas.DataFrame,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 $typing.Any$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , Tuple [EOL] import pathlib [EOL] import typing [EOL] import builtins [EOL] from pathlib import Path [EOL] from typing import Set , Tuple [EOL] [EOL] from gobbli . dataset . nested_file import NestedFileDataset [EOL] from gobbli . util import download_archive [EOL] [EOL] [EOL] class NewsgroupsDataset ( NestedFileDataset ) : [EOL] [docstring] [EOL] [EOL] def labels ( self ) : [EOL] return { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def download ( self , data_dir ) : [EOL] return download_archive ( [string] , data_dir , filename = [string] , ) [EOL] [EOL] def folders ( self ) : [EOL] return Path ( [string] ) , Path ( [string] ) [EOL] [EOL] def read_source_file ( self , file_path ) : [EOL] return file_path . read_text ( encoding = [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 $typing.Tuple[pathlib.Path,pathlib.Path]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $pathlib.Path$ 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0
from typing import Any , Set , Tuple , List [EOL] import pathlib [EOL] import typing [EOL] import builtins [EOL] from abc import abstractmethod [EOL] from pathlib import Path [EOL] from typing import List , Set , Tuple [EOL] [EOL] from gobbli . dataset . base import BaseDataset [EOL] [EOL] [EOL] class NestedFileDataset ( BaseDataset ) : [EOL] [docstring] [EOL] [EOL] TRAIN_X_FILE = [string] [EOL] TRAIN_Y_FILE = [string] [EOL] [EOL] TEST_X_FILE = [string] [EOL] TEST_Y_FILE = [string] [EOL] [EOL] [comment] [EOL] DELIMITER = [string] [EOL] [EOL] @ abstractmethod def labels ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def download ( self , data_dir ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def folders ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def read_source_file ( self , file_path ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def _build ( self ) : [EOL] data_dir = self . data_dir ( ) [EOL] data_dir . mkdir ( exist_ok = True , parents = True ) [EOL] [EOL] self . download ( data_dir ) [EOL] [EOL] train_folder , test_folder = self . folders ( ) [EOL] [EOL] self . _load_folder ( data_dir / train_folder , data_dir / self . TRAIN_X_FILE , data_dir / self . TRAIN_Y_FILE , ) [EOL] [EOL] self . _load_folder ( data_dir / test_folder , data_dir / self . TEST_X_FILE , data_dir / self . TEST_Y_FILE , ) [EOL] [EOL] def _load_folder ( self , folder , X_file , y_file ) : [EOL] [docstring] [EOL] X = [ ] [EOL] y = [ ] [EOL] [EOL] labels = self . labels ( ) [EOL] [EOL] for category_dir in folder . iterdir ( ) : [EOL] category_name = category_dir . name [EOL] [EOL] [comment] [EOL] if category_name not in labels : [EOL] continue [EOL] [EOL] for data_file in category_dir . iterdir ( ) : [EOL] X . append ( self . read_source_file ( data_file ) ) [EOL] y . append ( category_name ) [EOL] [EOL] X_file . write_text ( NestedFileDataset . DELIMITER . join ( X ) ) [EOL] y_file . write_text ( NestedFileDataset . DELIMITER . join ( y ) ) [EOL] [EOL] def _read_data_file ( self , filepath ) : [EOL] return filepath . read_text ( ) . split ( NestedFileDataset . DELIMITER ) [EOL] [EOL] def _is_built ( self ) : [EOL] data_files = ( self . TRAIN_X_FILE , self . TRAIN_Y_FILE , self . TEST_X_FILE , self . TEST_Y_FILE , ) [EOL] return all ( ( self . data_dir ( ) / data_file ) . exists ( ) for data_file in data_files ) [EOL] [EOL] def X_train ( self ) : [EOL] return self . _read_data_file ( self . data_dir ( ) / self . TRAIN_X_FILE ) [EOL] [EOL] def y_train ( self ) : [EOL] return self . _read_data_file ( self . data_dir ( ) / self . TRAIN_Y_FILE ) [EOL] [EOL] def X_test ( self ) : [EOL] return self . _read_data_file ( self . data_dir ( ) / self . TEST_X_FILE ) [EOL] [EOL] def y_test ( self ) : [EOL] return self . _read_data_file ( self . data_dir ( ) / self . TEST_Y_FILE ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[pathlib.Path,pathlib.Path]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $pathlib.Path$ 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,builtins.str,builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import typing [EOL] import builtins [EOL] from gobbli . dataset . base import BaseDataset [EOL] [EOL] [EOL] class TrivialDataset ( BaseDataset ) : [EOL] [docstring] [EOL] [EOL] DATASET = [ [string] , [string] ] [EOL] LABELS = [ [string] , [string] ] [EOL] [EOL] def _is_built ( self ) : [EOL] return True [EOL] [EOL] def _build ( self ) : [EOL] pass [EOL] [EOL] def X_train ( self ) : [EOL] return TrivialDataset . DATASET [EOL] [EOL] def y_train ( self ) : [EOL] return TrivialDataset . LABELS [EOL] [EOL] def X_test ( self ) : [EOL] return TrivialDataset . DATASET [EOL] [EOL] def y_test ( self ) : [EOL] return TrivialDataset . LABELS [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Tuple , List [EOL] import typing [EOL] import gobbli [EOL] import logging [EOL] import pathlib [EOL] import builtins [EOL] import logging [EOL] from abc import ABC , abstractmethod [EOL] from pathlib import Path [EOL] from timeit import default_timer as timer [EOL] from typing import Any , List , Optional , Tuple [EOL] [EOL] from sklearn . model_selection import train_test_split [EOL] [EOL] import gobbli . io [EOL] from gobbli . util import collect_labels , format_duration , gobbli_dir , shuffle_together [EOL] [EOL] LOGGER = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def dataset_dir ( ) : [EOL] return gobbli_dir ( ) / [string] [EOL] [EOL] [EOL] class BaseDataset ( ABC ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , * args , ** kwargs ) : [EOL] [docstring] [EOL] [EOL] @ classmethod def data_dir ( cls ) : [EOL] return dataset_dir ( ) / cls . __name__ [EOL] [EOL] @ classmethod def load ( cls , * args , ** kwargs ) : [EOL] ds = cls ( * args , ** kwargs ) [EOL] [EOL] if not ds . _is_built ( ) : [EOL] LOGGER . info ( [string] , cls . __name__ ) [EOL] start = timer ( ) [EOL] ds . _build ( ) [EOL] end = timer ( ) [EOL] [EOL] LOGGER . info ( f" [string] { format_duration ( end - start ) } [string] " ) [EOL] [EOL] return ds [EOL] [EOL] @ abstractmethod def _is_built ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def _build ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def X_train ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def y_train ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def X_test ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] @ abstractmethod def y_test ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] def _get_train_valid ( self , limit = None , shuffle_seed = [number] ) : [EOL] [docstring] [EOL] X_train_valid = self . X_train ( ) [EOL] y_train_valid = self . y_train ( ) [EOL] [EOL] [comment] [EOL] shuffle_together ( X_train_valid , y_train_valid , shuffle_seed ) [EOL] [EOL] if limit is not None : [EOL] X_train_valid = X_train_valid [ : limit ] [EOL] y_train_valid = y_train_valid [ : limit ] [EOL] [EOL] return X_train_valid , y_train_valid [EOL] [EOL] def train_input ( self , train_batch_size = [number] , valid_batch_size = [number] , num_train_epochs = [number] , valid_proportion = [number] , split_seed = [number] , shuffle_seed = [number] , limit = None , ) : [EOL] if not self . _is_built ( ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] X_train_valid , y_train_valid = self . _get_train_valid ( limit = limit , shuffle_seed = shuffle_seed ) [EOL] [EOL] X_train , X_valid , y_train , y_valid = train_test_split ( X_train_valid , y_train_valid , test_size = valid_proportion , random_state = split_seed , ) [EOL] [EOL] return gobbli . io . TrainInput ( X_train = X_train , X_valid = X_valid , y_train = y_train , y_valid = y_valid , train_batch_size = train_batch_size , valid_batch_size = valid_batch_size , num_train_epochs = num_train_epochs , ) [EOL] [EOL] def embed_input ( self , embed_batch_size = [number] , pooling = gobbli . io . EmbedPooling . MEAN , limit = None , ) : [EOL] if not self . _is_built ( ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] X_test = self . X_test ( ) [EOL] if limit is not None : [EOL] X_test = X_test [ : limit ] [EOL] [EOL] return gobbli . io . EmbedInput ( X = X_test , embed_batch_size = embed_batch_size , pooling = pooling ) [EOL] [EOL] def predict_input ( self , predict_batch_size = [number] , limit = None ) : [EOL] if not self . _is_built ( ) : [EOL] raise ValueError ( [string] ) [EOL] [EOL] _ , y_train_valid = self . _get_train_valid ( limit = limit ) [EOL] [EOL] X_test = self . X_test ( ) [EOL] if limit is not None : [EOL] X_test = X_test [ : limit ] [EOL] [EOL] labels = collect_labels ( y_train_valid ) [EOL] [EOL] return gobbli . io . PredictInput ( X = X_test , predict_batch_size = predict_batch_size , labels = labels ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"BaseDataset"$ 0 0 0 0 0 0 0 0 0 0 0 $gobbli.dataset.base.BaseDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.dataset.base.BaseDataset$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $gobbli.dataset.base.BaseDataset$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 $gobbli.dataset.base.BaseDataset$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.List[builtins.str],typing.List[typing.Any]]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Optional[builtins.int]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $gobbli.io.TrainInput$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $gobbli.io.EmbedInput$ 0 0 0 $builtins.int$ 0 0 0 $gobbli.io.EmbedPooling$ 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 $gobbli.io.EmbedPooling$ 0 $gobbli.io.EmbedPooling$ 0 0 0 0 $gobbli.io.PredictInput$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0
from typing import Dict , Tuple , List [EOL] import typing [EOL] import gobbli [EOL] import pandas as pd [EOL] [EOL] from gobbli . inspect . evaluate import ClassificationError , ClassificationEvaluation [EOL] from gobbli . util import multilabel_to_indicator_df [EOL] [EOL] [EOL] def test_classification_evaluation_multiclass ( ) : [EOL] results = ClassificationEvaluation ( labels = [ [string] , [string] ] , X = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , y_true = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , y_pred_proba = pd . DataFrame ( { [string] : [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [string] : [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , } ) , ) [EOL] [EOL] [comment] [EOL] expected_y_pred = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert results . y_pred_multiclass == expected_y_pred [EOL] [EOL] [comment] [EOL] [comment] [EOL] ea2 = ClassificationError ( X = [string] , y_true = [string] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] ea3 = ClassificationError ( X = [string] , y_true = [string] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] eb1 = ClassificationError ( X = [string] , y_true = [string] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] eb3 = ClassificationError ( X = [string] , y_true = [string] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] eb5 = ClassificationError ( X = [string] , y_true = [string] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] [EOL] [comment] [EOL] errors = results . errors ( k = [number] ) [EOL] [EOL] [comment] [EOL] a_false_positives = [ eb1 , eb5 ] [EOL] a_false_negatives = [ ea2 , ea3 ] [EOL] b_false_positives = a_false_negatives [EOL] b_false_negatives = a_false_positives [EOL] [EOL] a_errors = errors [ [string] ] [EOL] b_errors = errors [ [string] ] [EOL] assert a_errors [ [number] ] == a_false_positives [EOL] assert a_errors [ [number] ] == a_false_negatives [EOL] assert b_errors [ [number] ] == b_false_positives [EOL] assert b_errors [ [number] ] == b_false_negatives [EOL] [EOL] [EOL] def test_classification_evaluation_multilabel ( ) : [EOL] labels = [ [string] , [string] ] [EOL] results = ClassificationEvaluation ( labels = labels , X = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] , y_true = [ [ [string] ] , [ [string] ] , [ [string] , [string] ] , [ [string] , [string] ] , [ [string] ] , [ [string] ] , [ ] , [ ] , [ ] ] , y_pred_proba = pd . DataFrame ( { [string] : [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [string] : [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , } ) , ) [EOL] [EOL] [comment] [EOL] expected_y_pred = multilabel_to_indicator_df ( [ [ [string] ] , [ [string] ] , [ [string] ] , [ [string] , [string] ] , [ [string] ] , [ [string] ] , [ [string] ] , [ [string] ] , [ ] ] , labels ) [EOL] pd . testing . assert_frame_equal ( results . y_pred_multilabel , expected_y_pred ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] ea2 = ClassificationError ( X = [string] , y_true = [ [string] ] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] eab1 = ClassificationError ( X = [string] , y_true = [ [string] , [string] ] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] eb1 = ClassificationError ( X = [string] , y_true = [ [string] ] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] e01 = ClassificationError ( X = [string] , y_true = [ ] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] e02 = ClassificationError ( X = [string] , y_true = [ ] , y_pred_proba = { [string] : [number] , [string] : [number] } ) [EOL] [EOL] [comment] [EOL] errors = results . errors ( k = [number] ) [EOL] [EOL] [comment] [EOL] a_false_positives = [ eb1 , e01 ] [EOL] a_false_negatives = [ ea2 , eab1 ] [EOL] b_false_positives = [ ea2 , e02 ] [EOL] b_false_negatives = [ eb1 ] [EOL] [EOL] a_errors = errors [ [string] ] [EOL] b_errors = errors [ [string] ] [EOL] assert a_errors [ [number] ] == a_false_positives [EOL] assert a_errors [ [number] ] == a_false_negatives [EOL] assert b_errors [ [number] ] == b_false_positives [EOL] assert b_errors [ [number] ] == b_false_negatives [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import argparse [EOL] import argparse [EOL] [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from transformers import BertConfig , BertForMaskedLM , BertTokenizer [EOL] [EOL] [EOL] def batch_list ( l , batch_size ) : [EOL] for i in range ( [number] , len ( l ) , batch_size ) : [EOL] yield l [ i : i + batch_size ] [EOL] [EOL] [EOL] def encode_batch ( batch , tokenizer , config ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] encoded_texts = [ ] [EOL] for text in batch : [EOL] encoded_text = torch . tensor ( tokenizer . encode ( text ) ) [ : config . max_position_embeddings ] [EOL] encoded_texts . append ( encoded_text ) [EOL] [EOL] return torch . nn . utils . rnn . pad_sequence ( encoded_texts , batch_first = True ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] [EOL] parser . add_argument ( [string] , help = [string] ) [EOL] parser . add_argument ( [string] , help = [string] , ) [EOL] parser . add_argument ( [string] , help = [string] [string] [string] [string] , default = [string] , ) [EOL] parser . add_argument ( [string] , help = [string] , type = int , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , type = float , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , type = float , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , type = int , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , type = int , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , default = None , ) [EOL] parser . add_argument ( [string] , default = [string] , help = [string] , ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] if args . batch_size < [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if args . times < [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if not [number] <= args . probability <= [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if not [number] < args . diversity <= [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] tokenizer = BertTokenizer . from_pretrained ( args . bert_model , cache_dir = args . cache_dir ) [EOL] if tokenizer is None : [EOL] raise ValueError ( [string] ) [EOL] config = BertConfig . from_pretrained ( args . bert_model , cache_dir = args . cache_dir ) [EOL] if config is None : [EOL] raise ValueError ( [string] ) [EOL] model = BertForMaskedLM . from_pretrained ( args . bert_model , config = config , cache_dir = args . cache_dir ) [EOL] if model is None : [EOL] raise ValueError ( [string] ) [EOL] [EOL] model = model . to ( args . device ) [EOL] model . eval ( ) [EOL] [EOL] with open ( args . input_file , [string] , encoding = [string] ) as f_in : [EOL] with open ( args . output_file , [string] , encoding = [string] ) as f_out : [EOL] batches = batch_list ( f_in . readlines ( ) , args . batch_size ) [EOL] input_id_batches = [ encode_batch ( batch , tokenizer , config ) for batch in batches ] [EOL] [EOL] for time in range ( args . times ) : [EOL] for batch_id , input_ids in enumerate ( input_id_batches ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] should_replace = ( torch . rand_like ( input_ids , dtype = torch . float ) < args . probability ) & ( input_ids != tokenizer . vocab . get ( tokenizer . pad_token ) ) [EOL] [EOL] masked_ids = input_ids . clone ( ) . detach ( ) [EOL] masked_ids [ should_replace ] = tokenizer . vocab . get ( tokenizer . mask_token ) [EOL] masked_ids = masked_ids . to ( args . device ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] attention_mask = torch . ones_like ( masked_ids ) . to ( args . device ) [EOL] token_type_ids = torch . zeros_like ( masked_ids ) . to ( args . device ) [EOL] [EOL] with torch . no_grad ( ) : [EOL] ( output , ) = model ( masked_ids , attention_mask = attention_mask , token_type_ids = token_type_ids , ) [EOL] [EOL] [comment] [EOL] masked_ids = masked_ids . cpu ( ) [EOL] output = output . cpu ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] prediction_scores = torch . pow ( F . softmax ( output , dim = [number] ) , [number] / args . diversity ) [EOL] [EOL] max_seq_len = prediction_scores . size ( [number] ) [EOL] [EOL] output_ids = [ ] [EOL] for i , row in enumerate ( prediction_scores ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] candidate_ndxs = torch . argsort ( row , dim = [number] , descending = True ) [ : , : args . n_probable ] [EOL] [EOL] candidates = torch . stack ( [ tok [ candidate_ndxs [ i ] ] for i , tok in enumerate ( row ) ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] replacement_sorted_ndxs = torch . multinomial ( candidates , [number] ) . squeeze ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] replacements = torch . tensor ( [ candidate_ndxs [ i ] [ ndx ] for i , ndx in enumerate ( replacement_sorted_ndxs ) ] ) [EOL] [EOL] [comment] [EOL] output_ids . append ( torch . where ( should_replace [ i ] , replacements , masked_ids [ i ] ) . tolist ( ) ) [EOL] [EOL] [comment] [EOL] output_texts = [ tokenizer . decode ( [ tok for tok in row if not tok == tokenizer . vocab . get ( tokenizer . pad_token ) ] ) for row in output_ids ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for i , row in enumerate ( output_texts ) : [EOL] f_out . write ( row . replace ( [string] , [string] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not ( i == len ( output_texts ) - [number] [EOL] and batch_id == len ( input_id_batches ) - [number] [EOL] and time == args . times - [number] ) : [EOL] f_out . write ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import argparse [EOL] import argparse [EOL] [EOL] from transformers import MarianMTModel , MarianTokenizer [EOL] [EOL] [EOL] def batch_list ( l , batch_size ) : [EOL] for i in range ( [number] , len ( l ) , batch_size ) : [EOL] yield l [ i : i + batch_size ] [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] [EOL] parser . add_argument ( [string] , help = [string] [string] [string] [string] , ) [EOL] parser . add_argument ( [string] , help = [string] , ) [EOL] parser . add_argument ( [string] , help = [string] [string] [string] [string] [string] [string] , required = True , ) [EOL] parser . add_argument ( [string] , help = [string] [string] [string] [string] , required = True , ) [EOL] parser . add_argument ( [string] , help = [string] , type = int , default = [number] , ) [EOL] parser . add_argument ( [string] , help = [string] , default = None , ) [EOL] parser . add_argument ( [string] , default = [string] , help = [string] , ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] if args . batch_size < [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] tokenizer = MarianTokenizer . from_pretrained ( args . marian_model , cache_dir = args . cache_dir ) [EOL] if tokenizer is None : [EOL] raise ValueError ( [string] ) [EOL] model = MarianMTModel . from_pretrained ( args . marian_model , cache_dir = args . cache_dir ) [EOL] if model is None : [EOL] raise ValueError ( [string] ) [EOL] [EOL] model = model . to ( args . device ) [EOL] model . eval ( ) [EOL] [EOL] inv_tokenizer = MarianTokenizer . from_pretrained ( args . marian_inverse_model , cache_dir = args . cache_dir ) [EOL] if inv_tokenizer is None : [EOL] raise ValueError ( [string] ) [EOL] inv_model = MarianMTModel . from_pretrained ( args . marian_inverse_model , cache_dir = args . cache_dir ) [EOL] if inv_model is None : [EOL] raise ValueError ( [string] ) [EOL] [EOL] inv_model = inv_model . to ( args . device ) [EOL] inv_model . eval ( ) [EOL] [EOL] with open ( args . input_file , [string] , encoding = [string] ) as f_in : [EOL] with open ( args . output_file , [string] , encoding = [string] ) as f_out : [EOL] batches = list ( batch_list ( f_in . readlines ( ) , args . batch_size ) ) [EOL] for batch_id , batch in enumerate ( batches ) : [EOL] [comment] [EOL] [comment] [EOL] translated = model . generate ( ** tokenizer . prepare_translation_batch ( batch , max_length = tokenizer . model_max_length ) . to ( args . device ) ) [EOL] translated_texts = [ tokenizer . decode ( t , skip_special_tokens = True ) for t in translated ] [EOL] [EOL] backtranslated = inv_model . generate ( ** inv_tokenizer . prepare_translation_batch ( translated_texts , max_length = inv_tokenizer . model_max_length ) . to ( args . device ) ) [EOL] backtranslated_texts = [ inv_tokenizer . decode ( t , skip_special_tokens = True ) for t in backtranslated ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for i , row in enumerate ( backtranslated_texts ) : [EOL] f_out . write ( row . replace ( [string] , [string] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if not ( i == len ( translated_texts ) - [number] and batch_id == len ( batches ) - [number] ) : [EOL] f_out . write ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] from typing import Dict , Any , Set , List , Tuple [EOL] import pathlib [EOL] import typing [EOL] import argparse [EOL] import argparse [EOL] import ast [EOL] import json [EOL] import random [EOL] from pathlib import Path [EOL] [EOL] import numpy as np [EOL] import pandas as pd [EOL] import spacy [EOL] from spacy . gold import GoldParse [EOL] from spacy . util import minibatch [EOL] [EOL] import torch [EOL] from spacy_transformers import TransformersLanguage [EOL] from spacy_transformers . util import PIPES , cyclic_triangular_rate [EOL] [EOL] [EOL] def is_transformer ( nlp ) : [EOL] [docstring] [EOL] return isinstance ( nlp , TransformersLanguage ) [EOL] [EOL] [EOL] def read_unique_labels ( labels_path ) : [EOL] [docstring] [EOL] return labels_path . read_text ( encoding = [string] ) . split ( [string] ) [EOL] [EOL] [EOL] def read_data ( file_path , has_labels ) : [EOL] [docstring] [EOL] df = pd . read_csv ( file_path , sep = [string] , dtype = [string] , keep_default_na = False ) [EOL] X = df [ [string] ] . tolist ( ) [EOL] y = None [EOL] if has_labels : [EOL] [comment] [EOL] [comment] [EOL] y = df [ [string] ] . apply ( lambda labels : set ( ast . literal_eval ( labels ) ) ) . tolist ( ) [EOL] return X , y [EOL] [EOL] [EOL] def spacy_format_labels ( ys , labels ) : [EOL] [docstring] [EOL] return [ { l : int ( l in y ) for l in labels } for y in ys ] [EOL] [EOL] [EOL] def evaluate ( tokenizer , nlp , valid_data , labels ) : [EOL] [docstring] [EOL] texts , cats = zip ( * valid_data ) [EOL] [EOL] golds = [ ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] scores = np . zeros ( ( len ( cats ) , len ( labels ) ) , dtype = [string] ) [EOL] if is_transformer ( nlp ) : [EOL] textcat = nlp . get_pipe ( PIPES . textcat ) [EOL] else : [EOL] textcat = nlp . get_pipe ( [string] ) [EOL] scores = textcat . model . ops . asarray ( scores ) [EOL] [EOL] num_correct = [number] [EOL] for i , doc in enumerate ( nlp . pipe ( texts ) ) : [EOL] gold_cats = cats [ i ] [ [string] ] [EOL] for j , ( label , score ) in enumerate ( doc . cats . items ( ) ) : [EOL] if label not in gold_cats : [EOL] raise ValueError ( f" [string] { label }" ) [EOL] [EOL] scores [ i , j ] = score [EOL] [EOL] doc_prediction = score > [number] [EOL] if doc_prediction == bool ( gold_cats [ label ] ) : [EOL] num_correct += [number] [EOL] [EOL] golds . append ( GoldParse ( doc , cats = gold_cats ) ) [EOL] [EOL] accuracy = num_correct / ( ( len ( texts ) * len ( labels ) ) + [number] ) [EOL] loss , _ = textcat . get_loss ( texts , golds , scores ) [EOL] [EOL] return accuracy , loss [EOL] [EOL] [EOL] def train ( * , input_dir , output_dir , nlp , architecture , train_batch_size , num_train_epochs , labels , dropout , disabled_components , multilabel , ) : [EOL] [docstring] [EOL] if is_transformer ( nlp ) : [EOL] textcat_pipe_name = PIPES . textcat [EOL] textcat = nlp . create_pipe ( textcat_pipe_name , config = { [string] : [string] , [string] : not multilabel , [string] : nlp . get_pipe ( PIPES . tok2vec ) . model . nO , } , ) [EOL] else : [EOL] textcat_pipe_name = [string] [EOL] textcat = nlp . create_pipe ( textcat_pipe_name , config = { [string] : not multilabel , [string] : architecture } , ) [EOL] nlp . add_pipe ( textcat , last = True ) [EOL] [EOL] for label in labels : [EOL] textcat . add_label ( label ) [EOL] [EOL] X_train , y_train = read_data ( input_dir / [string] , True ) [EOL] X_valid , y_valid = read_data ( input_dir / [string] , True ) [EOL] [EOL] train_labels = spacy_format_labels ( y_train , labels ) [EOL] valid_labels = spacy_format_labels ( y_valid , labels ) [EOL] [EOL] train_data = list ( zip ( X_train , [ { [string] : cats } for cats in train_labels ] ) ) [EOL] valid_data = list ( zip ( X_valid , [ { [string] : cats } for cats in valid_labels ] ) ) [EOL] [EOL] with nlp . disable_pipes ( * disabled_components ) : [EOL] if is_transformer ( nlp ) : [EOL] optimizer = nlp . resume_training ( ) [EOL] optimizer . alpha = [number] [EOL] optimizer . trf_weight_decay = [number] [EOL] optimizer . L2 = [number] [EOL] learn_rate = [number] [EOL] learn_rates = cyclic_triangular_rate ( learn_rate / [number] , learn_rate * [number] , [number] * len ( train_data ) // train_batch_size ) [EOL] else : [EOL] optimizer = nlp . begin_training ( ) [EOL] for i in range ( num_train_epochs ) : [EOL] losses = { } [EOL] random . shuffle ( train_data ) [EOL] batches = minibatch ( train_data , train_batch_size ) [EOL] for batch in batches : [EOL] texts , annotations = zip ( * batch ) [EOL] if is_transformer ( nlp ) : [EOL] optimizer . trf_lr = next ( learn_rates ) [EOL] [EOL] nlp . update ( texts , annotations , sgd = optimizer , drop = dropout , losses = losses ) [EOL] [EOL] with textcat . model . use_params ( optimizer . averages ) : [EOL] accuracy , valid_loss = evaluate ( nlp . tokenizer , nlp , valid_data , labels ) [EOL] train_loss = losses [ textcat_pipe_name ] [EOL] print ( f" [string] { i } [string] { train_loss : [string] } [string] { valid_loss : [string] } [string] { accuracy : [string] }" ) [EOL] [EOL] checkpoint_dir = output_dir / [string] [EOL] checkpoint_dir . mkdir ( exist_ok = True , parents = True ) [EOL] [EOL] with nlp . use_params ( optimizer . averages ) : [EOL] nlp . to_disk ( checkpoint_dir ) [EOL] [EOL] metrics = { [string] : accuracy , [string] : losses [ textcat_pipe_name ] / len ( X_train ) , [string] : valid_loss / len ( X_valid ) , } [EOL] [EOL] with open ( output_dir / [string] , [string] ) as f : [EOL] json . dump ( metrics , f ) [EOL] [EOL] [EOL] def predict ( * , input_dir , output_dir , nlp , labels , disabled_components ) : [EOL] [docstring] [EOL] X_test , _ = read_data ( input_dir / [string] , False ) [EOL] [EOL] pred_probas = [ ] [EOL] with nlp . disable_pipes ( * disabled_components ) : [EOL] for doc in nlp . pipe ( X_test ) : [EOL] pred_probas . append ( { label : doc . cats . get ( label , [number] ) for label in labels } ) [EOL] df = pd . DataFrame ( pred_probas ) [EOL] df . to_csv ( output_dir / [string] , index = False , sep = [string] ) [EOL] [EOL] [EOL] def embed ( * , input_dir , output_dir , nlp , embed_pooling , disabled_components ) : [EOL] [docstring] [EOL] embeddings = [ ] [EOL] X_embed , _ = read_data ( input_dir / [string] , False ) [EOL] [EOL] with nlp . disable_pipes ( * disabled_components ) : [EOL] with open ( output_dir / [string] , [string] ) as f : [EOL] for doc in nlp . pipe ( X_embed ) : [EOL] if embed_pooling == [string] : [EOL] row_json = { [string] : doc . vector . tolist ( ) } [EOL] elif embed_pooling == [string] : [EOL] embeddings = [ ] [EOL] tokens = [ ] [EOL] for tok in doc : [EOL] embeddings . append ( tok . vector . tolist ( ) ) [EOL] tokens . append ( tok . text ) [EOL] row_json = { [string] : embeddings , [string] : tokens } [EOL] else : [EOL] raise ValueError ( f" [string] { embed_pooling }" ) [EOL] [EOL] f . write ( f"{ json . dumps ( row_json ) } [string] " ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] [EOL] parser . add_argument ( [string] , choices = [ [string] , [string] , [string] ] , help = [string] [string] [string] [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] [string] [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] , ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] [string] , ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] [string] [string] [string] , ) [EOL] parser . add_argument ( [string] , type = int , default = [number] , help = [string] , ) [EOL] parser . add_argument ( [string] , type = int , default = [number] , help = [string] , ) [EOL] parser . add_argument ( [string] , type = int , default = [number] , help = [string] , ) [EOL] parser . add_argument ( [string] , type = float , default = [number] , help = [string] ) [EOL] parser . add_argument ( [string] , choices = [ [string] , [string] ] , default = [string] , help = [string] [string] , ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] input_dir = Path ( args . input_dir ) [EOL] output_dir = Path ( args . output_dir ) [EOL] [EOL] using_gpu = spacy . prefer_gpu ( ) [EOL] if using_gpu : [EOL] torch . set_default_tensor_type ( [string] ) [EOL] [EOL] device = [string] if using_gpu else [string] [EOL] print ( f" [string] { device }" ) [EOL] [EOL] print ( [string] ) [EOL] print ( f" [string] { args . model }" ) [EOL] [EOL] nlp = spacy . load ( args . model ) [EOL] if not is_transformer ( nlp ) : [EOL] print ( f" [string] { args . architecture }" ) [EOL] [EOL] model_name = nlp . meta . get ( [string] , [string] ) [EOL] [EOL] print ( f" [string] { model_name } [string] " ) [EOL] [EOL] disabled_components = set ( ) [EOL] [EOL] if args . mode == [string] : [EOL] [comment] [EOL] for textcat_pipe in ( [string] , [string] ) : [EOL] if nlp . has_pipe ( textcat_pipe ) : [EOL] disabled_components . add ( textcat_pipe ) [EOL] [EOL] if model_name . endswith ( [string] ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] pass [EOL] else : [EOL] [comment] [EOL] for component in ( [string] , [string] , [string] ) : [EOL] if nlp . has_pipe ( component ) : [EOL] disabled_components . add ( component ) [EOL] [EOL] elif args . mode in ( [string] , [string] ) : [EOL] if args . full_pipeline : [EOL] [comment] [EOL] [comment] [EOL] pass [EOL] else : [EOL] [comment] [EOL] for component in ( [string] , [string] , [string] ) : [EOL] if nlp . has_pipe ( component ) : [EOL] disabled_components . add ( component ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] labels = None [EOL] if args . mode in ( [string] , [string] ) : [EOL] labels_file = input_dir / [string] [EOL] labels = read_unique_labels ( input_dir / [string] ) [EOL] num_labels = len ( labels ) [EOL] print ( f" [string] { num_labels }" ) [EOL] [EOL] if args . mode == [string] : [EOL] train ( input_dir = input_dir , output_dir = output_dir , nlp = nlp , architecture = args . architecture , labels = labels , train_batch_size = args . train_batch_size , num_train_epochs = args . num_train_epochs , dropout = args . dropout , disabled_components = disabled_components , multilabel = args . multilabel , ) [EOL] [EOL] elif args . mode == [string] : [EOL] predict ( input_dir = input_dir , output_dir = output_dir , nlp = nlp , labels = labels , disabled_components = disabled_components , ) [EOL] [EOL] elif args . mode == [string] : [EOL] embed ( input_dir = input_dir , output_dir = output_dir , nlp = nlp , embed_pooling = args . embed_pooling , disabled_components = disabled_components , ) [EOL] [EOL] else : [EOL] raise ValueError ( f" [string] { args . mode }" ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import argparse [EOL] import argparse [EOL] import json [EOL] [EOL] import tensorflow_hub as hub [EOL] [EOL] [EOL] def read_texts ( input_file ) : [EOL] with open ( input_file , [string] , encoding = [string] ) as f : [EOL] return f . readlines ( ) [EOL] [EOL] [EOL] def make_batches ( l , batch_size ) : [EOL] for i in range ( [number] , len ( l ) , batch_size ) : [EOL] yield l [ i : i + batch_size ] [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] [EOL] parser . add_argument ( [string] , required = True , help = [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] , ) [EOL] parser . add_argument ( [string] , required = True , help = [string] , ) [EOL] parser . add_argument ( [string] , default = [number] , type = int , help = [string] , ) [EOL] [EOL] args = parser . parse_args ( ) [EOL] [EOL] embed = hub . load ( args . module_dir ) [EOL] texts = read_texts ( args . input_file ) [EOL] [EOL] with open ( args . output_file , [string] ) as f : [EOL] for batch in make_batches ( texts , args . batch_size ) : [EOL] embeddings = embed ( batch ) . numpy ( ) [EOL] for embedding in embeddings . tolist ( ) : [EOL] json . dump ( embedding , f ) [EOL] f . write ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 $typing.Any$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $argparse.Namespace$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any , List [EOL] import typing [EOL] import gobbli [EOL] import tqdm [EOL] import unicodedata [EOL] [EOL] PAD = [string] [EOL] UNK = [string] [EOL] STA = [string] [EOL] END = [string] [EOL] [EOL] PAD_ID = [number] [EOL] UNK_ID = [number] [EOL] STA_ID = [number] [EOL] END_ID = [number] [EOL] [EOL] class Vocabulary ( object ) : [EOL] INIT_LEN = [number] [EOL] def __init__ ( self , neat = False ) : [EOL] self . neat = neat [EOL] if not neat : [EOL] self . tok2ind = { PAD : PAD_ID , UNK : UNK_ID , STA : STA_ID , END : END_ID } [EOL] self . ind2tok = { PAD_ID : PAD , UNK_ID : UNK , STA_ID : STA , END_ID : END } [EOL] else : [EOL] self . tok2ind = { } [EOL] self . ind2tok = { } [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . tok2ind ) [EOL] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . tok2ind ) [EOL] [EOL] def __contains__ ( self , key ) : [EOL] if type ( key ) == int : [EOL] return key in self . ind2tok [EOL] elif type ( key ) == str : [EOL] return key in self . tok2ind [EOL] [EOL] def __getitem__ ( self , key ) : [EOL] if type ( key ) == int : [EOL] return self . ind2tok . get ( key , - [number] ) if self . neat else self . ind2tok . get ( key , UNK ) [EOL] if type ( key ) == str : [EOL] return self . tok2ind . get ( key , None ) if self . neat else self . tok2ind . get ( key , self . tok2ind . get ( UNK ) ) [EOL] [EOL] def __setitem__ ( self , key , item ) : [EOL] if type ( key ) == int and type ( item ) == str : [EOL] self . ind2tok [ key ] = item [EOL] elif type ( key ) == str and type ( item ) == int : [EOL] self . tok2ind [ key ] = item [EOL] else : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] def add ( self , token ) : [EOL] if token not in self . tok2ind : [EOL] index = len ( self . tok2ind ) [EOL] self . tok2ind [ token ] = index [EOL] self . ind2tok [ index ] = token [EOL] [EOL] def get_vocab_list ( self , with_order = True ) : [EOL] if with_order : [EOL] words = [ self [ k ] for k in range ( [number] , len ( self ) ) ] [EOL] else : [EOL] words = [ k for k in self . tok2ind . keys ( ) if k not in { PAD , UNK , STA , END } ] [EOL] return words [EOL] [EOL] def toidx ( self , tokens ) : [EOL] return [ self [ tok ] for tok in tokens ] [EOL] [EOL] def copy ( self ) : [EOL] [docstring] [EOL] new_vocab = Vocabulary ( self . neat ) [EOL] for w in self : [EOL] new_vocab . add ( w ) [EOL] return new_vocab [EOL] [EOL] def build ( words , neat = False ) : [EOL] vocab = Vocabulary ( neat ) [EOL] for w in words : vocab . add ( w ) [EOL] return vocab [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.data_utils.vocab.Vocabulary$ 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] from sklearn . metrics import matthews_corrcoef [EOL] from sklearn . metrics import accuracy_score , f1_score [EOL] from scipy . stats import pearsonr , spearmanr [EOL] from torch . nn . functional import cross_entropy [EOL] [EOL] def compute_acc ( predicts , labels ) : [EOL] return [number] * accuracy_score ( labels , predicts ) [EOL] [EOL] def compute_f1 ( predicts , labels ) : [EOL] return [number] * f1_score ( labels , predicts ) [EOL] [EOL] def compute_mcc ( predicts , labels ) : [EOL] return [number] * matthews_corrcoef ( labels , predicts ) [EOL] [EOL] def compute_pearson ( predicts , labels ) : [EOL] pcof = pearsonr ( labels , predicts ) [ [number] ] [EOL] return [number] * pcof [EOL] [EOL] def compute_spearman ( predicts , labels ) : [EOL] scof = spearmanr ( labels , predicts ) [ [number] ] [EOL] return [number] * scof [EOL] [EOL] def compute_cross_entropy ( predicts , labels ) : [EOL] return cross_entropy ( predicts , labels ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import logging [EOL] import logging [EOL] from time import gmtime , strftime [EOL] import sys [EOL] [EOL] def create_logger ( name , silent = False , to_disk = False , log_file = None ) : [EOL] [docstring] [EOL] [comment] [EOL] log = logging . getLogger ( name ) [EOL] log . setLevel ( logging . DEBUG ) [EOL] log . propagate = False [EOL] formatter = logging . Formatter ( fmt = [string] , datefmt = [string] ) [EOL] if not silent : [EOL] ch = logging . StreamHandler ( sys . stdout ) [EOL] ch . setLevel ( logging . INFO ) [EOL] ch . setFormatter ( formatter ) [EOL] log . addHandler ( ch ) [EOL] if to_disk : [EOL] log_file = log_file if log_file is not None else strftime ( [string] , gmtime ( ) ) [EOL] fh = logging . FileHandler ( log_file ) [EOL] fh . setLevel ( logging . DEBUG ) [EOL] fh . setFormatter ( formatter ) [EOL] log . addHandler ( fh ) [EOL] return log [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Any , Tuple [EOL] import typing [EOL] import gobbli [EOL] from copy import deepcopy [EOL] import torch [EOL] from torch . nn import Parameter [EOL] from functools import wraps [EOL] [EOL] class EMA : [EOL] def __init__ ( self , gamma , model ) : [EOL] super ( EMA , self ) . __init__ ( ) [EOL] self . gamma = gamma [EOL] self . shadow = { } [EOL] self . model = model [EOL] self . setup ( ) [EOL] [EOL] def setup ( self ) : [EOL] for name , para in self . model . named_parameters ( ) : [EOL] if para . requires_grad : [EOL] self . shadow [ name ] = para . clone ( ) [EOL] def cuda ( self ) : [EOL] for k , v in self . shadow . items ( ) : [EOL] self . shadow [ k ] = v . cuda ( ) [EOL] [EOL] def update ( self ) : [EOL] for name , para in self . model . named_parameters ( ) : [EOL] if para . requires_grad : [EOL] self . shadow [ name ] = ( [number] - self . gamma ) * para + self . gamma * self . shadow [ name ] [EOL] [EOL] def swap_parameters ( self ) : [EOL] for name , para in self . model . named_parameters ( ) : [EOL] if para . requires_grad : [EOL] temp_data = para . data [EOL] para . data = self . shadow [ name ] . data [EOL] self . shadow [ name ] . data = temp_data [EOL] [EOL] def state_dict ( self ) : [EOL] return self . shadow [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def _norm ( p , dim ) : [EOL] [docstring] [EOL] if dim is None : [EOL] return p . norm ( ) [EOL] elif dim == [number] : [EOL] output_size = ( p . size ( [number] ) , ) + ( [number] , ) * ( p . dim ( ) - [number] ) [EOL] return p . contiguous ( ) . view ( p . size ( [number] ) , - [number] ) . norm ( dim = [number] ) . view ( * output_size ) [EOL] elif dim == p . dim ( ) - [number] : [EOL] output_size = ( [number] , ) * ( p . dim ( ) - [number] ) + ( p . size ( - [number] ) , ) [EOL] return p . contiguous ( ) . view ( - [number] , p . size ( - [number] ) ) . norm ( dim = [number] ) . view ( * output_size ) [EOL] else : [EOL] return _norm ( p . transpose ( [number] , dim ) , [number] ) . transpose ( [number] , dim ) [EOL] [EOL] [EOL] def _dummy ( * args , ** kwargs ) : [EOL] [comment] [EOL] return [EOL] [EOL] [EOL] class WeightNorm ( torch . nn . Module ) : [EOL] [EOL] def __init__ ( self , weights , dim ) : [EOL] super ( WeightNorm , self ) . __init__ ( ) [EOL] self . weights = weights [EOL] self . dim = dim [EOL] [EOL] def compute_weight ( self , module , name ) : [EOL] g = getattr ( module , name + [string] ) [EOL] v = getattr ( module , name + [string] ) [EOL] return v * ( g / _norm ( v , self . dim ) ) [EOL] [EOL] @ staticmethod def apply ( module , weights , dim ) : [EOL] [comment] [EOL] [comment] [EOL] if issubclass ( type ( module ) , torch . nn . RNNBase ) : [EOL] module . flatten_parameters = _dummy [EOL] if weights is None : [comment] [EOL] weights = [ w for w in module . _parameters . keys ( ) if [string] in w ] [EOL] fn = WeightNorm ( weights , dim ) [EOL] for name in weights : [EOL] if hasattr ( module , name ) : [EOL] print ( [string] . format ( str ( module ) , name ) ) [EOL] weight = getattr ( module , name ) [EOL] del module . _parameters [ name ] [EOL] module . register_parameter ( name + [string] , Parameter ( _norm ( weight , dim ) . data ) ) [EOL] module . register_parameter ( name + [string] , Parameter ( weight . data ) ) [EOL] setattr ( module , name , fn . compute_weight ( module , name ) ) [EOL] [EOL] module . register_forward_pre_hook ( fn ) [EOL] [EOL] return fn [EOL] [EOL] def remove ( self , module ) : [EOL] for name in self . weights : [EOL] weight = self . compute_weight ( module ) [EOL] delattr ( module , name ) [EOL] del module . _parameters [ name + [string] ] [EOL] del module . _parameters [ name + [string] ] [EOL] module . register_parameter ( name , Parameter ( weight . data ) ) [EOL] [EOL] def __call__ ( self , module , inputs ) : [EOL] for name in self . weights : [EOL] setattr ( module , name , self . compute_weight ( module , name ) ) [EOL] [EOL] [EOL] def weight_norm ( module , weights = None , dim = [number] ) : [EOL] WeightNorm . apply ( module , weights , dim ) [EOL] return module [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.module.my_optim.WeightNorm$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.module.my_optim.WeightNorm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gobbli.model.mtdnn.src.module.my_optim.WeightNorm$ 0 0 0 0 $gobbli.model.mtdnn.src.module.my_optim.WeightNorm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Dict , Any , List [EOL] import typing [EOL] import math [EOL] import torch [EOL] from torch . optim import Optimizer [EOL] from torch . nn . utils import clip_grad_norm_ [EOL] from pytorch_pretrained_bert . optimization import warmup_constant , warmup_cosine , warmup_linear [EOL] [EOL] def warmup_linear_xdl ( x , warmup = [number] ) : [EOL] if x < warmup : [EOL] return x / warmup [EOL] return ( [number] - x ) / ( [number] - warmup ) [EOL] [EOL] def schedule_func ( sch ) : [EOL] try : [EOL] f = eval ( sch ) [EOL] except : [EOL] f = warmup_linear [EOL] return f [EOL] [EOL] class Adamax ( Optimizer ) : [EOL] [docstring] [EOL] def __init__ ( self , params , lr , warmup = - [number] , t_total = - [number] , schedule = [string] , betas = ( [number] , [number] ) , eps = [number] , weight_decay_rate = [number] , max_grad_norm = [number] ) : [EOL] if not lr >= [number] : [EOL] raise ValueError ( [string] . format ( lr ) ) [EOL] if not [number] <= warmup < [number] and not warmup == - [number] : [EOL] raise ValueError ( [string] . format ( warmup ) ) [EOL] if not [number] <= eps : [EOL] raise ValueError ( [string] . format ( eps ) ) [EOL] if not [number] <= betas [ [number] ] < [number] : [EOL] raise ValueError ( [string] . format ( betas [ [number] ] ) ) [EOL] if not [number] <= betas [ [number] ] < [number] : [EOL] raise ValueError ( [string] . format ( betas [ [number] ] ) ) [EOL] defaults = dict ( lr = lr , schedule = schedule , warmup = warmup , t_total = t_total , betas = betas , eps = eps , weight_decay_rate = weight_decay_rate , max_grad_norm = max_grad_norm ) [EOL] super ( Adamax , self ) . __init__ ( params , defaults ) [EOL] [EOL] def get_lr ( self ) : [EOL] lr = [ ] [EOL] for group in self . param_groups : [EOL] for p in group [ [string] ] : [EOL] state = self . state [ p ] [EOL] if len ( state ) == [number] : [EOL] return [ [number] ] [EOL] if group [ [string] ] != - [number] : [EOL] schedule_fct = schedule_func ( group [ [string] ] ) [EOL] lr_scheduled = group [ [string] ] * schedule_fct ( state [ [string] ] / group [ [string] ] , group [ [string] ] ) [EOL] else : [EOL] lr_scheduled = group [ [string] ] [EOL] lr . append ( lr_scheduled ) [EOL] return lr [EOL] [EOL] def to ( self , device ) : [EOL] [docstring] [EOL] for state in self . state . values ( ) : [EOL] state [ [string] ] . to ( device ) [EOL] state [ [string] ] . to ( device ) [EOL] [EOL] def initialize_step ( self , initial_step ) : [EOL] [docstring] [EOL] for group in self . param_groups : [EOL] for p in group [ [string] ] : [EOL] state = self . state [ p ] [EOL] [comment] [EOL] state [ [string] ] = initial_step [EOL] [comment] [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] [comment] [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] [EOL] def step ( self , closure = None ) : [EOL] loss = None [EOL] if closure is not None : [EOL] loss = closure ( ) [EOL] [EOL] for group in self . param_groups : [EOL] for p in group [ [string] ] : [EOL] if p . grad is None : [EOL] continue [EOL] grad = p . grad . data [EOL] if grad . is_sparse : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] state = self . state [ p ] [EOL] [EOL] [comment] [EOL] if len ( state ) == [number] : [EOL] state [ [string] ] = [number] [EOL] [comment] [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] [EOL] exp_avg , exp_inf = state [ [string] ] , state [ [string] ] [EOL] beta1 , beta2 = group [ [string] ] [EOL] eps = group [ [string] ] [EOL] [comment] [EOL] if group [ [string] ] > [number] : [EOL] clip_grad_norm_ ( p , group [ [string] ] ) [EOL] [EOL] [comment] [EOL] exp_avg . mul_ ( beta1 ) . add_ ( [number] - beta1 , grad ) [EOL] [comment] [EOL] norm_buf = torch . cat ( [ exp_inf . mul_ ( beta2 ) . unsqueeze ( [number] ) , grad . abs ( ) . add_ ( eps ) . unsqueeze_ ( [number] ) ] , [number] ) [EOL] torch . max ( norm_buf , [number] , keepdim = False , out = ( exp_inf , exp_inf . new ( ) . long ( ) ) ) [EOL] update = exp_avg / ( exp_inf + eps ) [EOL] [EOL] if group [ [string] ] > [number] : [EOL] update += group [ [string] ] * p . data [EOL] [EOL] if group [ [string] ] != - [number] : [EOL] schedule_fct = schedule_func ( group [ [string] ] ) [EOL] lr_scheduled = group [ [string] ] * schedule_fct ( state [ [string] ] / group [ [string] ] , group [ [string] ] ) [EOL] else : [EOL] lr_scheduled = group [ [string] ] [EOL] [EOL] update_with_lr = lr_scheduled * update [EOL] p . data . add_ ( - update_with_lr ) [EOL] state [ [string] ] += [number] [EOL] [EOL] return loss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] class LayerNorm ( nn . Module ) : [EOL] [comment] [EOL] [comment] [EOL] def __init__ ( self , hidden_size , eps = [number] ) : [EOL] super ( LayerNorm , self ) . __init__ ( ) [EOL] self . alpha = Parameter ( torch . ones ( [number] , [number] , hidden_size ) ) [comment] [EOL] self . beta = Parameter ( torch . zeros ( [number] , [number] , hidden_size ) ) [comment] [EOL] self . eps = eps [EOL] [EOL] def forward ( self , x ) : [EOL] [docstring] [EOL] mu = torch . mean ( x , [number] , keepdim = True ) . expand_as ( x ) [EOL] sigma = torch . std ( x , [number] , keepdim = True ) . expand_as ( x ) [EOL] return ( x - mu ) / ( sigma + self . eps ) * self . alpha . expand_as ( x ) + self . beta . expand_as ( x ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any [EOL] import typing [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] from torch . autograd import Variable [EOL] [EOL] class DropoutWrapper ( nn . Module ) : [EOL] [docstring] [EOL] def __init__ ( self , dropout_p = [number] , enable_vbp = True ) : [EOL] super ( DropoutWrapper , self ) . __init__ ( ) [EOL] [docstring] [EOL] self . enable_variational_dropout = enable_vbp [EOL] self . dropout_p = dropout_p [EOL] [EOL] def forward ( self , x ) : [EOL] [docstring] [EOL] if self . training == False or self . dropout_p == [number] : [EOL] return x [EOL] [EOL] if len ( x . size ( ) ) == [number] : [EOL] mask = Variable ( [number] / ( [number] - self . dropout_p ) * torch . bernoulli ( ( [number] - self . dropout_p ) * ( x . data . new ( x . size ( [number] ) , x . size ( [number] ) ) . zero_ ( ) + [number] ) ) , requires_grad = False ) [EOL] return mask . unsqueeze ( [number] ) . expand_as ( x ) * x [EOL] else : [EOL] return F . dropout ( x , p = self . dropout_p , training = self . training ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Dict , Any [EOL] import typing [EOL] import argparse [EOL] import os [EOL] import argparse [EOL] from datetime import datetime [EOL] import torch [EOL] [EOL] def predict_config ( parser ) : [EOL] parser . add_argument ( [string] , default = [string] ) [EOL] parser . add_argument ( [string] , type = str , default = [string] ) [EOL] return parser [EOL] [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser = predict_config ( parser ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] def main ( ) : [EOL] opt = vars ( args ) [EOL] fout = args . fout [EOL] model_path = args . checkpoint [EOL] state_dict = None [EOL] if os . path . exists ( model_path ) : [EOL] state_dict = torch . load ( model_path ) [EOL] config = state_dict [ [string] ] [EOL] opt . update ( config ) [EOL] if state_dict [ [string] ] [ [string] ] > [number] : [EOL] new_state_dict = { [string] : state_dict [ [string] ] , [string] : state_dict [ [string] ] } [EOL] else : [EOL] new_state_dict = { [string] : state_dict [ [string] ] , [string] : state_dict [ [string] ] } [EOL] old_state_dict = { } [EOL] for key , val in new_state_dict [ [string] ] . items ( ) : [EOL] prefix = key . split ( [string] ) [ [number] ] [EOL] if prefix == [string] : [EOL] continue [EOL] old_state_dict [ key ] = val [EOL] my_config = { } [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] my_config [ [string] ] = config [ [string] ] [EOL] state_dict = { [string] : old_state_dict , [string] : my_config } [EOL] torch . save ( state_dict , args . fout ) [EOL] [EOL] if __name__ == [string] : [EOL] main ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse.ArgumentParser$ 0 0 $typing.Any$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Literal , DefaultDict , Union , Any , Dict , List , Optional , Tuple [EOL] import typing [EOL] import gobbli [EOL] import pandas [EOL] import numpy [EOL] import typing_extensions [EOL] import builtins [EOL] import copy [EOL] from collections import defaultdict [EOL] from pathlib import Path [EOL] from typing import Any , Dict , List , Optional , Tuple , Union [EOL] [EOL] import altair as alt [EOL] import click [EOL] import hdbscan [EOL] import numpy as np [EOL] import pandas as pd [EOL] import streamlit as st [EOL] from sklearn . cluster import KMeans [EOL] from sklearn . neighbors import VALID_METRICS as SKLEARN_DISTANCE_METRICS [EOL] from umap import UMAP [EOL] from umap . distances import named_distances [EOL] [EOL] from gobbli . interactive . util import ( get_label_indices , load_data , safe_sample , st_sample_data , st_select_model_checkpoint , st_select_untrained_model , ) [EOL] from gobbli . io import EmbedInput , EmbedPooling [EOL] from gobbli . model import FastText [EOL] from gobbli . model . mixin import EmbedMixin [EOL] from gobbli . util import TokenizeMethod , is_multilabel , tokenize , truncate_text [EOL] [EOL] DEFAULT_EMBED_BATCH_SIZE = EmbedInput . embed_batch_size [EOL] [EOL] [comment] [EOL] @ st . cache ( allow_output_mutation = True ) def get_tokens ( texts , tokenize_method , vocab_size ) : [EOL] try : [EOL] return tokenize ( tokenize_method , texts , vocab_size = vocab_size ) [EOL] except RuntimeError as e : [EOL] str_e = str ( e ) [EOL] if [string] in str_e and [string] in str_e : [EOL] st . error ( [string] [string] ) [EOL] return [EOL] else : [EOL] raise [EOL] [EOL] [EOL] def _show_example_documents ( texts , labels , truncate_len , ) : [EOL] df = pd . DataFrame ( { [string] : [ truncate_text ( t , truncate_len ) for t in texts ] } ) [EOL] if labels is not None : [EOL] if is_multilabel ( labels ) : [EOL] label_col = [string] [EOL] else : [EOL] label_col = [string] [EOL] df [ label_col ] = labels [EOL] st . table ( df ) [EOL] [EOL] [EOL] def show_example_documents ( texts , labels , filter_label , example_truncate_len , example_num_docs , ) : [EOL] st . header ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if filter_label is not None : [EOL] st . subheader ( f" [string] { filter_label }" ) [EOL] example_labels = None [EOL] else : [EOL] example_labels = labels [EOL] [EOL] example_indices = safe_sample ( range ( len ( texts ) ) , example_num_docs ) [EOL] _show_example_documents ( [ texts [ i ] for i in example_indices ] , [ example_labels [ i ] for i in example_indices ] [EOL] if example_labels is not None [EOL] else None , example_truncate_len , ) [EOL] [EOL] [EOL] def get_document_lengths ( tokens ) : [EOL] return [ len ( d ) for d in tokens ] [EOL] [EOL] [EOL] def _collect_label_counts ( labels ) : [EOL] if is_multilabel ( labels ) : [EOL] counts = defaultdict ( int ) [EOL] for row_labels in labels : [EOL] for label in row_labels : [EOL] counts [ label ] += [number] [EOL] label_counts = pd . Series ( counts ) / len ( labels ) [EOL] else : [EOL] label_counts = pd . Series ( labels ) . value_counts ( normalize = True ) [EOL] label_counts = label_counts . to_frame ( name = [string] ) [EOL] label_counts . index . name = [string] [EOL] return label_counts . reset_index ( ) [EOL] [EOL] [EOL] def show_label_distribution ( sample_labels , all_labels = None , ) : [EOL] if sample_labels is not None : [EOL] st . header ( [string] ) [EOL] label_counts = _collect_label_counts ( sample_labels ) [EOL] [EOL] if all_labels is None : [EOL] label_chart = ( alt . Chart ( label_counts , height = [number] , width = [number] ) . mark_bar ( ) . encode ( alt . X ( [string] , type = [string] ) , alt . Y ( [string] , type = [string] ) , ) ) [EOL] else : [EOL] label_counts [ [string] ] = [string] [EOL] all_label_counts = _collect_label_counts ( all_labels ) [EOL] all_label_counts [ [string] ] = [string] [EOL] label_counts = pd . concat ( [ label_counts , all_label_counts ] ) [EOL] [EOL] label_chart = ( alt . Chart ( label_counts , width = [number] ) . mark_bar ( ) . encode ( alt . X ( [string] , type = [string] , title = None , sort = [ [string] , [string] ] , ) , alt . Y ( [string] , type = [string] ) , alt . Column ( [string] , type = [string] , header = alt . Header ( labelAngle = [number] ) ) , alt . Color ( [string] , type = [string] , legend = None ) , ) ) [EOL] [EOL] st . altair_chart ( label_chart ) [EOL] [EOL] [EOL] def show_document_length_distribution ( tokens ) : [EOL] st . header ( [string] ) [EOL] document_lengths = get_document_lengths ( tokens ) [EOL] doc_lengths = pd . DataFrame ( { [string] : document_lengths } ) [EOL] doc_length_chart = ( alt . Chart ( doc_lengths , height = [number] , width = [number] ) . mark_bar ( ) . encode ( alt . X ( [string] , bin = alt . Bin ( maxbins = [number] ) ) , alt . Y ( [string] , type = [string] ) , ) ) [EOL] [EOL] st . altair_chart ( doc_length_chart ) [EOL] [EOL] [EOL] @ st . cache ( show_spinner = True ) def get_topics ( tokens , num_topics = [number] , train_chunksize = [number] , train_passes = [number] , train_iterations = [number] , do_bigrams = True , bigram_min_count = [number] , min_frequency = [number] , max_proportion = [number] , ) : [EOL] [docstring] [EOL] try : [EOL] import gensim [EOL] except ImportError : [EOL] raise ImportError ( [string] ) [EOL] [EOL] [comment] [EOL] gensim_tokens = copy . deepcopy ( tokens ) [EOL] [EOL] if do_bigrams : [EOL] bigram = gensim . models . Phrases ( gensim_tokens , min_count = bigram_min_count ) [EOL] for ndx in range ( len ( gensim_tokens ) ) : [EOL] for token in bigram [ gensim_tokens [ ndx ] ] : [EOL] if [string] in token : [EOL] [comment] [EOL] gensim_tokens [ ndx ] . append ( token ) [EOL] [EOL] dictionary = gensim . corpora . Dictionary ( tokens ) [EOL] dictionary . filter_extremes ( no_below = min_frequency , no_above = max_proportion ) [EOL] corpus = [ dictionary . doc2bow ( doc ) for doc in gensim_tokens ] [EOL] [EOL] _ = dictionary [ [number] ] [comment] [EOL] id2word = dictionary . id2token [EOL] [EOL] model = gensim . models . LdaModel ( corpus = corpus , id2word = id2word , chunksize = train_chunksize , alpha = [string] , eta = [string] , iterations = train_iterations , num_topics = num_topics , passes = train_passes , eval_every = None , ) [EOL] [EOL] topic_assignments = [ ] [EOL] for doc_bow in corpus : [EOL] topic_probs = model . get_document_topics ( doc_bow ) [EOL] assigned_topic , _ = max ( topic_probs , key = lambda t : t [ [number] ] ) [EOL] topic_assignments . append ( assigned_topic ) [EOL] [EOL] return model . top_topics ( corpus ) , topic_assignments [EOL] [EOL] [EOL] def corr_df_to_heatmap_df ( corr_df , index_col_name , columns_col_name ) : [EOL] [docstring] [EOL] heatmap_df = corr_df . copy ( ) [EOL] heatmap_df . index . name = index_col_name [EOL] heatmap_df = heatmap_df . reset_index ( ) [EOL] return heatmap_df . melt ( id_vars = index_col_name , var_name = columns_col_name , value_name = [string] ) [EOL] [EOL] [EOL] def st_heatmap ( heatmap_df , x_col_name , y_col_name , color_col_name ) : [EOL] heatmap = ( alt . Chart ( heatmap_df , height = [number] , width = [number] ) . mark_rect ( ) . encode ( alt . X ( x_col_name ) , alt . Y ( y_col_name ) , alt . Color ( color_col_name ) ) ) [EOL] st . altair_chart ( heatmap ) [EOL] [EOL] [EOL] def show_topic_model ( tokens , labels , filter_label , ** model_kwargs , ) : [EOL] topics , topic_assignments = get_topics ( tokens , ** model_kwargs ) [EOL] [EOL] if filter_label is not None : [EOL] st . subheader ( f" [string] { filter_label }" ) [EOL] [EOL] topic_df_data = [ ] [EOL] for i , ( topic , coherence ) in enumerate ( topics ) : [EOL] topic_data = { [string] : coherence } [EOL] [EOL] for j , ( probability , word ) in enumerate ( topic ) : [EOL] topic_data [ j + [number] ] = f"{ word } [string] { probability : [string] } [string] " [EOL] [EOL] topic_df_data . append ( topic_data ) [EOL] [EOL] topic_df = pd . DataFrame ( topic_df_data ) . T [EOL] topic_df . columns = [ f" [string] { i + [number] }" for i in range ( len ( topic_df . columns ) ) ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] st . dataframe ( topic_df , height = [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] topic_onehot = pd . get_dummies ( [ str ( t ) for t in topic_assignments ] ) [EOL] label_onehot = pd . get_dummies ( labels ) [EOL] heatmap_data = pd . concat ( [ topic_onehot , label_onehot ] , axis = [number] , ) [EOL] heatmap_corr = heatmap_data . corr ( ) [EOL] [EOL] [comment] [EOL] all_topics = topic_onehot . columns [EOL] topic_topic_matrix = heatmap_corr . loc [ all_topics , all_topics ] [EOL] [EOL] st . subheader ( [string] ) [EOL] st_heatmap ( corr_df_to_heatmap_df ( topic_topic_matrix , [string] , [string] ) , [string] , [string] , [string] , ) [EOL] [EOL] if labels is not None : [EOL] [comment] [EOL] all_labels = label_onehot . columns [EOL] topic_label_matrix = heatmap_corr . loc [ all_topics , all_labels ] [EOL] [EOL] st . subheader ( [string] ) [EOL] st_heatmap ( corr_df_to_heatmap_df ( topic_label_matrix , [string] , [string] ) , [string] , [string] , [string] , ) [EOL] [EOL] [EOL] @ st . cache ( show_spinner = True ) def get_embeddings ( model_cls , model_kwargs , texts , checkpoint_meta = None , batch_size = DEFAULT_EMBED_BATCH_SIZE , pooling = EmbedPooling . MEAN , ) : [EOL] [docstring] [EOL] embed_input = EmbedInput ( X = texts , checkpoint = None [EOL] if checkpoint_meta is None [EOL] else Path ( checkpoint_meta [ [string] ] ) , embed_batch_size = batch_size , pooling = pooling , ) [EOL] model = model_cls ( ** model_kwargs ) [EOL] model . build ( ) [EOL] embed_output = model . embed ( embed_input ) [EOL] if pooling == EmbedPooling . NONE : [EOL] return embed_output . X_embedded , embed_output . embed_tokens [EOL] else : [EOL] return embed_output . X_embedded , None [EOL] [EOL] [EOL] def show_embeddings ( model_cls , model_kwargs , texts , labels , checkpoint_meta = None , batch_size = DEFAULT_EMBED_BATCH_SIZE , umap_seed = [number] , umap_n_neighbors = [number] , umap_metric = [string] , umap_min_dist = [number] , cluster_when = [string] , clusterer = None , show_vocab_overlap = False , ) : [EOL] if cluster_when not in ( [string] , [string] ) : [EOL] raise ValueError ( f" [string] { cluster_when } [string] " ) [EOL] [EOL] embeddings , _ = get_embeddings ( model_cls , model_kwargs , texts , checkpoint_meta = checkpoint_meta , batch_size = batch_size , ) [EOL] X_embedded = pd . DataFrame ( embeddings ) [EOL] [EOL] umap = UMAP ( n_neighbors = umap_n_neighbors , metric = umap_metric , min_dist = umap_min_dist , random_state = umap_seed , ) [EOL] [EOL] clusters = None [EOL] if clusterer is not None and cluster_when == [string] : [EOL] clusterer . fit ( X_embedded ) [EOL] clusters = clusterer . labels_ [EOL] [EOL] umap_data = umap . fit_transform ( X_embedded ) [EOL] [EOL] if clusterer is not None and cluster_when == [string] : [EOL] clusterer . fit ( umap_data ) [EOL] clusters = clusterer . labels_ [EOL] [EOL] umap_df = pd . DataFrame ( umap_data , columns = [ [string] , [string] ] ) [EOL] tooltip_attrs = [ [string] ] [EOL] [EOL] dataset_is_multilabel = labels is not None and is_multilabel ( labels ) [EOL] label_col_name = [string] if dataset_is_multilabel else [string] [EOL] [EOL] if labels is not None : [EOL] tooltip_attrs . append ( label_col_name ) [EOL] umap_df [ label_col_name ] = labels [EOL] [EOL] if clusters is not None : [EOL] color_attr = [string] [EOL] tooltip_attrs . append ( [string] ) [EOL] umap_df [ [string] ] = clusters [EOL] umap_df [ [string] ] = umap_df [ [string] ] . astype ( str ) [EOL] elif labels is not None and not dataset_is_multilabel : [EOL] [comment] [EOL] color_attr = label_col_name [EOL] else : [EOL] color_attr = None [EOL] [EOL] [comment] [EOL] [comment] [EOL] umap_df [ [string] ] = texts [EOL] [EOL] umap_chart = ( alt . Chart ( umap_df , height = [number] , width = [number] ) . mark_circle ( size = [number] ) . encode ( alt . X ( [string] , scale = alt . Scale ( zero = False ) , axis = None ) , alt . Y ( [string] , scale = alt . Scale ( zero = False ) , axis = None ) , tooltip = alt . Tooltip ( tooltip_attrs ) , ) ) [EOL] if color_attr is not None : [EOL] umap_chart = umap_chart . encode ( alt . Color ( color_attr ) ) [EOL] [EOL] st . altair_chart ( umap_chart ) [EOL] [EOL] if show_vocab_overlap : [EOL] missing_token_counts = defaultdict ( int ) [EOL] oov_count = [number] [EOL] total_count = [number] [EOL] try : [EOL] embeddings , embed_tokens = get_embeddings ( model_cls , model_kwargs , texts , checkpoint_meta = checkpoint_meta , batch_size = batch_size , pooling = EmbedPooling . NONE , ) [EOL] except ValueError : [EOL] st . error ( [string] [string] ) [EOL] return [EOL] [EOL] for doc_embedding , doc_tokens in zip ( embeddings , embed_tokens ) : [EOL] oov_embeddings = np . abs ( doc_embedding ) . sum ( axis = [number] ) == [number] [EOL] oov_count += oov_embeddings . sum ( ) [EOL] total_count += len ( doc_tokens ) [EOL] for oov_token_ndx in np . argwhere ( oov_embeddings ) . ravel ( ) : [EOL] missing_token_counts [ doc_tokens [ oov_token_ndx ] ] += [number] [EOL] [EOL] st . subheader ( [string] ) [EOL] num_oov_tokens_show = [number] [EOL] st . markdown ( f""" [string] { total_count : [string] } [string] { oov_count : [string] } [string] { ( oov_count / total_count ) * [number] : [string] } [string] """ ) [EOL] if oov_count > [number] : [EOL] st . markdown ( f""" [string] { num_oov_tokens_show } [string] """ ) [EOL] oov_df = pd . DataFrame . from_dict ( { tok : count for tok , count in sorted ( missing_token_counts . items ( ) , key = lambda kv : - kv [ [number] ] ) } , orient = [string] , ) . tail ( num_oov_tokens_show ) [EOL] oov_df . columns = [ [string] ] [EOL] st . dataframe ( oov_df ) [EOL] [EOL] [EOL] @ click . command ( ) @ click . argument ( [string] , type = str ) @ click . option ( [string] , type = int , help = [string] , default = - [number] , show_default = True , ) @ click . option ( [string] , type = str , help = [string] [string] , default = None , ) @ click . option ( [string] , default = False , help = [string] [string] [string] , ) @ click . option ( [string] , default = [string] , help = [string] [string] , show_default = True , ) @ click . option ( [string] , default = False , help = [string] , ) @ click . option ( [string] , default = [string] , help = [string] [string] , show_default = True , ) def run ( data , n_rows , model_data_dir , multilabel , multilabel_sep , use_gpu , nvidia_visible_devices , ) : [EOL] st . title ( f" [string] { data }" ) [EOL] texts , labels = load_data ( data , multilabel , None if n_rows == - [number] else n_rows , multilabel_sep = multilabel_sep , ) [EOL] if labels is not None : [EOL] label_indices = get_label_indices ( labels ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] [EOL] filter_label = None [EOL] if labels is not None and st . sidebar . checkbox ( [string] , key = [string] ) : [EOL] filter_label = st . sidebar . selectbox ( [string] , list ( sorted ( label_indices . keys ( ) ) ) ) [EOL] [EOL] if filter_label is None : [EOL] filtered_texts = texts [EOL] filtered_labels = labels [EOL] else : [EOL] filtered_texts = [ texts [ i ] for i in label_indices [ filter_label ] ] [EOL] filtered_labels = [ labels [ i ] for i in label_indices [ filter_label ] ] [EOL] [EOL] sampled_texts , sampled_labels = st_sample_data ( filtered_texts , filtered_labels ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] example_truncate_len = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] example_num_docs = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] show_full_label_distribution = st . sidebar . checkbox ( [string] ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] tokenize_method = TokenizeMethod [ st . sidebar . selectbox ( [string] , tuple ( tm . name for tm in TokenizeMethod ) ) ] [EOL] vocab_size_input = st . sidebar . empty ( ) [EOL] vocab_size = None [EOL] if tokenize_method == TokenizeMethod . SENTENCEPIECE : [EOL] vocab_size = vocab_size_input . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] run_topic_model = False [EOL] if st . sidebar . checkbox ( [string] ) : [EOL] run_topic_model = st . sidebar . button ( [string] ) [EOL] num_topics = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] train_chunksize = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] train_passes = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] train_iterations = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] do_bigrams = st . sidebar . checkbox ( [string] , value = True ) [EOL] if do_bigrams : [EOL] bigram_min_count = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] min_frequency = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] max_proportion = st . sidebar . number_input ( [string] , min_value = [number] , max_value = [number] , value = [number] ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] run_embeddings = False [EOL] embeddings_error = False [EOL] embed_batch_size = DEFAULT_EMBED_BATCH_SIZE [EOL] if st . sidebar . checkbox ( [string] ) : [EOL] run_embeddings = st . sidebar . button ( [string] ) [EOL] model_type_options = [ [string] ] [EOL] if model_data_dir is not None : [EOL] model_type_options . insert ( [number] , [string] ) [EOL] else : [EOL] st . sidebar . markdown ( [string] [string] ) [EOL] model_type = st . sidebar . selectbox ( [string] , model_type_options ) [EOL] [EOL] if model_type == [string] and model_data_dir is not None : [EOL] result = st_select_model_checkpoint ( Path ( model_data_dir ) , use_gpu , nvidia_visible_devices ) [EOL] if result is None : [EOL] embeddings_error = True [EOL] st . sidebar . error ( [string] ) [EOL] else : [EOL] model_cls , model_kwargs , checkpoint_meta = result [EOL] else : [EOL] checkpoint_meta = None [EOL] model_result = st_select_untrained_model ( use_gpu , nvidia_visible_devices , predicate = lambda m : issubclass ( m , EmbedMixin ) and m is not FastText , ) [EOL] [EOL] if model_result is None : [EOL] run_embeddings = False [EOL] embeddings_error = True [EOL] else : [EOL] model_cls , model_kwargs = model_result [EOL] [EOL] embed_batch_size = st . sidebar . number_input ( [string] , min_value = [number] , max_value = len ( sampled_texts ) , value = min ( len ( sampled_texts ) , DEFAULT_EMBED_BATCH_SIZE ) , ) [EOL] [EOL] umap_seed = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] umap_n_neighbors = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] umap_metric = st . sidebar . selectbox ( [string] , list ( named_distances . keys ( ) ) ) [EOL] umap_min_dist = st . sidebar . number_input ( [string] , min_value = [number] , max_value = [number] , value = [number] ) [EOL] [EOL] show_vocab_overlap = st . sidebar . checkbox ( [string] ) [EOL] [EOL] cluster_when = [string] [EOL] clusterer = None [EOL] if st . sidebar . checkbox ( [string] ) : [EOL] clustering_algorithm = st . sidebar . selectbox ( [string] , [ [string] , [string] ] ) [EOL] cluster_when_labels = { [string] : [string] , [string] : [string] , } [EOL] cluster_when = st . sidebar . selectbox ( [string] , [ [string] , [string] ] , format_func = lambda l : cluster_when_labels [ l ] , ) [EOL] if clustering_algorithm == [string] : [EOL] n_clusters = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] max_iter = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] [EOL] kmeans_seed = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] clusterer = KMeans ( n_clusters = n_clusters , max_iter = max_iter , random_state = kmeans_seed ) [EOL] else : [EOL] hdbscan_min_cluster_size = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] hdbscan_min_samples = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] hdbscan_cluster_selection_epsilon = st . sidebar . number_input ( [string] , min_value = [number] , max_value = [number] , value = [number] , ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] hdbscan_metric_options = SKLEARN_DISTANCE_METRICS [ [string] ] [EOL] hdbscan_metric = st . sidebar . selectbox ( [string] , hdbscan_metric_options , index = hdbscan_metric_options . index ( [string] ) , ) [EOL] [EOL] clusterer = hdbscan . HDBSCAN ( min_cluster_size = hdbscan_min_cluster_size , min_samples = hdbscan_min_samples , cluster_selection_epsilon = hdbscan_cluster_selection_epsilon , metric = hdbscan_metric , ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] show_example_documents ( sampled_texts , sampled_labels , filter_label , example_truncate_len , example_num_docs , ) [EOL] [EOL] if filter_label is None : [EOL] show_label_distribution ( sampled_labels , all_labels = filtered_labels if show_full_label_distribution else None , ) [EOL] [EOL] tokens = get_tokens ( sampled_texts , tokenize_method , vocab_size ) [EOL] [EOL] show_document_length_distribution ( tokens ) [EOL] [EOL] st . header ( [string] ) [EOL] if not run_topic_model : [EOL] st . markdown ( [string] ) [EOL] else : [EOL] try : [EOL] show_topic_model ( tokens , sampled_labels , filter_label , num_topics = num_topics , train_chunksize = train_chunksize , train_passes = train_passes , train_iterations = train_iterations , do_bigrams = do_bigrams , bigram_min_count = bigram_min_count , min_frequency = min_frequency , max_proportion = max_proportion , ) [EOL] except ImportError as e : [EOL] st . error ( str ( e ) ) [EOL] [EOL] st . header ( [string] ) [EOL] [EOL] if embeddings_error : [EOL] st . error ( [string] ) [EOL] elif not run_embeddings : [EOL] st . markdown ( [string] ) [EOL] else : [EOL] show_embeddings ( model_cls , model_kwargs , sampled_texts , sampled_labels , checkpoint_meta , batch_size = embed_batch_size , umap_seed = umap_seed , umap_n_neighbors = umap_n_neighbors , umap_metric = umap_metric , umap_min_dist = umap_min_dist , clusterer = clusterer , cluster_when = cluster_when , show_vocab_overlap = show_vocab_overlap , ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] try : [EOL] run ( ) [EOL] except SystemExit : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , Callable , List [EOL] import pathlib [EOL] import typing [EOL] import builtins [EOL] import numpy [EOL] from pathlib import Path [EOL] from typing import Any , Callable , Dict , List [EOL] [EOL] import click [EOL] import eli5 [EOL] import numpy as np [EOL] import streamlit as st [EOL] from eli5 . lime import TextExplainer [EOL] [EOL] from gobbli . interactive . util import ( DEFAULT_PREDICT_BATCH_SIZE , get_predictions , load_data , st_model_metadata , st_select_model_checkpoint , ) [EOL] [EOL] [EOL] def make_predict_func ( model_cls , model_kwargs , unique_labels , checkpoint , batch_size , ) : [EOL] def predict ( texts ) : [EOL] preds = get_predictions ( model_cls , model_kwargs , texts , unique_labels , checkpoint , batch_size = batch_size , ) . values [EOL] [comment] [EOL] preds = preds / preds . sum ( axis = [number] , keepdims = [number] ) [EOL] return preds [EOL] [EOL] return predict [EOL] [EOL] [EOL] def st_lime_explanation ( text , predict_func , unique_labels , n_samples , position_dependent = True , ) : [EOL] [comment] [EOL] [comment] [EOL] with st . spinner ( [string] ) : [EOL] te = TextExplainer ( random_state = [number] , n_samples = n_samples , position_dependent = position_dependent ) [EOL] te . fit ( text , predict_func ) [EOL] st . json ( te . metrics_ ) [EOL] explanation = te . explain_prediction ( ) [EOL] explanation_df = eli5 . format_as_dataframe ( explanation ) [EOL] for target_ndx , target in enumerate ( sorted ( explanation . targets , key = lambda t : - t . proba ) ) : [EOL] target_explanation_df = explanation_df [ explanation_df [ [string] ] == target_ndx ] . copy ( ) [EOL] [EOL] target_explanation_df [ [string] ] = ( target_explanation_df [ [string] ] * target_explanation_df [ [string] ] ) [EOL] target_explanation_df [ [string] ] = abs ( target_explanation_df [ [string] ] ) [EOL] target_explanation_df = ( target_explanation_df . drop ( [string] , axis = [number] ) . sort_values ( by = [string] , ascending = False ) . reset_index ( drop = True ) ) [EOL] st . subheader ( f" [string] { unique_labels [ target_ndx ] } [string] { target . proba : [string] } [string] { target . score : [string] } [string] " ) [EOL] st . dataframe ( target_explanation_df ) [EOL] [EOL] [EOL] @ click . command ( ) @ click . argument ( [string] , type = str ) @ click . argument ( [string] , type = str ) @ click . option ( [string] , type = int , help = [string] , default = - [number] , show_default = True , ) @ click . option ( [string] , default = False , help = [string] , ) @ click . option ( [string] , default = False , help = [string] [string] [string] , ) @ click . option ( [string] , default = [string] , help = [string] [string] , show_default = True , ) @ click . option ( [string] , default = [string] , help = [string] [string] , show_default = True , ) def run ( model_data_dir , data , n_rows , use_gpu , multilabel , multilabel_sep , nvidia_visible_devices , ) : [EOL] st . sidebar . header ( [string] ) [EOL] model_data_path = Path ( model_data_dir ) [EOL] model_cls , model_kwargs , checkpoint_meta = st_select_model_checkpoint ( model_data_path , use_gpu , nvidia_visible_devices ) [EOL] st . title ( f" [string] { model_cls . __name__ } [string] { data }" ) [EOL] [EOL] model = model_cls ( ** model_kwargs ) [EOL] st_model_metadata ( model ) [EOL] [EOL] texts , labels = load_data ( data , multilabel , n_rows = None if n_rows == - [number] else n_rows , multilabel_sep = multilabel_sep , ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] example_ndx = st . sidebar . number_input ( f" [string] { len ( texts ) - [number] } [string] " , min_value = [number] , max_value = len ( texts ) - [number] , value = [number] , ) [EOL] [EOL] if labels is None : [EOL] header_text = [string] [EOL] else : [EOL] y_true = labels [ example_ndx ] [EOL] if isinstance ( y_true , list ) : [EOL] header_text = f" [string] { [string] . join ( y_true ) } [string] " [EOL] else : [EOL] header_text = f" [string] { y_true } [string] " [EOL] st . header ( header_text ) [EOL] st . text ( texts [ example_ndx ] ) [EOL] [EOL] st . sidebar . header ( [string] ) [EOL] predict_batch_size = st . sidebar . number_input ( [string] , min_value = [number] , max_value = len ( texts ) , value = min ( len ( texts ) , DEFAULT_PREDICT_BATCH_SIZE ) , ) [EOL] [EOL] predict_func = make_predict_func ( model_cls , model_kwargs , checkpoint_meta [ [string] ] , checkpoint_meta [ [string] ] , predict_batch_size , ) [EOL] [EOL] do_lime = st . sidebar . checkbox ( [string] ) [EOL] [EOL] do_run = st . sidebar . button ( [string] ) [EOL] [EOL] if do_lime : [EOL] st . sidebar . header ( [string] ) [EOL] n_samples = st . sidebar . number_input ( [string] , min_value = [number] , max_value = None , value = [number] ) [EOL] position_dependent = st . sidebar . checkbox ( [string] , value = True ) [EOL] if do_run : [EOL] st . header ( [string] ) [EOL] st_lime_explanation ( texts [ example_ndx ] , predict_func , checkpoint_meta [ [string] ] , n_samples , position_dependent = position_dependent , ) [EOL] [EOL] if not do_lime : [EOL] st . header ( [string] ) [EOL] st . markdown ( [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] try : [EOL] run ( ) [EOL] except SystemExit : [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[typing.List],numpy.ndarray]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $numpy.ndarray$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import pathlib [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] from typing import Any , Dict , List [EOL] [EOL] import click [EOL] import yaml [EOL] [EOL] import gobbli [EOL] from benchmark_util import BENCHMARK_DATA_DIR [EOL] from scenario import ( ClassImbalanceScenario , DataAugmentationScenario , DocumentWindowingScenario , IMDBClassificationScenario , IMDBEmbeddingScenario , LowResourceScenario , MovieSummaryClassificationScenario , NewsgroupsClassificationScenario , NewsgroupsEmbeddingScenario , load_scenario , ) [EOL] [EOL] LOGGER = logging . getLogger ( __name__ ) [EOL] [EOL] BENCHMARK_DIR = Path ( __file__ ) . parent [EOL] [EOL] BENCHMARK_SPECS_FILE = BENCHMARK_DIR / [string] [EOL] BENCHMARK_OUTPUT_DIR = BENCHMARK_DIR / [string] [EOL] [EOL] BENCHMARK_DEBUG_OUTPUT_DIR = BENCHMARK_DIR / [string] [EOL] BENCHMARK_DEBUG_SPECS_FILE = BENCHMARK_DIR / [string] [EOL] [EOL] ALL_SCENARIOS = { [string] : NewsgroupsClassificationScenario , [string] : NewsgroupsEmbeddingScenario , [string] : IMDBClassificationScenario , [string] : MovieSummaryClassificationScenario , [string] : IMDBEmbeddingScenario , [string] : ClassImbalanceScenario , [string] : LowResourceScenario , [string] : DataAugmentationScenario , [string] : DocumentWindowingScenario , } [EOL] [EOL] [EOL] def load_specs ( specs_file ) : [EOL] [docstring] [EOL] with open ( specs_file , [string] ) as f : [EOL] specs_list = yaml . safe_load ( f ) [EOL] [EOL] specs = { } [EOL] for spec in specs_list : [EOL] name = spec . pop ( [string] ) [EOL] specs [ name ] = spec [EOL] [EOL] return specs [EOL] [EOL] [EOL] @ click . command ( [string] ) @ click . option ( [string] , [string] , multiple = True , help = [string] , ) @ click . option ( [string] , default = False , help = [string] , ) @ click . option ( [string] , default = str ( BENCHMARK_OUTPUT_DIR ) , help = [string] , ) @ click . option ( [string] , default = [string] , show_default = True , help = [string] , ) @ click . option ( [string] , default = False , help = [string] [string] [string] , ) @ click . option ( [string] , default = False , help = [string] [string] , ) def run ( scenario_names , force , output_dir , log_level , debug , raise_exceptions , ) : [EOL] [comment] [EOL] [comment] [EOL] os . environ [ [string] ] = str ( BENCHMARK_DATA_DIR ) [EOL] [EOL] logging . basicConfig ( level = log_level , format = [string] ) [EOL] [EOL] output_path = Path ( output_dir ) [EOL] specs_file = BENCHMARK_SPECS_FILE [EOL] dataset_limit = None [EOL] if debug : [EOL] output_path = BENCHMARK_DEBUG_OUTPUT_DIR [EOL] specs_file = BENCHMARK_DEBUG_SPECS_FILE [EOL] dataset_limit = [number] [EOL] [EOL] output_path . mkdir ( exist_ok = True , parents = True ) [EOL] specs = load_specs ( specs_file ) [EOL] [EOL] if len ( scenario_names ) == [number] : [EOL] scenario_names = list ( specs . keys ( ) ) [EOL] [EOL] LOGGER . info ( f" [string] { scenario_names }" ) [EOL] [EOL] for scenario_name in scenario_names : [EOL] LOGGER . info ( f" [string] { scenario_name }" ) [EOL] if scenario_name not in ALL_SCENARIOS : [EOL] raise ValueError ( f" [string] { scenario_name } [string] " f"{ list ( ALL_SCENARIOS . keys ( ) ) }" ) [EOL] if scenario_name not in specs : [EOL] raise ValueError ( f" [string] { scenario_name } [string] " f"{ list ( specs . keys ( ) ) }" ) [EOL] [EOL] spec = specs [ scenario_name ] [EOL] scenario = load_scenario ( ALL_SCENARIOS [ scenario_name ] , output_path / scenario_name , spec . get ( [string] , { } ) , spec . get ( [string] , [ ] ) , force = force , dataset_limit = dataset_limit , ) [EOL] [EOL] LOGGER . info ( f" [string] { scenario_name }" ) [EOL] scenario . run ( raise_exceptions = raise_exceptions ) [EOL] LOGGER . info ( f" [string] { scenario_name }" ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] gobbli . util . cleanup ( force = True ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] run ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 $pathlib.Path$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 $pathlib.Path$ 0 0 0 $pathlib.Path$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0