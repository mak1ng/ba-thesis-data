[comment] [EOL] from typing import Any [EOL] import environments [EOL] import dqn [EOL] import typing [EOL] [docstring] [EOL] import gym [EOL] import torch [EOL] import torch . optim as optim [EOL] [EOL] from dqn . agents import DQNAgent [EOL] from dqn . networks import AtariQNetwork [EOL] from dqn . replays import NATUREDQN_ATARI_PREPROCESS_BATCH , CircularReplayBuffer [EOL] from environments import AtariPreprocessing , FrameStack [EOL] from train_eval import get_config , train_eval [EOL] from utils import get_logger , load_models , set_env_random_seeds , set_global_random_seeds [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] [comment] [EOL] CONFIG = get_config ( ) [EOL] [EOL] [comment] [EOL] logger = get_logger ( log_to_console = True , log_to_file = CONFIG . LOG_TO_FILE ) [EOL] [EOL] [comment] [EOL] torch . set_num_threads ( CONFIG . CPU_THREADS ) [EOL] device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] if not torch . cuda . is_available ( ) : [EOL] logger . warning ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] env = gym . make ( CONFIG . ENV_NAME ) [EOL] eval_env = gym . make ( CONFIG . ENV_NAME ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] env = AtariPreprocessing ( env , frame_skip = CONFIG . FRAME_SKIP , terminal_on_life_loss = True ) [EOL] [comment] [EOL] [comment] [EOL] eval_env = AtariPreprocessing ( eval_env , frame_skip = CONFIG . FRAME_SKIP , terminal_on_life_loss = False ) [EOL] [comment] [EOL] env = FrameStack ( env , stack_size = CONFIG . FRAME_STACK ) [EOL] eval_env = FrameStack ( eval_env , stack_size = CONFIG . FRAME_STACK ) [EOL] [EOL] [comment] [EOL] if CONFIG . RANDOM_SEED is not None : [EOL] set_global_random_seeds ( CONFIG . RANDOM_SEED , use_numpy = True , use_torch = True ) [EOL] [comment] [EOL] set_env_random_seeds ( env , CONFIG . RANDOM_SEED ) [EOL] set_env_random_seeds ( eval_env , CONFIG . RANDOM_SEED + [number] ) [EOL] else : [EOL] logger . warning ( [string] ) [EOL] [EOL] [comment] [EOL] q_net = AtariQNetwork ( CONFIG . FRAME_STACK , env . action_space . n ) . to ( device ) [EOL] optimizer = optim . RMSprop ( q_net . parameters ( ) , lr = CONFIG . RMSPROP_LR , alpha = CONFIG . RMSPROP_DECAY , eps = CONFIG . RMSPROP_EPSILON , momentum = CONFIG . RMSPROP_MOMENTUM , weight_decay = CONFIG . RMSPROP_WEIGHT_DECAY , centered = CONFIG . RMSPROP_IS_CENTERED , ) [EOL] if CONFIG . LOAD_PATH : [EOL] [comment] [EOL] load_models ( CONFIG . LOAD_PATH , q_net = q_net , optimizer = optimizer ) [EOL] dqn_agent = DQNAgent ( env , q_net , optimizer , device ) [EOL] replay_buffer = CircularReplayBuffer ( env , maxlen = CONFIG . REPLAY_BUFFER_SIZE , device = device , preprocess_batch = NATUREDQN_ATARI_PREPROCESS_BATCH , ) [EOL] [EOL] [comment] [EOL] train_eval ( dqn_agent , replay_buffer , env , eval_env , device , logger , CONFIG ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from typing import Any , List , Union , Literal [EOL] import typing_extensions [EOL] import dqn [EOL] import typing [EOL] [docstring] [EOL] import configargparse [EOL] import gym [EOL] import numpy as np [EOL] import torch [EOL] import torch . optim as optim [EOL] [EOL] from dqn . agents import DQNAgent [EOL] from dqn . networks import QNetwork [EOL] from dqn . replays import CircularReplayBuffer , Transition [EOL] from utils import ( get_linear_anneal_func , get_logger , get_timestamp , load_models , save_models , set_env_random_seeds , set_global_random_seeds , ) [EOL] [EOL] [EOL] def get_config ( ) : [EOL] [docstring] [EOL] parser = configargparse . ArgParser ( ) [EOL] parser . add ( [string] , [string] , required = True , is_config_file = True ) [EOL] parser . add ( [string] , type = str , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = ( [string] [string] ) , ) [EOL] parser . add ( [string] , type = int , help = ( [string] , [string] , ) , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = float , help = [string] , ) [EOL] parser . add ( [string] , type = float , help = [string] , ) [EOL] parser . add ( [string] , type = float , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] [EOL] parser . add ( [string] , type = float , help = [string] ) [EOL] parser . add ( [string] , type = float , help = [string] ) [EOL] parser . add ( [string] , type = float , help = [string] , ) [EOL] parser . add ( [string] , type = float , help = [string] ) [EOL] parser . add ( [string] , type = float , help = [string] ) [EOL] parser . add ( [string] , action = [string] , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = ( [string] [string] ) , ) [EOL] parser . add ( [string] , action = [string] , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = int , help = [string] , ) [EOL] parser . add ( [string] , type = float , help = [string] ) [EOL] parser . add ( [string] , type = str , help = [string] ) [EOL] parser . add ( [string] , type = str , help = [string] ) [EOL] parser . add ( [string] , type = int , help = [string] ) [EOL] parser . add ( [string] , action = [string] , help = [string] , ) [EOL] parser . add ( [string] , action = [string] , help = [string] , ) [EOL] CONFIG = parser . parse_args ( ) [EOL] if not hasattr ( CONFIG , [string] ) : [EOL] CONFIG . RMSPROP_IS_CENTERED = False [EOL] if not hasattr ( CONFIG , [string] ) : [EOL] CONFIG . LOG_TO_FILE = False [EOL] if not hasattr ( CONFIG , [string] ) : [EOL] CONFIG . USE_TENSORBOARD = False [EOL] if not hasattr ( CONFIG , [string] ) : [EOL] CONFIG . USE_WANDB = False [EOL] [EOL] [comment] [EOL] [comment] [EOL] CONFIG . ENV_STEPS = CONFIG . ENV_FRAMES // CONFIG . FRAME_SKIP [EOL] [EOL] print ( ) [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] for arg in vars ( CONFIG ) : [EOL] attr = getattr ( CONFIG , arg ) if getattr ( CONFIG , arg ) is not None else [string] [EOL] if type ( attr ) == bool : [EOL] attr = [string] if attr else [string] [EOL] [EOL] print ( [string] . format ( arg , attr ) ) [EOL] print ( [string] ) [EOL] print ( ) [EOL] [EOL] return CONFIG [EOL] [EOL] [EOL] def train_eval ( dqn_agent , replay_buffer , env , eval_env , device , logger , CONFIG ) : [EOL] [docstring] [EOL] [comment] [EOL] if CONFIG . USE_TENSORBOARD : [EOL] from torch . utils . tensorboard import SummaryWriter [EOL] [EOL] writer = SummaryWriter ( log_dir = [string] ) [EOL] if CONFIG . USE_WANDB : [EOL] import wandb [EOL] [EOL] wandb . init ( project = [string] , config = CONFIG ) [EOL] wandb . watch ( dqn_agent . q_net ) [EOL] [EOL] [comment] [EOL] if not CONFIG . SAVE_DIR : [EOL] logger . warning ( [string] ) [EOL] else : [EOL] [comment] [EOL] saved_model_eval_episode_return = - float ( [string] ) [EOL] [comment] [EOL] unique_save_dir = f"{ CONFIG . SAVE_DIR } [string] { CONFIG . ENV_NAME } [string] { get_timestamp ( ) } [string] " [EOL] [EOL] [comment] [EOL] get_epsilon = get_linear_anneal_func ( start_value = CONFIG . EPSILON_DECAY_START_VALUE , end_value = CONFIG . EPSILON_DECAY_END_VALUE , start_step = CONFIG . EPSILON_DECAY_START_STEP , end_step = CONFIG . EPSILON_DECAY_END_STEP , ) [EOL] [EOL] [comment] [EOL] episode_return = [number] [EOL] episode_i = [number] [EOL] eval_i = [number] [EOL] obs = env . reset ( ) [EOL] for step_i in range ( CONFIG . ENV_STEPS + [number] ) : [EOL] [comment] [EOL] epsilon = get_epsilon ( step_i ) [EOL] action = dqn_agent . select_action ( np . expand_dims ( obs , [number] ) , epsilon ) [EOL] next_obs , rew , done , info = env . step ( action ) [EOL] replay_buffer . append ( Transition ( obs , action , rew , next_obs , done ) ) [EOL] [EOL] [comment] [EOL] if ( step_i % CONFIG . UPDATE_FREQUENCY == [number] [EOL] and len ( replay_buffer ) >= CONFIG . MIN_REPLAY_BUFFER_SIZE ) : [EOL] experiences = replay_buffer . get_torch_batch ( CONFIG . BATCH_SIZE ) [EOL] td_loss = dqn_agent . train ( experiences , discount = CONFIG . DISCOUNT ) [EOL] [EOL] [comment] [EOL] if step_i % CONFIG . LOG_FREQUENCY == [number] : [EOL] logger . debug ( [string] . format ( episode_i , step_i , epsilon , td_loss ) ) [EOL] if CONFIG . USE_TENSORBOARD : [EOL] writer . add_scalar ( [string] , td_loss , step_i ) [EOL] writer . add_scalar ( [string] , epsilon , step_i ) [EOL] if CONFIG . USE_WANDB : [EOL] wandb . log ( { [string] : td_loss , [string] : epsilon } , step = step_i ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if step_i % CONFIG . TARGET_NET_UPDATE_FREQUENCY == [number] : [EOL] dqn_agent . update_target_q_net ( ) [EOL] [EOL] [comment] [EOL] episode_return += rew [EOL] obs = next_obs [EOL] [EOL] [comment] [EOL] if done : [EOL] [comment] [EOL] logger . info ( [string] . format ( episode_i , step_i , int ( episode_return ) ) ) [EOL] if CONFIG . USE_TENSORBOARD : [EOL] writer . add_scalar ( [string] , episode_return , episode_i ) [EOL] if CONFIG . USE_WANDB : [EOL] wandb . log ( { [string] : episode_return , [string] : episode_i , } , step = step_i , ) [EOL] [EOL] [comment] [EOL] env . reset ( ) [EOL] episode_return = [number] [EOL] episode_i += [number] [EOL] [EOL] [comment] [EOL] if step_i % CONFIG . EVAL_FREQUENCY == [number] and CONFIG . EVAL_EPISODES > [number] : [EOL] all_eval_episode_return = [ ] [EOL] [EOL] [comment] [EOL] for _ in range ( CONFIG . EVAL_EPISODES ) : [EOL] [comment] [EOL] eval_done = False [EOL] eval_obs = eval_env . reset ( ) [EOL] eval_episode_return = [number] [EOL] while not eval_done : [EOL] eval_action = dqn_agent . select_action ( np . expand_dims ( eval_obs , [number] ) , epsilon = CONFIG . EVAL_EPSILON ) [EOL] eval_obs , eval_rew , eval_done , _ = eval_env . step ( eval_action ) [EOL] eval_episode_return += eval_rew [EOL] all_eval_episode_return . append ( eval_episode_return ) [EOL] avg_eval_episode_return = np . mean ( all_eval_episode_return ) [EOL] [EOL] [comment] [EOL] logger . info ( [string] . format ( step_i , avg_eval_episode_return ) ) [EOL] if CONFIG . USE_TENSORBOARD : [EOL] writer . add_scalar ( [string] , avg_eval_episode_return , eval_i ) [EOL] writer . add_histogram ( [string] , np . array ( all_eval_episode_return ) ) [EOL] if CONFIG . USE_WANDB : [EOL] wandb . log ( { [string] : avg_eval_episode_return , [string] : wandb . Histogram ( all_eval_episode_return ) , [string] : eval_i , } , step = step_i , ) [EOL] [EOL] [comment] [EOL] if ( CONFIG . SAVE_DIR [EOL] and saved_model_eval_episode_return <= eval_episode_return ) : [EOL] saved_model_eval_episode_return = eval_episode_return [EOL] save_models ( unique_save_dir , filename = [string] , q_net = dqn_agent . q_net , optimizer = dqn_agent . optimizer , ) [EOL] logger . info ( f" [string] { unique_save_dir }" ) [EOL] [EOL] eval_i += [number] [EOL] [EOL] [comment] [EOL] if CONFIG . SAVE_DIR : [EOL] save_models ( unique_save_dir , filename = [string] , q_net = dqn_agent . q_net , optimizer = dqn_agent . optimizer , ) [EOL] logger . info ( f" [string] { unique_save_dir }" ) [EOL] [EOL] [EOL] def main ( ) : [EOL] [docstring] [EOL] [comment] [EOL] CONFIG = get_config ( ) [EOL] [EOL] [comment] [EOL] logger = get_logger ( log_to_console = True , log_to_file = CONFIG . LOG_TO_FILE ) [EOL] [EOL] [comment] [EOL] torch . set_num_threads ( CONFIG . CPU_THREADS ) [EOL] device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] if not torch . cuda . is_available ( ) : [EOL] logger . warning ( [string] ) [EOL] [EOL] [comment] [EOL] env = gym . make ( CONFIG . ENV_NAME ) [EOL] eval_env = gym . make ( CONFIG . ENV_NAME ) [EOL] [EOL] [comment] [EOL] if CONFIG . RANDOM_SEED is not None : [EOL] set_global_random_seeds ( CONFIG . RANDOM_SEED , use_numpy = True , use_torch = True ) [EOL] [comment] [EOL] set_env_random_seeds ( env , CONFIG . RANDOM_SEED ) [EOL] set_env_random_seeds ( eval_env , CONFIG . RANDOM_SEED + [number] ) [EOL] else : [EOL] logger . warning ( [string] ) [EOL] [EOL] [comment] [EOL] q_net = QNetwork ( env . observation_space . shape [ [number] ] , env . action_space . n ) . to ( device ) [EOL] optimizer = optim . RMSprop ( q_net . parameters ( ) , lr = CONFIG . RMSPROP_LR , alpha = CONFIG . RMSPROP_DECAY , eps = CONFIG . RMSPROP_EPSILON , momentum = CONFIG . RMSPROP_MOMENTUM , weight_decay = CONFIG . RMSPROP_WEIGHT_DECAY , centered = CONFIG . RMSPROP_IS_CENTERED , ) [EOL] if CONFIG . LOAD_PATH : [EOL] [comment] [EOL] load_models ( CONFIG . LOAD_PATH , q_net = q_net , optimizer = optimizer ) [EOL] dqn_agent = DQNAgent ( env , q_net , optimizer , device ) [EOL] [EOL] replay_buffer = CircularReplayBuffer ( env , maxlen = CONFIG . REPLAY_BUFFER_SIZE , device = device ) [EOL] [EOL] [comment] [EOL] train_eval ( dqn_agent , replay_buffer , env , eval_env , device , logger , CONFIG ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] [docstring] [EOL] import torch [EOL] import torch . nn as nn [EOL] [EOL] [EOL] class QNetwork ( nn . Module ) : [EOL] def __init__ ( self , in_dim , out_dim ) : [EOL] [docstring] [EOL] super ( ) . __init__ ( ) [EOL] [EOL] self . layers = nn . Sequential ( nn . Linear ( in_dim , [number] ) , nn . ReLU ( ) , nn . Linear ( [number] , [number] ) , nn . ReLU ( ) , nn . Linear ( [number] , out_dim ) , ) [EOL] [EOL] def weights_init ( m ) : [EOL] if isinstance ( m , nn . Linear ) : [EOL] torch . nn . init . kaiming_uniform_ ( m . weight ) [EOL] torch . nn . init . zeros_ ( m . bias ) [EOL] [EOL] self . layers . apply ( weights_init ) [EOL] [EOL] def forward ( self , x ) : [EOL] [docstring] [EOL] return self . layers ( x ) [EOL] [EOL] [EOL] class AtariQNetwork ( nn . Module ) : [EOL] def __init__ ( self , in_dim , out_dim ) : [EOL] [docstring] [EOL] super ( ) . __init__ ( ) [EOL] self . conv_layers = nn . Sequential ( nn . Conv2d ( in_dim , [number] , [number] , stride = [number] , padding = [number] ) , nn . ReLU ( ) , nn . Conv2d ( [number] , [number] , [number] , stride = [number] , padding = [number] ) , nn . ReLU ( ) , nn . Conv2d ( [number] , [number] , [number] , stride = [number] , padding = [number] ) , nn . ReLU ( ) , ) [EOL] self . fc_layers = nn . Sequential ( nn . Linear ( [number] , [number] ) , nn . ReLU ( ) , nn . Linear ( [number] , out_dim ) ) [EOL] [EOL] def weights_init ( m ) : [EOL] if isinstance ( m , nn . Conv2d ) or isinstance ( m , nn . Linear ) : [EOL] torch . nn . init . kaiming_uniform_ ( m . weight ) [EOL] torch . nn . init . zeros_ ( m . bias ) [EOL] [EOL] self . conv_layers . apply ( weights_init ) [EOL] self . fc_layers . apply ( weights_init ) [EOL] [EOL] def forward ( self , x ) : [EOL] [docstring] [EOL] x = self . conv_layers ( x ) [EOL] x = x . view ( x . size ( [number] ) , - [number] ) [EOL] x = self . fc_layers ( x ) [EOL] [EOL] return x [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
[docstring] [EOL]	0 0
from typing import Any , List , Tuple [EOL] import gym [EOL] import builtins [EOL] import torch [EOL] import numpy [EOL] import typing [EOL] [docstring] [EOL] import copy [EOL] import random [EOL] from typing import List , Tuple [EOL] [EOL] import gym [EOL] import numpy as np [EOL] import torch [EOL] import torch . nn as nn [EOL] import torch . nn . functional as F [EOL] import torch . optim as optim [EOL] [EOL] [EOL] class DQNAgent : [EOL] def __init__ ( self , env , q_net , optimizer , device , ) : [EOL] [docstring] [EOL] self . _env = env [EOL] self . q_net = q_net [EOL] self . optimizer = optimizer [EOL] [EOL] self . _target_q_net = copy . deepcopy ( q_net ) [EOL] [EOL] self . _device = device [EOL] [EOL] def select_action ( self , obs , epsilon = [number] ) : [EOL] [docstring] [EOL] assert [number] <= epsilon <= [number] [EOL] [EOL] [comment] [EOL] if random . random ( ) < epsilon : [EOL] return self . _env . action_space . sample ( ) [EOL] [EOL] [comment] [EOL] q_values = self . q_net ( torch . FloatTensor ( obs ) . to ( self . _device ) ) [EOL] return q_values . argmax ( ) . item ( ) [EOL] [EOL] def train ( self , experiences , discount , ) : [EOL] [docstring] [EOL] obs_b , action_b , rew_b , next_obs_b , done_b = experiences [EOL] assert obs_b . shape == ( len ( obs_b ) , ) + self . _env . observation_space . shape [EOL] assert action_b . shape == ( len ( obs_b ) , [number] ) [EOL] assert rew_b . shape == ( len ( obs_b ) , [number] ) [EOL] assert next_obs_b . shape == ( len ( obs_b ) , ) + self . _env . observation_space . shape [EOL] assert done_b . shape == ( len ( obs_b ) , [number] ) [EOL] [EOL] target = rew_b + ( [number] - done_b ) * discount * self . _target_q_net ( next_obs_b ) . max ( dim = - [number] ) [ [number] ] . unsqueeze ( [number] ) [EOL] prediction = self . q_net ( obs_b ) . gather ( [number] , action_b ) [EOL] assert target . shape == ( len ( obs_b ) , [number] ) [EOL] assert prediction . shape == ( len ( obs_b ) , [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] td_loss = F . mse_loss ( prediction , target ) [EOL] assert td_loss . shape == ( ) [EOL] [EOL] self . optimizer . zero_grad ( ) [EOL] td_loss . backward ( ) [EOL] nn . utils . clip_grad_norm_ ( self . q_net . parameters ( ) , [number] ) [EOL] self . optimizer . step ( ) [EOL] [EOL] return td_loss . item ( ) [EOL] [EOL] def update_target_q_net ( self ) : [EOL] [docstring] [EOL] self . _target_q_net = copy . deepcopy ( self . q_net ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Callable , Type , Tuple , Any [EOL] import gym [EOL] import builtins [EOL] import torch [EOL] import numpy [EOL] import typing [EOL] import dqn [EOL] [docstring] [EOL] import copy [EOL] import random [EOL] from collections import deque , namedtuple [EOL] from typing import Callable , Tuple [EOL] [EOL] import gym [EOL] import numpy as np [EOL] import torch [EOL] [EOL] Transition = namedtuple ( [string] , [ [string] , [string] , [string] , [string] , [string] ] ) [EOL] [EOL] [EOL] def NATUREDQN_ATARI_PREPROCESS_BATCH ( o , a , r , n , d ) : [EOL] [docstring] [EOL] o , a , r , n , d = NORMALIZE_OBSERVATION ( o , a , r , n , d ) [EOL] o , a , r , n , d = CLIP_REWARD ( o , a , r , n , d ) [EOL] [EOL] return o , a , r , n , d [EOL] [EOL] [EOL] def NORMALIZE_OBSERVATION ( obs_b , action_b , rew_b , next_obs_b , done_b ) : [EOL] [docstring] [EOL] return obs_b / [number] , action_b , rew_b , next_obs_b / [number] , done_b [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def CLIP_REWARD ( obs_b , action_b , rew_b , next_obs_b , done_b ) : [EOL] [docstring] [EOL] return obs_b , action_b , np . clip ( rew_b , - [number] , [number] ) , next_obs_b , done_b [EOL] [EOL] [EOL] [comment] [EOL] def FIXED_MAGNITUDE_REWARD ( obs_b , action_b , rew_b , next_obs_b , done_b ) : [EOL] [docstring] [EOL] return obs_b , action_b , np . sign ( rew_b ) , next_obs_b , done_b [EOL] [EOL] [EOL] class ReplayBuffer : [EOL] def __init__ ( self , maxlen , device , preprocess_batch = None ) : [EOL] [docstring] [EOL] self . buffer = deque ( maxlen = maxlen ) [EOL] self . _device = device [EOL] self . preprocess_batch = preprocess_batch [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . buffer ) [EOL] [EOL] def append ( self , transition ) : [EOL] [docstring] [EOL] assert type ( transition ) == Transition [EOL] self . buffer . append ( transition ) [EOL] [EOL] def get_numpy_batch ( self , batch_size ) : [EOL] [docstring] [EOL] transition_b = np . array ( random . sample ( self . buffer , batch_size ) ) [EOL] obs_b , action_b , rew_b , next_obs_b , done_b = transition_b . T [EOL] [EOL] [comment] [EOL] [comment] [EOL] obs_b = np . vstack ( [ np . expand_dims ( obs , [number] ) for obs in obs_b ] ) . astype ( np . float ) [EOL] action_b = np . vstack ( action_b ) . astype ( np . float ) [EOL] rew_b = np . vstack ( rew_b ) . astype ( np . float ) [EOL] next_obs_b = np . vstack ( [ np . expand_dims ( obs , [number] ) for obs in next_obs_b ] ) . astype ( np . float ) [EOL] done_b = np . vstack ( done_b ) . astype ( np . float ) [EOL] [EOL] if self . preprocess_batch is not None : [EOL] return self . preprocess_batch ( obs_b , action_b , rew_b , next_obs_b , done_b ) [EOL] else : [EOL] return obs_b , action_b , rew_b , next_obs_b , done_b [EOL] [EOL] def get_torch_batch ( self , batch_size ) : [EOL] [docstring] [EOL] obs_b , action_b , rew_b , next_obs_b , done_b = self . get_numpy_batch ( batch_size ) [EOL] obs_b = torch . FloatTensor ( obs_b ) . to ( self . _device ) [EOL] action_b = torch . LongTensor ( action_b ) . to ( self . _device ) [EOL] rew_b = torch . FloatTensor ( rew_b ) . to ( self . _device ) [EOL] next_obs_b = torch . FloatTensor ( next_obs_b ) . to ( self . _device ) [EOL] done_b = torch . FloatTensor ( done_b ) . to ( self . _device ) [EOL] [EOL] return obs_b , action_b , rew_b , next_obs_b , done_b [EOL] [EOL] [EOL] class CircularReplayBuffer : [EOL] def __init__ ( self , env , maxlen , device , preprocess_batch = None , ) : [EOL] [docstring] [EOL] assert maxlen > [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] stub_transition = Transition ( np . zeros_like ( env . observation_space . high ) , env . action_space . sample ( ) , [number] , np . ones_like ( env . observation_space . high ) , False , ) [EOL] self . buffer = [ None ] * maxlen [EOL] for i in range ( maxlen ) : [EOL] self . buffer [ i ] = copy . deepcopy ( stub_transition ) [EOL] assert not ( self . buffer [ [number] ] is self . buffer [ [number] ] ) [EOL] self . buffer = np . array ( self . buffer ) [EOL] [EOL] self . maxlen = maxlen [EOL] self . curlen = [number] [EOL] self . index = [number] [EOL] [EOL] self . preprocess_batch = preprocess_batch [EOL] self . _device = device [EOL] [EOL] def __len__ ( self ) : [EOL] return self . curlen [EOL] [EOL] def append ( self , transition ) : [EOL] [docstring] [EOL] assert type ( transition ) == Transition [EOL] self . buffer [ self . index ] = transition [EOL] if self . curlen < self . maxlen : [EOL] self . curlen += [number] [EOL] self . index = ( self . index + [number] ) % self . maxlen [EOL] [EOL] def get_numpy_batch ( self , batch_size ) : [EOL] [docstring] [EOL] indices = np . random . randint ( low = [number] , high = self . curlen , size = batch_size ) [EOL] transition_b = self . buffer [ indices , ... ] [EOL] assert transition_b . shape == ( batch_size , [number] ) [EOL] obs_b , action_b , rew_b , next_obs_b , done_b = transition_b . T [EOL] [EOL] [comment] [EOL] [comment] [EOL] obs_b = np . vstack ( [ np . expand_dims ( obs , [number] ) for obs in obs_b ] ) . astype ( np . float ) [EOL] action_b = np . vstack ( action_b ) . astype ( np . float ) [EOL] rew_b = np . vstack ( rew_b ) . astype ( np . float ) [EOL] next_obs_b = np . vstack ( [ np . expand_dims ( obs , [number] ) for obs in next_obs_b ] ) . astype ( np . float ) [EOL] done_b = np . vstack ( done_b ) . astype ( np . float ) [EOL] [EOL] if self . preprocess_batch is not None : [EOL] return self . preprocess_batch ( obs_b , action_b , rew_b , next_obs_b , done_b ) [EOL] else : [EOL] return obs_b , action_b , rew_b , next_obs_b , done_b [EOL] [EOL] def get_torch_batch ( self , batch_size ) : [EOL] [docstring] [EOL] obs_b , action_b , rew_b , next_obs_b , done_b = self . get_numpy_batch ( batch_size ) [EOL] obs_b = torch . FloatTensor ( obs_b ) . to ( self . _device ) [EOL] action_b = torch . LongTensor ( action_b ) . to ( self . _device ) [EOL] rew_b = torch . FloatTensor ( rew_b ) . to ( self . _device ) [EOL] next_obs_b = torch . FloatTensor ( next_obs_b ) . to ( self . _device ) [EOL] done_b = torch . FloatTensor ( done_b ) . to ( self . _device ) [EOL] [EOL] return obs_b , action_b , rew_b , next_obs_b , done_b [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor]$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0
[comment] [EOL] from . atari_preprocessing import AtariPreprocessing [EOL] from . atari_wrappers import make_atari , wrap_deepmind [EOL] from . frame_stack import FrameStack [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [comment] [EOL] [EOL] [EOL] import gym [EOL] import numpy as np [EOL] from gym . spaces import Box [EOL] from gym . wrappers import TimeLimit [EOL] [EOL] [EOL] [comment] [EOL] class AtariPreprocessing ( gym . Wrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , noop_max = [number] , frame_skip = [number] , screen_size = [number] , terminal_on_life_loss = False , grayscale_obs = True , ) : [EOL] super ( ) . __init__ ( env ) [EOL] assert frame_skip > [number] [EOL] assert screen_size > [number] [EOL] [EOL] self . noop_max = noop_max [EOL] assert env . unwrapped . get_action_meanings ( ) [ [number] ] == [string] [EOL] [EOL] self . frame_skip = frame_skip [EOL] self . screen_size = screen_size [EOL] self . terminal_on_life_loss = terminal_on_life_loss [EOL] self . grayscale_obs = grayscale_obs [EOL] [EOL] [comment] [EOL] if grayscale_obs : [EOL] self . obs_buffer = [ np . empty ( env . observation_space . shape [ : [number] ] , dtype = np . uint8 ) , np . empty ( env . observation_space . shape [ : [number] ] , dtype = np . uint8 ) , ] [EOL] else : [EOL] self . obs_buffer = [ np . empty ( env . observation_space . shape , dtype = np . uint8 ) , np . empty ( env . observation_space . shape , dtype = np . uint8 ) , ] [EOL] [EOL] self . ale = env . unwrapped . ale [EOL] self . lives = [number] [EOL] self . game_over = False [EOL] [EOL] if grayscale_obs : [EOL] self . observation_space = Box ( low = [number] , high = [number] , shape = ( screen_size , screen_size ) , dtype = np . uint8 ) [EOL] else : [EOL] self . observation_space = Box ( low = [number] , high = [number] , shape = ( screen_size , screen_size , [number] ) , dtype = np . uint8 ) [EOL] [EOL] def step ( self , action ) : [EOL] R = [number] [EOL] [EOL] for t in range ( self . frame_skip ) : [EOL] _ , reward , done , info = self . env . step ( action ) [EOL] R += reward [EOL] self . game_over = done [EOL] [EOL] if self . terminal_on_life_loss : [EOL] new_lives = self . ale . lives ( ) [EOL] done = done or new_lives < self . lives [EOL] self . lives = new_lives [EOL] [EOL] if done : [EOL] break [EOL] if t == self . frame_skip - [number] : [EOL] if self . grayscale_obs : [EOL] self . ale . getScreenGrayscale ( self . obs_buffer [ [number] ] ) [EOL] else : [EOL] self . ale . getScreenRGB2 ( self . obs_buffer [ [number] ] ) [EOL] elif t == self . frame_skip - [number] : [EOL] if self . grayscale_obs : [EOL] self . ale . getScreenGrayscale ( self . obs_buffer [ [number] ] ) [EOL] else : [EOL] self . ale . getScreenRGB2 ( self . obs_buffer [ [number] ] ) [EOL] return self . _get_obs ( ) , R , done , info [EOL] [EOL] def reset ( self , ** kwargs ) : [EOL] self . env . reset ( ** kwargs ) [EOL] [EOL] [comment] [EOL] if self . noop_max > [number] : [EOL] noops = self . env . unwrapped . np_random . randint ( [number] , self . noop_max + [number] ) [EOL] assert noops > [number] [EOL] for _ in range ( noops ) : [EOL] _ , _ , done , _ = self . env . step ( [number] ) [EOL] if done : [EOL] self . env . reset ( ** kwargs ) [EOL] [EOL] [comment] [EOL] action_meanings = self . env . unwrapped . get_action_meanings ( ) [EOL] if action_meanings [ [number] ] == [string] and len ( action_meanings ) >= [number] : [EOL] self . env . step ( [number] ) [EOL] self . env . step ( [number] ) [EOL] [EOL] self . lives = self . ale . lives ( ) [EOL] if self . grayscale_obs : [EOL] self . ale . getScreenGrayscale ( self . obs_buffer [ [number] ] ) [EOL] else : [EOL] self . ale . getScreenRGB2 ( self . obs_buffer [ [number] ] ) [EOL] self . obs_buffer [ [number] ] . fill ( [number] ) [EOL] return self . _get_obs ( ) [EOL] [EOL] def _get_obs ( self ) : [EOL] import cv2 [EOL] [EOL] if self . frame_skip > [number] : [comment] [EOL] np . maximum ( self . obs_buffer [ [number] ] , self . obs_buffer [ [number] ] , out = self . obs_buffer [ [number] ] ) [EOL] obs = cv2 . resize ( self . obs_buffer [ [number] ] , ( self . screen_size , self . screen_size ) , interpolation = cv2 . INTER_AREA , ) [EOL] obs = np . asarray ( obs , dtype = np . uint8 ) [EOL] return obs [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
from typing import Any , Tuple [EOL] import typing [EOL] [docstring] [EOL] [comment] [EOL] [EOL] import collections [EOL] [EOL] import gym [EOL] import numpy as np [EOL] [EOL] [EOL] class FrameStack ( gym . Wrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , env , stack_size = [number] ) : [EOL] super ( FrameStack , self ) . __init__ ( env ) [EOL] self . _env = env [EOL] self . _frames = collections . deque ( maxlen = stack_size ) [EOL] space = self . _env . observation_space [EOL] shape = ( stack_size , ) + space . shape [ [number] : [number] ] [EOL] self . observation_space = gym . spaces . Box ( low = [number] , high = [number] , shape = shape , dtype = space . dtype ) [EOL] [EOL] self . stack_size = stack_size [EOL] [EOL] def __getattr__ ( self , name ) : [EOL] [docstring] [EOL] return getattr ( self . _env , name ) [EOL] [EOL] def _generate_observation ( self ) : [EOL] return np . array ( self . _frames ) [EOL] [EOL] def reset ( self ) : [EOL] observation = self . _env . reset ( ) [EOL] for _ in range ( self . stack_size ) : [EOL] self . _frames . append ( observation ) [EOL] return self . _generate_observation ( ) [EOL] [EOL] def step ( self , action ) : [EOL] observation , reward , done , info = self . _env . step ( action ) [EOL] self . _frames . append ( observation ) [EOL] return self . _generate_observation ( ) , reward , done , info [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,...]$ 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Any,...]$ 0 $typing.Tuple[typing.Any,...]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL]	0 0