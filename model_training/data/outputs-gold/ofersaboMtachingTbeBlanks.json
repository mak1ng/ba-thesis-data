from typing import List [EOL] import typing [EOL] import json [EOL] import shutil [EOL] import sys [EOL] [EOL] from allennlp . commands import main [EOL] [EOL] model_location = [string] [EOL] output_file = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [docstring] [EOL] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , model_location , [string] , [string] , model_location + output_file , [string] , [string] , [string] , [string] ] [EOL] [EOL] main ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import numpy as np [EOL] data = [ [ [number] , [number] , [number] , [number] , [number] ] ] [EOL] label = [ [number] ] [EOL] [EOL] sum_loss = [number] [EOL] for x , l in zip ( data , label ) : [EOL] x = np . array ( x ) [EOL] x -= np . max ( x ) [EOL] print ( x ) [EOL] deno = sum ( np . exp ( x ) ) [EOL] proba = np . exp ( x ) / deno [EOL] sum_loss -= np . log ( proba [ l ] ) [EOL] [EOL] print ( sum_loss / len ( label ) )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.float]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.float]]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0
from typing import Any [EOL] import typing [EOL] import subprocess [EOL] import os [EOL] import random [EOL] import subprocess [EOL] import json [EOL] import numpy as np [EOL] accuracy = [ ] [EOL] random_seed = random . randint ( [number] , [number] ) [EOL] final_file_name = [string] [EOL] for i in range ( [number] ) : [EOL] os . system ( [string] + str ( i + random_seed ) + [string] ) [EOL] x = subprocess . Popen ( [string] , shell = True ) [EOL] [EOL] out , err = x . communicate ( ) [EOL] with open ( [string] ) as f : [EOL] data = json . load ( f ) [EOL] accuracy . append ( data [ [string] ] ) [EOL] [EOL] [EOL] accuracy = np . array ( accuracy ) [EOL] with open ( final_file_name , [string] ) as f : [EOL] acc = [string] + str ( np . mean ( accuracy ) ) [EOL] std = [string] + str ( np . std ( accuracy ) ) [EOL] f . write ( str ( accuracy ) + [string] ) [EOL] f . write ( acc + [string] ) [EOL] f . write ( std ) [EOL] [EOL] print ( acc ) [EOL] print ( std )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $subprocess.Popen[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $subprocess.Popen[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0
[comment] [EOL] from my_library . dataset_readers import * [EOL] from my_library . models import * [EOL] from my_library . predictors import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Union , Any , List , Dict [EOL] import logging [EOL] import torch [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Dict , List [EOL] import collections [EOL] import logging [EOL] import math [EOL] import allennlp [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . modules import FeedForward , Seq2VecEncoder , TextFieldEmbedder [EOL] from pytorch_pretrained_bert import BertForQuestionAnswering as HuggingFaceBertQA [EOL] from pytorch_pretrained_bert import BertModel as BertModel [EOL] from pytorch_pretrained_bert import BertConfig [EOL] from pytorch_pretrained_bert . tokenization import BasicTokenizer [EOL] from allennlp . nn . regularizers . regularizers import L2Regularizer [EOL] from torch . autograd import Variable [EOL] from allennlp . common import JsonDict [EOL] from allennlp . models . model import Model [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . modules . span_extractors import EndpointSpanExtractor [EOL] from allennlp . training . metrics import CategoricalAccuracy [EOL] from my_library . dataset_readers . mtb_reader import head_start_token , tail_start_token [EOL] from allennlp . nn import InitializerApplicator , RegularizerApplicator [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] BERT_LARGE_CONFIG = { [string] : [number] , [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } [EOL] [EOL] BERT_BASE_CONFIG = { [string] : [number] , [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } [EOL] linear = [string] [EOL] [EOL] @ Model . register ( [string] ) class BertEmbeddingsMTB ( Model ) : [EOL] def __init__ ( self , vocab , text_field_embedder , number_of_linear_layers = [number] , metrics = None , renorm_method = None , skip_connection = False , regularizer = None , bert_model = None , ) : [EOL] super ( ) . __init__ ( vocab , regularizer ) [EOL] self . embbedings = text_field_embedder [EOL] self . bert_type_model = BERT_BASE_CONFIG if [string] in bert_model else BERT_LARGE_CONFIG [EOL] self . extractor = EndpointSpanExtractor ( input_dim = self . bert_type_model [ [string] ] , combination = [string] ) [EOL] self . crossEntropyLoss = torch . nn . CrossEntropyLoss ( ) [EOL] self . device = torch . device ( [string] if torch . cuda . is_available ( ) else [string] ) [EOL] self . metrics = metrics or { [string] : CategoricalAccuracy ( ) } [EOL] self . first_liner_layer = torch . nn . Linear ( self . bert_type_model [ [string] ] * [number] , self . bert_type_model [ [string] ] * [number] ) [EOL] self . second_liner_layer = torch . nn . Linear ( self . bert_type_model [ [string] ] * [number] , self . bert_type_model [ [string] ] * [number] ) [EOL] self . do_skip_connection = skip_connection [EOL] [EOL] self . number_of_linear_layers = number_of_linear_layers [EOL] self . relation_layer_norm = torch . nn . LayerNorm ( torch . Size ( [ self . bert_type_model [ [string] ] * [number] ] ) , elementwise_affine = True ) [EOL] self . head_token_index = [number] [comment] [EOL] self . tail_token_index = [number] [EOL] self . tanh = torch . nn . Tanh ( ) [EOL] self . drop_layer = torch . nn . Dropout ( p = [number] ) [EOL] self . renorm_method = renorm_method or linear [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] @ overrides def forward ( self , sentences , test , clean_tokens , test_clean_text , label = None ) : [EOL] [EOL] tensor_of_matrices = torch . zeros ( [number] , [number] , self . bert_type_model [ [string] ] * [number] ) . to ( self . device ) [EOL] test_matrix = torch . zeros ( [number] , self . bert_type_model [ [string] ] * [number] ) . to ( self . device ) [EOL] [EOL] bert_context_for_relation = self . embbedings ( sentences ) [EOL] test_bert = self . embbedings ( test ) [EOL] [comment] [EOL] [EOL] for batch_input in range ( bert_context_for_relation . size ( [number] ) ) : [EOL] matrix_all_N_relation = torch . zeros ( [number] , self . bert_type_model [ [string] ] * [number] ) . to ( self . device ) [EOL] for i in range ( bert_context_for_relation . size ( [number] ) ) : [EOL] [EOL] head , tail = self . get_head_tail_locations ( sentences [ [string] ] [ batch_input , i , [number] , : ] ) [EOL] concat_represntentions = self . extract_embeddings_of_start_tokens ( bert_context_for_relation , i , batch_input , head , tail ) [EOL] [EOL] final_represnetation = self . renorm_vector ( concat_represntentions ) [EOL] matrix_all_N_relation = torch . cat ( ( matrix_all_N_relation , final_represnetation ) , [number] ) . to ( self . device ) [EOL] [EOL] matrix_all_N_relation = matrix_all_N_relation . unsqueeze ( [number] ) [EOL] tensor_of_matrices = torch . cat ( ( tensor_of_matrices , matrix_all_N_relation ) , [number] ) . to ( self . device ) [EOL] [EOL] [comment] [EOL] head , tail = self . get_head_tail_locations ( test [ [string] ] [ batch_input , : ] ) [EOL] [comment] [EOL] [EOL] test_concat = self . extract_embeddings_of_start_tokens ( test_bert , i , batch_input , head , tail ) [EOL] final_query_representation = self . renorm_vector ( test_concat ) [EOL] [EOL] test_matrix = torch . cat ( ( test_matrix , final_query_representation ) , [number] ) . to ( self . device ) [EOL] [EOL] test_matrix = test_matrix . unsqueeze ( [number] ) [EOL] tensor_of_matrices = tensor_of_matrices . permute ( [number] , [number] , [number] ) [EOL] scores = torch . matmul ( test_matrix , tensor_of_matrices ) . squeeze ( [number] ) . to ( self . device ) [EOL] output_dict = { [string] : scores } [EOL] if label is not None : [EOL] label = label . squeeze ( [number] ) [EOL] loss = self . crossEntropyLoss ( scores , label ) [EOL] output_dict [ [string] ] = loss [EOL] for metric in self . metrics . values ( ) : [EOL] metric ( scores , label ) [EOL] [EOL] return output_dict [EOL] [EOL] def debug_query_sentence ( self , test , test_bert , head , tail , batch_input , i ) : [EOL] if head > test_bert . size ( [number] ) or tail > test_bert . size ( [number] ) : [EOL] logger . warning ( [string] ) [EOL] toekns_list = self . reassemble_sentence_for_debug ( test , batch_input , i ) [EOL] test_bert = self . embbedings ( test ) [EOL] return test_bert [EOL] [EOL] def renorm_vector ( self , concat_represntentions ) : [EOL] if self . renorm_method != linear : [EOL] return torch . renorm ( concat_represntentions , [number] , [number] , [number] ) [EOL] [EOL] [comment] [EOL] x = self . first_liner_layer ( concat_represntentions ) [EOL] x = self . tanh ( x ) [EOL] x = self . drop_layer ( x ) [EOL] x = self . second_liner_layer ( x ) [EOL] if self . do_skip_connection : [EOL] x = x + concat_represntentions [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] return x [EOL] [EOL] def debug_issue ( self , bert_context_for_relation , sentences , test , test_bert ) : [EOL] if bert_context_for_relation . size ( - [number] ) != sentences [ [string] ] . size ( - [number] ) : [EOL] bert_context_for_relation = self . embbedings ( sentences ) [EOL] logger . warning ( [string] ) [EOL] exit ( ) [EOL] if test_bert . size ( - [number] ) != test [ [string] ] . size ( - [number] ) : [EOL] logger . warning ( [string] ) [EOL] test_bert = self . embbedings ( test ) [EOL] exit ( ) [EOL] [EOL] def reassemble_sentence_for_debug ( self , sentences , batch_input , i ) : [EOL] index_to_token = self . vocab . _index_to_token [ [string] ] [EOL] try : [EOL] this_sentence = sentences [ [string] ] [ batch_input , i , [number] , : ] [EOL] except IndexError : [EOL] this_sentence = sentences [ [string] ] [ batch_input ] [EOL] [EOL] tokens = [ ] [EOL] for i in this_sentence : [EOL] tokens . append ( index_to_token [ i . item ( ) ] ) [EOL] return tokens [EOL] [EOL] def get_head_tail_locations ( self , sentence ) : [EOL] head = None [EOL] tail = None [EOL] for i , index_value in enumerate ( sentence ) : [EOL] if index_value . item ( ) == self . tail_token_index : [EOL] assert tail is None [EOL] tail = i [EOL] if head is not None : [EOL] return head , tail [EOL] if index_value . item ( ) == self . head_token_index : [EOL] assert head is None [EOL] head = i [EOL] if tail is not None : [EOL] return head , tail [EOL] [EOL] return head , tail [EOL] [EOL] def assert_head_tail_correct_location ( self , batch_input , clean_tokens , head , i , tail ) : [EOL] assert clean_tokens [ batch_input ] [ i ] [ [number] ] [ tail ] . text == tail_start_token [comment] [EOL] assert clean_tokens [ batch_input ] [ i ] [ [number] ] [ head ] . text == head_start_token [EOL] [EOL] def extract_embeddings_of_start_tokens ( self , relation_representation , i , batch_input , head , tail ) : [EOL] indices = Variable ( torch . LongTensor ( [ [ head , tail ] ] ) ) . to ( self . device ) [EOL] try : [EOL] x = relation_representation [ batch_input , i , : , : , : ] . to ( self . device ) [EOL] length_of_seq = x . size ( [number] ) [EOL] except IndexError : [EOL] x = relation_representation [ batch_input , : , : ] . to ( self . device ) [EOL] length_of_seq = x . size ( [number] ) [EOL] [EOL] assert length_of_seq > head [EOL] assert length_of_seq > tail [EOL] concat_represntentions = self . extractor ( x , indices ) . to ( self . device ) [EOL] return concat_represntentions [EOL] [EOL] @ overrides def get_metrics ( self , reset = False ) : [EOL] return { metric_name : metric . get_metric ( reset ) for metric_name , metric in self . metrics . items ( ) } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import allennlp [EOL] from typing import Dict [EOL] from typing import List [EOL] import json [EOL] import logging [EOL] [EOL] from overrides import overrides [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import LabelField , TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . tokenizers import Tokenizer , WordTokenizer [EOL] from allennlp . data . token_indexers import TokenIndexer , SingleIdTokenIndexer [EOL] from allennlp . data . fields import ListField , IndexField , MetadataField , Field [EOL] [EOL] head_start_token = [string] [comment] [EOL] head_end_token = [string] [comment] [EOL] tail_start_token = [string] [EOL] tail_end_token = [string] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] def find_closest_distance_between_entities ( head_start_location , head_end_location , tail_start_location , tail_end_location ) : [EOL] min_distance = [number] [EOL] for i , x in enumerate ( head_start_location ) : [EOL] for j , y in enumerate ( tail_start_location ) : [EOL] if abs ( x - y ) < min_distance : [EOL] min_distance = abs ( x - y ) [EOL] h_start , h_end , t_start , t_end = x , head_end_location [ i ] , y , tail_end_location [ j ] [EOL] [EOL] return h_start , h_end , t_start , t_end [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class MTBDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , lazy = False , tokenizer = None , token_indexers = None ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . spacy_splitter = SpacyWordSplitter ( keep_spacy_tokens = True ) [EOL] self . TRAIN_DATA = [string] [EOL] self . TEST_DATA = [string] [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] with open ( cached_path ( file_path ) , [string] ) as data_file : [EOL] logger . info ( [string] , data_file ) [EOL] data = json . load ( data_file ) [EOL] labels = data [ [number] ] [EOL] data = data [ [number] ] [EOL] for x , l in zip ( data , labels ) : [EOL] yield self . text_to_instance ( x , l ) [EOL] [EOL] @ overrides def text_to_instance ( self , data , relation_type = None ) : [comment] [EOL] [comment] [EOL] N_relations = [ ] [EOL] location_list = [ ] [EOL] all_tokens_sentences = [ ] [EOL] for i , K_examples in enumerate ( data [ self . TRAIN_DATA ] ) : [EOL] toknized_sentences = [ ] [EOL] sentences_location = [ ] [EOL] clean_text_for_debug = [ ] [EOL] for relation in K_examples : [EOL] tokenized_tokens = self . _tokenizer . tokenize ( [string] . join ( relation [ [string] ] ) ) [EOL] head_location , tail_location = self . addStartEntityTokens ( tokenized_tokens , relation [ [string] ] , relation [ [string] ] ) [EOL] [EOL] assert tokenized_tokens [ head_location ] . text == head_start_token [EOL] assert tokenized_tokens [ tail_location ] . text == tail_start_token [EOL] [EOL] field_of_tokens = TextField ( tokenized_tokens , self . _token_indexers ) [EOL] locations_of_entities = MetadataField ( { [string] : head_location , [string] : tail_location } ) [EOL] clean_text_for_debug . append ( MetadataField ( tokenized_tokens ) ) [EOL] [EOL] sentences_location . append ( locations_of_entities ) [EOL] toknized_sentences . append ( field_of_tokens ) [EOL] assert len ( sentences_location ) == len ( toknized_sentences ) == len ( clean_text_for_debug ) [EOL] [EOL] sentences_location = ListField ( sentences_location ) [EOL] clean_text_for_debug = ListField ( clean_text_for_debug ) [EOL] toknized_sentences = ListField ( toknized_sentences ) [EOL] [EOL] all_tokens_sentences . append ( clean_text_for_debug ) [EOL] location_list . append ( sentences_location ) [EOL] N_relations . append ( toknized_sentences ) [EOL] [EOL] assert len ( N_relations ) == len ( location_list ) == len ( all_tokens_sentences ) [EOL] N_relations = ListField ( N_relations ) [EOL] location_list = ListField ( location_list ) [EOL] all_tokens_sentences = ListField ( all_tokens_sentences ) [EOL] fields = { [string] : N_relations , [string] : location_list , [string] : all_tokens_sentences } [EOL] [EOL] test_dict = data [ self . TEST_DATA ] [EOL] tokenized_tokens = self . _tokenizer . tokenize ( [string] . join ( test_dict [ [string] ] ) ) [EOL] head_location , tail_location = self . addStartEntityTokens ( tokenized_tokens , test_dict [ [string] ] , test_dict [ [string] ] ) [EOL] test_clean_text_for_debug = MetadataField ( tokenized_tokens ) [EOL] locations_of_entities = MetadataField ( { [string] : head_location , [string] : tail_location } ) [EOL] field_of_tokens = TextField ( tokenized_tokens , self . _token_indexers ) [EOL] [EOL] fields [ [string] ] = field_of_tokens [EOL] fields [ [string] ] = locations_of_entities [EOL] fields [ [string] ] = test_clean_text_for_debug [EOL] [EOL] if relation_type is not None : [EOL] fields [ [string] ] = IndexField ( relation_type , N_relations ) [EOL] return Instance ( fields ) [EOL] [EOL] def addStartEntityTokens ( self , tokens_list , head_full_data , tail_full_data ) : [EOL] if len ( head_full_data [ [number] ] ) > len ( tail_full_data [ [number] ] ) : [comment] [EOL] [comment] [EOL] [comment] [EOL] head_start_location , head_end_location = self . find_locations ( head_full_data , tokens_list ) [EOL] tail_start_location , tail_end_location = self . find_locations ( tail_full_data , tokens_list ) [EOL] if tail_start_location [ [number] ] >= head_start_location [ [number] ] and tail_start_location [ [number] ] <= head_end_location [ [number] ] : [EOL] tail_end_location , tail_start_location = self . deny_overlapping ( tokens_list , head_end_location , tail_full_data ) [EOL] [EOL] else : [EOL] tail_start_location , tail_end_location = self . find_locations ( tail_full_data , tokens_list ) [EOL] head_start_location , head_end_location = self . find_locations ( head_full_data , tokens_list ) [EOL] if head_start_location [ [number] ] >= tail_start_location [ [number] ] and head_start_location [ [number] ] <= tail_end_location [ [number] ] : [EOL] head_end_location , head_start_location = self . deny_overlapping ( tokens_list , tail_end_location , head_full_data ) [EOL] [EOL] [comment] [EOL] h_start_location , head_end_location , tail_start_location , tail_end_location = find_closest_distance_between_entities ( head_start_location , head_end_location , tail_start_location , tail_end_location ) [EOL] [EOL] x = self . _tokenizer . tokenize ( head_start_token ) [EOL] y = self . _tokenizer . tokenize ( head_end_token ) [EOL] z = self . _tokenizer . tokenize ( tail_start_token ) [EOL] w = self . _tokenizer . tokenize ( tail_end_token ) [EOL] [EOL] offset_tail = [number] * ( tail_start_location > h_start_location ) [EOL] tokens_list . insert ( h_start_location , x [ [number] ] ) [comment] [EOL] tokens_list . insert ( head_end_location + [number] + [number] , y [ [number] ] ) [comment] [EOL] tokens_list . insert ( tail_start_location + offset_tail , z [ [number] ] ) [comment] [EOL] tokens_list . insert ( tail_end_location + [number] + offset_tail , w [ [number] ] ) [comment] [EOL] [EOL] return h_start_location + [number] - offset_tail , tail_start_location + offset_tail [EOL] [EOL] def deny_overlapping ( self , tokens_list , longest_entity_end_location , shortest_entity_full_data ) : [EOL] start_location , end_location = self . find_locations ( shortest_entity_full_data , tokens_list [ longest_entity_end_location [ [number] ] + [number] : ] ) [EOL] start_location [ [number] ] = start_location [ [number] ] + longest_entity_end_location [ [number] ] [EOL] end_location [ [number] ] = end_location [ [number] ] + longest_entity_end_location [ [number] ] [EOL] return end_location , start_location [EOL] [EOL] def return_lower_text_from_tokens ( self , tokens ) : [EOL] return list ( map ( lambda x : x . text . lower ( ) , tokens ) ) [EOL] [EOL] def compare_two_token_lists ( self , x , y ) : [EOL] return self . return_lower_text_from_tokens ( x ) == self . return_lower_text_from_tokens ( y ) [EOL] [EOL] def spacy_work_toknizer ( self , text ) : [EOL] return list ( map ( lambda x : x . text , self . spacy_splitter . split_words ( text ) ) ) [EOL] [EOL] def find_locations ( self , head_full_data , token_list ) : [EOL] end_location , start_location = self . _find_entity_name ( token_list , head_full_data ) [EOL] if len ( end_location ) == [number] or len ( start_location ) == [number] : [EOL] end_location , start_location = self . _find_entity_name ( token_list , head_full_data , True ) [EOL] [EOL] assert len ( start_location ) == len ( end_location ) [EOL] assert len ( start_location ) == len ( head_full_data [ [number] ] ) [EOL] [EOL] return start_location , end_location [EOL] [EOL] def _find_entity_name ( self , token_list , head_full_data , use_spacy_toknizer_before = False ) : [EOL] if use_spacy_toknizer_before : [EOL] spacy_head_tokens = self . spacy_work_toknizer ( head_full_data [ [number] ] ) [EOL] head = self . _tokenizer . tokenize ( [string] . join ( spacy_head_tokens ) ) [EOL] else : [EOL] head = self . _tokenizer . tokenize ( [string] . join ( [ head_full_data [ [number] ] ] ) ) [EOL] start_head_entity_name = head [ [number] ] [EOL] start_location = [ ] [EOL] end_location = [ ] [EOL] for i , token in enumerate ( token_list ) : [EOL] if self . compare_two_token_lists ( [ token ] , [ start_head_entity_name ] ) : [EOL] if self . compare_two_token_lists ( token_list [ i : i + len ( head ) ] , head ) : [EOL] start_location . append ( i ) [EOL] end_location . append ( i + len ( head ) - [number] ) [EOL] if len ( start_location ) == len ( head_full_data [ [number] ] ) : [EOL] break [EOL] return end_location , start_location [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.bool$ 0 0 0 $allennlp.data.tokenizers.Tokenizer$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $allennlp.data.tokenizers.Tokenizer$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.dict$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0
	0
from typing import Any , Dict [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import allennlp [EOL] from typing import Dict [EOL] from typing import List [EOL] import json [EOL] import logging [EOL] [EOL] from overrides import overrides [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import LabelField , TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . tokenizers import Tokenizer , WordTokenizer [EOL] from allennlp . data . token_indexers import TokenIndexer , SingleIdTokenIndexer [EOL] from allennlp . data . fields import ListField , IndexField , MetadataField , Field [EOL] [EOL] head_start_token = [string] [comment] [EOL] head_end_token = [string] [comment] [EOL] tail_start_token = [string] [EOL] tail_end_token = [string] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] def find_closest_distance_between_entities ( head_start_location , head_end_location , tail_start_location , tail_end_location ) : [EOL] min_distance = [number] [EOL] for i , x in enumerate ( head_start_location ) : [EOL] for j , y in enumerate ( tail_start_location ) : [EOL] if abs ( x - y ) < min_distance : [EOL] min_distance = abs ( x - y ) [EOL] h_start , h_end , t_start , t_end = x , head_end_location [ i ] , y , tail_end_location [ j ] [EOL] [EOL] return h_start , h_end , t_start , t_end [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class OnlyEntitiesDatasetReader ( DatasetReader ) : [EOL] def __init__ ( self , lazy = False , tokenizer = None , single_entity = None , token_indexers = None ) : [EOL] super ( ) . __init__ ( lazy ) [EOL] self . _tokenizer = tokenizer or WordTokenizer ( ) [EOL] self . _token_indexers = token_indexers or { [string] : SingleIdTokenIndexer ( ) } [EOL] self . spacy_splitter = SpacyWordSplitter ( keep_spacy_tokens = True ) [EOL] self . TRAIN_DATA = [string] [EOL] self . TEST_DATA = [string] [EOL] self . single_entity = single_entity [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] with open ( cached_path ( file_path ) , [string] ) as data_file : [EOL] logger . info ( [string] , data_file ) [EOL] data = json . load ( data_file ) [EOL] labels = data [ [number] ] [EOL] data = data [ [number] ] [EOL] for x , l in zip ( data , labels ) : [EOL] yield self . text_to_instance ( x , l ) [EOL] [EOL] @ overrides def text_to_instance ( self , data , relation_type = None ) : [comment] [EOL] [comment] [EOL] N_relations = [ ] [EOL] all_tokens_sentences = [ ] [EOL] for i , K_examples in enumerate ( data [ self . TRAIN_DATA ] ) : [EOL] toknized_sentences = [ ] [EOL] clean_text_for_debug = [ ] [EOL] for relation in K_examples : [EOL] head_tail = self . create_head_tail_sentence ( relation ) [EOL] tokenized_tokens = self . _tokenizer . tokenize ( head_tail ) [EOL] [EOL] field_of_tokens = TextField ( tokenized_tokens , self . _token_indexers ) [EOL] clean_text_for_debug . append ( MetadataField ( tokenized_tokens ) ) [EOL] [EOL] toknized_sentences . append ( field_of_tokens ) [EOL] assert len ( toknized_sentences ) == len ( clean_text_for_debug ) [EOL] [EOL] clean_text_for_debug = ListField ( clean_text_for_debug ) [EOL] toknized_sentences = ListField ( toknized_sentences ) [EOL] [EOL] all_tokens_sentences . append ( clean_text_for_debug ) [EOL] N_relations . append ( toknized_sentences ) [EOL] [EOL] assert len ( N_relations ) == len ( all_tokens_sentences ) [EOL] N_relations = ListField ( N_relations ) [EOL] all_tokens_sentences = ListField ( all_tokens_sentences ) [EOL] fields = { [string] : N_relations , [string] : all_tokens_sentences } [EOL] [EOL] test_dict = data [ self . TEST_DATA ] [EOL] head_tail = self . create_head_tail_sentence ( test_dict ) [EOL] tokenized_tokens = self . _tokenizer . tokenize ( head_tail ) [EOL] test_clean_text_for_debug = MetadataField ( tokenized_tokens ) [EOL] field_of_tokens = TextField ( tokenized_tokens , self . _token_indexers ) [EOL] [EOL] fields [ [string] ] = field_of_tokens [EOL] fields [ [string] ] = test_clean_text_for_debug [EOL] [EOL] if relation_type is not None : [EOL] fields [ [string] ] = IndexField ( relation_type , N_relations ) [EOL] return Instance ( fields ) [EOL] [EOL] def create_head_tail_sentence ( self , relation ) : [EOL] if self . single_entity is None : [EOL] return head_start_token + [string] + relation [ [string] ] [ [number] ] + [string] + head_end_token + [string] + tail_start_token + [string] + relation [ [string] ] [ [number] ] + [string] + tail_end_token [EOL] elif self . single_entity == [string] : [EOL] return head_start_token + [string] + relation [ [string] ] [ [number] ] + [string] + head_end_token + [string] + tail_start_token + [string] + [string] + [string] + tail_end_token [EOL] elif self . single_entity == [string] : [EOL] return head_start_token + [string] + [string] + [string] + head_end_token + [string] + tail_start_token + [string] + relation [ [string] ] [ [number] ] + [string] + tail_end_token [EOL] else : [EOL] raise AttributeError ( [string] . format ( self . single_entity ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.TokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.dict$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.dict$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.dict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import builtins [EOL] import io [EOL] import allennlp [EOL] from overrides import overrides [EOL] import numpy as np [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] from allennlp . predictors . predictor import Predictor [EOL] import json [EOL] [EOL] json_file = open ( [string] , [string] ) [EOL] id2rel_name = json . load ( json_file ) [EOL] def softmax ( x ) : [EOL] x = np . array ( x ) [EOL] x -= np . max ( x ) [EOL] deno = sum ( np . exp ( x ) ) [EOL] proba = np . exp ( x ) / deno [EOL] np . set_printoptions ( precision = [number] ) [EOL] new_datalist = [ [string] . format ( value ) for value in proba ] [EOL] return str ( new_datalist ) [EOL] [EOL] @ Predictor . register ( [string] ) class MTBClassifierPredictor ( Predictor ) : [EOL] def predict_json ( self , inputs ) : [EOL] mapping_set_index_to_realtion_type , gold_pred = self . extract_mapping_and_correct_answer ( inputs ) [EOL] output_dict = { } [EOL] [EOL] instance = self . _json_to_instance ( inputs ) [EOL] scores = self . predict_instance ( instance ) [ [string] ] [EOL] prediction = np . argmax ( scores ) [EOL] answer = [string] if prediction == gold_pred else [string] [EOL] output_dict [ [string] ] = answer [EOL] output_dict [ [string] ] = str ( prediction ) [EOL] output_dict [ [string] ] = str ( gold_pred ) [EOL] output_dict [ [string] ] = str ( softmax ( scores ) ) [EOL] output_dict [ [string] ] = scores [EOL] [EOL] for i , relation in enumerate ( inputs [ self . _dataset_reader . TRAIN_DATA ] ) : [EOL] relation = relation [ [number] ] [EOL] output_dict [ [string] + str ( i ) ] = [string] . join ( relation [ [string] ] ) [EOL] output_dict [ [string] + str ( i ) + [string] ] = relation [ [string] ] [ [number] ] [EOL] output_dict [ [string] + str ( i ) + [string] ] = relation [ [string] ] [ [number] ] [EOL] try : [EOL] rel_name = id2rel_name [ mapping_set_index_to_realtion_type [ i ] . lower ( ) ] [EOL] except KeyError : [EOL] rel_name = mapping_set_index_to_realtion_type [ i ] [EOL] output_dict [ [string] + str ( i ) + [string] ] = rel_name [EOL] [EOL] relation = inputs [ self . _dataset_reader . TEST_DATA ] [EOL] output_dict [ [string] ] = [string] . join ( relation [ [string] ] ) [EOL] output_dict [ [string] ] = relation [ [string] ] [ [number] ] [EOL] output_dict [ [string] ] = relation [ [string] ] [ [number] ] [EOL] try : [EOL] rel_name = id2rel_name [ mapping_set_index_to_realtion_type [ gold_pred ] . lower ( ) ] [EOL] except KeyError : [EOL] rel_name = mapping_set_index_to_realtion_type [ gold_pred ] [EOL] [EOL] true_relation = rel_name [EOL] output_dict [ [string] ] = true_relation [EOL] [EOL] return output_dict [EOL] [EOL] def extract_mapping_and_correct_answer ( self , inputs ) : [EOL] this_set_mapping = [ ] [EOL] for i , examples in enumerate ( inputs [ self . _dataset_reader . TRAIN_DATA ] ) : [EOL] this_set_mapping . append ( examples [ [number] ] ) [EOL] inputs [ self . _dataset_reader . TRAIN_DATA ] [ i ] = examples [ [number] ] [EOL] x = inputs [ self . _dataset_reader . TEST_DATA ] [EOL] correct = x [ [number] ] [EOL] inputs [ self . _dataset_reader . TEST_DATA ] = x [ [number] ] [EOL] return this_set_mapping , correct [EOL] [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [comment] [EOL] this_instance = self . _dataset_reader . text_to_instance ( data = json_dict ) [EOL] return this_instance [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $builtins.str$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $typing.Any$ 0
from my_library . predictors . fewrel_analyze_predictor import MTBClassifierPredictor [EOL]	0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import allennlp [EOL] from overrides import overrides [EOL] import numpy as np [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class MTBClassifierPredictor ( Predictor ) : [EOL] def predict_json ( self , inputs ) : [EOL] [comment] [EOL] output_dict = { } [EOL] [EOL] instance = self . _json_to_instance ( inputs ) [EOL] scores = self . predict_instance ( instance ) [ [string] ] [EOL] prediction = np . argmax ( scores ) [EOL] output_dict [ [string] ] = prediction [EOL] [EOL] return output_dict [EOL] [EOL] def extract_mapping_and_correct_answer ( self , inputs ) : [EOL] this_set_mapping = [ ] [EOL] for i , examples in enumerate ( inputs [ self . _dataset_reader . TRAIN_DATA ] ) : [EOL] this_set_mapping . append ( examples [ [number] ] ) [EOL] inputs [ self . _dataset_reader . TRAIN_DATA ] [ i ] = examples [ [number] ] [EOL] x = inputs [ self . _dataset_reader . TEST_DATA ] [EOL] correct = x [ [number] ] [EOL] inputs [ self . _dataset_reader . TEST_DATA ] = x [ [number] ] [EOL] return this_set_mapping , correct [EOL] [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [comment] [EOL] this_instance = self . _dataset_reader . text_to_instance ( data = json_dict ) [EOL] return this_instance [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $typing.Any$ 0
	0