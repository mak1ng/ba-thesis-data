from typing import List [EOL] import typing [EOL] from setuptools import setup , find_packages [EOL] import os [EOL] from os import path [EOL] [EOL] [comment] [EOL] NAME = [string] [EOL] [comment] [EOL] VERSION = os . environ . get ( [string] , [string] ) [EOL] [EOL] [comment] [EOL] with open ( [string] ) as r : [EOL] DEPENDENCIES = [ dep for dep in map ( str . strip , r . readlines ( ) ) if all ( [ not dep . startswith ( [string] ) , not dep . endswith ( [string] ) , len ( dep ) > [number] ] ) ] [EOL] [EOL] [comment] [EOL] with open ( [string] ) as f : [EOL] LONG_DESCRIPTION = f . read ( ) [EOL] [EOL] [EOL] setup ( name = NAME , version = VERSION , description = [string] , long_description = LONG_DESCRIPTION , author = [string] , author_email = [string] , license = [string] , packages = find_packages ( exclude = ( [string] , [string] ) ) , entry_points = { [string] : [ [string] . format ( name = NAME ) ] , } , install_requires = DEPENDENCIES ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0
from typing import Any [EOL] import typing [EOL] import pandas [EOL] import pandas as pd [EOL] from sklearn . cluster import MiniBatchKMeans [EOL] [EOL] from squad_3_ad_data_science import config [EOL] [EOL] [EOL] def create_mini_batch_kmeans ( input_data , return_labels = False ) : [EOL] [docstring] [EOL] [EOL] model = MiniBatchKMeans ( n_clusters = config . N_CLUSTERS , batch_size = [number] ) [EOL] labels = model . fit_predict ( input_data ) [EOL] [EOL] if return_labels : [EOL] return ( model , labels ) [EOL] [EOL] else : [EOL] return model [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] import builtins [EOL] import pandas [EOL] import pandas as pd [EOL] from loguru import logger [EOL] from sklearn . metrics . pairwise import cosine_similarity [EOL] [EOL] from squad_3_ad_data_science import config [EOL] [EOL] [EOL] def make_recomendation ( market_list , user_ids , use_clusters = False , use_sp_labels = True ) : [EOL] [docstring] [EOL] [EOL] user_rows = market_list . loc [ user_ids ] [EOL] market_no_user = market_list . drop ( labels = user_ids ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if use_clusters : [EOL] logger . info ( f' [string] ' ) [EOL] [comment] [EOL] if config . CLUSTER_LABEL not in market_list . columns : [EOL] logger . error ( [string] + f' [string] { config . CLUSTER_LABEL } [string] ' ) [EOL] exit ( [number] ) [EOL] [EOL] user_labels = user_rows [ config . CLUSTER_LABEL ] . value_counts ( ) . index [EOL] [comment] [EOL] [comment] [EOL] market_no_user = market_no_user [ [ d in user_labels for d in market_no_user [ config . CLUSTER_LABEL ] ] ] [EOL] [EOL] [comment] [EOL] market_no_user . drop ( config . CLUSTER_LABEL , inplace = True , axis = [number] ) [EOL] user_rows . drop ( config . CLUSTER_LABEL , inplace = True , axis = [number] ) [EOL] if use_sp_labels : [EOL] [comment] [EOL] [comment] [EOL] logger . info ( f' [string] ' + f' [string] { config . SPECIAL_LABELS }' ) [EOL] [EOL] for col in config . SPECIAL_LABELS : [EOL] sp = [string] + col [EOL] user_sp_label = list ( user_rows [ sp ] . unique ( ) ) [EOL] selection = market_no_user [ sp ] . isin ( user_sp_label ) [EOL] market_no_user = market_no_user . loc [ selection ] [EOL] [EOL] [comment] [EOL] for col in config . SPECIAL_LABELS : [EOL] user_rows . drop ( [string] + col , axis = [number] ) [EOL] market_no_user . drop ( [string] + col , axis = [number] ) [EOL] [EOL] sim = cosine_similarity ( market_no_user , user_rows ) [EOL] [comment] [EOL] scores = sim . sum ( axis = [number] ) [EOL] [EOL] market_no_user [ [string] ] = scores [EOL] [EOL] market_no_user . sort_values ( by = [ [string] ] , inplace = True , ascending = False ) [EOL] [EOL] return list ( market_no_user . index ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List , Dict [EOL] import typing [EOL] import pandas [EOL] import pandas as pd [EOL] import numpy as np [EOL] import typing [EOL] from loguru import logger [EOL] from sklearn . compose import make_column_transformer [EOL] from sklearn . decomposition import PCA [EOL] from sklearn . preprocessing import StandardScaler , OrdinalEncoder [EOL] [EOL] from squad_3_ad_data_science import config [EOL] from squad_3_ad_data_science . exceptions import ConfigMissException [EOL] [EOL] [EOL] def pipeline_data ( input_data ) : [EOL] [docstring] [EOL] [comment] [EOL] try : [EOL] input_data . set_index ( [string] , inplace = True ) [EOL] [EOL] except KeyError as e : [EOL] logger . error ( [string] ) [EOL] raise e [EOL] [EOL] [comment] [EOL] input_data . drop ( axis = [number] , labels = config . TO_REMOVE , inplace = True ) [EOL] [EOL] [comment] [EOL] removed_columns = [ ] [EOL] for col in input_data . iloc [ : ] : [EOL] nan_pctg = input_data [ col ] . isna ( ) . sum ( ) / input_data [ col ] . shape [ [number] ] [EOL] if nan_pctg > config . NAN_THRESH : [EOL] removed_columns . append ( col ) [EOL] [EOL] input_data . drop ( removed_columns , inplace = True , axis = [number] ) [EOL] [EOL] [comment] [EOL] try : [EOL] for col in config . TO_FIX_OBJ2BOOL : [EOL] input_data [ col ] = input_data [ col ] . astype ( [string] ) [EOL] [EOL] except KeyError as e : [EOL] logger . error ( f' [string] { col } [string] ' + [string] ) [EOL] raise e [EOL] [EOL] [comment] [EOL] try : [EOL] input_data = impute_as_config ( input_data ) [EOL] [EOL] except ConfigMissException as e : [EOL] logger . error ( f' [string] ' ) [EOL] raise e [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] [comment] [EOL] try : [EOL] input_data = manual_ordinal_encoder ( input_data ) [EOL] [EOL] except ConfigMissException : [EOL] logger . error ( f' [string] ' ) [EOL] raise [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] [comment] [EOL] if input_data . isna ( ) . sum ( ) . sum ( ) != [number] : [EOL] cols = input_data . columns [ input_data . isna ( ) . any ( ) ] . tolist ( ) [EOL] [EOL] logger . error ( f' [string] { cols } [string] ' + f' [string] { len ( cols ) } [string] ' ) [EOL] exit ( [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] logger . info ( f' [string] { config . SPECIAL_LABELS } [string] ' ) [EOL] special_labels_df = create_sp_labels ( input_data ) [EOL] [EOL] [comment] [EOL] input_data = pd . get_dummies ( input_data , sparse = True , drop_first = True ) [EOL] [EOL] [comment] [EOL] scaler = StandardScaler ( ) [EOL] input_data [ : ] = scaler . fit_transform ( input_data ) [EOL] [EOL] return input_data , special_labels_df [EOL] [EOL] [EOL] def manual_ordinal_encoder ( input_data ) : [EOL] [docstring] [EOL] [EOL] for col , info in config . ORDINAL_ENCODE . items ( ) : [EOL] try : [EOL] input_data [ col ] [EOL] except KeyError as e : [EOL] logger . error ( f' [string] { col } [string] ' + [string] ) [EOL] raise e [EOL] [EOL] try : [EOL] for key , value in info . items ( ) : [EOL] input_data [ col ] = input_data [ col ] . replace ( key , value ) [EOL] [EOL] except KeyError as e : [EOL] logger . error ( f' [string] ' + f' [string] { col } [string] { info }' ) [EOL] raise e [EOL] [EOL] return input_data [EOL] [EOL] [EOL] def create_sp_labels ( input_data ) : [EOL] [docstring] [EOL] transformer = make_column_transformer ( ( OrdinalEncoder ( ) , config . SPECIAL_LABELS ) , remainder = [string] ) [EOL] [EOL] data = transformer . fit_transform ( input_data ) [EOL] [EOL] return pd . DataFrame ( data = data , index = input_data . index , columns = [ [string] + col for col in config . SPECIAL_LABELS ] ) [EOL] [EOL] [EOL] def impute_as_config ( input_data ) : [EOL] [docstring] [EOL] for col , info in config . NAN_FIXES . items ( ) : [EOL] [comment] [EOL] info = typing . cast ( typing . Dict , info ) [EOL] [EOL] try : [EOL] input_data [ col ] [EOL] except KeyError : [EOL] logger . error ( f' [string] { col } [string] ' + [string] ) [EOL] raise ConfigMissException ( [string] ) [EOL] [EOL] try : [EOL] if info [ [string] ] == [string] : [EOL] input_data [ col ] = input_data [ col ] . fillna ( value = info [ [string] ] ) [EOL] [EOL] elif info [ [string] ] == [string] : [EOL] median = input_data [ col ] . quantile ( ) [EOL] input_data [ col ] = input_data [ col ] . fillna ( value = median ) [EOL] [EOL] elif info [ [string] ] == [string] : [EOL] mean = input_data [ col ] . quantile ( ) [EOL] input_data [ col ] = input_data [ col ] . fillna ( value = mean ) [EOL] [EOL] elif info [ [string] ] == [string] : [EOL] mode = input_data [ col ] . quantile ( ) [EOL] input_data [ col ] = input_data [ col ] . fillna ( value = mode ) [EOL] [EOL] else : [EOL] raise KeyError [EOL] [EOL] except KeyError : [EOL] logger . error ( f' [string] ' + f' [string] { col } [string] { info }' ) [EOL] raise ConfigMissException ( [string] ) [EOL] [EOL] return input_data [EOL] [EOL] [EOL] def select_features ( input_data ) : [EOL] [docstring] [EOL] [EOL] [comment] [EOL] pca = PCA ( ) [EOL] pca . fit_transform ( input_data ) [EOL] [EOL] ratio = pca . explained_variance_ratio_ [EOL] [EOL] counter = [number] [EOL] value = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] while value <= config . EXPLAINED_VARIANCE_RATIO : [EOL] value = np . cumsum ( ratio ) [ counter ] [EOL] counter += [number] [EOL] [EOL] num_components = int ( counter ) [EOL] pca = PCA ( n_components = num_components ) [EOL] input_data = pd . DataFrame ( data = pca . fit_transform ( input_data ) , index = input_data . index ) [EOL] [EOL] return input_data [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] def custom_apk ( market_list , test_list , k = [number] ) : [EOL] [EOL] [docstring] [EOL] [EOL] if len ( market_list ) > k : [EOL] market_list = market_list [ : k ] [EOL] [EOL] score = [number] [EOL] num_hits = [number] [EOL] [EOL] for i , p in enumerate ( market_list ) : [EOL] if p in test_list : [EOL] num_hits += [number] [EOL] score += num_hits / ( i + [number] ) [EOL] [EOL] return score / len ( test_list ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
class ConfigMissException ( Exception ) : [EOL] [docstring] [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Dict , Any [EOL] import typing [EOL] import pickle [comment] [EOL] import json [EOL] from os . path import isfile , join , basename , splitext [EOL] from datetime import datetime [EOL] [EOL] import pandas as pd [EOL] import fire [EOL] from loguru import logger [EOL] from sklearn . model_selection import train_test_split [EOL] [EOL] from squad_3_ad_data_science import config [comment] [EOL] from squad_3_ad_data_science . model_training import create_mini_batch_kmeans [EOL] from squad_3_ad_data_science . feature_engineering import ( pipeline_data , select_features ) [EOL] from squad_3_ad_data_science . recomendations import make_recomendation [EOL] from squad_3_ad_data_science . validation import custom_apk [EOL] [EOL] [EOL] def features ( ** kwargs ) : [EOL] [docstring] [EOL] print ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if not isfile ( config . train_dataset ) : [EOL] logger . error ( f' [string] { config . train_dataset } [string] ' + [string] ) [EOL] [EOL] exit ( [number] ) [EOL] [EOL] else : [EOL] logger . info ( f' [string] { config . train_dataset } [string] ' ) [EOL] [EOL] for test_ds in config . test_datasets : [EOL] if not isfile ( test_ds ) : [EOL] logger . error ( f' [string] { test_ds } [string] ' + [string] ) [EOL] [EOL] else : [EOL] logger . info ( f' [string] { test_ds } [string] ' ) [EOL] [EOL] logger . info ( [string] ) [EOL] pure_data = pd . read_csv ( config . train_dataset , index_col = [number] ) [EOL] [EOL] old_shape = pure_data . shape [EOL] logger . info ( [string] ) [EOL] try : [EOL] data , special_labels_df = pipeline_data ( pure_data ) [EOL] except Exception as e : [EOL] logger . error ( f' [string] { e }' ) [EOL] exit ( [number] ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] data = select_features ( data ) [EOL] [comment] [EOL] logger . info ( f' [string] { len ( data . columns ) - [number] } [string] ' + f' [string] { config . EXPLAINED_VARIANCE_RATIO * [number] } [string] ' + f' [string] ' ) [EOL] logger . info ( f' [string] ' ) [EOL] [EOL] data = data . join ( special_labels_df ) [EOL] [EOL] logger . info ( f' [string] { old_shape }' ) [EOL] logger . info ( f' [string] { data . shape }' ) [EOL] [EOL] [comment] [EOL] data_path = join ( config . data_path , config . TRAIN_DATA_PKL ) [EOL] data . to_pickle ( data_path ) [EOL] logger . info ( f' [string] { data_path } [string] ' ) [EOL] [EOL] [EOL] def train ( ** kwargs ) : [EOL] [docstring] [EOL] print ( [string] ) [EOL] [EOL] [comment] [EOL] data = pd . read_pickle ( join ( config . data_path , config . TRAIN_DATA_PKL ) ) [EOL] model , labels = create_mini_batch_kmeans ( data , return_labels = True ) [EOL] [EOL] [comment] [EOL] data [ config . CLUSTER_LABEL ] = labels [EOL] labeld_data_pkl_path = join ( config . data_path , config . TRAIN_DATA_PKL_LABELED ) [EOL] data . to_pickle ( labeld_data_pkl_path ) [EOL] [EOL] logger . info ( f' [string] { labeld_data_pkl_path } [string] ' ) [EOL] [EOL] [comment] [EOL] model_path = join ( config . models_path , config . MBK_PICKLE ) [EOL] with open ( model_path , [string] ) as f : [EOL] pickle . dump ( model , file = f ) [EOL] [EOL] logger . info ( f' [string] { model_path } [string] ' ) [EOL] [EOL] [EOL] def metadata ( ** kwargs ) : [EOL] [docstring] [EOL] print ( [string] ) [EOL] [EOL] [comment] [EOL] if [string] in kwargs . keys ( ) : [EOL] update = kwargs [ [string] ] [EOL] logger . info ( f' [string] ' ) [EOL] [EOL] else : [EOL] update = True [EOL] [EOL] [comment] [EOL] labeld_data_pkl_path = join ( config . data_path , config . TRAIN_DATA_PKL_LABELED ) [EOL] data = pd . read_pickle ( labeld_data_pkl_path ) [EOL] [EOL] [comment] [EOL] if isfile ( config . performance_metadata_path ) : [EOL] logger . info ( f' [string] { config . performance_metadata_path } [string] ' + [string] ) [EOL] with open ( config . performance_metadata_path ) as jf : [EOL] perf = json . load ( jf ) [EOL] [EOL] else : [EOL] logger . info ( f' [string] ' ) [EOL] perf = dict ( ) [EOL] [EOL] [comment] [EOL] for f in config . test_datasets : [EOL] if not isfile ( f ) : [EOL] logger . error ( f' [string] { f } [string] ' ) [EOL] continue [EOL] [EOL] logger . info ( f' [string] { f } [string] ' ) [EOL] [EOL] [comment] [EOL] base = basename ( f ) [EOL] name = splitext ( base ) [ [number] ] [EOL] [EOL] user_data = pd . read_csv ( f , index_col = [number] ) [EOL] user_ids = user_data [ [string] ] . tolist ( ) [EOL] [EOL] train , test = train_test_split ( user_ids , test_size = config . TEST_SIZE , random_state = [number] ) [EOL] [EOL] apk_values = dict ( ) [EOL] for k in config . APK_VALUES : [EOL] ordered_recs = make_recomendation ( data , train ) [EOL] [EOL] score = custom_apk ( market_list = ordered_recs , test_list = test , k = k ) [EOL] [EOL] apk_values [ f' [string] { k }' ] = score [EOL] [EOL] [comment] [EOL] if not update and name in perf . keys ( ) : [EOL] c = [number] [EOL] while name + f' [string] { c }' in perf . keys ( ) : [EOL] c += [number] [EOL] [EOL] name = name + f' [string] { c }' [EOL] [EOL] perf [ name ] = { [string] : str ( datetime . now ( ) ) , [string] : f , [string] : config . TEST_SIZE , [string] : apk_values , } [EOL] [EOL] with open ( config . performance_metadata_path , [string] ) as f : [EOL] json . dump ( perf , f , indent = [number] ) [EOL] [EOL] logger . info ( f' [string] ' + f' [string] { config . performance_metadata_path } [string] ' ) [EOL] [EOL] [EOL] def predict ( input_data , ** kwargs ) : [EOL] [docstring] [EOL] print ( [string] . format ( input_data ) ) [EOL] print ( [string] . format ( kwargs ) ) [EOL] [EOL] [comment] [EOL] if [string] in kwargs . keys ( ) : [EOL] k = kwargs [ [string] ] [EOL] logger . info ( f' [string] { k }' ) [EOL] [EOL] else : [EOL] logger . info ( f' [string] ' ) [EOL] k = [number] [EOL] [EOL] if [string] in kwargs . keys ( ) : [EOL] use_clusters = kwargs [ [string] ] [EOL] else : [EOL] use_clusters = False [EOL] [EOL] if [string] in kwargs . keys ( ) : [EOL] use_sp_labels = kwargs [ [string] ] [EOL] else : [EOL] use_sp_labels = True [EOL] [EOL] if [string] in kwargs . keys ( ) : [EOL] return_recs = kwargs [ [string] ] [EOL] [EOL] else : [EOL] return_recs = False [EOL] [EOL] [comment] [EOL] if not isfile ( input_data ) : [EOL] logger . error ( f' [string] { input_data } [string] ' ) [EOL] exit ( [number] ) [EOL] [EOL] data = pd . read_csv ( input_data ) [EOL] [EOL] try : [EOL] ids = data [ [string] ] [EOL] [EOL] except KeyError : [EOL] logger . error ( f' [string] ' ) [EOL] exit ( [number] ) [EOL] [EOL] [comment] [EOL] labeld_data_pkl_path = join ( config . data_path , config . TRAIN_DATA_PKL_LABELED ) [EOL] if not isfile ( labeld_data_pkl_path ) : [EOL] logger . error ( f' [string] ' + [string] ) [EOL] exit ( [number] ) [EOL] [EOL] train_data = pd . read_pickle ( labeld_data_pkl_path ) [EOL] [EOL] ordered_ids = make_recomendation ( train_data , ids , use_clusters = use_clusters , use_sp_labels = use_sp_labels ) [EOL] [EOL] recomendations = ordered_ids [ : k ] [EOL] if not return_recs : [EOL] print ( [string] ) [EOL] print ( f' [string] ' ) [EOL] for i , r in enumerate ( recomendations ) : [EOL] print ( f' [string] { i } [string] { r }' ) [EOL] [EOL] else : [EOL] return recomendations [EOL] [EOL] [EOL] [comment] [EOL] def run ( ** kwargs ) : [EOL] [docstring] [EOL] print ( [string] . format ( kwargs ) ) [EOL] print ( [string] ) [EOL] features ( ** kwargs ) [comment] [EOL] train ( ** kwargs ) [comment] [EOL] metadata ( ** kwargs ) [comment] [EOL] [EOL] [EOL] def cli ( ) : [EOL] [docstring] [EOL] return fire . Fire ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] cli ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import pytest [EOL] import pandas as pd [EOL] [EOL] TEST_DATA = [string] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def fixture_test_data ( ) : [EOL] return pd . read_pickle ( TEST_DATA ) [EOL]	0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0
	0
from typing import Any [EOL] import typing [EOL] import pandas as pd [EOL] [EOL] from squad_3_ad_data_science . feature_engineering import ( pipeline_data , select_features ) [EOL] from squad_3_ad_data_science . model_training import create_mini_batch_kmeans [EOL] from squad_3_ad_data_science import config [EOL] [EOL] [EOL] def test_model_creation ( fixture_test_data ) : [EOL] pipelined_data , sp_labels = pipeline_data ( fixture_test_data ) [EOL] feature_selected = select_features ( pipelined_data ) [EOL] [EOL] ret = create_mini_batch_kmeans ( feature_selected . copy ( ) ) [EOL] [EOL] [comment] [EOL] assert type ( ret ) != tuple [EOL] ret = create_mini_batch_kmeans ( feature_selected , return_labels = True ) [EOL] assert type ( ret ) == tuple [EOL] assert len ( ret [ [number] ] ) == len ( feature_selected . index ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import pandas as pd [EOL] [EOL] from squad_3_ad_data_science . feature_engineering import pipeline_data , select_features [EOL] from squad_3_ad_data_science import config [EOL] [EOL] def test_data ( fixture_test_data ) : [EOL] assert type ( fixture_test_data ) == pd . DataFrame [EOL] [EOL] def test_pipeline_data ( fixture_test_data ) : [EOL] pipelined_data , sp_labels = pipeline_data ( fixture_test_data . copy ( ) ) [EOL] [EOL] [comment] [EOL] assert ( set ( pipelined_data . index ) - set ( fixture_test_data [ [string] ] ) ) == set ( ) [EOL] [comment] [EOL] assert [string] not in pipelined_data . columns [EOL] [comment] [EOL] assert all ( [ [string] + col in sp_labels . columns for col in config . SPECIAL_LABELS ] ) [EOL] [comment] [EOL] assert pipelined_data . isna ( ) . sum ( ) . sum ( ) == [number] [EOL] [EOL] def test_feature_selection ( fixture_test_data ) : [EOL] pipelined_data , sp_labels = pipeline_data ( fixture_test_data ) [EOL] feature_selected = select_features ( pipelined_data ) [EOL] [EOL] [comment] [EOL] assert ( set ( feature_selected . index ) - set ( pipelined_data . index ) ) == set ( )	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import Any , List [EOL] import typing [EOL] import os [EOL] import sys [EOL] import datetime [EOL] import re [EOL] sys . path . insert ( [number] , os . path . abspath ( [string] ) ) [EOL] [EOL] [comment] [EOL] def convert ( word ) : [EOL] return [string] . join ( x . capitalize ( ) for x in re . split ( [string] , word ) ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] project = convert ( [string] ) [EOL] copyright = str ( datetime . datetime . now ( ) . year ) + [string] [EOL] author = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] version = [string] [EOL] release = version [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] extensions = [ [string] , [string] , [string] ] [EOL] [EOL] [comment] [EOL] templates_path = [ [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] exclude_patterns = [ ] [EOL] [EOL] [comment] [EOL] master_doc = [string] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_theme = [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] html_static_path = [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0