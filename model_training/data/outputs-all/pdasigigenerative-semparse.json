	0
	0
[comment] [EOL] from flaky import flaky [EOL] from allennlp . common . testing import ModelTestCase [EOL] [EOL] from gensem import WikiTablesQuestionGenerator , WikiTablesQuestionGeneratorReader [EOL] [EOL] class TestWikiTablesQuestionGenerator ( ModelTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . set_up_model ( [string] , [string] ) [EOL] [EOL] @ flaky def test_model_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_simple_seq2seq_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
[comment] [EOL] from typing import Dict , Any , List , Union [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] from gensem . models import WikiTablesQuestionGenerator [EOL] from gensem . predictors import WikiTablesReranker [EOL] [EOL] class TestWikiTablesReranker ( AllenNlpTestCase ) : [EOL] def test_ranked_logical_forms_present ( self ) : [EOL] archive_path = [string] [EOL] archive = load_archive ( archive_path ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] inputs = { [string] : [string] , [string] : [string] , [string] : [ [string] , [string] ] } [EOL] result = predictor . predict_json ( inputs ) [EOL] assert result [ [string] ] == [ [string] , [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.str],builtins.str]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Dict , Any , List , Union [EOL] import typing [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import START_SYMBOL , END_SYMBOL [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . semparse . domain_languages import WikiTablesLanguage [EOL] [EOL] from gensem . readers . dataset_reader import WikiTablesQuestionGeneratorReader [EOL] [EOL] class TestWikiTablesBackTranslationDatasetReader ( AllenNlpTestCase ) : [EOL] def test_reader_reads_jsonl ( self ) : [EOL] params = { [string] : False , [string] : [string] , } [EOL] reader = WikiTablesQuestionGeneratorReader . from_params ( Params ( params ) ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = list ( dataset ) [EOL] assert len ( instances ) == [number] [EOL] instance = instances [ [number] ] [EOL] [EOL] assert instance . fields . keys ( ) == { [string] , [string] , [string] , [string] , } [EOL] [EOL] question_tokens = [ START_SYMBOL , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , END_SYMBOL ] [EOL] assert [ t . text for t in instance . fields [ [string] ] . tokens ] == question_tokens [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert isinstance ( instance . fields [ [string] ] . as_tensor ( { } ) , WikiTablesLanguage ) [EOL] [EOL] all_action_fields = instance . fields [ [string] ] . field_list [EOL] actions = [ [ action_field . rule for action_field in action_fields ] for action_fields in all_action_fields ] [EOL] [EOL] [comment] [EOL] expected_sequence = [ [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] assert actions == expected_sequence [EOL] assert [ [ t . text for t in text_field . tokens ] for text_field in instance . fields [ [string] ] ] == expected_sequence [EOL] [EOL] def test_reader_reads_examples ( self ) : [EOL] params = { [string] : False , [string] : [string] , [string] : [string] } [EOL] reader = WikiTablesQuestionGeneratorReader . from_params ( Params ( params ) ) [EOL] dataset = reader . read ( [string] ) [EOL] instances = list ( dataset ) [EOL] assert len ( instances ) == [number] [EOL] instance = instances [ [number] ] [EOL] [EOL] assert instance . fields . keys ( ) == { [string] , [string] , [string] , [string] , } [EOL] [EOL] question_tokens = [ START_SYMBOL , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , END_SYMBOL ] [EOL] assert [ t . text for t in instance . fields [ [string] ] . tokens ] == question_tokens [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert isinstance ( instance . fields [ [string] ] . as_tensor ( { } ) , WikiTablesLanguage ) [EOL] all_action_fields = instance . fields [ [string] ] . field_list [EOL] assert len ( all_action_fields ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.list]$ 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.bool,builtins.str]]$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
[comment] [EOL] from typing import Optional , Any , List [EOL] import gensem [EOL] import builtins [EOL] import typing [EOL] import pytest [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] from gensem . modules . rnn_grammar_state import RnnGrammarState [EOL] [EOL] [EOL] def is_nonterminal ( symbol ) : [EOL] return symbol . isupper ( ) or symbol == [string] [EOL] [EOL] [EOL] class TestRnnGrammarStatelet ( AllenNlpTestCase ) : [EOL] def setUp ( self ) : [EOL] super ( ) . setUp ( ) [EOL] self . action_rnn_state_mapping = { [string] : object ( ) , [string] : object ( ) , [string] : object ( ) , [string] : object ( ) } [EOL] [EOL] def test_update_works ( self ) : [EOL] [comment] [EOL] grammar_state = RnnGrammarState ( [ ] , is_nonterminal ) [EOL] action1 = [string] [EOL] grammar_state = grammar_state . update ( action1 , self . action_rnn_state_mapping [ action1 ] ) [EOL] assert grammar_state . _nonterminal_stack == [ ( [string] , self . action_rnn_state_mapping [ action1 ] ) ] [EOL] action2 = [string] [EOL] grammar_state = grammar_state . update ( action2 , self . action_rnn_state_mapping [ action2 ] ) [EOL] assert grammar_state . _nonterminal_stack == [ ( [string] , self . action_rnn_state_mapping [ action1 ] ) , ( [string] , self . action_rnn_state_mapping [ action2 ] ) ] [EOL] action3 = [string] [EOL] grammar_state = grammar_state . update ( action3 , self . action_rnn_state_mapping [ action3 ] ) [EOL] assert grammar_state . _nonterminal_stack == [ ( [string] , self . action_rnn_state_mapping [ action3 ] ) ] [EOL] [EOL] def test_get_child_rnn_states ( self ) : [EOL] grammar_state = RnnGrammarState ( [ ] , is_nonterminal ) [EOL] assert grammar_state . get_child_rnn_states ( [string] ) is None [EOL] with pytest . raises ( AssertionError ) : [EOL] grammar_state . get_child_rnn_states ( [string] ) [EOL] [EOL] grammar_state = RnnGrammarState ( [ ( [string] , self . action_rnn_state_mapping [ [string] ] ) , ( [string] , self . action_rnn_state_mapping [ [string] ] ) ] , is_nonterminal ) [EOL] child_rnn_states = grammar_state . get_child_rnn_states ( [string] ) [EOL] assert child_rnn_states == [ self . action_rnn_state_mapping [ [string] ] , self . action_rnn_state_mapping [ [string] ] ] [EOL] with pytest . raises ( AssertionError ) : [EOL] grammar_state . get_child_rnn_states ( [string] ) [EOL] [EOL] def test_update_followed_by_get_states ( self ) : [EOL] grammar_state = RnnGrammarState ( [ ] , is_nonterminal ) [EOL] grammar_state = grammar_state . update ( [string] , self . action_rnn_state_mapping [ [string] ] ) [EOL] grammar_state = grammar_state . update ( [string] , self . action_rnn_state_mapping [ [string] ] ) [EOL] child_states = grammar_state . get_child_rnn_states ( [string] ) [EOL] assert child_states == [ self . action_rnn_state_mapping [ [string] ] , self . action_rnn_state_mapping [ [string] ] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.Any]]$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.Any]]$ 0 $gensem.modules.rnn_grammar_state.RnnGrammarState$ 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] from gensem . models import * [EOL] from gensem . readers import * [EOL] from gensem . modules import * [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from gensem . models . wikitables_question_generator import WikiTablesQuestionGenerator [EOL]	0 0 0 0 0 0 0 0 0
from gensem . readers . dataset_reader import WikiTablesQuestionGeneratorReader [EOL]	0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import logging [EOL] import typing [EOL] import builtins [EOL] import allennlp [EOL] import logging [EOL] from typing import Dict , List [EOL] import os [EOL] import json [EOL] import gzip [EOL] import tarfile [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . util import START_SYMBOL , END_SYMBOL [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . fields import ( Field , TextField , MetadataField , ProductionRuleField , ListField ) [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . dataset_readers . semantic_parsing . wikitables import util as wtq_data_util [EOL] from allennlp . data . tokenizers import WordTokenizer , Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] from allennlp . data . tokenizers . word_splitter import SpacyWordSplitter [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] from allennlp . semparse import ParsingError [EOL] from allennlp . semparse . contexts import TableQuestionContext [EOL] from allennlp . semparse . domain_languages import WikiTablesLanguage [EOL] [EOL] from gensem . readers import utils as reader_utils [EOL] [EOL] logger = logging . getLogger ( __name__ ) [comment] [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class WikiTablesQuestionGeneratorReader ( DatasetReader ) : [EOL] def __init__ ( self , lazy = False , tables_directory = None , offline_logical_forms_directory = None , max_num_logical_forms = [number] , tokenizer = None , question_token_indexers = None , rule_indexers = None ) : [EOL] super ( ) . __init__ ( lazy = lazy ) [EOL] self . _tables_directory = tables_directory [EOL] self . _offline_logical_forms_directory = offline_logical_forms_directory [EOL] self . _max_num_logical_forms = max_num_logical_forms [EOL] self . _tokenizer = tokenizer or WordTokenizer ( SpacyWordSplitter ( pos_tags = True ) ) [EOL] self . _question_token_indexers = question_token_indexers or { [string] : SingleIdTokenIndexer ( [string] ) } [EOL] self . _rule_indexers = rule_indexers or { [string] : SingleIdTokenIndexer ( [string] ) } [EOL] [EOL] @ overrides def _read ( self , file_path ) : [EOL] tarball_with_all_lfs = None [EOL] for filename in os . listdir ( self . _offline_logical_forms_directory ) : [EOL] if filename . endswith ( [string] ) : [EOL] tarball_with_all_lfs = os . path . join ( self . _offline_logical_forms_directory , filename ) [EOL] break [EOL] if tarball_with_all_lfs is not None : [EOL] logger . info ( f" [string] { tarball_with_all_lfs }" ) [EOL] logger . info ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . _offline_logical_forms_directory = [string] [EOL] tarfile . open ( tarball_with_all_lfs , mode = [string] ) . extractall ( path = self . _offline_logical_forms_directory ) [EOL] data = [ ] [EOL] if file_path . endswith ( [string] ) : [EOL] with open ( file_path , [string] ) as data_file : [EOL] for line in data_file : [EOL] if not line : [EOL] continue [EOL] line_data = json . loads ( line ) [EOL] line_data [ [string] ] = [ line_data [ [string] ] ] [EOL] data . append ( line_data ) [EOL] elif file_path . endswith ( [string] ) : [EOL] num_examples = [number] [EOL] num_examples_without_lf = [number] [EOL] if self . _offline_logical_forms_directory is None : [EOL] raise RuntimeError ( [string] ) [EOL] with open ( file_path , [string] ) as data_file : [EOL] for line in data_file : [EOL] num_examples += [number] [EOL] line_data = wtq_data_util . parse_example_line ( line ) [EOL] example_id = line_data [ [string] ] [EOL] logical_forms_file = os . path . join ( self . _offline_logical_forms_directory , f"{ example_id } [string] " ) [EOL] if not os . path . exists ( logical_forms_file ) : [EOL] num_examples_without_lf += [number] [EOL] continue [EOL] logical_forms = None [EOL] with gzip . open ( logical_forms_file , [string] ) as lf_file : [EOL] logical_forms = [ x . strip ( ) for x in lf_file . readlines ( ) ] [ : self . _max_num_logical_forms ] [EOL] line_data [ [string] ] = logical_forms [EOL] data . append ( line_data ) [EOL] logger . info ( f" [string] { num_examples_without_lf } [string] { num_examples } [string] " ) [EOL] else : [EOL] raise RuntimeError ( f" [string] { file_path } [string] " ) [EOL] [EOL] for datum in data : [EOL] [comment] [EOL] table_filename = os . path . join ( self . _tables_directory , datum [ [string] ] . replace ( [string] , [string] ) ) [EOL] [EOL] table_lines = [ line . split ( [string] ) for line in open ( table_filename ) . readlines ( ) ] [EOL] instance = self . text_to_instance ( logical_forms = datum [ [string] ] , table_lines = table_lines , question = datum [ [string] ] ) [EOL] if instance is not None : [EOL] yield instance [EOL] [EOL] def text_to_instance ( self , logical_forms , table_lines , question ) : [EOL] [comment] [EOL] tokenized_question = self . _tokenizer . tokenize ( question . lower ( ) ) [EOL] tokenized_question . insert ( [number] , Token ( START_SYMBOL ) ) [EOL] tokenized_question . append ( Token ( END_SYMBOL ) ) [EOL] question_field = TextField ( tokenized_question , self . _question_token_indexers ) [EOL] table_context = TableQuestionContext . read_from_lines ( table_lines , tokenized_question ) [EOL] world = WikiTablesLanguage ( table_context ) [EOL] [EOL] action_sequences_list = [ ] [EOL] action_sequence_fields_list = [ ] [EOL] for logical_form in logical_forms : [EOL] try : [EOL] action_sequence = world . logical_form_to_action_sequence ( logical_form ) [EOL] action_sequence = reader_utils . make_bottom_up_action_sequence ( action_sequence , world . is_nonterminal ) [EOL] action_sequence_field = TextField ( [ Token ( rule ) for rule in action_sequence ] , self . _rule_indexers ) [EOL] action_sequences_list . append ( action_sequence ) [EOL] action_sequence_fields_list . append ( action_sequence_field ) [EOL] except ParsingError as error : [EOL] logger . debug ( f' [string] { error . message } [string] ' ) [EOL] logger . debug ( f' [string] { question }' ) [EOL] logger . debug ( f' [string] { logical_form }' ) [EOL] logger . debug ( f' [string] { table_lines }' ) [EOL] except : [EOL] logger . error ( logical_form ) [EOL] raise [EOL] [EOL] if not action_sequences_list : [EOL] return None [EOL] [EOL] all_production_rule_fields = [ ] [EOL] for action_sequence in action_sequences_list : [EOL] all_production_rule_fields . append ( [ ] ) [EOL] for production_rule in action_sequence : [EOL] _ , rule_right_side = production_rule . split ( [string] ) [EOL] is_global_rule = not world . is_instance_specific_entity ( rule_right_side ) [EOL] field = ProductionRuleField ( production_rule , is_global_rule = is_global_rule ) [EOL] all_production_rule_fields [ - [number] ] . append ( field ) [EOL] action_field = ListField ( [ ListField ( production_rule_fields ) for production_rule_fields in all_production_rule_fields ] ) [EOL] [EOL] fields = { [string] : ListField ( action_sequence_fields_list ) , [string] : question_field , [string] : MetadataField ( world ) , [string] : action_field } [EOL] [EOL] return Instance ( fields ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,builtins.str]]$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[typing.List[builtins.str]]$ 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 $typing.List[allennlp.data.fields.TextField]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[allennlp.data.fields.TextField]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 $typing.List[typing.List[allennlp.data.fields.Field]]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[typing.List[builtins.str]]$ 0 0 $typing.List[typing.List[allennlp.data.fields.Field]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 $typing.List[typing.List[allennlp.data.fields.Field]]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[allennlp.data.fields.Field]]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.List[allennlp.data.fields.TextField]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0
from typing import Callable , Tuple , List [EOL] import typing [EOL] import builtins [EOL] from typing import List , Callable , Tuple [EOL] [EOL] [EOL] def get_nonterminals_from_list ( right_side ) : [EOL] return right_side [ [number] : - [number] ] . split ( [string] ) [comment] [EOL] [EOL] [EOL] def make_bottom_up_action_sequence ( top_down_sequence , is_nonterminal ) : [EOL] [docstring] [EOL] nonterminal_stack = [ ( [string] , None ) ] [comment] [EOL] bottom_up_sequence = [ ] [EOL] for action in top_down_sequence : [EOL] left_side , right_side = action . split ( [string] ) [EOL] [comment] [EOL] while nonterminal_stack [ - [number] ] [ [number] ] is not None : [EOL] next_action = nonterminal_stack [ - [number] ] [ [number] ] [EOL] bottom_up_sequence . append ( next_action ) [EOL] nonterminal_stack = nonterminal_stack [ : - [number] ] [EOL] [comment] [EOL] assert nonterminal_stack [ - [number] ] [ [number] ] == left_side , f" [string] { top_down_sequence }" [EOL] [comment] [EOL] nonterminal_stack = nonterminal_stack [ : - [number] ] [EOL] [comment] [EOL] if not is_nonterminal ( right_side ) and not right_side [ [number] ] == [string] : [EOL] [comment] [EOL] bottom_up_sequence . append ( action ) [EOL] else : [EOL] if right_side [ [number] ] == [string] : [EOL] right_side_list = get_nonterminals_from_list ( right_side ) [EOL] else : [EOL] right_side_list = [ right_side ] [EOL] [comment] [EOL] nonterminal_stack . append ( ( left_side , action ) ) [EOL] [comment] [EOL] for symbol in reversed ( right_side_list ) : [EOL] nonterminal_stack . append ( ( symbol , None ) ) [EOL] [EOL] for _ , action in reversed ( nonterminal_stack ) : [EOL] assert action is not None , f" [string] { top_down_sequence }" [EOL] bottom_up_sequence . append ( action ) [EOL] [EOL] return bottom_up_sequence [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from gensem . predictors . wikitables_reranker import WikiTablesReranker [EOL]	0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import typing [EOL] import allennlp [EOL] from overrides import overrides [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class WikiTablesReranker ( Predictor ) : [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] logical_forms = json_dict [ [string] ] [EOL] table_lines = json_dict [ [string] ] . split ( [string] ) [EOL] question_text = json_dict [ [string] ] [EOL] instance = self . _dataset_reader . text_to_instance ( logical_forms = logical_forms , table_lines = table_lines , question = question_text ) [EOL] return instance [EOL] [EOL] [EOL] @ overrides def predict_json ( self , inputs ) : [EOL] outputs = super ( ) . predict_json ( inputs ) [EOL] logical_form_indices = outputs [ [string] ] [EOL] ranked_logical_forms = [ inputs [ [string] ] [ index ] for index in logical_form_indices ] [EOL] outputs_to_return = { [string] : inputs [ [string] ] , [string] : inputs [ [string] ] , [string] : ranked_logical_forms } [EOL] return outputs_to_return [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0
from gensem . modules . tree_lstm import TreeLSTM [EOL]	0 0 0 0 0 0 0 0 0
from typing import Callable , Tuple , Any , List [EOL] import torch [EOL] import typing [EOL] import builtins [EOL] from typing import List , Tuple , Callable [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] [EOL] from gensem . modules . rnn_grammar_state import RnnGrammarState [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class TreeLSTM ( Seq2SeqEncoder ) : [EOL] [docstring] [EOL] def __init__ ( self , input_dim , output_dim ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _lstm_cell = torch . nn . LSTMCell ( input_dim , output_dim ) [EOL] [comment] [EOL] self . _leaf_hidden_state = torch . nn . Parameter ( torch . Tensor ( output_dim ) ) [EOL] self . _leaf_memory_cell = torch . nn . Parameter ( torch . Tensor ( output_dim ) ) [EOL] self . _child_representation_aggregator = torch . nn . GRU ( output_dim , output_dim ) [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _lstm_cell . input_size [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _lstm_cell . hidden_size [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return False [EOL] [EOL] def _aggregate_child_states ( self , child_rnn_states ) : [EOL] [comment] [EOL] child_hidden_states = torch . stack ( [ rnn_state [ [number] ] for rnn_state in child_rnn_states ] ) . unsqueeze ( [number] ) [EOL] [comment] [EOL] _ , aggregated_hidden_state = self . _child_representation_aggregator ( child_hidden_states ) [EOL] [comment] [EOL] aggregated_hidden_state = aggregated_hidden_state . squeeze ( [number] ) [EOL] [comment] [EOL] child_memory_cells = torch . stack ( [ rnn_state [ [number] ] for rnn_state in child_rnn_states ] ) . unsqueeze ( [number] ) [EOL] [comment] [EOL] _ , aggregated_memory_cell = self . _child_representation_aggregator ( child_memory_cells ) [EOL] [comment] [EOL] aggregated_memory_cell = aggregated_memory_cell . squeeze ( [number] ) [EOL] return aggregated_hidden_state , aggregated_memory_cell [EOL] [EOL] @ overrides def forward ( self , inputs , mask , production_rules , is_nonterminal ) : [EOL] [comment] [EOL] grammar_state = RnnGrammarState ( [ ] , is_nonterminal ) [EOL] outputs = [ ] [EOL] batch_size , _ , _ = inputs . size ( ) [EOL] for i in range ( batch_size ) : [EOL] instance_outputs = [ ] [EOL] rule_index = [number] [EOL] for input_ , input_mask in zip ( inputs [ i ] , mask [ i ] ) : [EOL] if not input_mask : [EOL] instance_outputs . append ( input_ . new ( torch . zeros ( self . get_output_dim ( ) ) ) ) [EOL] continue [EOL] production_rule = production_rules [ i ] [ rule_index ] [EOL] rule_index += [number] [EOL] child_rnn_states = grammar_state . get_child_rnn_states ( production_rule ) [EOL] if child_rnn_states is None : [EOL] [comment] [EOL] aggregated_hidden_state = self . _leaf_hidden_state . unsqueeze ( [number] ) [EOL] aggregated_memory_cell = self . _leaf_memory_cell . unsqueeze ( [number] ) [EOL] else : [EOL] aggregated_hidden_state , aggregated_memory_cell = self . _aggregate_child_states ( child_rnn_states ) [EOL] [EOL] hidden_state , memory_cell = self . _lstm_cell ( input_ . unsqueeze ( [number] ) , ( aggregated_hidden_state , aggregated_memory_cell ) ) [EOL] hidden_state = hidden_state . squeeze ( [number] ) [EOL] memory_cell = memory_cell . squeeze ( [number] ) [EOL] grammar_state = grammar_state . update ( production_rule , ( hidden_state , memory_cell ) ) [EOL] instance_outputs . append ( hidden_state ) [EOL] [comment] [EOL] outputs . append ( torch . stack ( instance_outputs ) ) [EOL] [comment] [EOL] return torch . stack ( outputs ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.LongTensor$ 0 $typing.List[typing.List[builtins.str]]$ 0 $typing.Callable[[builtins.str],builtins.bool]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Callable[[builtins.str],builtins.bool]$ 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 $torch.LongTensor$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0
from typing import Optional , Tuple , Any , List , Callable [EOL] import typing [EOL] import builtins [EOL] from typing import List , Callable , Any , Tuple , Optional [EOL] [EOL] from gensem . readers import utils as reader_utils [EOL] [EOL] [EOL] class RnnGrammarState : [EOL] [docstring] [EOL] def __init__ ( self , nonterminal_stack , is_nonterminal ) : [EOL] self . _nonterminal_stack = nonterminal_stack [EOL] self . _is_nonterminal = is_nonterminal [EOL] [EOL] def _get_nonterminals_from_rule ( self , production_rule ) : [EOL] [docstring] [EOL] left_side , right_side = production_rule . split ( [string] ) [EOL] right_nonterminals = [ ] [EOL] if right_side [ [number] ] == [string] : [EOL] right_nonterminals = reader_utils . get_nonterminals_from_list ( right_side ) [EOL] elif self . _is_nonterminal ( right_side ) : [EOL] right_nonterminals = [ right_side ] [EOL] return left_side , right_nonterminals [EOL] [EOL] def update ( self , production_rule , rnn_state ) : [EOL] [docstring] [EOL] left_side , nonterminals_to_close = self . _get_nonterminals_from_rule ( production_rule ) [EOL] [EOL] if nonterminals_to_close : [EOL] assert len ( self . _nonterminal_stack ) >= len ( nonterminals_to_close ) , f" [string] { production_rule }" [EOL] [comment] [EOL] num_nonterminals = len ( nonterminals_to_close ) [EOL] stack_nonterminals = [ x [ [number] ] for x in self . _nonterminal_stack [ - num_nonterminals : ] ] [EOL] assert stack_nonterminals == nonterminals_to_close , f" [string] { production_rule } [string] { stack_nonterminals }" [EOL] new_stack = self . _nonterminal_stack [ : - len ( nonterminals_to_close ) ] [EOL] else : [EOL] new_stack = list ( self . _nonterminal_stack ) [EOL] [EOL] new_stack . append ( ( left_side , rnn_state ) ) [EOL] [EOL] return RnnGrammarState ( nonterminal_stack = new_stack , is_nonterminal = self . _is_nonterminal ) [EOL] [EOL] def get_child_rnn_states ( self , production_rule ) : [EOL] [docstring] [EOL] _ , right_nonterminals = self . _get_nonterminals_from_rule ( production_rule ) [EOL] if not right_nonterminals : [EOL] return None [EOL] [EOL] assert len ( self . _nonterminal_stack ) >= len ( right_nonterminals ) , f" [string] { production_rule }" [EOL] child_rnn_states = [ ] [EOL] for i , nonterminal in enumerate ( reversed ( right_nonterminals ) ) : [EOL] stack_info = self . _nonterminal_stack [ - ( i + [number] ) ] [EOL] assert nonterminal == stack_info [ [number] ] , f" [string] { production_rule } [string] { nonterminal } [string] { stack_info [ [number] ] }" [EOL] child_rnn_states . append ( stack_info [ [number] ] ) [EOL] return child_rnn_states [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 $typing.Callable[[builtins.str],builtins.bool]$ 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 0 0 $typing.Callable[[builtins.str],builtins.bool]$ 0 $typing.Callable[[builtins.str],builtins.bool]$ 0 0 0 $typing.Tuple[builtins.str,typing.List[builtins.str]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $'RnnGrammarState'$ 0 0 0 $builtins.str$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.List[typing.Any]]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Tuple[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Tuple[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0
from typing import Dict , Any , List , Set [EOL] import argparse [EOL] import typing [EOL] import builtins [EOL] import argparse [EOL] from typing import List , Dict , Set [EOL] import random [EOL] import os [EOL] import gzip [EOL] import json [EOL] [EOL] from allennlp . data . dataset_readers . semantic_parsing . wikitables import util as wtq_util [EOL] [EOL] [EOL] MAX_NUM_LOGICAL_FORMS_TO_SHOW = [number] [EOL] [EOL] [EOL] def main ( examples_file , tables_directory , logical_forms_directory , output_file ) : [EOL] examples = [ ] [EOL] with open ( examples_file ) as input_file : [EOL] for line in input_file : [EOL] examples . append ( wtq_util . parse_example_line ( line ) ) [EOL] random . shuffle ( examples ) [comment] [EOL] [EOL] processed_examples = set ( ) [EOL] if os . path . exists ( output_file ) : [EOL] with open ( output_file ) as output_file_for_reading : [EOL] for line in output_file_for_reading : [EOL] example_id = json . loads ( line ) [ [string] ] [EOL] processed_examples . add ( example_id ) [EOL] [EOL] with open ( output_file , [string] ) as output_file_for_appending : [EOL] for example in examples : [EOL] example_id = example [ [string] ] [EOL] if example_id in processed_examples : [EOL] [comment] [EOL] continue [EOL] question = example [ [string] ] [EOL] table_filename = example [ [string] ] [EOL] full_table_filename = os . path . join ( tables_directory , table_filename ) [EOL] table_lines = [ ] [EOL] with open ( full_table_filename . replace ( [string] , [string] ) ) as table_file : [EOL] table_lines = table_file . readlines ( ) [EOL] logical_forms_file = os . path . join ( logical_forms_directory , f"{ example_id } [string] " ) [EOL] if not os . path . exists ( logical_forms_file ) : [EOL] continue [EOL] print ( [string] . join ( table_lines ) ) [EOL] print ( ) [EOL] with gzip . open ( logical_forms_file , [string] ) as lf_file : [EOL] for i , logical_form in enumerate ( lf_file ) : [EOL] logical_form = logical_form . strip ( ) [EOL] print ( question ) [EOL] print ( logical_form ) [EOL] print ( ) [EOL] user_input = None [EOL] while user_input not in [ [string] , [string] , [string] , [string] ] : [EOL] user_input = input ( [string] ) [EOL] user_input = user_input . lower ( ) [EOL] if user_input == [string] : [EOL] break [EOL] elif user_input == [string] : [EOL] instance_output = { [string] : example_id , [string] : question , [string] : table_filename , [string] : logical_form } [EOL] print ( json . dumps ( instance_output ) , file = output_file_for_appending ) [EOL] break [EOL] elif user_input == [string] : [EOL] correct_logical_form = input ( [string] ) [EOL] instance_output = { [string] : example_id , [string] : question , [string] : table_filename , [string] : correct_logical_form } [EOL] print ( json . dumps ( instance_output ) , file = output_file_for_appending ) [EOL] break [EOL] if i >= MAX_NUM_LOGICAL_FORMS_TO_SHOW : [EOL] break [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [comment] [EOL] argparser = argparse . ArgumentParser ( [string] ) [EOL] argparser . add_argument ( [string] , required = True , type = str , help = [string] ) [EOL] argparser . add_argument ( [string] , required = True , type = str , help = [string] ) [EOL] argparser . add_argument ( [string] , required = True , type = str , help = [string] ) [EOL] argparser . add_argument ( [string] , required = True , type = str , help = [string] ) [EOL] args = argparser . parse_args ( ) [EOL] main ( args . examples , args . tables , args . lfs , args . output ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0