import pytest [EOL] [EOL] from src . utils import create_views [EOL] [EOL] [EOL] class TestCreateViews ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] ] ) def test_view_is_created ( self , cursor , view_name ) : [EOL] create_views ( cursor ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] cursor . execute ( f" [string] { view_name }" ) [EOL] cursor . fetchone ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import sqlite3 [EOL] import pytest [EOL] import sqlite3 [EOL] [EOL] [EOL] with open ( [string] ) as fh : [EOL] setup_sql = fh . read ( ) [EOL] [EOL] @ pytest . fixture def cursor ( ) : [EOL] conn = sqlite3 . connect ( [string] ) [EOL] conn . executescript ( setup_sql ) [EOL] yield conn . cursor ( ) [EOL] conn . close ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] from typing import List , Type , Optional [EOL] import src [EOL] import tests [EOL] import typing [EOL] import html [EOL] [EOL] from src . scrape . extract_links import extract [EOL] [EOL] [EOL] class TestExtract ( object ) : [EOL] doc = html . escape ( [string] ) [EOL] [EOL] def test_extracts_media_links ( self ) : [EOL] expected_urls = [ [string] , [string] , ] [EOL] expected_txts = [ [string] , [string] ] [EOL] [EOL] medias = extract ( [string] , self . doc ) [EOL] urls = [ media . url for media in medias ] [EOL] txts = [ media . txt for media in medias ] [EOL] [EOL] assert urls == expected_urls [EOL] assert txts == expected_txts [EOL] [EOL] def test_propagates_submission_id ( self ) : [EOL] submission_id = [string] [EOL] medias = extract ( submission_id , self . doc ) [EOL] assert all ( media . submission_id == submission_id for media in medias ) [EOL] [EOL] def test_never_direct ( self ) : [EOL] medias = extract ( [string] , self . doc ) [EOL] assert all ( not media . is_direct for media in medias ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0
from typing import Any , Optional , Dict , Tuple , List [EOL] import src [EOL] import typing [EOL] from requests import HTTPError [EOL] import json [EOL] import pytest [EOL] import responses [EOL] [EOL] from src . scrape . images import ( Client , ImgurClient , RateLimitError , get_id , get_links , ingest_albums , ingest_standalones , is_album , is_imgur , make_imgur_url , sniff_imgur_resource , strip_imgur_subdomain , ) [EOL] [EOL] [EOL] @ pytest . fixture def imgur_album ( ) : [EOL] with open ( [string] ) as fh : [EOL] data = json . load ( fh ) [EOL] return data [EOL] [EOL] def add_image ( url , body ) : [EOL] mimetype = [string] [EOL] headers = { [string] : mimetype } [EOL] responses . add ( responses . GET , url , body = body , headers = headers ) [EOL] return mimetype [EOL] [EOL] def add_album_response ( raw_album , img_body ) : [EOL] data = raw_album [ [string] ] [EOL] album_id = data . get ( [string] , [string] ) [EOL] [EOL] url = f" [string] { album_id }" [EOL] api_url = f" [string] { album_id }" [EOL] [EOL] [comment] [EOL] data [ [string] ] = album_id [EOL] data [ [string] ] = url [EOL] [EOL] [comment] [EOL] for i , img in enumerate ( data [ [string] ] ) : [EOL] img [ [string] ] = str ( i ) [EOL] img_url = f" [string] { i } [string] " [EOL] md_url = f" [string] { i }" [EOL] img [ [string] ] = img_url [EOL] [EOL] add_image ( img_url , img_body ) [EOL] responses . add ( responses . GET , md_url , json = { [string] : img } ) [EOL] [EOL] responses . add ( responses . GET , url , status = [number] ) [EOL] responses . add ( responses . GET , api_url , json = raw_album ) [EOL] [EOL] [comment] [EOL] return raw_album , api_url [EOL] [EOL] def insert_submission ( cursor , s_id ) : [EOL] cursor . execute ( [string] , ( s_id , ) ) [EOL] return [EOL] [EOL] def insert_media ( cursor , m_id , s_id , url ) : [EOL] cursor . execute ( [string] , ( m_id , s_id , url ) ) [EOL] return [EOL] [EOL] [EOL] class TestSniffImgurResource ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] , ids = [ [string] , [string] , [string] , [string] ] ) def test_gets_resource ( self , url , expected ) : [EOL] resource_type = sniff_imgur_resource ( url ) [EOL] assert resource_type == expected [EOL] [EOL] def test_handles_trailing_slash ( self ) : [EOL] url = [string] [EOL] resource_type = sniff_imgur_resource ( url ) [EOL] assert resource_type == [string] [EOL] [EOL] def test_ignores_extra_path_components ( self ) : [EOL] url = [string] [EOL] resource_type = sniff_imgur_resource ( url ) [EOL] assert resource_type == [string] [EOL] [EOL] def test_returns_none_for_unknown_resource ( self ) : [EOL] url = [string] [EOL] resource_type = sniff_imgur_resource ( url ) [EOL] assert resource_type is None [EOL] [EOL] def test_strips_album_anchor_link ( self ) : [EOL] url = [string] [EOL] resource_type = sniff_imgur_resource ( url ) [EOL] assert resource_type == [string] [EOL] [EOL] class TestGetID ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] , ids = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] ) def test_gets_id ( self , url , expected ) : [EOL] hash_ = get_id ( url ) [EOL] assert hash_ == expected [EOL] [EOL] class TestMakeImgurURL ( object ) : [EOL] def test_imgur_api_url ( self ) : [EOL] api_url = make_imgur_url ( [string] , [string] ) [EOL] assert api_url . endswith ( [string] ) [EOL] [EOL] class TestStripImgurSubdomain ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , ] , ids = [ [string] , [string] , [string] ] ) def test_strips ( self , url ) : [EOL] expected = [string] [EOL] assert strip_imgur_subdomain ( url ) == expected [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , ] , ids = [ [string] , [string] ] ) def test_passes ( self , url ) : [EOL] assert strip_imgur_subdomain ( url ) == url [EOL] [EOL] class TestIsImgur ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , ] , ids = [ [string] , [string] , [string] ] ) def test_is_imgur ( self , url ) : [EOL] assert is_imgur ( url ) [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , [string] , ] , ids = [ [string] , [string] , [string] , [string] ] ) def test_not_imgur ( self , url ) : [EOL] assert not is_imgur ( url ) [EOL] [EOL] class TestIsAlbum ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , ] , ids = [ [string] , [string] , [string] ] ) def test_is_album ( self , url ) : [EOL] assert is_album ( url ) [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , [string] , ] , ids = [ [string] , [string] , [string] , [string] ] ) def test_not_album ( self , url ) : [EOL] assert not is_album ( url ) [EOL] [EOL] class TestGetLinks ( object ) : [EOL] def test_unprocessed_links ( self , cursor ) : [EOL] [comment] [EOL] insert_submission ( cursor , [string] ) [EOL] medias = [ ( [number] , [string] ) , ( [number] , [string] ) , ( [number] , [string] ) , ] [EOL] for id_ , url in medias : [EOL] insert_media ( cursor , id_ , [string] , url ) [EOL] [EOL] [comment] [EOL] ( processed_media_id , _ ) , * expected = medias [EOL] cursor . execute ( [string] , ( processed_media_id , ) ) [EOL] [EOL] medias = get_links ( cursor ) [EOL] [EOL] assert medias == expected [EOL] [EOL] class TestClient ( object ) : [EOL] url = [string] [EOL] body = [string] [EOL] [EOL] @ responses . activate def test_gets_image ( self ) : [EOL] mimetype = add_image ( self . url , self . body ) [EOL] [EOL] client = Client ( ) [EOL] image = client . get_image ( self . url ) [EOL] [EOL] assert image . url == self . url [EOL] assert image . mimetype == mimetype [EOL] assert image . img == self . body [EOL] [EOL] @ responses . activate def test_metadata_is_propagated ( self ) : [EOL] add_image ( self . url , self . body ) [EOL] [EOL] client = Client ( ) [EOL] image = client . get_image ( self . url , id = [string] , media_id = [number] ) [EOL] [EOL] assert image . id == [string] [EOL] assert image . media_id == [number] [EOL] [EOL] @ responses . activate def test_returns_metadata_if_request_fails ( self ) : [EOL] responses . add ( responses . GET , self . url , status = [number] ) [EOL] [EOL] client = Client ( ) [EOL] image = client . get_image ( self . url , media_id = [number] , id = [string] ) [EOL] [EOL] assert image . url == self . url [EOL] assert image . mimetype is None [EOL] assert image . img is None [EOL] [EOL] @ pytest . mark . parametrize ( [string] , Client . fail_on_statuses ) @ responses . activate def test_raises_error_on_auth_failure ( self , status ) : [EOL] responses . add ( responses . GET , self . url , status = status ) [EOL] [EOL] with pytest . raises ( HTTPError ) : [EOL] client = Client ( ) [EOL] client . get_image ( self . url ) [EOL] [EOL] class TestImgurClient ( object ) : [EOL] @ responses . activate def test_get_image_has_auth_header ( self ) : [EOL] def cb ( request ) : [EOL] auth = request . headers . get ( [string] ) [EOL] assert auth is not None [EOL] assert auth == [string] [EOL] return ( [number] , { } , [string] ) [EOL] [EOL] url = [string] [EOL] responses . add_callback ( responses . GET , url , cb ) [EOL] [EOL] client = ImgurClient ( [string] ) [EOL] client . get_image ( url ) [EOL] [EOL] @ responses . activate def test_get_album_has_auth_header ( self ) : [EOL] body = json . dumps ( { [string] : [number] , [string] : { [string] : [string] , [string] : [ ] , } , } ) [EOL] [EOL] def cb ( request ) : [EOL] auth = request . headers . get ( [string] ) [EOL] assert auth is not None [EOL] assert auth == [string] [EOL] return ( [number] , { } , body . encode ( ) ) [EOL] [EOL] url = [string] [EOL] responses . add_callback ( responses . GET , url , cb ) [EOL] [EOL] client = ImgurClient ( [string] ) [EOL] client . get_album ( url , media_id = [number] ) [EOL] [EOL] @ responses . activate def test_raises_error_on_rate_limited ( self ) : [EOL] url = [string] [EOL] responses . add ( responses . GET , url , status = [number] , body = [string] , headers = { [string] : [string] , [string] : [string] , [string] : str ( [number] ) , } ) [EOL] [EOL] with pytest . raises ( RateLimitError ) : [EOL] client = ImgurClient ( [string] ) [EOL] client . get_album ( url , media_id = [number] ) [EOL] [EOL] @ responses . activate def test_gets_album ( self , imgur_album ) : [EOL] imgur_album , url = add_album_response ( imgur_album , img_body = [string] ) [EOL] album_data = imgur_album [ [string] ] [EOL] [EOL] client = ImgurClient ( [string] ) [EOL] album , images = client . get_album ( url , media_id = [number] ) [EOL] [EOL] assert album . id == album_data [ [string] ] [EOL] assert album . media_id == [number] [EOL] assert len ( images ) == len ( album_data [ [string] ] ) [EOL] assert all ( image . album_id == album . id for image in images ) [EOL] assert all ( image . media_id == [number] for image in images ) [EOL] [EOL] class TestIngestStandalones ( object ) : [EOL] @ responses . activate def test_reddit_standalone ( self , cursor ) : [EOL] url = [string] [EOL] body = [string] [EOL] mimetype = add_image ( url , body ) [EOL] [EOL] [comment] [EOL] media_id = [number] [EOL] insert_submission ( cursor , [string] ) [EOL] insert_media ( cursor , media_id , [string] , url ) [EOL] [EOL] expected = [ ( [string] , media_id , None , None , None , None , mimetype , url , None , body ) ] [EOL] [EOL] medias = [ ( media_id , url ) ] [EOL] client = Client ( ) [EOL] imgur_client = ImgurClient ( [string] ) [EOL] ingest_standalones ( cursor , client , imgur_client , medias ) [EOL] [EOL] cursor . execute ( [string] ) [EOL] records = cursor . fetchall ( ) [EOL] assert records == expected [EOL] [EOL] @ responses . activate def test_imgur_standalone ( self , cursor ) : [EOL] url = [string] [EOL] body = [string] [EOL] mimetype = add_image ( url , body ) [EOL] [EOL] md_url = [string] [EOL] md = { [string] : { [string] : [string] , [string] : [string] , [string] : [number] , [string] : mimetype , [string] : [number] , [string] : url , } } [EOL] responses . add ( responses . GET , md_url , json = md ) [EOL] [EOL] [comment] [EOL] media_id = [number] [EOL] insert_submission ( cursor , [string] ) [EOL] insert_media ( cursor , media_id , [string] , url ) [EOL] [EOL] expected = [ ( [string] , media_id , None , None , md [ [string] ] [ [string] ] , md [ [string] ] [ [string] ] , mimetype , url , md [ [string] ] [ [string] ] , body ) ] [EOL] [EOL] medias = [ ( media_id , url ) ] [EOL] client = Client ( ) [EOL] imgur_client = ImgurClient ( [string] ) [EOL] ingest_standalones ( cursor , client , imgur_client , medias ) [EOL] [EOL] cursor . execute ( [string] ) [EOL] records = cursor . fetchall ( ) [EOL] assert records == expected [EOL] [EOL] class TestIngestAlbums ( object ) : [EOL] @ responses . activate def test_imgur_album ( self , cursor , imgur_album ) : [EOL] img_body = [string] [EOL] imgur_album , _ = add_album_response ( imgur_album , img_body = img_body ) [EOL] [EOL] album_data = imgur_album [ [string] ] [EOL] url = album_data [ [string] ] [comment] [EOL] album_id = album_data [ [string] ] [EOL] [EOL] [comment] [EOL] media_id = [number] [EOL] insert_submission ( cursor , [string] ) [EOL] insert_media ( cursor , media_id , [string] , url ) [EOL] [EOL] expected_albums = [ ( album_id , media_id , album_data [ [string] ] , album_data [ [string] ] , album_data [ [string] ] , url , album_data [ [string] ] , ) ] [EOL] expected_images = [ ( img [ [string] ] , media_id , album_id , img [ [string] ] , img [ [string] ] , img [ [string] ] , [string] , img [ [string] ] , img [ [string] ] , img_body ) for img in album_data [ [string] ] ] [EOL] [EOL] medias = [ ( media_id , url ) ] [EOL] imgur_client = ImgurClient ( [string] ) [EOL] ingest_albums ( cursor , imgur_client , medias ) [EOL] [EOL] cursor . execute ( [string] ) [EOL] albums = cursor . fetchall ( ) [EOL] assert albums == expected_albums [EOL] [EOL] cursor . execute ( [string] ) [EOL] images = cursor . fetchall ( ) [EOL] assert images == expected_images [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.models.Image$ 0 $src.scrape.images.Client$ 0 0 0 0 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 0 0 $src.scrape.models.Image$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.models.Image$ 0 $src.scrape.images.Client$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.models.Image$ 0 $src.scrape.images.Client$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 $src.scrape.models.Image$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.int,None,None,None,None,typing.Any,builtins.str,None,builtins.bytes]]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 $builtins.int$ 0 $builtins.str$ 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 $src.scrape.images.ImgurClient$ 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[builtins.str,builtins.int,None,None,None,None,typing.Any,builtins.str,None,builtins.bytes]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.int,None,None,typing.Any,typing.Any,typing.Any,builtins.str,typing.Any,builtins.bytes]]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 $builtins.int$ 0 $builtins.str$ 0 0 0 $src.scrape.images.Client$ 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.Client$ 0 $src.scrape.images.ImgurClient$ 0 $typing.List[typing.Tuple[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[builtins.str,builtins.int,None,None,typing.Any,typing.Any,typing.Any,builtins.str,typing.Any,builtins.bytes]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.int,typing.Any,typing.Any,typing.Any,typing.Any,typing.Any]]$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.int,typing.Any,typing.Any,typing.Any,typing.Any,builtins.str,typing.Any,typing.Any,builtins.bytes]]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,typing.Any]]$ 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 $src.scrape.images.ImgurClient$ 0 0 0 0 0 0 0 0 0 0 $src.scrape.images.ImgurClient$ 0 $typing.List[typing.Tuple[builtins.int,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[typing.Any,builtins.int,typing.Any,typing.Any,typing.Any,typing.Any,typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Tuple[typing.Any,builtins.int,typing.Any,typing.Any,typing.Any,typing.Any,builtins.str,typing.Any,typing.Any,builtins.bytes]]$ 0
from typing import Any , Optional , Dict , Tuple , List , Union [EOL] import requests [EOL] import src [EOL] import tests [EOL] import typing [EOL] import json [EOL] import pytest [EOL] import responses [EOL] [EOL] from src . scrape . subreddit import extract_submissions , ingest , paginated_search [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def listing ( ) : [EOL] with open ( [string] ) as fh : [EOL] data = json . load ( fh ) [EOL] return data [EOL] [EOL] [EOL] class TestExtractSubmissions ( object ) : [EOL] query = [string] [EOL] subreddit = [string] [EOL] [EOL] def test_extracts_all_submissions ( self , listing ) : [EOL] expected_len = [number] [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] [EOL] assert len ( extracted ) == expected_len [EOL] [EOL] def test_saves_additional_metadata ( self , listing ) : [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] [EOL] submissions = [ s for s , _ in extracted ] [EOL] assert all ( s . search_query == self . query for s in submissions ) [EOL] assert all ( s . subreddit == self . subreddit for s in submissions ) [EOL] [EOL] def test_propagates_empty ( self ) : [EOL] [comment] [EOL] listing = { [string] : [string] , [string] : { [string] : None , [string] : [ ] , } , } [EOL] n_expected = [number] [EOL] [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] [EOL] assert len ( extracted ) == n_expected [EOL] [EOL] def test_medias_are_associated ( self , listing ) : [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] assert all ( m is None or s . id == m . submission_id for s , m in extracted ) [EOL] [EOL] def test_medias_are_direct ( self , listing ) : [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] medias = [ m for _ , m in extracted if m is not None ] [EOL] assert all ( m . is_direct for m in medias ) [EOL] [EOL] def test_medias_dont_have_text ( self , listing ) : [EOL] extracted = extract_submissions ( listing , self . subreddit , self . query ) [EOL] medias = [ m for _ , m in extracted if m is not None ] [EOL] assert all ( m . txt is None for m in medias ) [EOL] [EOL] class MockSearchResults ( object ) : [EOL] limit = [number] [comment] [EOL] headers = { [string] : [string] } [EOL] [EOL] def __init__ ( self , listing ) : [EOL] self . kind = listing [ [string] ] [EOL] self . children = listing [ [string] ] [ [string] ] [EOL] [EOL] @ property def max_responses ( self ) : [EOL] n , remainder = divmod ( len ( self . children ) , self . limit ) [EOL] if remainder > [number] : [EOL] n += [number] [EOL] return n [EOL] [EOL] def listing ( self , submissions , after ) : [EOL] data = { [string] : self . kind , [string] : { [string] : submissions , [string] : after , } , } [EOL] return json . dumps ( data ) [EOL] [EOL] def get ( self , request ) : [EOL] qs = responses . urlparse ( request . url ) . query [EOL] params = { k : v for k , v in responses . parse_qsl ( qs ) } [EOL] after = params . get ( [string] ) [EOL] [EOL] if after is None : [EOL] start_pos = [number] [EOL] else : [EOL] s_ids = [ s [ [string] ] [ [string] ] for s in self . children ] [EOL] try : [EOL] start_pos = s_ids . index ( after ) + [number] [EOL] except IndexError : [EOL] return ( [number] , { } , [string] ) [EOL] [EOL] end_pos = start_pos + self . limit [EOL] submissions = self . children [ start_pos : end_pos ] [EOL] [EOL] if end_pos >= len ( self . children ) : [EOL] resp_after = None [EOL] else : [EOL] resp_after = submissions [ - [number] ] [ [string] ] [ [string] ] [EOL] [EOL] return ( [number] , self . headers , self . listing ( submissions , resp_after ) ) [EOL] [EOL] class TestPaginatedSearch ( object ) : [EOL] @ responses . activate def test_mock ( self , listing ) : [EOL] subreddit = [string] [EOL] url = f" [string] { subreddit } [string] " [EOL] [EOL] mock_search = MockSearchResults ( listing ) [EOL] responses . add_callback ( responses . GET , url , mock_search . get ) [EOL] [EOL] out = list ( paginated_search ( subreddit , query = [string] , after = None ) ) [EOL] [EOL] assert all ( r . ok for r in out ) [EOL] assert len ( out ) == mock_search . max_responses [EOL] [EOL] class TestIngest ( object ) : [EOL] @ responses . activate def test_mock_without_resume ( self , cursor , listing ) : [EOL] subreddit = [string] [EOL] url = f" [string] { subreddit } [string] " [EOL] [EOL] mock_search = MockSearchResults ( listing ) [EOL] responses . add_callback ( responses . GET , url , mock_search . get ) [EOL] [EOL] ingest ( cursor , query = [string] , subreddit = subreddit ) [EOL] [EOL] cursor . execute ( [string] ) [EOL] count = cursor . fetchone ( ) [ [number] ] [EOL] [EOL] assert count == len ( mock_search . children ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Submission]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Submission]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Submission]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.Dict[builtins.str,typing.Optional[typing.List[typing.Any]]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.Dict[builtins.str,typing.Optional[typing.List[typing.Any]]],builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0 0 $typing.List[requests.models.Response]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[requests.models.Response]$ 0 0 0 0 0 $typing.List[requests.models.Response]$ 0 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $tests.scrape.test_subreddit.MockSearchResults$ 0 0 0 0
from typing import Dict , Any [EOL] import builtins [EOL] import tests [EOL] import sqlite3 [EOL] import typing [EOL] from dataclasses import InitVar , dataclass , field [EOL] import pytest [EOL] import sqlite3 [EOL] [EOL] from src . scrape . common import from_json , insert_or_ignore , is_media_url [EOL] [EOL] [EOL] @ dataclass class Post : [EOL] author = field ( init = False ) [EOL] user = ... [EOL] content = ... [EOL] [EOL] def __post_init__ ( self , user , ** _ ) : [EOL] self . author = user [EOL] [EOL] @ pytest . fixture def cursor ( ) : [EOL] conn = sqlite3 . connect ( [string] ) [EOL] cursor = conn . cursor ( ) [EOL] cursor . execute ( [string] ) [EOL] yield cursor [EOL] conn . close ( ) [EOL] [EOL] [EOL] class TestFromJson ( object ) : [EOL] def test_initializes_dataclass ( self ) : [EOL] data = { [string] : [string] , [string] : [string] } [EOL] expected_post = Post ( user = None , content = [string] ) [EOL] expected_post . author = [string] [EOL] [EOL] post = from_json ( Post , ** data ) [EOL] [EOL] assert post == expected_post [EOL] [EOL] def test_only_selects_init_fields ( self ) : [EOL] data = { [string] : [string] , [string] : [string] , [string] : [string] } [EOL] expected_post = Post ( user = None , content = [string] ) [EOL] expected_post . author = [string] [EOL] [EOL] post = from_json ( Post , ** data ) [EOL] [EOL] assert post == expected_post [EOL] [EOL] def test_allows_missing_fields ( self ) : [EOL] data = { [string] : [string] } [EOL] expected_post = Post ( user = None , content = None ) [EOL] expected_post . author = [string] [EOL] [EOL] post = from_json ( Post , ** data ) [EOL] [EOL] assert post == expected_post [EOL] [EOL] class TestInsertOrIgnore ( object ) : [EOL] table = [string] [EOL] [EOL] def test_inserts ( self , cursor ) : [EOL] post = Post ( user = [string] , content = [string] ) [EOL] [EOL] insert_or_ignore ( cursor , self . table , post ) [EOL] cursor . execute ( f" [string] { self . table }" ) [EOL] results = cursor . fetchall ( ) [EOL] [EOL] assert len ( results ) == [number] [EOL] author , content = results [ [number] ] [EOL] assert author == post . author [EOL] assert content == post . content [EOL] [EOL] def test_ignores ( self , cursor ) : [EOL] [comment] [EOL] post = Post ( user = [string] , content = None ) [EOL] [EOL] insert_or_ignore ( cursor , self . table , post ) [EOL] cursor . execute ( f" [string] { self . table }" ) [EOL] results = cursor . fetchall ( ) [EOL] [EOL] assert len ( results ) == [number] [EOL] [EOL] class TestIsMediaURL ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , [string] ] , ids = [ [string] , [string] , [string] , [string] ] ) def test_is_media ( self , url ) : [EOL] assert is_media_url ( url ) [EOL] [EOL] def test_reddit ( self ) : [EOL] url = [string] [EOL] assert not is_media_url ( url ) [EOL] [EOL] def test_unknown ( self ) : [EOL] url = [string] [EOL] assert not is_media_url ( url ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 $builtins.str$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 $builtins.str$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 $builtins.str$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $tests.scrape.test_common.Post$ 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.scrape.test_common.Post$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0
from typing import Any , Dict , Literal , Tuple , List , Union [EOL] import src [EOL] import typing_extensions [EOL] import urllib [EOL] import typing [EOL] from time import time [EOL] from unittest . mock import patch [EOL] from urllib . parse import parse_qsl , urlparse [EOL] import json [EOL] import pytest [EOL] import requests [EOL] import responses [EOL] [EOL] from src . scrape . models import ProductSearchResult [EOL] from src . scrape . zappos import ( SKIP_PRODUCT_STATUSES , ZapposClient , extract_description , get_products , paginated_search , reset_time , strip_legal_signs , ) [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] ) def product_response ( ) : [EOL] with open ( [string] ) as fh : [EOL] response_data = json . load ( fh ) [EOL] return response_data [EOL] [EOL] [EOL] class TestResetTime ( object ) : [EOL] def test_when_reset_is_unnecessary ( self ) : [EOL] headers = { [string] : [string] , [string] : [string] , } [EOL] wait_seconds = reset_time ( headers ) [EOL] assert wait_seconds == [number] [EOL] [EOL] def test_when_short_limit_is_low ( self ) : [EOL] [comment] [EOL] ms = int ( time ( ) * [number] ) + [number] [EOL] headers = { [string] : [string] , [string] : str ( ms ) , [string] : [string] , } [EOL] wait_seconds = reset_time ( headers ) [EOL] assert wait_seconds > [number] [EOL] [EOL] def test_when_long_limit_is_low ( self ) : [EOL] [comment] [EOL] ms = int ( time ( ) * [number] ) + [number] [EOL] headers = { [string] : [string] , [string] : [string] , [string] : str ( ms ) , } [EOL] wait_seconds = reset_time ( headers ) [EOL] assert wait_seconds > [number] [EOL] [EOL] def test_long_reset_when_limits_are_both_low ( self ) : [EOL] [comment] [EOL] [comment] [EOL] ms = int ( time ( ) * [number] ) + [number] [EOL] headers = { [string] : [string] , [string] : [string] , [string] : [string] , [string] : str ( ms ) , } [EOL] wait_seconds = reset_time ( headers ) [EOL] assert wait_seconds > [number] [EOL] [EOL] class TestStripLegalSigns ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] , ids = [ [string] , [string] , [string] , [string] ] ) def test_strips ( self , string , expected ) : [EOL] assert strip_legal_signs ( string ) == expected [EOL] [EOL] class TestExtractDescription ( object ) : [EOL] def test_ignores_link ( self ) : [EOL] expected = [string] [EOL] html = [string] . join ( [ [string] , [string] , f" [string] { expected } [string] " , [string] , ] ) [EOL] assert extract_description ( html , [string] ) == expected [EOL] [EOL] def test_returns_null_if_no_match ( self ) : [EOL] html = [string] . join ( [ [string] , [string] , [string] , ] ) [EOL] assert extract_description ( html , [string] ) is None [EOL] [EOL] def test_gets_text_within_markup ( self ) : [EOL] expected = [string] [EOL] html = [string] . join ( [ [string] , f" [string] { expected } [string] " , [string] , ] ) [EOL] assert extract_description ( html , [string] ) == expected [EOL] [EOL] def test_gets_first_occurrence ( self ) : [EOL] expected = [string] [EOL] html = [string] . join ( [ [string] , f" [string] { expected } [string] " , [string] , [string] , ] ) [EOL] assert extract_description ( html , [string] ) == expected [EOL] [EOL] def test_is_stripped ( self ) : [EOL] expected = [string] [EOL] html = [string] . join ( [ [string] , f" [string] { expected } [string] " , [string] , [string] , ] ) [EOL] assert extract_description ( html , [string] ) == expected [EOL] [EOL] def headers ( ) : [EOL] return { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , } [EOL] [EOL] class TestZapposClient ( object ) : [EOL] url = [string] [EOL] key = [string] [EOL] [EOL] def test_with_key ( self ) : [EOL] key = [string] [EOL] params = { [string] : [number] , [string] : [number] } [EOL] expected = { [string] : [number] , [string] : [number] , [string] : key } [EOL] [EOL] client = ZapposClient ( key ) [EOL] out = client . with_key ( params ) [EOL] [EOL] assert out == expected [EOL] assert params == { [string] : [number] , [string] : [number] } [comment] [EOL] [EOL] @ responses . activate def test_dispatch_adds_key ( self ) : [EOL] def has_key ( request ) : [EOL] parsed = urlparse ( request . url ) [EOL] param_names , param_values = zip ( * parse_qsl ( parsed . query ) ) [EOL] idx = param_names . index ( [string] ) [EOL] assert param_values [ idx ] == self . key [EOL] return ( [number] , { } , None ) [EOL] [EOL] responses . add_callback ( responses . GET , self . url , callback = has_key ) [EOL] [EOL] client = ZapposClient ( self . key ) [EOL] client . dispatch ( [string] , self . url ) [EOL] [EOL] @ responses . activate def test_dispatch_retries_when_throttled ( self ) : [EOL] responses . add ( responses . GET , self . url , status = [number] ) [EOL] responses . add ( responses . GET , self . url , status = [number] ) [EOL] [EOL] client = ZapposClient ( self . key ) [EOL] client . retry_delay_seconds = [number] [EOL] client . dispatch ( [string] , self . url ) [EOL] [EOL] @ responses . activate def test_dispatch_raises_unknown_error ( self ) : [EOL] responses . add ( responses . GET , self . url , status = [number] ) [EOL] [EOL] with pytest . raises ( requests . RequestException ) : [EOL] client = ZapposClient ( self . key ) [EOL] client . dispatch ( [string] , self . url ) [EOL] [EOL] @ responses . activate def test_dispatch_pre_empts_rate_limiting ( self ) : [EOL] delay_seconds = [number] [EOL] [EOL] def cb ( request ) : [EOL] reset_ms = int ( [number] * ( time ( ) + delay_seconds ) ) [EOL] headers = { [string] : [string] , [string] : str ( reset_ms ) , [string] : [string] , } [EOL] return ( [number] , headers , None ) [EOL] [EOL] responses . add_callback ( responses . GET , self . url , callback = cb ) [EOL] [EOL] with patch ( [string] , return_value = None ) as patched_sleep : [EOL] client = ZapposClient ( self . key ) [EOL] client . dispatch ( [string] , self . url ) [EOL] [EOL] assert patched_sleep . call_count == [number] [EOL] [EOL] @ responses . activate def test_gets_product ( self , product_response ) : [EOL] product_id = [number] [EOL] [EOL] def has_includes ( request ) : [EOL] parsed = urlparse ( request . url ) [EOL] param_names , _ = zip ( * parse_qsl ( parsed . query ) ) [EOL] assert [string] in param_names [EOL] [EOL] headers = { [string] : [string] } [EOL] return ( [number] , headers , json . dumps ( product_response ) ) [EOL] [EOL] responses . add_callback ( responses . GET , f" [string] { product_id }" , callback = has_includes , ) [EOL] [EOL] client = ZapposClient ( self . key ) [EOL] product = client . product_description ( product_id ) [EOL] [EOL] assert product . id == product_id [EOL] assert product . description is None [comment] [EOL] [EOL] @ responses . activate def test_search_has_params ( self ) : [EOL] url = [string] [EOL] term = [string] [EOL] page = [string] [EOL] limit = [string] [EOL] includes = [ [string] ] [EOL] filters = { [string] : [ [string] , [string] ] } [EOL] expected_params = { [string] : term , [string] : page , [string] : limit , [string] : json . dumps ( includes ) , [string] : json . dumps ( filters ) , } [EOL] [EOL] def has_params ( request ) : [EOL] parsed = urlparse ( request . url ) [EOL] params = { name : value for name , value in parse_qsl ( parsed . query ) if name != [string] } [EOL] assert params == expected_params [EOL] return ( [number] , { } , None ) [EOL] [EOL] responses . add_callback ( responses . GET , url , callback = has_params ) [EOL] [EOL] client = ZapposClient ( self . key ) [EOL] client . search ( term , int ( page ) , int ( limit ) ) [EOL] [EOL] class TestGetProducts ( object ) : [EOL] @ pytest . mark . parametrize ( [string] , SKIP_PRODUCT_STATUSES ) @ responses . activate def test_skips_skippable_errors ( self , status_code ) : [EOL] psr = ProductSearchResult ( [string] , [number] , [string] , [string] , [string] ) [EOL] url = f" [string] { psr . product_id }" [EOL] responses . add ( responses . GET , url , status = status_code ) [EOL] [EOL] client = ZapposClient ( [string] ) [EOL] products = list ( get_products ( client , [ psr ] ) ) [EOL] [EOL] assert products == [ ] [EOL] [EOL] @ responses . activate def test_gets_multiple_products ( self , product_response ) : [EOL] p_ids = ( [number] , [number] ) [EOL] psrs = [ ] [EOL] for p_id in p_ids : [EOL] psr = ProductSearchResult ( [string] , p_id , [string] , [string] , [string] ) [EOL] url = f" [string] { psr . product_id }" [EOL] responses . add ( responses . GET , url , json = product_response ) [EOL] psrs . append ( psr ) [EOL] [EOL] client = ZapposClient ( [string] ) [EOL] products = list ( get_products ( client , psrs ) ) [EOL] [EOL] assert len ( products ) == len ( psrs ) [EOL] assert products [ [number] ] . id == p_ids [ [number] ] [EOL] assert products [ [number] ] . id == p_ids [ [number] ] [EOL] [EOL] [EOL] class TestPaginatedSearch ( object ) : [EOL] key = [string] [EOL] [EOL] @ responses . activate def test_paginated_search ( self ) : [EOL] url = [string] [EOL] expected_count = [number] [EOL] [EOL] def cb ( request ) : [EOL] cb . request_count += [number] [EOL] content = json . dumps ( { [string] : str ( expected_count ) , [string] : [string] , [string] : [ { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] , } ] , } ) [EOL] headers = { [string] : [string] } [EOL] return ( [number] , headers , content ) [EOL] [EOL] cb . request_count = [number] [EOL] responses . add_callback ( responses . GET , url , callback = cb ) [EOL] [EOL] client = ZapposClient ( self . key ) [EOL] search_results = paginated_search ( client , term = [string] ) [EOL] [EOL] assert len ( search_results ) == expected_count [EOL] assert cb . request_count == expected_count [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 $builtins.str$ 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 $src.scrape.zappos.ZapposClient$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 $typing.Dict[builtins.str,typing.Union[builtins.int,builtins.str]]$ 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.models.Product$ 0 $src.scrape.zappos.ZapposClient$ 0 0 0 $builtins.int$ 0 0 0 0 $src.scrape.models.Product$ 0 0 0 $builtins.int$ 0 0 $src.scrape.models.Product$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $urllib.parse.ParseResult$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Tuple[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $src.scrape.models.ProductSearchResult$ 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 0 0 $typing.Tuple[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 0 $typing.List[src.scrape.models.Product]$ 0 0 0 0 0 0 $typing.Tuple[typing_extensions.Literal,typing_extensions.Literal]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.ProductSearchResult]$ 0 0 0 $src.scrape.zappos.ZapposClient$ 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.ProductSearchResult]$ 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0
	0
from typing import Optional [EOL] import sqlite3 [EOL] import typing [EOL] from pkgutil import get_data [EOL] import sqlite3 [EOL] [EOL] [EOL] _rollups_sql = get_data ( [string] , [string] ) [EOL] [EOL] [EOL] def create_views ( cursor ) : [EOL] if _rollups_sql is None : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] cursor . executescript ( _rollups_sql . decode ( ) ) [EOL] return [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.bytes]$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 $typing.Optional[builtins.bytes]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.bytes]$ 0 0 0 0 0 0 0 0
from typing import Any , Optional , Dict , Iterator , Tuple , List , Union [EOL] import src [EOL] import argparse [EOL] import requests [EOL] import builtins [EOL] import sqlite3 [EOL] import typing [EOL] [docstring] [EOL] [EOL] from typing import Any , Dict , Iterator , List , Tuple , Optional [EOL] import logging [EOL] import requests [EOL] import sqlite3 [EOL] import sys [EOL] [EOL] from src . scrape . common import ( base_parser , from_json , insert_or_ignore , is_media_url , setup_logging , ) [EOL] from src . scrape . models import Media , Submission [EOL] [EOL] [EOL] MAX_LIMIT = [number] [EOL] TIMEOUT = [number] [EOL] SUBREDDIT = [string] [EOL] USER_AGENT = ( [string] [string] [string] [string] ) [EOL] [EOL] [EOL] def search ( subreddit , query , after = None ) : [EOL] url = f" [string] { subreddit } [string] " [EOL] headers = { [string] : USER_AGENT } [EOL] [EOL] params = { [string] : query , [string] : MAX_LIMIT , [string] : [string] , [string] : [string] , [string] : [string] , } [EOL] if after is not None : [EOL] params . update ( { [string] : after } ) [EOL] [EOL] response = requests . request ( [string] , url , params = params , headers = headers , timeout = TIMEOUT ) [EOL] return response [EOL] [EOL] def paginated_search ( subreddit , query , after ) : [comment] [EOL] while True : [EOL] response = search ( subreddit , query , after = after ) [EOL] if not response . ok : [EOL] logging . error ( [string] , response . status_code , response . reason ) [EOL] break [EOL] [EOL] yield response [EOL] [EOL] after = response . json ( ) [ [string] ] [ [string] ] [EOL] if after is None : [EOL] break [EOL] [EOL] return [EOL] [EOL] def extract_submissions ( listing , subreddit , query ) : [comment] [EOL] children = listing [ [string] ] . get ( [string] , [ ] ) [EOL] modeled = [ ] [EOL] for child in children : [EOL] data = { ** child [ [string] ] , [string] : query , [string] : subreddit } [EOL] submission = from_json ( Submission , ** data ) [EOL] media = None [EOL] if is_media_url ( data [ [string] ] ) : [EOL] media = Media ( submission_id = submission . id , url = data [ [string] ] , is_direct = True , txt = None ) [EOL] modeled . append ( ( submission , media ) ) [EOL] [EOL] return modeled [EOL] [EOL] def ingest ( cursor , query , subreddit ) : [EOL] responses = paginated_search ( subreddit , query , after = None ) [EOL] for response in responses : [EOL] listing = response . json ( ) [EOL] extracted = extract_submissions ( listing , subreddit , query ) [EOL] for submission , media in extracted : [EOL] insert_or_ignore ( cursor , [string] , submission ) [EOL] if media is not None : [EOL] insert_or_ignore ( cursor , [string] , media ) [EOL] [EOL] return [EOL] [EOL] def main ( ) : [EOL] setup_logging ( ) [EOL] parser = base_parser ( description = __doc__ ) [EOL] parser . add_argument ( [string] , [string] , type = str , help = [string] ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] conn = sqlite3 . connect ( args . conn ) [EOL] cursor = conn . cursor ( ) [EOL] logging . info ( [string] ) [EOL] [EOL] status = [number] [EOL] try : [EOL] logging . info ( [string] , args . query ) [EOL] ingest ( cursor , args . query , SUBREDDIT ) [EOL] except sqlite3 . Error as e : [EOL] logging . error ( [string] , e ) [EOL] conn . rollback ( ) [EOL] status = [number] [EOL] else : [EOL] conn . commit ( ) [EOL] finally : [EOL] conn . close ( ) [EOL] logging . info ( [string] , args . query ) [EOL] [EOL] return status [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] sys . exit ( main ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $requests.Response$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Iterator[requests.Response]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[src.scrape.models.Submission,typing.Optional[src.scrape.models.Media]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Optional , MutableMapping , Dict , Literal , Pattern , Tuple , Iterable , List , Type , Union [EOL] import datetime [EOL] import src [EOL] import argparse [EOL] import requests [EOL] import builtins [EOL] import typing_extensions [EOL] import sqlite3 [EOL] import typing [EOL] [docstring] [EOL] [EOL] from bs4 import BeautifulSoup , element [EOL] from datetime import datetime [EOL] from time import sleep [EOL] from typing import Dict , Iterable , List , MutableMapping , Optional [EOL] import json [EOL] import logging [EOL] import re [EOL] import requests [EOL] import sqlite3 [EOL] import sys [EOL] [EOL] from src . scrape . common import base_parser , insert_or_ignore , from_json , setup_logging [EOL] from src . scrape . models import Product , ProductSearchResult [EOL] [EOL] [EOL] [comment] [EOL] MAX_SEARCH_LIMIT = [number] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] SHOE_CATEGORIES = ( [string] , [string] ) [EOL] LOG_INTERVAL = [number] [EOL] [comment] [EOL] SKIP_PRODUCT_STATUSES = ( [number] , [number] ) [EOL] [EOL] [EOL] def reset_time ( headers ) : [EOL] def le_threshold ( type_ ) : [EOL] remaining = headers . get ( f" [string] { type_ } [string] " ) [EOL] if remaining is None : [EOL] return False [EOL] return int ( remaining ) <= [number] [EOL] [EOL] reset_long = le_threshold ( [string] ) [EOL] reset_short = le_threshold ( [string] ) [EOL] [EOL] if not reset_long and not reset_short : [EOL] return [number] [EOL] [EOL] type_ = [string] if reset_long else [string] [EOL] reset_ms = int ( headers . get ( f" [string] { type_ } [string] " , [number] ) ) [EOL] reset_time = datetime . fromtimestamp ( reset_ms / [number] ) [EOL] wait_seconds = ( reset_time - datetime . now ( ) ) . total_seconds ( ) [EOL] return wait_seconds [EOL] [EOL] def strip_legal_signs ( string ) : [EOL] signs = ( [string] , [string] , [string] , ) [EOL] for sign in signs : [EOL] string = string . replace ( sign , [string] ) [EOL] return string [EOL] [EOL] def extract_description ( description_html , brand ) : [EOL] pattern = re . compile ( rf" [string] { brand } [string] " , flags = re . IGNORECASE ) [EOL] soup = BeautifulSoup ( description_html , [string] ) [EOL] [EOL] for item in soup . find_all ( [string] ) : [EOL] for child in item . children : [EOL] [comment] [EOL] [comment] [EOL] if isinstance ( child , element . Tag ) and child . has_attr ( [string] ) : [EOL] continue [EOL] [EOL] [comment] [EOL] [comment] [EOL] text = getattr ( child , [string] , str ( child ) ) [EOL] normalized = strip_legal_signs ( text ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if re . search ( pattern , normalized ) : [EOL] return normalized [EOL] [EOL] return None [EOL] [EOL] class ZapposClient ( object ) : [EOL] base_url = [string] [EOL] max_retries = [number] [EOL] retry_delay_seconds = [number] * [number] [EOL] [EOL] def __init__ ( self , api_key ) : [EOL] self . api_key = api_key [EOL] [EOL] def with_key ( self , params ) : [EOL] return { ** params , [string] : self . api_key } [EOL] [EOL] def dispatch ( self , method , url , ** kwds ) : [EOL] params = self . with_key ( kwds . pop ( [string] , { } ) ) [EOL] [EOL] for _ in range ( self . max_retries + [number] ) : [EOL] response = requests . request ( method , url , params = params , ** kwds ) [EOL] if response . status_code != [number] : [EOL] break [EOL] [EOL] logging . warning ( [string] , self . retry_delay_seconds ) [EOL] sleep ( self . retry_delay_seconds ) [EOL] [EOL] response . raise_for_status ( ) [EOL] [EOL] [comment] [EOL] wait_time = reset_time ( response . headers ) [EOL] if wait_time > [number] : [EOL] logging . info ( [string] , wait_time ) [EOL] sleep ( wait_time ) [EOL] return response [EOL] [EOL] def search ( self , term , page , limit ) : [EOL] search_url = f"{ self . base_url } [string] " [EOL] params = { [string] : term , [string] : str ( page ) , [string] : str ( limit ) , [string] : json . dumps ( [ [string] ] ) , [string] : json . dumps ( { [string] : SHOE_CATEGORIES } ) , } [EOL] return self . dispatch ( [string] , search_url , params = params ) [EOL] [EOL] def product_description ( self , product_id ) : [EOL] if product_id < [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] product_url = f"{ self . base_url } [string] { product_id }" [EOL] includes = [ [string] ] [EOL] params = { [string] : json . dumps ( includes ) } [EOL] response = self . dispatch ( [string] , product_url , params = params ) [EOL] [EOL] data = response . json ( ) [EOL] [comment] [EOL] product_raw , * unexpected = data [ [string] ] [EOL] if unexpected : [EOL] logging . warning ( [string] , len ( unexpected ) , product_id ) [EOL] [EOL] product = from_json ( Product , ** product_raw , id = product_id ) [EOL] if product . description is not None : [EOL] sentence = extract_description ( product . description , product . brand ) [EOL] product . description = sentence [EOL] return product [EOL] [EOL] def paginated_search ( client , term ) : [EOL] stop_at = None [EOL] page = [number] [EOL] results = [ ] [EOL] while stop_at is None or len ( results ) < stop_at : [EOL] response = client . search ( term , page = page , limit = MAX_SEARCH_LIMIT ) [EOL] [EOL] data = response . json ( ) [EOL] if stop_at is None : [EOL] stop_at = int ( data [ [string] ] ) [EOL] [EOL] results . extend ( [ from_json ( ProductSearchResult , ** result , search_query = term ) for result in data [ [string] ] ] ) [EOL] page += [number] [EOL] [EOL] return results [EOL] [EOL] def get_products ( client , records ) : [comment] [EOL] for i , record in enumerate ( records , start = [number] ) : [EOL] if i % LOG_INTERVAL == [number] : [EOL] logging . info ( f" [string] { i } [string] { len ( records ) }" ) [EOL] [EOL] try : [EOL] product = client . product_description ( record . product_id ) [EOL] except requests . RequestException as e : [EOL] if e . response . status_code in SKIP_PRODUCT_STATUSES : [EOL] continue [EOL] raise e [EOL] yield product [EOL] [EOL] def main ( ) : [EOL] setup_logging ( ) [EOL] parser = base_parser ( description = __doc__ ) [EOL] parser . add_argument ( [string] , required = True , type = str , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , default = [string] , help = [string] ) [EOL] parser . add_argument ( [string] , action = [string] , help = [string] ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] if args . search and args . fetch : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] if not args . search and not args . fetch : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] conn = sqlite3 . connect ( args . conn ) [EOL] cursor = conn . cursor ( ) [EOL] [EOL] client = ZapposClient ( args . api_key ) [EOL] [EOL] status = [number] [EOL] try : [EOL] if args . search : [EOL] logging . info ( [string] , args . query ) [EOL] search_results = paginated_search ( client , term = args . query ) [EOL] logging . info ( [string] , len ( search_results ) ) [EOL] for result in search_results : [EOL] insert_or_ignore ( cursor , [string] , result ) [EOL] elif args . fetch : [EOL] logging . info ( [string] ) [EOL] cursor . execute ( [string] ) [EOL] records = [ ProductSearchResult ( * row ) for row in cursor ] [EOL] logging . info ( [string] , len ( records ) ) [EOL] [EOL] logging . info ( [string] ) [EOL] for product in get_products ( client , records ) : [EOL] insert_or_ignore ( cursor , [string] , product ) [EOL] except Exception as e : [EOL] status = [number] [EOL] logging . error ( [string] , e ) [EOL] if not args . fetch : [EOL] conn . rollback ( ) [EOL] else : [EOL] conn . commit ( ) [EOL] else : [EOL] logging . info ( [string] ) [EOL] conn . commit ( ) [EOL] finally : [EOL] conn . close ( ) [EOL] [EOL] return status [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] sys . exit ( main ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $src.scrape.models.Product$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import List , Iterator , Any , Tuple [EOL] import src [EOL] import argparse [EOL] import builtins [EOL] import sqlite3 [EOL] import typing [EOL] [docstring] [EOL] [EOL] from bs4 import BeautifulSoup [EOL] from itertools import chain , starmap [EOL] from typing import List , Tuple [EOL] import html [EOL] import logging [EOL] import sqlite3 [EOL] import sys [EOL] [EOL] from src . scrape . common import base_parser , insert_or_ignore , is_media_url , setup_logging [EOL] from src . scrape . models import Media [EOL] [EOL] [EOL] def get_submission_contents ( cursor ) : [EOL] cursor . execute ( [string] ) [EOL] return cursor . fetchall ( ) [EOL] [EOL] def extract ( submission_id , selftext_html ) : [EOL] raw = html . unescape ( selftext_html ) [EOL] soup = BeautifulSoup ( raw , [string] ) [EOL] links = soup . find_all ( [string] ) [EOL] [EOL] medias = [ Media ( submission_id = submission_id , url = link . attrs [ [string] ] , is_direct = False , txt = link . text ) for link in links if is_media_url ( link . attrs [ [string] ] ) ] [EOL] return medias [EOL] [EOL] def main ( ) : [EOL] setup_logging ( ) [EOL] parser = base_parser ( description = __doc__ ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] conn = sqlite3 . connect ( args . conn ) [EOL] cursor = conn . cursor ( ) [EOL] logging . info ( [string] ) [EOL] [EOL] status = [number] [EOL] try : [EOL] records = get_submission_contents ( cursor ) [EOL] g = chain . from_iterable ( starmap ( extract , records ) ) [EOL] medias = list ( g ) [EOL] for media in medias : [EOL] insert_or_ignore ( cursor , [string] , media ) [EOL] except sqlite3 . Error as e : [EOL] logging . error ( [string] , e ) [EOL] conn . rollback ( ) [EOL] status = [number] [EOL] else : [EOL] conn . commit ( ) [EOL] finally : [EOL] conn . close ( ) [EOL] logging . info ( [string] ) [EOL] [EOL] return status [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] sys . exit ( main ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[src.scrape.models.Media]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional [EOL] import builtins [EOL] import typing [EOL] [docstring] [EOL] [EOL] from dataclasses import InitVar , dataclass , field [EOL] from typing import Optional [EOL] [EOL] [EOL] @ dataclass class Submission : [EOL] id = ... [EOL] title = ... [EOL] author_fullname = ... [EOL] author = ... [EOL] subreddit = ... [EOL] permalink = ... [EOL] created_utc = ... [EOL] [EOL] selftext_html = ... [EOL] num_comments = ... [EOL] comments = field ( init = False ) [EOL] gilded = ... [EOL] downs = ... [EOL] ups = ... [EOL] score = ... [EOL] [EOL] search_query = ... [EOL] [EOL] def __post_init__ ( self , num_comments , ** _ ) : [EOL] self . comments = num_comments [EOL] [EOL] @ dataclass class Media : [EOL] submission_id = ... [EOL] url = ... [EOL] is_direct = ... [EOL] txt = ... [EOL] [EOL] @ dataclass class Album : [EOL] id = ... [EOL] media_id = ... [EOL] title = ... [EOL] description = ... [EOL] datetime = ... [EOL] uploaded_utc = field ( init = False ) [EOL] link = ... [EOL] url = field ( init = False ) [EOL] views = ... [EOL] [EOL] def __post_init__ ( self , datetime , link , ** _ ) : [EOL] self . uploaded_utc = datetime [EOL] self . url = link [EOL] [EOL] @ dataclass class Image : [EOL] id = ... [EOL] media_id = ... [EOL] album_id = ... [EOL] title = ... [EOL] description = ... [EOL] datetime = ... [EOL] uploaded_utc = field ( init = False ) [EOL] type = ... [EOL] mimetype = field ( init = False ) [EOL] link = ... [EOL] url = field ( init = False ) [EOL] views = ... [EOL] img = ... [EOL] [EOL] def __post_init__ ( self , datetime , type , link , ** _ ) : [EOL] self . uploaded_utc = datetime [EOL] self . mimetype = type [EOL] self . url = link [EOL] [EOL] @ dataclass class ProductSearchResult : [EOL] brandName = ... [EOL] brand = field ( init = False ) [EOL] productId = ... [EOL] product_id = field ( init = False ) [EOL] productName = ... [EOL] product_name = field ( init = False ) [EOL] categoryFacet = ... [EOL] category = field ( init = False ) [EOL] search_query = ... [EOL] [EOL] def __post_init__ ( self , brandName , productId , productName , categoryFacet , ** _ ) : [comment] [EOL] self . brand = brandName [EOL] self . product_id = int ( productId ) [EOL] self . product_name = productName [EOL] self . category = categoryFacet [EOL] [EOL] @ dataclass class Product : [EOL] id = ... [EOL] brandName = ... [EOL] brand = field ( init = False ) [EOL] productName = ... [EOL] name = field ( init = False ) [EOL] defaultProductUrl = ... [EOL] default_url = field ( init = False ) [EOL] description = ... [EOL] [EOL] def __post_init__ ( self , brandName , productName , defaultProductUrl , ** _ ) : [comment] [EOL] self . brand = brandName [EOL] self . name = productName [EOL] self . default_url = defaultProductUrl [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $dataclasses.InitVar[builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $dataclasses.InitVar[builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $dataclasses.InitVar[typing.Optional[builtins.int]]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[typing.Optional[builtins.str]]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.bytes]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $dataclasses.InitVar[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0