	0
	0
	0
from typing import Any [EOL] import typing [EOL] from allennlp . data . tokenizers import CharacterTokenizer [EOL] [EOL] [EOL] tokenizer = CharacterTokenizer ( ) [EOL] passage = ( [string] [string] [string] [string] [string] [string] ) [EOL] [EOL] [EOL] def bench_character_tokenizer ( benchmark ) : [EOL] benchmark ( tokenizer . tokenize , passage ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0
	0
from d . d import D [EOL]	0 0 0 0 0 0 0
import argparse [EOL] import argparse [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands import Subcommand [EOL] [EOL] [EOL] def do_nothing ( _ ) : [EOL] pass [EOL] [EOL] [EOL] @ Subcommand . register ( [string] ) class D ( Subcommand ) : [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] subparser = parser . add_parser ( self . name , description = [string] , help = [string] ) [EOL] subparser . set_defaults ( func = do_nothing ) [EOL] return subparser [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse._SubParsersAction$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0
import os [EOL] [EOL] _MAJOR = [string] [EOL] _MINOR = [string] [EOL] [comment] [EOL] [comment] [EOL] _PATCH = [string] [EOL] [comment] [EOL] [comment] [EOL] _SUFFIX = os . environ . get ( [string] , [string] ) [EOL] [EOL] VERSION_SHORT = [string] . format ( _MAJOR , _MINOR ) [EOL] VERSION = [string] . format ( _MAJOR , _MINOR , _PATCH , _SUFFIX ) [EOL]	0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0
[docstring] [EOL] [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . archival import archive_model , load_archive , Archive [EOL] from allennlp . models . simple_tagger import SimpleTagger [EOL] from allennlp . models . basic_classifier import BasicClassifier [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , Any , Dict , List , Union , Tuple [EOL] import logging [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] [docstring] [EOL] [EOL] import logging [EOL] import re [EOL] import math [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import torch [EOL] import transformers [EOL] [EOL] from allennlp . common import Params , Registrable [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def make_parameter_groups ( model_parameters , groups = None , ) : [EOL] [docstring] [EOL] if groups : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] parameter_groups = [ { [string] : [ ] } for _ in range ( len ( groups ) + [number] ) ] [EOL] [comment] [EOL] for k in range ( len ( groups ) ) : [EOL] parameter_groups [ k ] . update ( groups [ k ] [ [number] ] ) [EOL] [EOL] regex_use_counts = { } [EOL] parameter_group_names = [ set ( ) for _ in range ( len ( groups ) + [number] ) ] [EOL] for name , param in model_parameters : [EOL] [comment] [EOL] group_index = None [EOL] for k , group_regexes in enumerate ( groups ) : [EOL] for regex in group_regexes [ [number] ] : [EOL] if regex not in regex_use_counts : [EOL] regex_use_counts [ regex ] = [number] [EOL] if re . search ( regex , name ) : [EOL] if group_index is not None and group_index != k : [EOL] raise ValueError ( [string] . format ( name ) ) [EOL] group_index = k [EOL] regex_use_counts [ regex ] += [number] [EOL] [EOL] if group_index is not None : [EOL] parameter_groups [ group_index ] [ [string] ] . append ( param ) [EOL] parameter_group_names [ group_index ] . add ( name ) [EOL] else : [EOL] [comment] [EOL] parameter_groups [ - [number] ] [ [string] ] . append ( param ) [EOL] parameter_group_names [ - [number] ] . add ( name ) [EOL] [EOL] [comment] [EOL] no_grad_group_indices = [ ] [EOL] for k , ( names , group ) in enumerate ( zip ( parameter_group_names , parameter_groups ) ) : [EOL] if group . get ( [string] ) is False : [EOL] no_grad_group_indices . append ( k ) [EOL] logger . info ( [string] , names ) [EOL] for param in group [ [string] ] : [EOL] param . requires_grad_ ( False ) [EOL] [EOL] [comment] [EOL] unused_options = { key : val for key , val in group . items ( ) if key not in ( [string] , [string] ) } [EOL] if unused_options : [EOL] logger . warning ( [string] , unused_options , names ) [EOL] parameter_group_names = [ names for ( k , names ) in enumerate ( parameter_group_names ) if k not in no_grad_group_indices ] [EOL] parameter_groups = [ group for ( k , group ) in enumerate ( parameter_groups ) if k not in no_grad_group_indices ] [EOL] [EOL] [comment] [EOL] logger . info ( [string] ) [EOL] for k in range ( len ( parameter_groups ) ) : [EOL] group_options = { key : val for key , val in parameter_groups [ k ] . items ( ) if key != [string] } [EOL] logger . info ( [string] , k , list ( parameter_group_names [ k ] ) , group_options ) [EOL] [EOL] [comment] [EOL] for regex , count in regex_use_counts . items ( ) : [EOL] if count == [number] : [EOL] logger . warning ( [string] , regex , ) [EOL] [EOL] else : [EOL] parameter_groups = [ param for name , param in model_parameters ] [EOL] [EOL] [comment] [EOL] num_parameters = [number] [EOL] for parameter_group in parameter_groups : [EOL] if isinstance ( parameter_group , dict ) : [EOL] num_parameters += sum ( parameter . numel ( ) for parameter in parameter_group [ [string] ] ) [EOL] else : [EOL] num_parameters += parameter_group . numel ( ) [comment] [EOL] logger . info ( [string] , num_parameters ) [EOL] return parameter_groups [EOL] [EOL] [EOL] class Optimizer ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] @ staticmethod def default ( model_parameters ) : [EOL] return Optimizer . from_params ( model_parameters = model_parameters , params = Params ( { } ) ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AdamOptimizer ( Optimizer , torch . optim . Adam ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , weight_decay = [number] , amsgrad = False , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class SparseAdamOptimizer ( Optimizer , torch . optim . SparseAdam ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , betas = betas , eps = eps , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AdamaxOptimizer ( Optimizer , torch . optim . Adamax ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , weight_decay = [number] , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AdamWOptimizer ( Optimizer , torch . optim . AdamW ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , weight_decay = [number] , amsgrad = False , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , amsgrad = amsgrad , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class HuggingfaceAdamWOptimizer ( Optimizer , transformers . AdamW ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , weight_decay = [number] , correct_bias = True , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , betas = betas , eps = eps , weight_decay = weight_decay , correct_bias = correct_bias , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AdagradOptimizer ( Optimizer , torch . optim . Adagrad ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , lr_decay = [number] , weight_decay = [number] , initial_accumulator_value = [number] , eps = [number] , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , lr_decay = lr_decay , weight_decay = weight_decay , initial_accumulator_value = initial_accumulator_value , eps = eps , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AdadeltaOptimizer ( Optimizer , torch . optim . Adadelta ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , rho = [number] , eps = [number] , weight_decay = [number] , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , rho = rho , eps = eps , weight_decay = weight_decay , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class SgdOptimizer ( Optimizer , torch . optim . SGD ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , lr , parameter_groups = None , momentum = [number] , dampening = [number] , weight_decay = [number] , nesterov = False , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , momentum = momentum , dampening = dampening , weight_decay = weight_decay , nesterov = nesterov , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class RmsPropOptimizer ( Optimizer , torch . optim . RMSprop ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , alpha = [number] , eps = [number] , weight_decay = [number] , momentum = [number] , centered = False , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , alpha = alpha , eps = eps , weight_decay = weight_decay , momentum = momentum , centered = centered , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class AveragedSgdOptimizer ( Optimizer , torch . optim . ASGD ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , lambd = [number] , alpha = [number] , t0 = [number] , weight_decay = [number] , ) : [EOL] super ( ) . __init__ ( params = make_parameter_groups ( model_parameters , parameter_groups ) , lr = lr , lambd = lambd , alpha = alpha , t0 = t0 , weight_decay = weight_decay , ) [EOL] [EOL] [EOL] @ Optimizer . register ( [string] ) class DenseSparseAdam ( Optimizer , torch . optim . Optimizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_parameters , parameter_groups = None , lr = [number] , betas = ( [number] , [number] ) , eps = [number] , ) : [EOL] if not [number] <= lr : [EOL] raise ValueError ( [string] . format ( lr ) ) [EOL] if not [number] <= eps : [EOL] raise ValueError ( [string] . format ( eps ) ) [EOL] if not [number] <= betas [ [number] ] < [number] : [EOL] raise ValueError ( [string] . format ( betas [ [number] ] ) ) [EOL] if not [number] <= betas [ [number] ] < [number] : [EOL] raise ValueError ( [string] . format ( betas [ [number] ] ) ) [EOL] defaults = dict ( lr = lr , betas = betas , eps = eps ) [EOL] super ( ) . __init__ ( make_parameter_groups ( model_parameters , parameter_groups ) , defaults ) [EOL] [EOL] def step ( self , closure = None ) : [EOL] [docstring] [EOL] loss = None [EOL] if closure is not None : [EOL] loss = closure ( ) [EOL] [EOL] for group in self . param_groups : [EOL] for p in group [ [string] ] : [EOL] if p . grad is None : [EOL] continue [EOL] grad = p . grad . data [EOL] [EOL] state = self . state [ p ] [EOL] [EOL] [comment] [EOL] if len ( state ) == [number] : [EOL] state [ [string] ] = [number] [EOL] [comment] [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] [comment] [EOL] state [ [string] ] = torch . zeros_like ( p . data ) [EOL] [EOL] state [ [string] ] += [number] [EOL] [EOL] exp_avg , exp_avg_sq = state [ [string] ] , state [ [string] ] [EOL] beta1 , beta2 = group [ [string] ] [EOL] [EOL] if grad . is_sparse : [EOL] grad = grad . coalesce ( ) [comment] [EOL] grad_indices = grad . _indices ( ) [EOL] grad_values = grad . _values ( ) [EOL] size = grad . size ( ) [EOL] [EOL] def make_sparse ( values ) : [EOL] constructor = grad . new [EOL] if grad_indices . dim ( ) == [number] or values . dim ( ) == [number] : [EOL] return constructor ( ) . resize_as_ ( grad ) [EOL] return constructor ( grad_indices , values , size ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] old_exp_avg_values = exp_avg . sparse_mask ( grad ) . _values ( ) [EOL] exp_avg_update_values = grad_values . sub ( old_exp_avg_values ) . mul_ ( [number] - beta1 ) [EOL] exp_avg . add_ ( make_sparse ( exp_avg_update_values ) ) [EOL] old_exp_avg_sq_values = exp_avg_sq . sparse_mask ( grad ) . _values ( ) [EOL] exp_avg_sq_update_values = ( grad_values . pow ( [number] ) . sub_ ( old_exp_avg_sq_values ) . mul_ ( [number] - beta2 ) ) [EOL] exp_avg_sq . add_ ( make_sparse ( exp_avg_sq_update_values ) ) [EOL] [EOL] [comment] [EOL] numer = exp_avg_update_values . add_ ( old_exp_avg_values ) [EOL] exp_avg_sq_update_values . add_ ( old_exp_avg_sq_values ) [EOL] denom = exp_avg_sq_update_values . sqrt_ ( ) . add_ ( group [ [string] ] ) [EOL] del exp_avg_update_values , exp_avg_sq_update_values [EOL] [EOL] bias_correction1 = [number] - beta1 ** state [ [string] ] [EOL] bias_correction2 = [number] - beta2 ** state [ [string] ] [EOL] step_size = group [ [string] ] * math . sqrt ( bias_correction2 ) / bias_correction1 [EOL] [EOL] p . data . add_ ( make_sparse ( - step_size * numer . div_ ( denom ) ) ) [EOL] [EOL] else : [EOL] [comment] [EOL] exp_avg . mul_ ( beta1 ) . add_ ( grad , alpha = [number] - beta1 ) [EOL] exp_avg_sq . mul_ ( beta2 ) . addcmul_ ( grad , grad , value = [number] - beta2 ) [EOL] denom = exp_avg_sq . sqrt ( ) . add_ ( group [ [string] ] ) [EOL] [EOL] bias_correction1 = [number] - beta1 ** state [ [string] ] [EOL] bias_correction2 = [number] - beta2 ** state [ [string] ] [EOL] step_size = group [ [string] ] * math . sqrt ( bias_correction2 ) / bias_correction1 [EOL] [EOL] p . data . addcdiv_ ( exp_avg , denom , value = - step_size ) [EOL] [EOL] return loss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[typing.List[typing.Dict[builtins.str,typing.Any]],typing.List[torch.nn.Parameter]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $"Optimizer"$ 0 $typing.List$ 0 0 0 0 0 0 0 0 $typing.List$ 0 $typing.List$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $typing.Tuple[builtins.float,builtins.float]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $typing.Tuple[builtins.float,builtins.float]$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $builtins.float$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.str,torch.nn.Parameter]]$ 0 $typing.List[typing.Tuple[typing.List[builtins.str],typing.Dict[builtins.str,typing.Any]]]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0
from typing import Optional , Any , Dict , List , Union , Iterator , Tuple [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import datetime [EOL] import logging [EOL] import math [EOL] import os [EOL] import re [EOL] import time [EOL] import traceback [EOL] from contextlib import contextmanager [EOL] from typing import Any , Dict , Iterator , List , Optional , Tuple , Union [EOL] [EOL] from allennlp . common . util import int_to_device [EOL] [EOL] import torch [EOL] import torch . distributed as dist [EOL] from torch . cuda import amp [EOL] import torch . optim . lr_scheduler [EOL] from torch . nn . parallel import DistributedDataParallel [EOL] from torch . nn . utils import clip_grad_norm_ [EOL] [EOL] from allennlp . common import Lazy , Registrable , Tqdm [EOL] from allennlp . common import util as common_util [EOL] from allennlp . common . checks import ConfigurationError , check_for_gpu [EOL] from allennlp . data import DataLoader [EOL] from allennlp . data . dataloader import TensorDict [EOL] from allennlp . models . model import Model [EOL] from allennlp . nn import util as nn_util [EOL] from allennlp . training import util as training_util [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] from allennlp . training . metric_tracker import MetricTracker [EOL] from allennlp . training . momentum_schedulers import MomentumScheduler [EOL] from allennlp . training . moving_average import MovingAverage [EOL] from allennlp . training . optimizers import Optimizer [EOL] from allennlp . training . tensorboard_writer import TensorboardWriter [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class Trainer ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def __init__ ( self , serialization_dir , cuda_device = None , distributed = False , local_rank = [number] , world_size = [number] , ) : [EOL] if cuda_device is None : [EOL] from torch import cuda [EOL] [EOL] if cuda . device_count ( ) > [number] : [EOL] cuda_device = [number] [EOL] else : [EOL] cuda_device = - [number] [EOL] [EOL] check_for_gpu ( cuda_device ) [EOL] self . _serialization_dir = serialization_dir [EOL] [EOL] if isinstance ( cuda_device , list ) : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] [EOL] if distributed and world_size <= [number] : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] self . cuda_device = int_to_device ( cuda_device ) [EOL] [EOL] self . _distributed = distributed [EOL] self . _rank = local_rank [EOL] self . _master = self . _rank == [number] [EOL] self . _world_size = world_size [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ contextmanager def get_checkpoint_state ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] [EOL] class BatchCallback ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] pass [EOL] [EOL] [EOL] @ BatchCallback . register ( [string] ) class TensoboardBatchMemoryUsage ( BatchCallback ) : [EOL] [docstring] [EOL] [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] [comment] [EOL] [comment] [EOL] cpu_memory_usage = common_util . peak_memory_mb ( ) [EOL] [comment] [EOL] [comment] [EOL] if is_master : [EOL] gpu_memory_usage = common_util . gpu_memory_mb ( ) [EOL] trainer . _tensorboard . log_memory_usage ( cpu_memory_usage , gpu_memory_usage ) [EOL] [EOL] [EOL] BatchCallback . register ( [string] ) ( BatchCallback ) [EOL] [EOL] [EOL] class EpochCallback ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __call__ ( self , trainer , metrics , epoch , is_master , ) : [EOL] pass [EOL] [EOL] [EOL] EpochCallback . register ( [string] ) ( EpochCallback ) [EOL] [EOL] [EOL] @ EpochCallback . register ( [string] ) class TrackEpochCallback : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] def __call__ ( self , trainer , metrics , epoch , is_master , ) : [EOL] trainer . model . epoch = epoch + [number] [EOL] [EOL] [EOL] @ Trainer . register ( [string] , constructor = [string] ) class GradientDescentTrainer ( Trainer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model , optimizer , data_loader , patience = None , validation_metric = [string] , validation_data_loader = None , num_epochs = [number] , serialization_dir = None , checkpointer = None , cuda_device = None , grad_norm = None , grad_clipping = None , learning_rate_scheduler = None , momentum_scheduler = None , tensorboard_writer = None , moving_average = None , batch_callbacks = None , epoch_callbacks = None , distributed = False , local_rank = [number] , world_size = [number] , num_gradient_accumulation_steps = [number] , use_amp = False , ) : [EOL] super ( ) . __init__ ( serialization_dir , cuda_device , distributed , local_rank , world_size ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . model = model [EOL] [EOL] self . data_loader = data_loader [EOL] self . _validation_data_loader = validation_data_loader [EOL] self . optimizer = optimizer [EOL] [EOL] if patience is None : [comment] [EOL] if validation_data_loader is not None : [EOL] logger . warning ( [string] [string] ) [EOL] elif ( not isinstance ( patience , int ) ) or patience <= [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( patience ) ) [EOL] [EOL] [comment] [EOL] self . _metric_tracker = MetricTracker ( patience , validation_metric ) [EOL] [comment] [EOL] self . _validation_metric = validation_metric [ [number] : ] [EOL] [EOL] self . _num_epochs = num_epochs [EOL] [EOL] if checkpointer is not None : [EOL] self . _checkpointer = checkpointer [EOL] else : [EOL] self . _checkpointer = Checkpointer ( serialization_dir ) [EOL] [EOL] self . _grad_norm = grad_norm [EOL] self . _grad_clipping = grad_clipping [EOL] [EOL] self . _learning_rate_scheduler = learning_rate_scheduler [EOL] self . _momentum_scheduler = momentum_scheduler [EOL] self . _moving_average = moving_average [EOL] self . _batch_callbacks = batch_callbacks or [ ] [EOL] self . _epoch_callbacks = epoch_callbacks or [ ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . _batch_num_total = [number] [EOL] [EOL] self . _tensorboard = tensorboard_writer or TensorboardWriter ( serialization_dir ) [EOL] self . _tensorboard . get_batch_num_total = lambda : self . _batch_num_total [EOL] self . _tensorboard . enable_activation_logging ( self . model ) [EOL] [EOL] self . _last_log = [number] [comment] [EOL] [EOL] self . _num_gradient_accumulation_steps = num_gradient_accumulation_steps [EOL] [EOL] [comment] [EOL] self . _scaler = None [EOL] self . _use_amp = use_amp [EOL] if self . _use_amp : [EOL] if self . cuda_device == torch . device ( [string] ) : [EOL] raise ValueError ( [string] ) [EOL] self . _scaler = amp . GradScaler ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if self . _distributed : [EOL] self . _pytorch_model = DistributedDataParallel ( self . model , device_ids = None if self . cuda_device == torch . device ( [string] ) else [ self . cuda_device ] , find_unused_parameters = True , ) [EOL] else : [EOL] self . _pytorch_model = self . model [EOL] [EOL] def rescale_gradients ( self ) : [EOL] [docstring] [EOL] parameters_to_clip = [ p for p in self . model . parameters ( ) if p . grad is not None ] [EOL] if self . _grad_norm : [EOL] if self . _scaler is not None : [EOL] [comment] [EOL] self . _scaler . unscale_ ( self . optimizer ) [EOL] return clip_grad_norm_ ( parameters_to_clip , self . _grad_norm ) [EOL] else : [EOL] return torch . norm ( torch . stack ( [ torch . norm ( p . grad . detach ( ) ) for p in parameters_to_clip ] ) ) [EOL] [EOL] def batch_outputs ( self , batch , for_training ) : [EOL] [docstring] [EOL] batch = nn_util . move_to_device ( batch , self . cuda_device ) [EOL] output_dict = self . _pytorch_model ( ** batch ) [EOL] [EOL] if for_training : [EOL] try : [EOL] assert [string] in output_dict [EOL] regularization_penalty = self . model . get_regularization_penalty ( ) [EOL] [EOL] if regularization_penalty is not None : [EOL] output_dict [ [string] ] = regularization_penalty [EOL] output_dict [ [string] ] += regularization_penalty [EOL] [EOL] except AssertionError : [EOL] if for_training : [EOL] raise RuntimeError ( [string] [string] ) [EOL] [EOL] return output_dict [EOL] [EOL] def _train_epoch ( self , epoch ) : [EOL] [docstring] [EOL] logger . info ( [string] , epoch , self . _num_epochs - [number] ) [EOL] cpu_memory_usage = [ ] [EOL] for worker , memory in common_util . peak_memory_mb ( ) . items ( ) : [EOL] cpu_memory_usage . append ( ( worker , memory ) ) [EOL] logger . info ( f" [string] { worker } [string] { memory }" ) [EOL] gpu_memory_usage = [ ] [EOL] for gpu , memory in common_util . gpu_memory_mb ( ) . items ( ) : [EOL] gpu_memory_usage . append ( ( gpu , memory ) ) [EOL] logger . info ( f" [string] { gpu } [string] { memory }" ) [EOL] [EOL] regularization_penalty = self . model . get_regularization_penalty ( ) [EOL] [EOL] train_loss = [number] [EOL] batch_loss = [number] [EOL] [EOL] if regularization_penalty is not None : [EOL] train_reg_loss = [number] [EOL] batch_reg_loss = [number] [EOL] else : [EOL] train_reg_loss = None [EOL] batch_reg_loss = None [EOL] [comment] [EOL] self . _pytorch_model . train ( ) [EOL] [EOL] [comment] [EOL] batch_generator = iter ( self . data_loader ) [EOL] batch_group_generator = common_util . lazy_groups_of ( batch_generator , self . _num_gradient_accumulation_steps ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] num_training_batches = ... [EOL] try : [EOL] len_data_loader = len ( self . data_loader ) [EOL] num_training_batches = math . ceil ( len_data_loader / self . _num_gradient_accumulation_steps ) [EOL] except TypeError : [EOL] num_training_batches = float ( [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _master : [EOL] batch_group_generator_tqdm = Tqdm . tqdm ( batch_group_generator , total = num_training_batches ) [EOL] else : [EOL] batch_group_generator_tqdm = batch_group_generator [EOL] [EOL] self . _last_log = time . time ( ) [EOL] [EOL] batches_this_epoch = [number] [EOL] if self . _batch_num_total is None : [EOL] self . _batch_num_total = [number] [EOL] [EOL] done_early = False [EOL] for batch_group in batch_group_generator_tqdm : [EOL] if self . _distributed : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] done = torch . tensor ( [number] , device = self . cuda_device ) [EOL] torch . distributed . all_reduce ( done , torch . distributed . ReduceOp . SUM ) [EOL] if done . item ( ) > [number] : [EOL] done_early = True [EOL] logger . warning ( f" [string] { torch . distributed . get_rank ( ) } [string] " [string] [string] [string] [string] [string] ) [EOL] break [EOL] [EOL] batches_this_epoch += [number] [EOL] self . _batch_num_total += [number] [EOL] batch_num_total = self . _batch_num_total [EOL] [EOL] self . optimizer . zero_grad ( ) [EOL] [EOL] batch_group_outputs = [ ] [EOL] for batch in batch_group : [EOL] with amp . autocast ( self . _use_amp ) : [EOL] batch_outputs = self . batch_outputs ( batch , for_training = True ) [EOL] batch_group_outputs . append ( batch_outputs ) [EOL] loss = batch_outputs . get ( [string] ) [EOL] reg_loss = batch_outputs . get ( [string] ) [EOL] if torch . isnan ( loss ) : [EOL] raise ValueError ( [string] ) [EOL] loss = loss / len ( batch_group ) [EOL] [EOL] batch_loss = loss . item ( ) [EOL] train_loss += batch_loss [EOL] if reg_loss is not None : [EOL] reg_loss = reg_loss / len ( batch_group ) [EOL] batch_reg_loss = reg_loss . item ( ) [EOL] train_reg_loss += batch_reg_loss [EOL] [EOL] if self . _scaler is not None : [EOL] self . _scaler . scale ( loss ) . backward ( ) [EOL] else : [EOL] loss . backward ( ) [EOL] [EOL] batch_grad_norm = self . rescale_gradients ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _learning_rate_scheduler : [EOL] self . _learning_rate_scheduler . step_batch ( batch_num_total ) [EOL] if self . _momentum_scheduler : [EOL] self . _momentum_scheduler . step_batch ( batch_num_total ) [EOL] [EOL] param_updates = None [EOL] if self . _tensorboard . should_log_histograms_this_batch ( ) and self . _master : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] param_updates = { name : param . detach ( ) . cpu ( ) . clone ( ) for name , param in self . model . named_parameters ( ) } [EOL] [EOL] if self . _scaler is not None : [EOL] self . _scaler . step ( self . optimizer ) [EOL] self . _scaler . update ( ) [EOL] else : [EOL] self . optimizer . step ( ) [EOL] [EOL] for name , param in self . model . named_parameters ( ) : [EOL] param_updates [ name ] . sub_ ( param . detach ( ) . cpu ( ) ) [EOL] else : [EOL] if self . _scaler is not None : [EOL] self . _scaler . step ( self . optimizer ) [EOL] self . _scaler . update ( ) [EOL] else : [EOL] self . optimizer . step ( ) [EOL] [EOL] [comment] [EOL] if self . _moving_average is not None : [EOL] self . _moving_average . apply ( batch_num_total ) [EOL] [EOL] [comment] [EOL] metrics = training_util . get_metrics ( self . model , train_loss , train_reg_loss , batch_loss , batch_reg_loss , batches_this_epoch , world_size = self . _world_size , cuda_device = self . cuda_device , ) [EOL] [EOL] if self . _master : [EOL] [comment] [EOL] description = training_util . description_from_metrics ( metrics ) [EOL] batch_group_generator_tqdm . set_description ( description , refresh = False ) [EOL] self . _tensorboard . log_batch ( self . model , self . optimizer , batch_grad_norm , metrics , batch_group , param_updates , ) [EOL] [EOL] self . _checkpointer . maybe_save_checkpoint ( self , epoch , batches_this_epoch ) [EOL] for callback in self . _batch_callbacks : [EOL] callback ( self , batch_group , batch_group_outputs , epoch , batches_this_epoch , is_training = True , is_master = self . _master , ) [EOL] [EOL] if self . _distributed and not done_early : [EOL] logger . warning ( f" [string] { torch . distributed . get_rank ( ) } [string] " ) [EOL] [comment] [EOL] done = torch . tensor ( [number] , device = self . cuda_device ) [EOL] torch . distributed . all_reduce ( done , torch . distributed . ReduceOp . SUM ) [EOL] assert done . item ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _distributed : [EOL] dist . barrier ( ) [EOL] [EOL] metrics = training_util . get_metrics ( self . model , train_loss , train_reg_loss , batch_loss = None , batch_reg_loss = None , num_batches = batches_this_epoch , reset = True , world_size = self . _world_size , cuda_device = self . cuda_device , ) [EOL] [EOL] for ( worker , memory ) in cpu_memory_usage : [EOL] metrics [ [string] + str ( worker ) + [string] ] = memory [EOL] for ( gpu_num , memory ) in gpu_memory_usage : [EOL] metrics [ [string] + str ( gpu_num ) + [string] ] = memory [EOL] return metrics [EOL] [EOL] def _validation_loss ( self , epoch ) : [EOL] [docstring] [EOL] logger . info ( [string] ) [EOL] [EOL] self . _pytorch_model . eval ( ) [EOL] [EOL] [comment] [EOL] if self . _moving_average is not None : [EOL] self . _moving_average . assign_average_value ( ) [EOL] [EOL] if self . _validation_data_loader is not None : [EOL] validation_data_loader = self . _validation_data_loader [EOL] else : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] regularization_penalty = self . model . get_regularization_penalty ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _master : [EOL] val_generator_tqdm = Tqdm . tqdm ( validation_data_loader ) [EOL] else : [EOL] val_generator_tqdm = validation_data_loader [EOL] [EOL] batches_this_epoch = [number] [EOL] val_loss = [number] [EOL] val_batch_loss = [number] [EOL] if regularization_penalty is not None : [EOL] val_reg_loss = [number] [EOL] val_batch_reg_loss = [number] [EOL] else : [EOL] val_reg_loss = None [EOL] val_batch_reg_loss = None [EOL] done_early = False [EOL] for batch in val_generator_tqdm : [EOL] if self . _distributed : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] done = torch . tensor ( [number] , device = self . cuda_device ) [EOL] torch . distributed . all_reduce ( done , torch . distributed . ReduceOp . SUM ) [EOL] if done . item ( ) > [number] : [EOL] done_early = True [EOL] logger . warning ( f" [string] { torch . distributed . get_rank ( ) } [string] " [string] [string] [string] [string] [string] ) [EOL] break [EOL] [EOL] with amp . autocast ( self . _use_amp ) : [EOL] batch_outputs = self . batch_outputs ( batch , for_training = False ) [EOL] loss = batch_outputs . get ( [string] ) [EOL] reg_loss = batch_outputs . get ( [string] ) [EOL] if loss is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] batches_this_epoch += [number] [EOL] val_batch_loss = loss . detach ( ) . cpu ( ) . numpy ( ) [EOL] val_loss += val_batch_loss [EOL] if reg_loss is not None : [EOL] val_batch_reg_loss = reg_loss . detach ( ) . cpu ( ) . numpy ( ) [EOL] val_reg_loss += val_batch_reg_loss [EOL] [EOL] [comment] [EOL] val_metrics = training_util . get_metrics ( self . model , val_loss , val_reg_loss , val_batch_loss , val_batch_reg_loss , batches_this_epoch , world_size = self . _world_size , cuda_device = self . cuda_device , ) [EOL] [EOL] description = training_util . description_from_metrics ( val_metrics ) [EOL] if self . _master : [EOL] val_generator_tqdm . set_description ( description , refresh = False ) [EOL] [EOL] for callback in self . _batch_callbacks : [EOL] callback ( self , [ batch ] , [ batch_outputs ] , epoch , batches_this_epoch , is_training = False , is_master = self . _master , ) [EOL] [EOL] if self . _distributed and not done_early : [EOL] logger . warning ( f" [string] { torch . distributed . get_rank ( ) } [string] " ) [EOL] [comment] [EOL] done = torch . tensor ( [number] , device = self . cuda_device ) [EOL] torch . distributed . all_reduce ( done , torch . distributed . ReduceOp . SUM ) [EOL] assert done . item ( ) [EOL] [EOL] [comment] [EOL] if self . _moving_average is not None : [EOL] self . _moving_average . restore ( ) [EOL] [EOL] return val_loss , val_reg_loss , batches_this_epoch [EOL] [EOL] def train ( self ) : [EOL] [docstring] [EOL] try : [EOL] epoch_counter = self . _restore_checkpoint ( ) [EOL] except RuntimeError : [EOL] traceback . print_exc ( ) [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] [EOL] training_util . enable_gradient_clipping ( self . model , self . _grad_clipping ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] val_metrics = { } [EOL] this_epoch_val_metric = None [EOL] metrics = { } [EOL] epochs_trained = [number] [EOL] training_start_time = time . time ( ) [EOL] [EOL] metrics [ [string] ] = self . _metric_tracker . best_epoch [EOL] for key , value in self . _metric_tracker . best_epoch_metrics . items ( ) : [EOL] metrics [ [string] + key ] = value [EOL] [EOL] for callback in self . _epoch_callbacks : [EOL] callback ( self , metrics = { } , epoch = - [number] , is_master = self . _master ) [EOL] [EOL] for epoch in range ( epoch_counter , self . _num_epochs ) : [EOL] epoch_start_time = time . time ( ) [EOL] train_metrics = self . _train_epoch ( epoch ) [EOL] [EOL] [comment] [EOL] for key , value in train_metrics . items ( ) : [EOL] if key . startswith ( [string] ) and key . endswith ( [string] ) : [EOL] metrics [ [string] + key ] = max ( metrics . get ( [string] + key , [number] ) , value ) [EOL] elif key . startswith ( [string] ) and key . endswith ( [string] ) : [EOL] metrics [ [string] + key ] = max ( metrics . get ( [string] + key , [number] ) , value ) [EOL] [EOL] if self . _validation_data_loader is not None : [EOL] with torch . no_grad ( ) : [EOL] [comment] [EOL] val_loss , val_reg_loss , num_batches = self . _validation_loss ( epoch ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _distributed : [EOL] dist . barrier ( ) [EOL] [EOL] val_metrics = training_util . get_metrics ( self . model , val_loss , val_reg_loss , batch_loss = None , batch_reg_loss = None , num_batches = num_batches , reset = True , world_size = self . _world_size , cuda_device = self . cuda_device , ) [EOL] [EOL] [comment] [EOL] this_epoch_val_metric = val_metrics [ self . _validation_metric ] [EOL] self . _metric_tracker . add_metric ( this_epoch_val_metric ) [EOL] [EOL] if self . _metric_tracker . should_stop_early ( ) : [EOL] logger . info ( [string] ) [EOL] break [EOL] [EOL] if self . _master : [EOL] self . _tensorboard . log_metrics ( train_metrics , val_metrics = val_metrics , log_to_console = True , epoch = epoch + [number] ) [comment] [EOL] [EOL] [comment] [EOL] training_elapsed_time = time . time ( ) - training_start_time [EOL] metrics [ [string] ] = str ( datetime . timedelta ( seconds = training_elapsed_time ) ) [EOL] metrics [ [string] ] = epoch_counter [EOL] metrics [ [string] ] = epochs_trained [EOL] metrics [ [string] ] = epoch [EOL] [EOL] for key , value in train_metrics . items ( ) : [EOL] metrics [ [string] + key ] = value [EOL] for key , value in val_metrics . items ( ) : [EOL] metrics [ [string] + key ] = value [EOL] [EOL] if self . _metric_tracker . is_best_so_far ( ) : [EOL] [comment] [EOL] [comment] [EOL] metrics [ [string] ] = epoch [EOL] for key , value in val_metrics . items ( ) : [EOL] metrics [ [string] + key ] = value [EOL] [EOL] self . _metric_tracker . best_epoch_metrics = val_metrics [EOL] [EOL] if self . _serialization_dir and self . _master : [EOL] common_util . dump_metrics ( os . path . join ( self . _serialization_dir , f" [string] { epoch } [string] " ) , metrics ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _learning_rate_scheduler : [EOL] self . _learning_rate_scheduler . step ( this_epoch_val_metric ) [EOL] if self . _momentum_scheduler : [EOL] self . _momentum_scheduler . step ( this_epoch_val_metric ) [EOL] [EOL] if self . _master : [EOL] self . _checkpointer . save_checkpoint ( epoch , self , is_best_so_far = self . _metric_tracker . is_best_so_far ( ) ) [EOL] [EOL] [comment] [EOL] if self . _distributed : [EOL] dist . barrier ( ) [EOL] [EOL] for callback in self . _epoch_callbacks : [EOL] callback ( self , metrics = metrics , epoch = epoch , is_master = self . _master ) [EOL] [EOL] epoch_elapsed_time = time . time ( ) - epoch_start_time [EOL] logger . info ( [string] , datetime . timedelta ( seconds = epoch_elapsed_time ) ) [EOL] [EOL] if epoch < self . _num_epochs - [number] : [EOL] training_elapsed_time = time . time ( ) - training_start_time [EOL] estimated_time_remaining = training_elapsed_time * ( ( self . _num_epochs - epoch_counter ) / float ( epoch - epoch_counter + [number] ) - [number] ) [EOL] formatted_time = str ( datetime . timedelta ( seconds = int ( estimated_time_remaining ) ) ) [EOL] logger . info ( [string] , formatted_time ) [EOL] [EOL] epochs_trained += [number] [EOL] [EOL] [comment] [EOL] self . _tensorboard . close ( ) [EOL] [EOL] [comment] [EOL] best_model_state = self . _checkpointer . best_model_state ( ) [EOL] if best_model_state : [EOL] self . model . load_state_dict ( best_model_state ) [EOL] [EOL] return metrics [EOL] [EOL] @ contextmanager def get_checkpoint_state ( self ) : [EOL] if self . _moving_average is not None : [EOL] [comment] [EOL] [comment] [EOL] self . _moving_average . assign_average_value ( ) [EOL] [EOL] model_state = self . model . state_dict ( ) [EOL] [EOL] [comment] [EOL] training_states = { [string] : self . _metric_tracker . state_dict ( ) , [string] : self . optimizer . state_dict ( ) , [string] : self . _batch_num_total , } [EOL] [EOL] [comment] [EOL] if self . _learning_rate_scheduler is not None : [EOL] training_states [ [string] ] = self . _learning_rate_scheduler . state_dict ( ) [EOL] if self . _momentum_scheduler is not None : [EOL] training_states [ [string] ] = self . _momentum_scheduler . state_dict ( ) [EOL] [EOL] try : [EOL] yield model_state , training_states [EOL] finally : [EOL] if self . _moving_average is not None : [EOL] self . _moving_average . restore ( ) [EOL] [EOL] def _restore_checkpoint ( self ) : [EOL] [docstring] [EOL] model_state , training_state = self . _checkpointer . restore_checkpoint ( ) [EOL] [EOL] if not training_state : [EOL] [comment] [EOL] return [number] [EOL] [EOL] self . model . load_state_dict ( model_state ) [EOL] self . optimizer . load_state_dict ( training_state [ [string] ] ) [EOL] if ( self . _learning_rate_scheduler is not None [EOL] and [string] in training_state ) : [EOL] self . _learning_rate_scheduler . load_state_dict ( training_state [ [string] ] ) [EOL] if self . _momentum_scheduler is not None and [string] in training_state : [EOL] self . _momentum_scheduler . load_state_dict ( training_state [ [string] ] ) [EOL] training_util . move_optimizer_to_cuda ( self . optimizer ) [EOL] [EOL] [comment] [EOL] if [string] in training_state : [EOL] self . _metric_tracker . load_state_dict ( training_state [ [string] ] ) [EOL] [comment] [EOL] elif [string] in training_state : [EOL] self . _metric_tracker . clear ( ) [EOL] self . _metric_tracker . add_metrics ( training_state [ [string] ] ) [EOL] [comment] [EOL] else : [EOL] self . _metric_tracker . clear ( ) [EOL] [EOL] if isinstance ( training_state [ [string] ] , int ) : [EOL] epoch_to_return = training_state [ [string] ] + [number] [EOL] else : [EOL] epoch_to_return = int ( training_state [ [string] ] . split ( [string] ) [ [number] ] ) + [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] batch_num_total = training_state . get ( [string] ) [EOL] if batch_num_total is not None : [EOL] self . _batch_num_total = batch_num_total [EOL] [EOL] return epoch_to_return [EOL] [EOL] @ classmethod def from_partial_objects ( cls , model , serialization_dir , data_loader , validation_data_loader = None , local_rank = [number] , patience = None , validation_metric = [string] , num_epochs = [number] , cuda_device = None , grad_norm = None , grad_clipping = None , distributed = None , world_size = [number] , num_gradient_accumulation_steps = [number] , use_amp = False , no_grad = None , optimizer = None , learning_rate_scheduler = None , momentum_scheduler = None , tensorboard_writer = None , moving_average = None , checkpointer = None , batch_callbacks = None , epoch_callbacks = None , ) : [EOL] [docstring] [EOL] if cuda_device is None : [EOL] from torch import cuda [EOL] [EOL] if cuda . device_count ( ) > [number] : [EOL] cuda_device = [number] [EOL] else : [EOL] cuda_device = - [number] [EOL] [EOL] check_for_gpu ( cuda_device ) [EOL] if cuda_device >= [number] : [EOL] [comment] [EOL] [comment] [EOL] model = model . cuda ( cuda_device ) [EOL] [EOL] if no_grad : [EOL] for name , parameter in model . named_parameters ( ) : [EOL] if any ( re . search ( regex , name ) for regex in no_grad ) : [EOL] parameter . requires_grad_ ( False ) [EOL] [EOL] parameters = [ [ n , p ] for n , p in model . named_parameters ( ) if p . requires_grad ] [EOL] optimizer_ = optimizer . construct ( model_parameters = parameters ) [EOL] if not optimizer_ : [EOL] optimizer_ = Optimizer . default ( parameters ) [EOL] [EOL] common_util . log_frozen_and_tunable_parameter_names ( model ) [EOL] [EOL] batches_per_epoch = ... [EOL] try : [EOL] batches_per_epoch = len ( data_loader ) [EOL] batches_per_epoch = math . ceil ( batches_per_epoch / num_gradient_accumulation_steps ) [EOL] except TypeError : [EOL] batches_per_epoch = None [EOL] [EOL] moving_average_ = moving_average . construct ( parameters = parameters ) [EOL] learning_rate_scheduler_ = learning_rate_scheduler . construct ( optimizer = optimizer_ , num_epochs = num_epochs , num_steps_per_epoch = batches_per_epoch ) [EOL] momentum_scheduler_ = momentum_scheduler . construct ( optimizer = optimizer_ ) [EOL] [EOL] checkpointer_ = checkpointer . construct ( ) or Checkpointer ( serialization_dir ) [EOL] tensorboard_writer_ = tensorboard_writer . construct ( ) or TensorboardWriter ( serialization_dir ) [EOL] [EOL] return cls ( model , optimizer_ , data_loader , patience = patience , validation_metric = validation_metric , validation_data_loader = validation_data_loader , num_epochs = num_epochs , serialization_dir = serialization_dir , cuda_device = cuda_device , grad_norm = grad_norm , grad_clipping = grad_clipping , learning_rate_scheduler = learning_rate_scheduler_ , momentum_scheduler = momentum_scheduler_ , tensorboard_writer = tensorboard_writer_ , checkpointer = checkpointer_ , moving_average = moving_average_ , batch_callbacks = batch_callbacks , epoch_callbacks = epoch_callbacks , distributed = distributed , local_rank = local_rank , world_size = world_size , num_gradient_accumulation_steps = num_gradient_accumulation_steps , use_amp = use_amp , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"Trainer"$ 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[BatchCallback]$ 0 0 0 $typing.List[EpochCallback]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 $allennlp.models.model.Model$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.optimizers.Optimizer$ 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 $allennlp.training.optimizers.Optimizer$ 0 0 $allennlp.training.optimizers.Optimizer$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.training.optimizers.Optimizer$ 0 0 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.training.optimizers.Optimizer$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 $allennlp.training.optimizers.Optimizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.List[BatchCallback]$ 0 $typing.List[BatchCallback]$ 0 $typing.List[EpochCallback]$ 0 $typing.List[EpochCallback]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Iterator , Any , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import os [EOL] from contextlib import contextmanager [EOL] from typing import Any , Dict , Iterator , Tuple [EOL] [EOL] from allennlp . models import Model [EOL] from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . trainer import Trainer [EOL] [EOL] [EOL] @ Trainer . register ( [string] ) class NoOpTrainer ( Trainer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , serialization_dir , model ) : [EOL] [docstring] [EOL] [EOL] super ( ) . __init__ ( serialization_dir , cuda_device = - [number] ) [EOL] self . model = model [EOL] [EOL] def train ( self ) : [EOL] self . model . vocab . save_to_files ( os . path . join ( self . _serialization_dir , [string] ) ) [EOL] [EOL] checkpointer = Checkpointer ( self . _serialization_dir ) [EOL] checkpointer . save_checkpoint ( epoch = [number] , trainer = self , is_best_so_far = True ) [EOL] return { } [EOL] [EOL] @ contextmanager def get_checkpoint_state ( self ) : [EOL] yield self . model . state_dict ( ) , { } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 $allennlp.models.model.Model$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Tuple[typing.Dict[builtins.str,typing.Any],typing.Dict[builtins.str,typing.Any]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . training . checkpointer import Checkpointer [EOL] from allennlp . training . tensorboard_writer import TensorboardWriter [EOL] from allennlp . training . no_op_trainer import NoOpTrainer [EOL] from allennlp . training . trainer import ( Trainer , GradientDescentTrainer , BatchCallback , EpochCallback , TrackEpochCallback , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Set , Any , Callable , Dict , List [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Any , Callable , Dict , List , Optional , Set [EOL] import logging [EOL] import os [EOL] [EOL] from tensorboardX import SummaryWriter [EOL] import torch [EOL] [EOL] from allennlp . common . from_params import FromParams [EOL] from allennlp . data . dataloader import TensorDict [EOL] from allennlp . nn import util as nn_util [EOL] from allennlp . training . optimizers import Optimizer [EOL] from allennlp . training import util as training_util [EOL] from allennlp . models . model import Model [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class TensorboardWriter ( FromParams ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , serialization_dir = None , summary_interval = [number] , histogram_interval = None , batch_size_interval = None , should_log_parameter_statistics = True , should_log_learning_rate = False , get_batch_num_total = None , ) : [EOL] if serialization_dir is not None : [EOL] [comment] [EOL] [comment] [EOL] train_ser_dir = os . path . join ( serialization_dir , [string] , [string] ) [EOL] os . makedirs ( train_ser_dir , exist_ok = True ) [EOL] self . _train_log = SummaryWriter ( train_ser_dir ) [EOL] val_ser_dir = os . path . join ( serialization_dir , [string] , [string] ) [EOL] os . makedirs ( val_ser_dir , exist_ok = True ) [EOL] self . _validation_log = SummaryWriter ( val_ser_dir ) [EOL] else : [EOL] self . _train_log = self . _validation_log = None [EOL] [EOL] self . _summary_interval = summary_interval [EOL] self . _histogram_interval = histogram_interval [EOL] self . _batch_size_interval = batch_size_interval [EOL] self . _should_log_parameter_statistics = should_log_parameter_statistics [EOL] self . _should_log_learning_rate = should_log_learning_rate [EOL] self . get_batch_num_total = get_batch_num_total [EOL] [EOL] self . _cumulative_batch_group_size = [number] [EOL] self . _batches_this_epoch = [number] [EOL] self . _histogram_parameters = None [EOL] [EOL] @ staticmethod def _item ( value ) : [EOL] if hasattr ( value , [string] ) : [EOL] val = value . item ( ) [EOL] else : [EOL] val = value [EOL] return val [EOL] [EOL] def log_memory_usage ( self , cpu_memory_usage , gpu_memory_usage ) : [EOL] cpu_memory_usage_total = [number] [EOL] for worker , memory in cpu_memory_usage . items ( ) : [EOL] self . add_train_scalar ( f" [string] { worker } [string] " , memory ) [EOL] cpu_memory_usage_total += memory [EOL] self . add_train_scalar ( [string] , cpu_memory_usage_total ) [EOL] for gpu , memory in gpu_memory_usage . items ( ) : [EOL] self . add_train_scalar ( f" [string] { gpu }" , memory ) [EOL] [EOL] def log_batch ( self , model , optimizer , batch_grad_norm , metrics , batch_group , param_updates , ) : [EOL] if self . should_log_this_batch ( ) : [EOL] self . log_parameter_and_gradient_statistics ( model , batch_grad_norm ) [EOL] self . log_learning_rates ( model , optimizer ) [EOL] [EOL] self . add_train_scalar ( [string] , metrics [ [string] ] ) [EOL] self . log_metrics ( { [string] + k : v for k , v in metrics . items ( ) } ) [EOL] [EOL] if self . should_log_histograms_this_batch ( ) : [EOL] self . log_histograms ( model ) [EOL] self . log_gradient_updates ( model , param_updates ) [EOL] [EOL] if self . _batch_size_interval : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] batch_group_size = sum ( training_util . get_batch_size ( batch ) for batch in batch_group ) [EOL] self . _batches_this_epoch += [number] [EOL] self . _cumulative_batch_group_size += batch_group_size [EOL] [EOL] if ( self . _batches_this_epoch - [number] ) % self . _batch_size_interval == [number] : [EOL] average = self . _cumulative_batch_group_size / self . _batches_this_epoch [EOL] logger . info ( f" [string] { batch_group_size } [string] { average }" ) [EOL] self . add_train_scalar ( [string] , batch_group_size ) [EOL] self . add_train_scalar ( [string] , average ) [EOL] [EOL] def reset_epoch ( self ) : [EOL] self . _cumulative_batch_group_size = [number] [EOL] self . _batches_this_epoch = [number] [EOL] [EOL] def should_log_this_batch ( self ) : [EOL] return self . get_batch_num_total ( ) % self . _summary_interval == [number] [EOL] [EOL] def should_log_histograms_this_batch ( self ) : [EOL] return ( self . _histogram_interval is not None [EOL] and self . get_batch_num_total ( ) % self . _histogram_interval == [number] ) [EOL] [EOL] def add_train_scalar ( self , name , value , timestep = None ) : [EOL] timestep = timestep or self . get_batch_num_total ( ) [EOL] [comment] [EOL] if self . _train_log is not None : [EOL] self . _train_log . add_scalar ( name , self . _item ( value ) , timestep ) [EOL] [EOL] def add_train_histogram ( self , name , values ) : [EOL] if self . _train_log is not None : [EOL] if isinstance ( values , torch . Tensor ) : [EOL] values_to_write = values . cpu ( ) . data . numpy ( ) . flatten ( ) [EOL] self . _train_log . add_histogram ( name , values_to_write , self . get_batch_num_total ( ) ) [EOL] [EOL] def add_validation_scalar ( self , name , value , timestep = None ) : [EOL] timestep = timestep or self . get_batch_num_total ( ) [EOL] if self . _validation_log is not None : [EOL] self . _validation_log . add_scalar ( name , self . _item ( value ) , timestep ) [EOL] [EOL] def log_parameter_and_gradient_statistics ( self , model , batch_grad_norm ) : [EOL] [docstring] [EOL] if self . _should_log_parameter_statistics : [EOL] [comment] [EOL] for name , param in model . named_parameters ( ) : [EOL] if param . data . numel ( ) > [number] : [EOL] self . add_train_scalar ( [string] + name , param . data . mean ( ) ) [EOL] if param . data . numel ( ) > [number] : [EOL] self . add_train_scalar ( [string] + name , param . data . std ( ) ) [EOL] if param . grad is not None : [EOL] if param . grad . is_sparse : [EOL] [EOL] grad_data = param . grad . data . _values ( ) [EOL] else : [EOL] grad_data = param . grad . data [EOL] [EOL] [comment] [EOL] if torch . prod ( torch . tensor ( grad_data . shape ) ) . item ( ) > [number] : [EOL] self . add_train_scalar ( [string] + name , grad_data . mean ( ) ) [EOL] if grad_data . numel ( ) > [number] : [EOL] self . add_train_scalar ( [string] + name , grad_data . std ( ) ) [EOL] else : [EOL] [comment] [EOL] logger . info ( [string] , name ) [EOL] [comment] [EOL] if batch_grad_norm is not None : [EOL] self . add_train_scalar ( [string] , batch_grad_norm ) [EOL] [EOL] def log_learning_rates ( self , model , optimizer ) : [EOL] [docstring] [EOL] if self . _should_log_learning_rate : [EOL] [comment] [EOL] [comment] [EOL] names = { param : name for name , param in model . named_parameters ( ) } [EOL] for group in optimizer . param_groups : [EOL] if [string] not in group : [EOL] continue [EOL] rate = group [ [string] ] [EOL] for param in group [ [string] ] : [EOL] [comment] [EOL] effective_rate = rate * float ( param . requires_grad ) [EOL] self . add_train_scalar ( [string] + names [ param ] , effective_rate ) [EOL] [EOL] def log_histograms ( self , model ) : [EOL] [docstring] [EOL] if not self . _histogram_parameters : [EOL] [comment] [EOL] [comment] [EOL] self . _histogram_parameters = set ( model . get_parameters_for_histogram_tensorboard_logging ( ) ) [EOL] for name , param in model . named_parameters ( ) : [EOL] if name in self . _histogram_parameters : [EOL] self . add_train_histogram ( [string] + name , param ) [EOL] [EOL] def log_gradient_updates ( self , model , param_updates ) : [EOL] for name , param in model . named_parameters ( ) : [EOL] update_norm = torch . norm ( param_updates [ name ] . view ( - [number] ) ) [EOL] param_norm = torch . norm ( param . view ( - [number] ) ) . cpu ( ) [EOL] self . add_train_scalar ( [string] + name , update_norm / ( param_norm + nn_util . tiny_value_of_dtype ( param_norm . dtype ) ) , ) [EOL] [EOL] def log_metrics ( self , train_metrics , val_metrics = None , epoch = None , log_to_console = False , ) : [EOL] [docstring] [EOL] metric_names = set ( train_metrics . keys ( ) ) [EOL] if val_metrics is not None : [EOL] metric_names . update ( val_metrics . keys ( ) ) [EOL] val_metrics = val_metrics or { } [EOL] [EOL] [comment] [EOL] if log_to_console : [EOL] dual_message_template = [string] [EOL] no_val_message_template = [string] [EOL] no_train_message_template = [string] [EOL] header_template = [string] [EOL] name_length = max ( len ( x ) for x in metric_names ) [EOL] logger . info ( header_template , [string] . rjust ( name_length + [number] ) , [string] ) [EOL] [EOL] for name in sorted ( metric_names ) : [EOL] [comment] [EOL] train_metric = train_metrics . get ( name ) [EOL] if train_metric is not None : [EOL] self . add_train_scalar ( name , train_metric , timestep = epoch ) [EOL] val_metric = val_metrics . get ( name ) [EOL] if val_metric is not None : [EOL] self . add_validation_scalar ( name , val_metric , timestep = epoch ) [EOL] [EOL] [comment] [EOL] if log_to_console and val_metric is not None and train_metric is not None : [EOL] logger . info ( dual_message_template , name . ljust ( name_length ) , train_metric , val_metric ) [EOL] elif log_to_console and val_metric is not None : [EOL] logger . info ( no_train_message_template , name . ljust ( name_length ) , [string] , val_metric ) [EOL] elif log_to_console and train_metric is not None : [EOL] logger . info ( no_val_message_template , name . ljust ( name_length ) , train_metric , [string] ) [EOL] [EOL] def enable_activation_logging ( self , model ) : [EOL] if self . _histogram_interval is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for _ , module in model . named_modules ( ) : [EOL] if not getattr ( module , [string] , False ) : [EOL] [comment] [EOL] continue [EOL] [EOL] def hook ( module_ , inputs , outputs ) : [EOL] [EOL] log_prefix = [string] . format ( module_ . __class__ ) [EOL] if self . should_log_histograms_this_batch ( ) : [EOL] self . log_activation_histogram ( outputs , log_prefix ) [EOL] [EOL] module . register_forward_hook ( hook ) [EOL] [EOL] def log_activation_histogram ( self , outputs , log_prefix ) : [EOL] if isinstance ( outputs , torch . Tensor ) : [EOL] log_name = log_prefix [EOL] self . add_train_histogram ( log_name , outputs ) [EOL] elif isinstance ( outputs , ( list , tuple ) ) : [EOL] for i , output in enumerate ( outputs ) : [EOL] log_name = [string] . format ( log_prefix , i ) [EOL] self . add_train_histogram ( log_name , output ) [EOL] elif isinstance ( outputs , dict ) : [EOL] for k , tensor in outputs . items ( ) : [EOL] log_name = [string] . format ( log_prefix , k ) [EOL] self . add_train_histogram ( log_name , tensor ) [EOL] else : [EOL] [comment] [EOL] pass [EOL] [EOL] def close ( self ) : [EOL] [docstring] [EOL] if self . _train_log is not None : [EOL] self . _train_log . close ( ) [EOL] if self . _validation_log is not None : [EOL] self . _validation_log . close ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import builtins [EOL] import torch [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . training . learning_rate_schedulers . learning_rate_scheduler import LearningRateScheduler [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class NoamLR ( LearningRateScheduler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , model_size , warmup_steps , factor = [number] , last_epoch = - [number] , ) : [EOL] self . warmup_steps = warmup_steps [EOL] self . factor = factor [EOL] self . model_size = model_size [EOL] super ( ) . __init__ ( optimizer , last_epoch = last_epoch ) [EOL] [EOL] @ overrides def step ( self , metric = None ) : [EOL] pass [EOL] [EOL] def step_batch ( self , batch_num_total = None ) : [EOL] if batch_num_total is None : [EOL] self . last_epoch += [number] [comment] [EOL] else : [EOL] self . last_epoch = batch_num_total [EOL] for param_group , learning_rate in zip ( self . optimizer . param_groups , self . get_values ( ) ) : [EOL] param_group [ [string] ] = learning_rate [EOL] [EOL] def get_values ( self ) : [EOL] step = max ( self . last_epoch , [number] ) [EOL] scale = self . factor * ( self . model_size ** ( - [number] ) * min ( step ** ( - [number] ) , step * self . warmup_steps ** ( - [number] ) ) ) [EOL] [EOL] return [ scale for _ in range ( len ( self . base_values ) ) ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $None$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List , Union [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Union [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . training . scheduler import Scheduler [EOL] from allennlp . training . optimizers import Optimizer [EOL] [EOL] [EOL] class LearningRateScheduler ( Scheduler , Registrable ) : [EOL] def __init__ ( self , optimizer , last_epoch = - [number] ) : [EOL] super ( ) . __init__ ( optimizer , [string] , last_epoch ) [EOL] [EOL] @ overrides def get_values ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] [EOL] class _PyTorchLearningRateSchedulerWrapper ( LearningRateScheduler ) : [EOL] def __init__ ( self , lr_scheduler ) : [EOL] self . lr_scheduler = lr_scheduler [EOL] [EOL] def get_values ( self ) : [EOL] return self . lr_scheduler . get_last_lr ( ) [EOL] [EOL] @ overrides def step ( self , metric = None ) : [EOL] self . lr_scheduler . step ( ) [EOL] [EOL] @ overrides def state_dict ( self ) : [EOL] return self . lr_scheduler . state_dict ( ) [EOL] [EOL] @ overrides def load_state_dict ( self , state_dict ) : [EOL] self . lr_scheduler . load_state_dict ( state_dict ) [EOL] [EOL] [EOL] class _PyTorchLearningRateSchedulerWithMetricsWrapper ( _PyTorchLearningRateSchedulerWrapper ) : [EOL] @ overrides def step ( self , metric = None ) : [EOL] if metric is None : [EOL] raise ConfigurationError ( [string] [string] [string] ) [EOL] self . lr_scheduler . step ( metric ) [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class StepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , step_size , gamma = [number] , last_epoch = - [number] ) : [EOL] lr_scheduler = torch . optim . lr_scheduler . StepLR ( optimizer = optimizer , step_size = step_size , gamma = gamma , last_epoch = last_epoch ) [EOL] super ( ) . __init__ ( lr_scheduler ) [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class MultiStepLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , milestones , gamma = [number] , last_epoch = - [number] ) : [EOL] lr_scheduler = torch . optim . lr_scheduler . MultiStepLR ( optimizer = optimizer , milestones = milestones , gamma = gamma , last_epoch = last_epoch ) [EOL] super ( ) . __init__ ( lr_scheduler ) [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class ExponentialLearningRateScheduler ( _PyTorchLearningRateSchedulerWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , gamma = [number] , last_epoch = - [number] ) : [EOL] lr_scheduler = torch . optim . lr_scheduler . ExponentialLR ( optimizer = optimizer , gamma = gamma , last_epoch = last_epoch ) [EOL] super ( ) . __init__ ( lr_scheduler ) [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class ReduceOnPlateauLearningRateScheduler ( _PyTorchLearningRateSchedulerWithMetricsWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , mode = [string] , factor = [number] , patience = [number] , verbose = False , threshold_mode = [string] , threshold = [number] , cooldown = [number] , min_lr = [number] , eps = [number] , ) : [EOL] lr_scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer = optimizer , mode = mode , factor = factor , patience = patience , verbose = verbose , threshold_mode = threshold_mode , threshold = threshold , cooldown = cooldown , min_lr = min_lr , eps = eps , ) [EOL] super ( ) . __init__ ( lr_scheduler ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
import builtins [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . training . learning_rate_schedulers import PolynomialDecay [EOL] from allennlp . training . learning_rate_schedulers . learning_rate_scheduler import LearningRateScheduler [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class LinearWithWarmup ( PolynomialDecay ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , num_epochs , num_steps_per_epoch = None , warmup_steps = [number] , last_epoch = - [number] , ) : [EOL] super ( ) . __init__ ( optimizer , num_epochs , num_steps_per_epoch , power = [number] , warmup_steps = warmup_steps , end_learning_rate = [number] , last_epoch = last_epoch , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0
from typing import Any [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . training . learning_rate_schedulers . learning_rate_scheduler import LearningRateScheduler [EOL] [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class PolynomialDecay ( LearningRateScheduler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , optimizer , num_epochs , num_steps_per_epoch , power = [number] , warmup_steps = [number] , end_learning_rate = [number] , last_epoch = - [number] , ) : [EOL] super ( ) . __init__ ( optimizer , last_epoch ) [EOL] [EOL] self . power = power [EOL] self . warmup_steps = warmup_steps [EOL] self . total_steps = num_epochs * num_steps_per_epoch [EOL] self . end_learning_rate = end_learning_rate [EOL] [EOL] self . steps = [number] [EOL] [EOL] self . step_batch ( [number] ) [EOL] [EOL] @ overrides def get_values ( self ) : [EOL] if self . warmup_steps > [number] and self . steps < self . warmup_steps : [EOL] f = self . steps / self . warmup_steps [EOL] return [ f * lr for lr in self . base_values ] [EOL] [EOL] if self . steps >= self . total_steps : [EOL] return [ self . end_learning_rate for _ in self . base_values ] [EOL] [EOL] current_decay_steps = self . total_steps - self . steps [EOL] total_decay_steps = self . total_steps - self . warmup_steps [EOL] f = ( current_decay_steps / total_decay_steps ) ** self . power [EOL] return [ f * ( lr - self . end_learning_rate ) + self . end_learning_rate for lr in self . base_values ] [EOL] [EOL] @ overrides def step ( self , metric = None ) : [EOL] pass [EOL] [EOL] @ overrides def step_batch ( self , batch_num_total = None ) : [EOL] if batch_num_total is None : [EOL] self . steps += [number] [EOL] else : [EOL] self . steps = batch_num_total [EOL] [EOL] for param_group , lr in zip ( self . optimizer . param_groups , self . get_values ( ) ) : [EOL] param_group [ self . param_group_field ] = lr [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] from overrides import overrides [EOL] [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Average ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] self . _total_value = [number] [EOL] self . _count = [number] [EOL] [EOL] @ overrides def __call__ ( self , value ) : [EOL] [docstring] [EOL] _total_value = list ( self . detach_tensors ( value ) ) [ [number] ] [EOL] _count = [number] [EOL] if is_distributed ( ) : [EOL] device = torch . device ( [string] if dist . get_backend ( ) == [string] else [string] ) [EOL] count = torch . tensor ( _count ) . to ( device ) [EOL] total_value = torch . tensor ( _total_value ) . to ( device ) [EOL] dist . all_reduce ( count , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( total_value , op = dist . ReduceOp . SUM ) [EOL] _count = count . item ( ) [EOL] _total_value = total_value . item ( ) [EOL] self . _count += _count [EOL] self . _total_value += _total_value [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] [EOL] average_value = self . _total_value / self . _count if self . _count > [number] else [number] [EOL] if reset : [EOL] self . reset ( ) [EOL] return float ( average_value ) [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _total_value = [number] [EOL] self . _count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0
from typing import Dict , Any [EOL] import builtins [EOL] import typing [EOL] from typing import Dict [EOL] [EOL] from allennlp . training . metrics . metric import Metric [EOL] from allennlp . training . metrics . fbeta_measure import FBetaMeasure [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class F1Measure ( FBetaMeasure ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , positive_label ) : [EOL] super ( ) . __init__ ( beta = [number] , labels = [ positive_label ] ) [EOL] self . _positive_label = positive_label [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] metric = super ( ) . get_metric ( reset = reset ) [EOL] [comment] [EOL] [comment] [EOL] precision = metric [ [string] ] [ [number] ] [EOL] recall = metric [ [string] ] [ [number] ] [EOL] f1 = metric [ [string] ] [ [number] ] [EOL] return { [string] : precision , [string] : recall , [string] : f1 } [EOL] [EOL] @ property def _true_positives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_positive_sum is None : [EOL] return [number] [EOL] else : [EOL] return self . _true_positive_sum [ self . _positive_label ] [EOL] [EOL] @ property def _true_negatives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_negative_sum is None : [EOL] return [number] [EOL] else : [EOL] return self . _true_negative_sum [ self . _positive_label ] [EOL] [EOL] @ property def _false_positives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _pred_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _pred_sum [ self . _positive_label ] - self . _true_positives [EOL] [EOL] @ property def _false_negatives ( self ) : [EOL] [comment] [EOL] [comment] [EOL] if self . _true_sum is None : [EOL] return [number] [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] return self . _true_sum [ self . _positive_label ] - self . _true_positives [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Entropy ( Metric ) : [EOL] def __init__ ( self ) : [EOL] self . _entropy = [number] [EOL] self . _count = [number] [EOL] [EOL] @ overrides def __call__ ( self , logits , mask = None , ) : [EOL] [docstring] [EOL] logits , mask = self . detach_tensors ( logits , mask ) [EOL] device = logits . device [EOL] [EOL] if mask is None : [EOL] mask = torch . ones ( logits . size ( ) [ : - [number] ] , device = logits . device ) . bool ( ) [EOL] [EOL] log_probs = torch . nn . functional . log_softmax ( logits , dim = - [number] ) [EOL] probabilities = torch . exp ( log_probs ) * mask . unsqueeze ( - [number] ) [EOL] weighted_negative_likelihood = - log_probs * probabilities [EOL] entropy = weighted_negative_likelihood . sum ( - [number] ) [EOL] [EOL] _entropy = entropy . sum ( ) / mask . sum ( ) [EOL] _count = [number] [EOL] [EOL] if is_distributed ( ) : [EOL] count = torch . tensor ( _count ) . to ( device ) [EOL] dist . all_reduce ( _entropy , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( count , op = dist . ReduceOp . SUM ) [EOL] _count = count . item ( ) [EOL] self . _entropy += _entropy . item ( ) [EOL] self . _count += _count [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] average_value = self . _entropy / self . _count if self . _count > [number] else [number] [EOL] if reset : [EOL] self . reset ( ) [EOL] return { [string] : average_value } [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _entropy = [number] [EOL] self . _count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.int$ 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] from overrides import overrides [EOL] import math [EOL] [EOL] from allennlp . training . metrics . average import Average [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Perplexity ( Average ) : [EOL] [docstring] [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] average_loss = super ( ) . get_metric ( reset ) [EOL] if average_loss == [number] : [EOL] return [number] [EOL] [EOL] [comment] [EOL] return math . exp ( average_loss ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
[docstring] [EOL] [EOL] from allennlp . training . metrics . attachment_scores import AttachmentScores [EOL] from allennlp . training . metrics . average import Average [EOL] from allennlp . training . metrics . boolean_accuracy import BooleanAccuracy [EOL] from allennlp . training . metrics . bleu import BLEU [EOL] from allennlp . training . metrics . rouge import ROUGE [EOL] from allennlp . training . metrics . categorical_accuracy import CategoricalAccuracy [EOL] from allennlp . training . metrics . covariance import Covariance [EOL] from allennlp . training . metrics . entropy import Entropy [EOL] from allennlp . training . metrics . evalb_bracketing_scorer import ( EvalbBracketingScorer , DEFAULT_EVALB_DIR , ) [EOL] from allennlp . training . metrics . fbeta_measure import FBetaMeasure [EOL] from allennlp . training . metrics . fbeta_multi_label_measure import FBetaMultiLabelMeasure [EOL] from allennlp . training . metrics . f1_measure import F1Measure [EOL] from allennlp . training . metrics . mean_absolute_error import MeanAbsoluteError [EOL] from allennlp . training . metrics . metric import Metric [EOL] from allennlp . training . metrics . pearson_correlation import PearsonCorrelation [EOL] from allennlp . training . metrics . spearman_correlation import SpearmanCorrelation [EOL] from allennlp . training . metrics . perplexity import Perplexity [EOL] from allennlp . training . metrics . sequence_accuracy import SequenceAccuracy [EOL] from allennlp . training . metrics . span_based_f1_measure import SpanBasedF1Measure [EOL] from allennlp . training . metrics . unigram_recall import UnigramRecall [EOL] from allennlp . training . metrics . auc import Auc [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] import sys [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class UnigramRecall ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] self . correct_count = [number] [EOL] self . total_count = [number] [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , end_index = sys . maxsize , ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . detach_tensors ( predictions , gold_labels , mask ) [EOL] device = predictions . device [EOL] [EOL] [comment] [EOL] if gold_labels . dim ( ) != predictions . dim ( ) - [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( gold_labels . size ( ) ) ) [EOL] if mask is not None and mask . size ( ) != gold_labels . size ( ) : [EOL] raise ConfigurationError ( [string] [string] . format ( mask . size ( ) ) ) [EOL] [EOL] batch_size = predictions . size ( ) [ [number] ] [EOL] correct = [number] [EOL] for i in range ( batch_size ) : [EOL] beams = predictions [ i ] [EOL] cur_gold = gold_labels [ i ] [EOL] [EOL] if mask is not None : [EOL] masked_gold = cur_gold * mask [ i ] [EOL] else : [EOL] masked_gold = cur_gold [EOL] cleaned_gold = [ x for x in masked_gold if x not in ( [number] , end_index ) ] [EOL] [EOL] retval = [number] [EOL] for word in cleaned_gold : [EOL] stillsearch = True [EOL] for beam in beams : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if stillsearch and word in beam : [EOL] retval += [number] / len ( cleaned_gold ) [EOL] stillsearch = False [EOL] correct += retval [EOL] [EOL] _correct_count = correct [EOL] _total_count = predictions . size ( ) [ [number] ] [EOL] [EOL] if is_distributed ( ) : [EOL] correct_count = torch . tensor ( _correct_count ) . to ( device ) [EOL] total_count = torch . tensor ( _total_count ) . to ( device ) [EOL] dist . all_reduce ( correct_count , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( total_count , op = dist . ReduceOp . SUM ) [EOL] _correct_count = correct_count . item ( ) [EOL] _total_count = total_count . item ( ) [EOL] [EOL] self . correct_count += _correct_count [EOL] self . total_count += _total_count [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] [EOL] recall = self . correct_count / self . total_count if self . total_count > [number] else [number] [EOL] if reset : [EOL] self . reset ( ) [EOL] return { [string] : recall } [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . correct_count = [number] [EOL] self . total_count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 $typing.Any$ 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class MeanAbsoluteError ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] self . _absolute_error = [number] [EOL] self . _total_count = [number] [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . detach_tensors ( predictions , gold_labels , mask ) [EOL] device = gold_labels . device [EOL] [EOL] absolute_errors = torch . abs ( predictions - gold_labels ) [EOL] [EOL] if mask is not None : [EOL] absolute_errors *= mask [EOL] _total_count = torch . sum ( mask ) [EOL] else : [EOL] _total_count = gold_labels . numel ( ) [EOL] _absolute_error = torch . sum ( absolute_errors ) [EOL] [EOL] if is_distributed ( ) : [EOL] absolute_error = torch . tensor ( _absolute_error ) . to ( device ) [EOL] total_count = torch . tensor ( _total_count ) . to ( device ) [EOL] dist . all_reduce ( absolute_error , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( total_count , op = dist . ReduceOp . SUM ) [EOL] _absolute_error = absolute_error . item ( ) [EOL] _total_count = total_count . item ( ) [EOL] [EOL] self . _absolute_error += _absolute_error [EOL] self . _total_count += _total_count [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] mean_absolute_error = self . _absolute_error / self . _total_count [EOL] if reset : [EOL] self . reset ( ) [EOL] return { [string] : mean_absolute_error } [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _absolute_error = [number] [EOL] self . _total_count = [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0
from typing import Optional , Any , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import List , Optional [EOL] [EOL] import torch [EOL] import torch . distributed as dist [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics import FBetaMeasure [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class FBetaMultiLabelMeasure ( FBetaMeasure ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , beta = [number] , average = None , labels = None , threshold = [number] , ) : [EOL] super ( ) . __init__ ( beta , average , labels ) [EOL] self . _threshold = threshold [EOL] [EOL] @ overrides def __call__ ( self , predictions , gold_labels , mask = None , ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . detach_tensors ( predictions , gold_labels , mask ) [EOL] device = gold_labels . device [EOL] [EOL] [comment] [EOL] num_classes = predictions . size ( - [number] ) [EOL] if ( gold_labels >= num_classes ) . any ( ) : [EOL] raise ConfigurationError ( [string] f" [string] { num_classes } [string] " ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if self . _true_positive_sum is None : [EOL] self . _true_positive_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] self . _true_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] self . _pred_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] self . _total_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] [EOL] if mask is None : [EOL] mask = torch . ones_like ( gold_labels ) . bool ( ) [EOL] gold_labels = gold_labels . float ( ) [EOL] [EOL] [comment] [EOL] pred_mask = ( predictions . sum ( dim = - [number] ) != [number] ) . unsqueeze ( - [number] ) [EOL] threshold_predictions = ( predictions >= self . _threshold ) . float ( ) [EOL] [EOL] class_indices = ( torch . arange ( num_classes , device = predictions . device ) . unsqueeze ( [number] ) . repeat ( gold_labels . size ( [number] ) , [number] ) ) [EOL] true_positives = ( gold_labels * threshold_predictions ) . bool ( ) & mask & pred_mask [EOL] true_positives_bins = class_indices [ true_positives ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] if true_positives_bins . shape [ [number] ] == [number] : [EOL] true_positive_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] else : [EOL] true_positive_sum = torch . bincount ( true_positives_bins . long ( ) , minlength = num_classes ) . float ( ) [EOL] [EOL] pred_bins = class_indices [ threshold_predictions . bool ( ) & mask & pred_mask ] [EOL] [comment] [EOL] [comment] [EOL] if pred_bins . shape [ [number] ] != [number] : [EOL] pred_sum = torch . bincount ( pred_bins , minlength = num_classes ) . float ( ) [EOL] else : [EOL] pred_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] [EOL] gold_labels_bins = class_indices [ gold_labels . bool ( ) & mask ] [EOL] if gold_labels_bins . shape [ [number] ] != [number] : [EOL] true_sum = torch . bincount ( gold_labels_bins , minlength = num_classes ) . float ( ) [EOL] else : [EOL] true_sum = torch . zeros ( num_classes , device = predictions . device ) [EOL] [EOL] self . _total_sum += mask . expand_as ( gold_labels ) . sum ( ) . to ( torch . float ) [EOL] [EOL] if is_distributed ( ) : [EOL] true_positive_sum = torch . tensor ( true_positive_sum ) . to ( device ) [EOL] pred_sum = torch . tensor ( pred_sum ) . to ( device ) [EOL] true_sum = torch . tensor ( true_sum ) . to ( device ) [EOL] dist . all_reduce ( true_positive_sum , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( pred_sum , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( true_sum , op = dist . ReduceOp . SUM ) [EOL] [EOL] self . _true_positive_sum += true_positive_sum [EOL] self . _pred_sum += pred_sum [EOL] self . _true_sum += true_sum [EOL] [EOL] @ property def _true_negative_sum ( self ) : [EOL] if self . _total_sum is None : [EOL] return None [EOL] else : [EOL] true_negative_sum = ( self . _total_sum [ [number] ] / self . _true_positive_sum . size ( [number] ) - self . _pred_sum - self . _true_sum + self . _true_positive_sum ) [EOL] return true_negative_sum [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class F1MultiLabelMeasure ( FBetaMultiLabelMeasure ) : [EOL] def __init__ ( self , average = None , labels = None , threshold = [number] ) : [EOL] super ( ) . __init__ ( [number] , average , labels , threshold ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.List[builtins.int]$ 0 $builtins.float$ 0 0
from typing import Optional , Any , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] from sklearn import metrics [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class Auc ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , positive_label = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _positive_label = positive_label [EOL] self . _all_predictions = torch . FloatTensor ( ) [EOL] self . _all_gold_labels = torch . LongTensor ( ) [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , ) : [EOL] [docstring] [EOL] [EOL] predictions , gold_labels , mask = self . detach_tensors ( predictions , gold_labels , mask ) [EOL] [EOL] [comment] [EOL] if gold_labels . dim ( ) != [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( gold_labels . size ( ) ) ) [EOL] if predictions . dim ( ) != [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( predictions . size ( ) ) ) [EOL] [EOL] unique_gold_labels = torch . unique ( gold_labels ) [EOL] if unique_gold_labels . numel ( ) > [number] : [EOL] raise ConfigurationError ( [string] [string] . format ( unique_gold_labels . numel ( ) ) ) [EOL] [EOL] gold_labels_is_binary = set ( unique_gold_labels . tolist ( ) ) <= { [number] , [number] } [EOL] if not gold_labels_is_binary and self . _positive_label not in unique_gold_labels : [EOL] raise ConfigurationError ( [string] [string] . format ( self . _positive_label ) ) [EOL] [EOL] if mask is None : [EOL] batch_size = gold_labels . shape [ [number] ] [EOL] mask = torch . ones ( batch_size , device = gold_labels . device ) . bool ( ) [EOL] [EOL] self . _all_predictions = self . _all_predictions . to ( predictions . device ) [EOL] self . _all_gold_labels = self . _all_gold_labels . to ( gold_labels . device ) [EOL] [EOL] self . _all_predictions = torch . cat ( [ self . _all_predictions , torch . masked_select ( predictions , mask ) . float ( ) ] , dim = [number] ) [EOL] self . _all_gold_labels = torch . cat ( [ self . _all_gold_labels , torch . masked_select ( gold_labels , mask ) . long ( ) ] , dim = [number] ) [EOL] [EOL] if is_distributed ( ) : [EOL] world_size = dist . get_world_size ( ) [EOL] device = gold_labels . device [EOL] [EOL] [comment] [EOL] _all_batch_lengths = [ torch . tensor ( [number] ) for i in range ( world_size ) ] [EOL] dist . all_gather ( _all_batch_lengths , torch . tensor ( len ( self . _all_predictions ) , device = device ) ) [EOL] _all_batch_lengths = [ batch_length . item ( ) for batch_length in _all_batch_lengths ] [EOL] [EOL] if len ( set ( _all_batch_lengths ) ) > [number] : [EOL] [comment] [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] _all_predictions = [ torch . zeros ( self . _all_predictions . shape , device = device ) for i in range ( world_size ) ] [EOL] [EOL] _all_gold_labels = [ torch . zeros ( self . _all_gold_labels . shape , device = device , dtype = torch . long ) for i in range ( world_size ) ] [EOL] dist . all_gather ( _all_predictions , self . _all_predictions ) [EOL] dist . all_gather ( _all_gold_labels , self . _all_gold_labels ) [EOL] self . _all_predictions = torch . cat ( _all_predictions , dim = [number] ) [EOL] self . _all_gold_labels = torch . cat ( _all_gold_labels , dim = [number] ) [EOL] [EOL] def get_metric ( self , reset = False ) : [EOL] [EOL] if self . _all_gold_labels . shape [ [number] ] == [number] : [EOL] return [number] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( self . _all_gold_labels . cpu ( ) . numpy ( ) , self . _all_predictions . cpu ( ) . numpy ( ) , pos_label = self . _positive_label , ) [EOL] auc = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] [EOL] if reset : [EOL] self . reset ( ) [EOL] return auc [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _all_predictions = torch . FloatTensor ( ) [EOL] self . _all_gold_labels = torch . LongTensor ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Optional , Iterable , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Dict , Iterable , Optional , Any [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] [EOL] class Metric ( Registrable ) : [EOL] [docstring] [EOL] [EOL] supports_distributed = False [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_metric ( self , reset ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def reset ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] @ staticmethod def detach_tensors ( * tensors ) : [EOL] [docstring] [EOL] [comment] [EOL] return ( x . detach ( ) if isinstance ( x , torch . Tensor ) else x for x in tensors ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 $typing.Optional[torch.BoolTensor]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[torch.Tensor]$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0
from typing import Optional , Any , List [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] import scipy . stats as stats [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class SpearmanCorrelation ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] self . total_predictions = torch . zeros ( [number] ) [EOL] self . total_gold_labels = torch . zeros ( [number] ) [EOL] [EOL] def __call__ ( self , predictions , gold_labels , mask = None , ) : [EOL] [docstring] [EOL] predictions , gold_labels , mask = self . detach_tensors ( predictions , gold_labels , mask ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] predictions = predictions . reshape ( - [number] ) [EOL] gold_labels = gold_labels . reshape ( - [number] ) [EOL] [EOL] self . total_predictions = self . total_predictions . to ( predictions . device ) [EOL] self . total_gold_labels = self . total_gold_labels . to ( gold_labels . device ) [EOL] [EOL] if mask is not None : [EOL] mask = mask . reshape ( - [number] ) [EOL] self . total_predictions = torch . cat ( ( self . total_predictions , predictions * mask ) , [number] ) [EOL] self . total_gold_labels = torch . cat ( ( self . total_gold_labels , gold_labels * mask ) , [number] ) [EOL] else : [EOL] self . total_predictions = torch . cat ( ( self . total_predictions , predictions ) , [number] ) [EOL] self . total_gold_labels = torch . cat ( ( self . total_gold_labels , gold_labels ) , [number] ) [EOL] [EOL] if is_distributed ( ) : [EOL] world_size = dist . get_world_size ( ) [EOL] device = gold_labels . device [EOL] [comment] [EOL] _all_batch_lengths = [ torch . tensor ( [number] ) for i in range ( world_size ) ] [EOL] dist . all_gather ( _all_batch_lengths , torch . tensor ( self . total_predictions . shape [ [number] ] , device = device ) ) [EOL] _all_batch_lengths = [ batch_length . item ( ) for batch_length in _all_batch_lengths ] [EOL] [EOL] if len ( set ( _all_batch_lengths ) ) > [number] : [EOL] [comment] [EOL] raise RuntimeError ( [string] [string] ) [EOL] _total_predictions = [ torch . zeros ( self . total_predictions . shape , device = device ) for i in range ( world_size ) ] [EOL] _total_gold_labels = [ torch . zeros ( self . total_gold_labels . shape , device = device ) for i in range ( world_size ) ] [EOL] dist . all_gather ( _total_predictions , self . total_predictions ) [EOL] dist . all_gather ( _total_gold_labels , self . total_gold_labels ) [EOL] self . total_predictions = torch . cat ( _total_predictions , dim = [number] ) [EOL] self . total_gold_labels = torch . cat ( _total_gold_labels , dim = [number] ) [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] [EOL] spearman_correlation = stats . spearmanr ( self . total_predictions . cpu ( ) . numpy ( ) , self . total_gold_labels . cpu ( ) . numpy ( ) ) [EOL] if reset : [EOL] self . reset ( ) [EOL] [EOL] return spearman_correlation [ [number] ] [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . total_predictions = torch . zeros ( [number] ) [EOL] self . total_gold_labels = torch . zeros ( [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Set , Any , Tuple [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from collections import defaultdict [EOL] from typing import Tuple , Dict , Set [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class ROUGE ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , ngram_size = [number] , exclude_indices = None , ) : [EOL] self . _ngram_size = ngram_size [EOL] self . _exclude_indices = exclude_indices or set ( ) [EOL] [EOL] self . _total_rouge_n_recalls = defaultdict ( lambda : [number] ) [EOL] self . _total_rouge_n_precisions = defaultdict ( lambda : [number] ) [EOL] self . _total_rouge_n_f1s = defaultdict ( lambda : [number] ) [EOL] [EOL] self . _total_rouge_l_f1 = [number] [EOL] [EOL] self . _total_sequence_count = [number] [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _total_rouge_n_recalls = defaultdict ( lambda : [number] ) [EOL] self . _total_rouge_n_precisions = defaultdict ( lambda : [number] ) [EOL] self . _total_rouge_n_f1s = defaultdict ( lambda : [number] ) [EOL] [EOL] self . _total_rouge_l_f1 = [number] [EOL] [EOL] self . _total_sequence_count = [number] [EOL] [EOL] def _longest_common_subsequence ( self , seq_1 , seq_2 ) : [EOL] [docstring] [EOL] m = len ( seq_1 ) [EOL] n = len ( seq_2 ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] if m < n : [EOL] seq_1 , seq_2 = seq_2 , seq_1 [EOL] m , n = n , m [EOL] [EOL] prev_lcs = torch . zeros ( n + [number] , dtype = torch . long ) [EOL] [EOL] for i in range ( m - [number] , - [number] , - [number] ) : [EOL] [comment] [EOL] if seq_1 [ i ] . item ( ) in self . _exclude_indices : [EOL] continue [EOL] [EOL] cur_lcs = torch . zeros_like ( prev_lcs ) [EOL] for j in range ( n - [number] , - [number] , - [number] ) : [EOL] if seq_1 [ i ] == seq_2 [ j ] : [EOL] cur_lcs [ j ] = [number] + prev_lcs [ j + [number] ] [EOL] else : [EOL] cur_lcs [ j ] = max ( cur_lcs [ j + [number] ] , prev_lcs [ j ] ) [EOL] prev_lcs = cur_lcs [EOL] [EOL] return prev_lcs [ [number] ] . item ( ) [EOL] [EOL] def _get_rouge_l_score ( self , predicted_tokens , reference_tokens ) : [EOL] [docstring] [EOL] total_f1 = [number] [EOL] [EOL] for predicted_seq , reference_seq in zip ( predicted_tokens , reference_tokens ) : [EOL] from allennlp . training . util import get_valid_tokens_mask [EOL] [EOL] m = get_valid_tokens_mask ( reference_seq , self . _exclude_indices ) . sum ( ) . item ( ) [EOL] n = get_valid_tokens_mask ( predicted_seq , self . _exclude_indices ) . sum ( ) . item ( ) [EOL] [EOL] lcs = self . _longest_common_subsequence ( reference_seq , predicted_seq ) [EOL] [EOL] [comment] [EOL] if lcs == [number] : [EOL] continue [EOL] [EOL] recall_lcs = lcs / m [EOL] precision_lcs = lcs / n [EOL] [EOL] f1 = [number] * recall_lcs * precision_lcs / ( recall_lcs + precision_lcs ) [EOL] [EOL] total_f1 += f1 [EOL] [EOL] if is_distributed ( ) : [EOL] device = predicted_tokens . device [EOL] _total_f1 = torch . tensor ( total_f1 ) . to ( device ) [EOL] dist . all_reduce ( _total_f1 , op = dist . ReduceOp . SUM ) [EOL] total_f1 = _total_f1 . item ( ) [EOL] [EOL] return total_f1 [EOL] [EOL] def _get_rouge_n_stats ( self , predicted_tokens , reference_tokens , ngram_size , ) : [EOL] [docstring] [EOL] total_recall = [number] [EOL] total_precision = [number] [EOL] total_f1 = [number] [EOL] [EOL] for predicted_seq , reference_seq in zip ( predicted_tokens , reference_tokens ) : [EOL] from allennlp . training . util import ngrams [EOL] [EOL] predicted_ngram_counts = ngrams ( predicted_seq , ngram_size , self . _exclude_indices ) [EOL] reference_ngram_counts = ngrams ( reference_seq , ngram_size , self . _exclude_indices ) [EOL] [EOL] matches = [number] [EOL] total_reference_ngrams = [number] [EOL] for ngram , count in reference_ngram_counts . items ( ) : [EOL] matches += min ( predicted_ngram_counts [ ngram ] , count ) [EOL] total_reference_ngrams += count [EOL] [EOL] total_predicted_ngrams = sum ( predicted_ngram_counts . values ( ) ) [EOL] [EOL] if total_reference_ngrams == [number] or total_predicted_ngrams == [number] or matches == [number] : [EOL] continue [EOL] [EOL] recall = matches / total_reference_ngrams [EOL] precision = matches / total_predicted_ngrams [EOL] [EOL] f1 = [number] * recall * precision / ( recall + precision ) [EOL] [EOL] [comment] [EOL] total_recall += recall [EOL] total_precision += precision [EOL] total_f1 += f1 [EOL] [EOL] if is_distributed ( ) : [EOL] device = predicted_tokens . device [EOL] _total_recall = torch . tensor ( total_recall ) . to ( device ) [EOL] _total_precision = torch . tensor ( total_precision ) . to ( device ) [EOL] _total_f1 = torch . tensor ( total_f1 ) . to ( device ) [EOL] dist . all_reduce ( _total_recall , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( _total_precision , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( _total_f1 , op = dist . ReduceOp . SUM ) [EOL] total_recall = _total_recall . item ( ) [EOL] total_precision = _total_precision . item ( ) [EOL] total_f1 = _total_f1 . item ( ) [EOL] [EOL] return total_recall , total_precision , total_f1 [EOL] [EOL] @ overrides def __call__ ( self , predictions , gold_targets , ) : [EOL] [docstring] [EOL] [comment] [EOL] predictions , gold_targets = self . detach_tensors ( predictions , gold_targets ) [EOL] for n in range ( [number] , self . _ngram_size + [number] ) : [EOL] [EOL] recall , precision , f1 = self . _get_rouge_n_stats ( predictions , gold_targets , n ) [EOL] self . _total_rouge_n_recalls [ n ] += recall [EOL] self . _total_rouge_n_precisions [ n ] += precision [EOL] self . _total_rouge_n_f1s [ n ] += f1 [EOL] [EOL] [comment] [EOL] self . _total_rouge_l_f1 += self . _get_rouge_l_score ( predictions , gold_targets ) [EOL] [EOL] self . _total_sequence_count += len ( predictions ) [EOL] [EOL] def _metric_mean ( self , metric_sum ) : [EOL] if self . _total_sequence_count == [number] : [EOL] return [number] [EOL] return metric_sum / self . _total_sequence_count [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [docstring] [EOL] [EOL] metrics = { } [EOL] [EOL] [comment] [EOL] [comment] [EOL] metrics . update ( { f" [string] { i } [string] " : self . _metric_mean ( self . _total_rouge_n_recalls [ i ] ) for i in range ( [number] , self . _ngram_size + [number] ) } ) [EOL] [EOL] [comment] [EOL] metrics . update ( { f" [string] { i } [string] " : self . _metric_mean ( self . _total_rouge_n_precisions [ i ] ) for i in range ( [number] , self . _ngram_size + [number] ) } ) [EOL] [EOL] [comment] [EOL] metrics . update ( { f" [string] { i } [string] " : self . _metric_mean ( self . _total_rouge_n_f1s [ i ] ) for i in range ( [number] , self . _ngram_size + [number] ) } ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] metrics [ [string] ] = self . _metric_mean ( self . _total_rouge_l_f1 ) [EOL] [EOL] if reset : [EOL] self . reset ( ) [EOL] [EOL] return metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 $typing.Set[builtins.int]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Set[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $torch.LongTensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Tuple[builtins.float,builtins.float,builtins.float]$ 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $torch.LongTensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0
from typing import Set , Any , Dict , Generator , Iterable , Tuple [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] from collections import Counter [EOL] import math [EOL] from typing import Iterable , Tuple , Dict , Set [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . util import is_distributed [EOL] from allennlp . training . metrics . metric import Metric [EOL] [EOL] [EOL] @ Metric . register ( [string] ) class BLEU ( Metric ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , ngram_weights = ( [number] , [number] , [number] , [number] ) , exclude_indices = None , ) : [EOL] self . _ngram_weights = ngram_weights [EOL] self . _exclude_indices = exclude_indices or set ( ) [EOL] self . _precision_matches = Counter ( ) [EOL] self . _precision_totals = Counter ( ) [EOL] self . _prediction_lengths = [number] [EOL] self . _reference_lengths = [number] [EOL] [EOL] @ overrides def reset ( self ) : [EOL] self . _precision_matches = Counter ( ) [EOL] self . _precision_totals = Counter ( ) [EOL] self . _prediction_lengths = [number] [EOL] self . _reference_lengths = [number] [EOL] [EOL] def _get_modified_precision_counts ( self , predicted_tokens , reference_tokens , ngram_size , ) : [EOL] [docstring] [EOL] clipped_matches = [number] [EOL] total_predicted = [number] [EOL] from allennlp . training . util import ngrams [EOL] [EOL] for predicted_row , reference_row in zip ( predicted_tokens , reference_tokens ) : [EOL] predicted_ngram_counts = ngrams ( predicted_row , ngram_size , self . _exclude_indices ) [EOL] reference_ngram_counts = ngrams ( reference_row , ngram_size , self . _exclude_indices ) [EOL] for ngram , count in predicted_ngram_counts . items ( ) : [EOL] clipped_matches += min ( count , reference_ngram_counts [ ngram ] ) [EOL] total_predicted += count [EOL] return clipped_matches , total_predicted [EOL] [EOL] def _get_brevity_penalty ( self ) : [EOL] if self . _prediction_lengths > self . _reference_lengths : [EOL] return [number] [EOL] if self . _reference_lengths == [number] or self . _prediction_lengths == [number] : [EOL] return [number] [EOL] return math . exp ( [number] - self . _reference_lengths / self . _prediction_lengths ) [EOL] [EOL] @ overrides def __call__ ( self , predictions , gold_targets , ) : [EOL] [docstring] [EOL] predictions , gold_targets = self . detach_tensors ( predictions , gold_targets ) [EOL] device = gold_targets . device [EOL] if is_distributed ( ) : [EOL] world_size = dist . get_world_size ( ) [EOL] [EOL] for ngram_size , _ in enumerate ( self . _ngram_weights , start = [number] ) : [EOL] precision_matches , precision_totals = self . _get_modified_precision_counts ( predictions , gold_targets , ngram_size ) [EOL] if is_distributed ( ) : [EOL] _precision_matches = torch . tensor ( precision_matches ) . to ( device ) [EOL] _precision_totals = torch . tensor ( precision_totals ) . to ( device ) [EOL] dist . all_reduce ( _precision_matches , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( _precision_totals , op = dist . ReduceOp . SUM ) [EOL] precision_matches = _precision_matches . item ( ) / world_size [EOL] precision_totals = _precision_totals . item ( ) / world_size [EOL] [EOL] self . _precision_matches [ ngram_size ] += precision_matches [EOL] self . _precision_totals [ ngram_size ] += precision_totals [EOL] [EOL] if not self . _exclude_indices : [EOL] _prediction_lengths = predictions . size ( [number] ) * predictions . size ( [number] ) [EOL] _reference_lengths = gold_targets . size ( [number] ) * gold_targets . size ( [number] ) [EOL] [EOL] else : [EOL] from allennlp . training . util import get_valid_tokens_mask [EOL] [EOL] valid_predictions_mask = get_valid_tokens_mask ( predictions , self . _exclude_indices ) [EOL] valid_gold_targets_mask = get_valid_tokens_mask ( gold_targets , self . _exclude_indices ) [EOL] _prediction_lengths = valid_predictions_mask . sum ( ) . item ( ) [EOL] _reference_lengths = valid_gold_targets_mask . sum ( ) . item ( ) [EOL] [EOL] if is_distributed ( ) : [EOL] prediction_lengths = torch . tensor ( _prediction_lengths ) . to ( device ) [EOL] reference_lengths = torch . tensor ( _reference_lengths ) . to ( device ) [EOL] dist . all_reduce ( prediction_lengths , op = dist . ReduceOp . SUM ) [EOL] dist . all_reduce ( reference_lengths , op = dist . ReduceOp . SUM ) [EOL] _prediction_lengths = prediction_lengths . item ( ) [EOL] _reference_lengths = reference_lengths . item ( ) [EOL] [EOL] self . _prediction_lengths += _prediction_lengths [EOL] self . _reference_lengths += _reference_lengths [EOL] [EOL] @ overrides def get_metric ( self , reset = False ) : [EOL] [EOL] brevity_penalty = self . _get_brevity_penalty ( ) [EOL] ngram_scores = ( weight * ( math . log ( self . _precision_matches [ n ] + [number] ) - math . log ( self . _precision_totals [ n ] + [number] ) ) for n , weight in enumerate ( self . _ngram_weights , start = [number] ) ) [EOL] bleu = brevity_penalty * math . exp ( sum ( ngram_scores ) ) [EOL] [EOL] if reset : [EOL] self . reset ( ) [EOL] return { [string] : bleu } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Iterable[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.int]$ 0 0 0 0 0 0 0 0 $typing.Iterable[builtins.float]$ 0 $typing.Iterable[builtins.float]$ 0 0 0 0 0 $typing.Set[builtins.int]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.int,builtins.int]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Tuple[builtins.int,builtins.int]$ 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.LongTensor$ 0 $torch.LongTensor$ 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Dict[typing.Tuple[builtins.int,...],builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Generator[builtins.float,None,None]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Generator[builtins.float,None,None]$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.float$ 0 0
import builtins [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . training . scheduler import Scheduler [EOL] [EOL] [EOL] class MomentumScheduler ( Scheduler , Registrable ) : [EOL] def __init__ ( self , optimizer , last_epoch = - [number] ) : [EOL] super ( ) . __init__ ( optimizer , [string] , last_epoch ) [EOL] [EOL] def get_values ( self ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 0 0 $builtins.int$ 0 0 0 0 $None$ 0 0 0 0 0 0 0 0
from allennlp . training . momentum_schedulers . momentum_scheduler import MomentumScheduler [EOL] from allennlp . training . momentum_schedulers . inverted_triangular import InvertedTriangular [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . nn . activations import Activation [EOL] from allennlp . nn . initializers import Initializer , InitializerApplicator [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] [EOL] class Regularizer ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def __call__ ( self , parameter ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 0 0
from typing import Any , List , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import re [EOL] from typing import List , Tuple [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import FromParams [EOL] from allennlp . nn . regularizers . regularizer import Regularizer [EOL] [EOL] [EOL] class RegularizerApplicator ( FromParams ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , regexes = None ) : [EOL] [docstring] [EOL] self . _regularizers = regexes or [ ] [EOL] [EOL] def __call__ ( self , module ) : [EOL] [docstring] [EOL] accumulator = [number] [EOL] for name , parameter in module . named_parameters ( ) : [EOL] [comment] [EOL] if parameter . requires_grad : [EOL] [comment] [EOL] for regex , regularizer in self . _regularizers : [EOL] if re . search ( regex , name ) : [EOL] penalty = regularizer ( parameter ) [EOL] accumulator = accumulator + penalty [EOL] break [EOL] return accumulator [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . modules . attention import Attention [EOL] from allennlp . modules . bimpm_matching import BiMpmMatching [EOL] from allennlp . modules . conditional_random_field import ConditionalRandomField [EOL] from allennlp . modules . elmo import Elmo [EOL] from allennlp . modules . feedforward import FeedForward [EOL] from allennlp . modules . gated_sum import GatedSum [EOL] from allennlp . modules . highway import Highway [EOL] from allennlp . modules . input_variational_dropout import InputVariationalDropout [EOL] from allennlp . modules . layer_norm import LayerNorm [EOL] from allennlp . modules . matrix_attention import MatrixAttention [EOL] from allennlp . modules . maxout import Maxout [EOL] from allennlp . modules . residual_with_layer_dropout import ResidualWithLayerDropout [EOL] from allennlp . modules . scalar_mix import ScalarMix [EOL] from allennlp . modules . seq2seq_encoders import Seq2SeqEncoder [EOL] from allennlp . modules . seq2vec_encoders import Seq2VecEncoder [EOL] from allennlp . modules . text_field_embedders import TextFieldEmbedder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders import TokenEmbedder , Embedding [EOL] from allennlp . modules . softmax_loss import SoftmaxLoss [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] class LayerNorm ( torch . nn . Module ) : [EOL] [EOL] [docstring] [comment] [EOL] [EOL] def __init__ ( self , dimension ) : [EOL] super ( ) . __init__ ( ) [EOL] self . gamma = torch . nn . Parameter ( torch . ones ( dimension ) ) [EOL] self . beta = torch . nn . Parameter ( torch . zeros ( dimension ) ) [EOL] [EOL] def forward ( self , tensor ) : [EOL] mean = tensor . mean ( - [number] , keepdim = True ) [EOL] std = tensor . std ( - [number] , unbiased = False , keepdim = True ) [EOL] return ( self . gamma * ( tensor - mean ) / ( std + util . tiny_value_of_dtype ( std . dtype ) ) + self . beta ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] import torch [EOL] import numpy as np [EOL] [EOL] [EOL] class SoftmaxLoss ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , num_words , embedding_dim ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] [comment] [EOL] self . tie_embeddings = False [EOL] [EOL] self . softmax_w = torch . nn . Parameter ( torch . randn ( embedding_dim , num_words ) / np . sqrt ( embedding_dim ) ) [EOL] self . softmax_b = torch . nn . Parameter ( torch . zeros ( num_words ) ) [EOL] [EOL] def forward ( self , embeddings , targets ) : [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] probs = torch . nn . functional . log_softmax ( torch . matmul ( embeddings , self . softmax_w ) + self . softmax_b , dim = - [number] ) [EOL] [EOL] return torch . nn . functional . nll_loss ( probs , targets . long ( ) , reduction = [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . nn import Activation [EOL] [EOL] [EOL] class GatedSum ( torch . nn . Module ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_dim , activation = torch . nn . Sigmoid ( ) ) : [EOL] super ( ) . __init__ ( ) [EOL] self . input_dim = input_dim [EOL] self . _gate = torch . nn . Linear ( input_dim * [number] , [number] ) [EOL] self . _activation = activation [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . input_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . input_dim [EOL] [EOL] def forward ( self , input_a , input_b ) : [EOL] if input_a . size ( ) != input_b . size ( ) : [EOL] raise ValueError ( [string] ) [EOL] if input_a . size ( - [number] ) != self . input_dim : [EOL] raise ValueError ( [string] ) [EOL] gate_value = self . _activation ( self . _gate ( torch . cat ( [ input_a , input_b ] , - [number] ) ) ) [EOL] return gate_value * input_a + ( [number] - gate_value ) * input_b [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.Activation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 $typing.Any$ 0 0 $torch.Tensor$ 0
from typing import Any [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] @ SpanExtractor . register ( [string] ) class SelfAttentiveSpanExtractor ( SpanExtractor ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_dim ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _input_dim = input_dim [EOL] self . _global_attention = TimeDistributed ( torch . nn . Linear ( input_dim , [number] ) ) [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . _input_dim [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _input_dim [EOL] [EOL] @ overrides def forward ( self , sequence_tensor , span_indices , span_indices_mask = None , ) : [EOL] [comment] [EOL] global_attention_logits = self . _global_attention ( sequence_tensor ) [EOL] [EOL] [comment] [EOL] concat_tensor = torch . cat ( [ sequence_tensor , global_attention_logits ] , - [number] ) [EOL] [EOL] concat_output , span_mask = util . batched_span_select ( concat_tensor , span_indices ) [EOL] [EOL] [comment] [EOL] span_embeddings = concat_output [ : , : , : , : - [number] ] [EOL] [comment] [EOL] span_attention_logits = concat_output [ : , : , : , - [number] ] [EOL] [EOL] [comment] [EOL] span_attention_weights = util . masked_softmax ( span_attention_logits , span_mask ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] attended_text_embeddings = util . weighted_sum ( span_embeddings , span_attention_weights ) [EOL] [EOL] if span_indices_mask is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return attended_text_embeddings * span_indices_mask . unsqueeze ( - [number] ) [EOL] [EOL] return attended_text_embeddings [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.FloatTensor$ 0 0 0 $torch.FloatTensor$ 0 $torch.LongTensor$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.FloatTensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $torch.FloatTensor$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.LongTensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
from allennlp . modules . span_extractors . span_extractor import SpanExtractor [EOL] from allennlp . modules . span_extractors . endpoint_span_extractor import EndpointSpanExtractor [EOL] from allennlp . modules . span_extractors . self_attentive_span_extractor import ( SelfAttentiveSpanExtractor , ) [EOL] from allennlp . modules . span_extractors . bidirectional_endpoint_span_extractor import ( BidirectionalEndpointSpanExtractor , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] [EOL] import torch [EOL] import torch . nn [EOL] [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class BertPooler ( Seq2VecEncoder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , pretrained_model , * , override_weights_file = None , override_weights_strip_prefix = None , requires_grad = True , dropout = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] from allennlp . common import cached_transformers [EOL] [EOL] model = cached_transformers . get ( pretrained_model , False , override_weights_file , override_weights_strip_prefix ) [EOL] [EOL] self . _dropout = torch . nn . Dropout ( p = dropout ) [EOL] [EOL] import copy [EOL] [EOL] self . pooler = copy . deepcopy ( model . pooler ) [EOL] for param in self . pooler . parameters ( ) : [EOL] param . requires_grad = requires_grad [EOL] self . _embedding_dim = model . config . hidden_size [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] def forward ( self , tokens , mask = None , num_wrapping_dims = [number] ) : [EOL] pooler = self . pooler [EOL] for _ in range ( num_wrapping_dims ) : [EOL] from allennlp . modules import TimeDistributed [EOL] [EOL] pooler = TimeDistributed ( pooler ) [EOL] pooled = pooler ( tokens ) [EOL] pooled = self . _dropout ( pooled ) [EOL] return pooled [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0
import builtins [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] import torch . nn [EOL] [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] from allennlp . nn . util import get_final_encoder_states [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class ClsPooler ( Seq2VecEncoder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , embedding_dim = None , cls_is_last_token = False ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _embedding_dim = embedding_dim [EOL] self . _cls_is_last_token = cls_is_last_token [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _embedding_dim [EOL] [EOL] @ overrides def forward ( self , tokens , mask = None ) : [EOL] [comment] [EOL] [comment] [EOL] if not self . _cls_is_last_token : [EOL] return tokens [ : , [number] , : ] [EOL] else : [comment] [EOL] if mask is None : [EOL] raise ValueError ( [string] ) [EOL] return get_final_encoder_states ( tokens , mask ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 0
from typing import Any [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules . augmented_lstm import AugmentedLstm [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] from allennlp . modules . stacked_alternating_lstm import StackedAlternatingLstm [EOL] from allennlp . modules . stacked_bidirectional_lstm import StackedBidirectionalLstm [EOL] [EOL] [EOL] class PytorchSeq2VecWrapper ( Seq2VecEncoder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , module ) : [EOL] [comment] [EOL] super ( ) . __init__ ( stateful = False ) [EOL] self . _module = module [EOL] try : [EOL] if not self . _module . batch_first : [EOL] raise ConfigurationError ( [string] ) [EOL] except AttributeError : [EOL] pass [EOL] [EOL] def get_input_dim ( self ) : [EOL] return self . _module . input_size [EOL] [EOL] def get_output_dim ( self ) : [EOL] try : [EOL] is_bidirectional = self . _module . bidirectional [EOL] except AttributeError : [EOL] is_bidirectional = False [EOL] return self . _module . hidden_size * ( [number] if is_bidirectional else [number] ) [EOL] [EOL] def forward ( self , inputs , mask , hidden_state = None ) : [EOL] [EOL] if mask is None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] return self . _module ( inputs , hidden_state ) [ [number] ] [ : , - [number] , : ] [EOL] [EOL] batch_size = mask . size ( [number] ) [EOL] [EOL] ( _ , state , restoration_indices , ) = self . sort_and_run_forward ( self . _module , inputs , mask , hidden_state ) [EOL] [EOL] [comment] [EOL] if isinstance ( state , tuple ) : [EOL] state = state [ [number] ] [EOL] [EOL] num_layers_times_directions , num_valid , encoding_dim = state . size ( ) [EOL] [comment] [EOL] if num_valid < batch_size : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] zeros = state . new_zeros ( num_layers_times_directions , batch_size - num_valid , encoding_dim ) [EOL] state = torch . cat ( [ state , zeros ] , [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] unsorted_state = state . transpose ( [number] , [number] ) . index_select ( [number] , restoration_indices ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] try : [EOL] last_state_index = [number] if self . _module . bidirectional else [number] [EOL] except AttributeError : [EOL] last_state_index = [number] [EOL] last_layer_state = unsorted_state [ : , - last_state_index : , : ] [EOL] return last_layer_state . contiguous ( ) . view ( [ - [number] , self . get_output_dim ( ) ] ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class GruSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , bias = True , dropout = [number] , bidirectional = False , ) : [EOL] module = torch . nn . GRU ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class LstmSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , bias = True , dropout = [number] , bidirectional = False , ) : [EOL] module = torch . nn . LSTM ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class RnnSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , nonlinearity = [string] , bias = True , dropout = [number] , bidirectional = False , ) : [EOL] module = torch . nn . RNN ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , nonlinearity = nonlinearity , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class AugmentedLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , go_forward = True , recurrent_dropout_probability = [number] , use_highway = True , use_input_projection_bias = True , ) : [EOL] module = AugmentedLstm ( input_size = input_size , hidden_size = hidden_size , go_forward = go_forward , recurrent_dropout_probability = recurrent_dropout_probability , use_highway = use_highway , use_input_projection_bias = use_input_projection_bias , ) [EOL] super ( ) . __init__ ( module = module ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class StackedAlternatingLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers , recurrent_dropout_probability = [number] , use_highway = True , use_input_projection_bias = True , ) : [EOL] module = StackedAlternatingLstm ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , recurrent_dropout_probability = recurrent_dropout_probability , use_highway = use_highway , use_input_projection_bias = use_input_projection_bias , ) [EOL] super ( ) . __init__ ( module = module ) [EOL] [EOL] [EOL] @ Seq2VecEncoder . register ( [string] ) class StackedBidirectionalLstmSeq2VecEncoder ( PytorchSeq2VecWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers , recurrent_dropout_probability = [number] , layer_dropout_probability = [number] , use_highway = True , ) : [EOL] module = StackedBidirectionalLstm ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , recurrent_dropout_probability = recurrent_dropout_probability , layer_dropout_probability = layer_dropout_probability , use_highway = use_highway , ) [EOL] super ( ) . __init__ ( module = module ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $torch.nn.modules.RNNBase$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.nn.modules.RNNBase$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import torch [EOL] import typing [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . modules . token_embedders import PretrainedTransformerEmbedder , TokenEmbedder [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class PretrainedTransformerMismatchedEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_name , max_length = None , train_parameters = True , last_layer_only = True , gradient_checkpointing = None , ) : [EOL] super ( ) . __init__ ( ) [EOL] [comment] [EOL] self . _matched_embedder = PretrainedTransformerEmbedder ( model_name , max_length = max_length , train_parameters = train_parameters , last_layer_only = last_layer_only , gradient_checkpointing = gradient_checkpointing , ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _matched_embedder . get_output_dim ( ) [EOL] [EOL] @ overrides def forward ( self , token_ids , mask , offsets , wordpiece_mask , type_ids = None , segment_concat_mask = None , ) : [comment] [EOL] [docstring] [EOL] [comment] [EOL] embeddings = self . _matched_embedder ( token_ids , wordpiece_mask , type_ids = type_ids , segment_concat_mask = segment_concat_mask ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] span_embeddings , span_mask = util . batched_span_select ( embeddings . contiguous ( ) , offsets ) [EOL] span_mask = span_mask . unsqueeze ( - [number] ) [EOL] span_embeddings *= span_mask [comment] [EOL] [EOL] span_embeddings_sum = span_embeddings . sum ( [number] ) [EOL] span_embeddings_len = span_mask . sum ( [number] ) [EOL] [comment] [EOL] orig_embeddings = span_embeddings_sum / torch . clamp_min ( span_embeddings_len , [number] ) [EOL] [EOL] [comment] [EOL] orig_embeddings [ ( span_embeddings_len == [number] ) . expand ( orig_embeddings . shape ) ] = [number] [EOL] [EOL] return orig_embeddings [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Optional[builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $typing.Optional[builtins.bool]$ 0 $typing.Optional[builtins.bool]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.LongTensor$ 0 $torch.BoolTensor$ 0 $torch.LongTensor$ 0 $torch.BoolTensor$ 0 $typing.Optional[torch.LongTensor]$ 0 0 0 $typing.Optional[torch.BoolTensor]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.LongTensor$ 0 $torch.BoolTensor$ 0 $typing.Optional[torch.LongTensor]$ 0 $typing.Optional[torch.LongTensor]$ 0 $typing.Optional[torch.BoolTensor]$ 0 $typing.Optional[torch.BoolTensor]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.LongTensor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
from typing import Optional , Any , Dict , List , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import torch [EOL] import math [EOL] from typing import Optional , Tuple [EOL] [EOL] from overrides import overrides [EOL] [EOL] import torch [EOL] import torch . nn . functional as F [EOL] from transformers import XLNetConfig [EOL] [EOL] from allennlp . data . tokenizers import PretrainedTransformerTokenizer [EOL] from allennlp . modules . scalar_mix import ScalarMix [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] from allennlp . nn . util import batched_index_select [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class PretrainedTransformerEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] authorized_missing_keys = [ [string] ] [EOL] [EOL] def __init__ ( self , model_name , * , max_length = None , sub_module = None , train_parameters = True , last_layer_only = True , override_weights_file = None , override_weights_strip_prefix = None , gradient_checkpointing = None , ) : [EOL] super ( ) . __init__ ( ) [EOL] from allennlp . common import cached_transformers [EOL] [EOL] self . transformer_model = cached_transformers . get ( model_name , True , override_weights_file , override_weights_strip_prefix ) [EOL] [EOL] if gradient_checkpointing is not None : [EOL] self . transformer_model . config . update ( { [string] : gradient_checkpointing } ) [EOL] [EOL] self . config = self . transformer_model . config [EOL] if sub_module : [EOL] assert hasattr ( self . transformer_model , sub_module ) [EOL] self . transformer_model = getattr ( self . transformer_model , sub_module ) [EOL] self . _max_length = max_length [EOL] [EOL] [comment] [EOL] [comment] [EOL] self . output_dim = self . config . hidden_size [EOL] [EOL] self . _scalar_mix = None [EOL] if not last_layer_only : [EOL] self . _scalar_mix = ScalarMix ( self . config . num_hidden_layers ) [EOL] self . config . output_hidden_states = True [EOL] [EOL] tokenizer = PretrainedTransformerTokenizer ( model_name ) [EOL] self . _num_added_start_tokens = len ( tokenizer . single_sequence_start_tokens ) [EOL] self . _num_added_end_tokens = len ( tokenizer . single_sequence_end_tokens ) [EOL] self . _num_added_tokens = self . _num_added_start_tokens + self . _num_added_end_tokens [EOL] [EOL] if not train_parameters : [EOL] for param in self . transformer_model . parameters ( ) : [EOL] param . requires_grad = False [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] def _number_of_token_type_embeddings ( self ) : [EOL] if isinstance ( self . config , XLNetConfig ) : [EOL] return [number] [comment] [EOL] elif hasattr ( self . config , [string] ) : [EOL] return self . config . type_vocab_size [EOL] else : [EOL] return [number] [EOL] [EOL] @ overrides def forward ( self , token_ids , mask , type_ids = None , segment_concat_mask = None , ) : [comment] [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if type_ids is not None : [EOL] max_type_id = type_ids . max ( ) [EOL] if max_type_id == [number] : [EOL] type_ids = None [EOL] else : [EOL] if max_type_id >= self . _number_of_token_type_embeddings ( ) : [EOL] raise ValueError ( [string] ) [EOL] assert token_ids . shape == type_ids . shape [EOL] [EOL] fold_long_sequences = self . _max_length is not None and token_ids . size ( [number] ) > self . _max_length [EOL] if fold_long_sequences : [EOL] batch_size , num_segment_concat_wordpieces = token_ids . size ( ) [EOL] token_ids , segment_concat_mask , type_ids = self . _fold_long_sequences ( token_ids , segment_concat_mask , type_ids ) [EOL] [EOL] transformer_mask = segment_concat_mask if self . _max_length is not None else mask [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] parameters = { [string] : token_ids , [string] : transformer_mask . float ( ) } [EOL] if type_ids is not None : [EOL] parameters [ [string] ] = type_ids [EOL] [EOL] transformer_output = self . transformer_model ( ** parameters ) [EOL] if self . _scalar_mix is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] hidden_states = transformer_output [ - [number] ] [ [number] : ] [EOL] embeddings = self . _scalar_mix ( hidden_states ) [EOL] else : [EOL] embeddings = transformer_output [ [number] ] [EOL] [EOL] if fold_long_sequences : [EOL] embeddings = self . _unfold_long_sequences ( embeddings , segment_concat_mask , batch_size , num_segment_concat_wordpieces ) [EOL] [EOL] return embeddings [EOL] [EOL] def _fold_long_sequences ( self , token_ids , mask , type_ids = None , ) : [EOL] [docstring] [EOL] num_segment_concat_wordpieces = token_ids . size ( [number] ) [EOL] num_segments = math . ceil ( num_segment_concat_wordpieces / self . _max_length ) [EOL] padded_length = num_segments * self . _max_length [EOL] length_to_pad = padded_length - num_segment_concat_wordpieces [EOL] [EOL] def fold ( tensor ) : [comment] [EOL] [comment] [EOL] tensor = F . pad ( tensor , [ [number] , length_to_pad ] , value = [number] ) [EOL] [comment] [EOL] return tensor . reshape ( - [number] , self . _max_length ) [EOL] [EOL] return fold ( token_ids ) , fold ( mask ) , fold ( type_ids ) if type_ids is not None else None [EOL] [EOL] def _unfold_long_sequences ( self , embeddings , mask , batch_size , num_segment_concat_wordpieces , ) : [EOL] [docstring] [EOL] [EOL] def lengths_to_mask ( lengths , max_len , device ) : [EOL] return torch . arange ( max_len , device = device ) . expand ( lengths . size ( [number] ) , max_len ) < lengths . unsqueeze ( [number] ) [EOL] [EOL] device = embeddings . device [EOL] num_segments = int ( embeddings . size ( [number] ) / batch_size ) [EOL] embedding_size = embeddings . size ( [number] ) [EOL] [EOL] [comment] [EOL] num_wordpieces = num_segment_concat_wordpieces - ( num_segments - [number] ) * self . _num_added_tokens [EOL] [EOL] embeddings = embeddings . reshape ( batch_size , num_segments * self . _max_length , embedding_size ) [EOL] mask = mask . reshape ( batch_size , num_segments * self . _max_length ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] seq_lengths = mask . sum ( - [number] ) [EOL] if not ( lengths_to_mask ( seq_lengths , mask . size ( [number] ) , device ) == mask ) . all ( ) : [EOL] raise ValueError ( [string] ) [EOL] [comment] [EOL] end_token_indices = ( seq_lengths . unsqueeze ( - [number] ) - torch . arange ( self . _num_added_end_tokens , device = device ) - [number] ) [EOL] [EOL] [comment] [EOL] start_token_embeddings = embeddings [ : , : self . _num_added_start_tokens , : ] [EOL] [comment] [EOL] end_token_embeddings = batched_index_select ( embeddings , end_token_indices ) [EOL] [EOL] embeddings = embeddings . reshape ( batch_size , num_segments , self . _max_length , embedding_size ) [EOL] embeddings = embeddings [ : , : , self . _num_added_start_tokens : - self . _num_added_end_tokens , : ] [comment] [EOL] embeddings = embeddings . reshape ( batch_size , - [number] , embedding_size ) [comment] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] num_effective_segments = ( seq_lengths + self . _max_length - [number] ) // self . _max_length [EOL] [comment] [EOL] num_removed_non_end_tokens = ( num_effective_segments * self . _num_added_tokens - self . _num_added_end_tokens ) [EOL] [comment] [EOL] end_token_indices -= num_removed_non_end_tokens . unsqueeze ( - [number] ) [EOL] assert ( end_token_indices >= self . _num_added_start_tokens ) . all ( ) [EOL] [comment] [EOL] embeddings = torch . cat ( [ embeddings , torch . zeros_like ( end_token_embeddings ) ] , [number] ) [EOL] [comment] [EOL] embeddings . scatter_ ( [number] , end_token_indices . unsqueeze ( - [number] ) . expand_as ( end_token_embeddings ) , end_token_embeddings ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] embeddings = torch . cat ( [ start_token_embeddings , embeddings ] , [number] ) [EOL] [EOL] [comment] [EOL] embeddings = embeddings [ : , : num_wordpieces , : ] [EOL] return embeddings [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.FloatTensor$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0
import builtins [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] [EOL] [EOL] class TokenEmbedder ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def get_output_dim ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . modules . token_embedders . embedding import Embedding [EOL] from allennlp . modules . seq2vec_encoders . seq2vec_encoder import Seq2VecEncoder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class TokenCharactersEncoder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , embedding , encoder , dropout = [number] ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _embedding = TimeDistributed ( embedding ) [EOL] self . _encoder = TimeDistributed ( encoder ) [EOL] if dropout > [number] : [EOL] self . _dropout = torch . nn . Dropout ( p = dropout ) [EOL] else : [EOL] self . _dropout = lambda x : x [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . _encoder . _module . get_output_dim ( ) [EOL] [EOL] def forward ( self , token_characters ) : [EOL] mask = ( token_characters != [number] ) . long ( ) [EOL] return self . _dropout ( self . _encoder ( self . _embedding ( token_characters ) , mask ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 $typing.Any$ 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import List [EOL] import torch [EOL] [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] from allennlp . modules . elmo import Elmo [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class ElmoTokenEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , options_file = [string] + [string] , weight_file = [string] + [string] , do_layer_norm = False , dropout = [number] , requires_grad = False , projection_dim = None , vocab_to_cache = None , scalar_mix_parameters = None , ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] self . _elmo = Elmo ( options_file , weight_file , [number] , do_layer_norm = do_layer_norm , dropout = dropout , requires_grad = requires_grad , vocab_to_cache = vocab_to_cache , scalar_mix_parameters = scalar_mix_parameters , ) [EOL] if projection_dim : [EOL] self . _projection = torch . nn . Linear ( self . _elmo . get_output_dim ( ) , projection_dim ) [EOL] self . output_dim = projection_dim [EOL] else : [EOL] self . _projection = None [EOL] self . output_dim = self . _elmo . get_output_dim ( ) [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] def forward ( self , elmo_tokens , word_inputs = None ) : [EOL] [docstring] [EOL] elmo_output = self . _elmo ( elmo_tokens , word_inputs ) [EOL] elmo_representations = elmo_output [ [string] ] [ [number] ] [EOL] if self . _projection : [EOL] projection = self . _projection [EOL] for _ in range ( elmo_representations . dim ( ) - [number] ) : [EOL] projection = TimeDistributed ( projection ) [EOL] elmo_representations = projection ( elmo_representations ) [EOL] return elmo_representations [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0
import builtins [EOL] import torch [EOL] import torch [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class PassThroughTokenEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , hidden_dim ) : [EOL] self . hidden_dim = hidden_dim [EOL] super ( ) . __init__ ( ) [EOL] [EOL] def get_output_dim ( self ) : [EOL] return self . hidden_dim [EOL] [EOL] def forward ( self , tokens ) : [EOL] return tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 0 0 0 $torch.Tensor$ 0
import torch [EOL] import torch [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] [EOL] @ TokenEmbedder . register ( [string] ) class EmptyEmbedder ( TokenEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] def get_output_dim ( self ) : [EOL] return [number] [EOL] [EOL] def forward ( self , * inputs , ** kwargs ) : [EOL] return None [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import torch [EOL] import typing [EOL] import torch [EOL] from overrides import overrides [EOL] from allennlp . modules . attention . attention import Attention [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class CosineAttention ( Attention ) : [EOL] [docstring] [EOL] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] a_norm = vector / ( vector . norm ( p = [number] , dim = - [number] , keepdim = True ) + util . tiny_value_of_dtype ( vector . dtype ) ) [EOL] b_norm = matrix / ( matrix . norm ( p = [number] , dim = - [number] , keepdim = True ) + util . tiny_value_of_dtype ( matrix . dtype ) ) [EOL] return torch . bmm ( a_norm . unsqueeze ( dim = [number] ) , b_norm . transpose ( - [number] , - [number] ) ) . squeeze ( [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . parameter import Parameter [EOL] [EOL] from allennlp . modules . attention . attention import Attention [EOL] [EOL] [EOL] @ Attention . register ( [string] ) class AdditiveAttention ( Attention ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vector_dim , matrix_dim , normalize = True ) : [EOL] super ( ) . __init__ ( normalize ) [EOL] self . _w_matrix = Parameter ( torch . Tensor ( vector_dim , vector_dim ) ) [EOL] self . _u_matrix = Parameter ( torch . Tensor ( matrix_dim , vector_dim ) ) [EOL] self . _v_vector = Parameter ( torch . Tensor ( vector_dim , [number] ) ) [EOL] self . reset_parameters ( ) [EOL] [EOL] def reset_parameters ( self ) : [EOL] torch . nn . init . xavier_uniform_ ( self . _w_matrix ) [EOL] torch . nn . init . xavier_uniform_ ( self . _u_matrix ) [EOL] torch . nn . init . xavier_uniform_ ( self . _v_vector ) [EOL] [EOL] @ overrides def _forward_internal ( self , vector , matrix ) : [EOL] intermediate = vector . matmul ( self . _w_matrix ) . unsqueeze ( [number] ) + matrix . matmul ( self . _u_matrix ) [EOL] intermediate = torch . tanh ( intermediate ) [EOL] return intermediate . matmul ( self . _v_vector ) . squeeze ( [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import allennlp [EOL] import torch [EOL] import typing [EOL] from overrides import overrides [EOL] import torch [EOL] from typing import List [EOL] [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class ComposeEncoder ( Seq2SeqEncoder ) : [EOL] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , encoders ) : [EOL] super ( ) . __init__ ( ) [EOL] self . encoders = encoders [EOL] for idx , encoder in enumerate ( encoders ) : [EOL] self . add_module ( [string] % idx , encoder ) [EOL] [EOL] [comment] [EOL] all_bidirectional = all ( encoder . is_bidirectional ( ) for encoder in encoders ) [EOL] any_bidirectional = any ( encoder . is_bidirectional ( ) for encoder in encoders ) [EOL] self . bidirectional = all_bidirectional [EOL] [EOL] if all_bidirectional != any_bidirectional : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if len ( self . encoders ) < [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] last_enc = None [EOL] for enc in encoders : [EOL] if last_enc is not None and last_enc . get_output_dim ( ) != enc . get_input_dim ( ) : [EOL] raise ValueError ( [string] ) [EOL] last_enc = enc [EOL] [EOL] @ overrides def forward ( self , inputs , mask = None ) : [EOL] [docstring] [EOL] for encoder in self . encoders : [EOL] inputs = encoder ( inputs , mask ) [EOL] return inputs [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . encoders [ [number] ] . get_input_dim ( ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . encoders [ - [number] ] . get_output_dim ( ) [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return self . bidirectional [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $torch.BoolTensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . modules . feedforward import FeedForward [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class FeedForwardEncoder ( Seq2SeqEncoder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , feedforward ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _feedforward = feedforward [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _feedforward . get_input_dim ( ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _feedforward . get_output_dim ( ) [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return False [EOL] [EOL] @ overrides def forward ( self , inputs , mask = None ) : [EOL] [docstring] [EOL] if mask is None : [EOL] return self . _feedforward ( inputs ) [EOL] else : [EOL] outputs = self . _feedforward ( inputs ) [EOL] return outputs * mask . unsqueeze ( dim = - [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.modules.feedforward.FeedForward$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.feedforward.FeedForward$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import Optional [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] from torch import nn [EOL] [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] from allennlp . nn . util import add_positional_features [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class PytorchTransformer ( Seq2SeqEncoder ) : [EOL] [docstring] [comment] [EOL] [EOL] def __init__ ( self , input_dim , num_layers , feedforward_hidden_dim = [number] , num_attention_heads = [number] , positional_encoding = None , positional_embedding_size = [number] , dropout_prob = [number] , activation = [string] , ) : [EOL] super ( ) . __init__ ( ) [EOL] [EOL] layer = nn . TransformerEncoderLayer ( d_model = input_dim , nhead = num_attention_heads , dim_feedforward = feedforward_hidden_dim , dropout = dropout_prob , activation = activation , ) [EOL] self . _transformer = nn . TransformerEncoder ( layer , num_layers ) [EOL] self . _input_dim = input_dim [EOL] [EOL] [comment] [EOL] [comment] [EOL] for p in self . parameters ( ) : [EOL] if p . dim ( ) > [number] : [EOL] nn . init . xavier_uniform_ ( p ) [EOL] [EOL] if positional_encoding is None : [EOL] self . _sinusoidal_positional_encoding = False [EOL] self . _positional_embedding = None [EOL] elif positional_encoding == [string] : [EOL] self . _sinusoidal_positional_encoding = True [EOL] self . _positional_embedding = None [EOL] elif positional_encoding == [string] : [EOL] self . _sinusoidal_positional_encoding = False [EOL] self . _positional_embedding = nn . Embedding ( positional_embedding_size , input_dim ) [EOL] else : [EOL] raise ValueError ( [string] ) [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _input_dim [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _input_dim [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return False [EOL] [EOL] @ overrides def forward ( self , inputs , mask ) : [EOL] output = inputs [EOL] if self . _sinusoidal_positional_encoding : [EOL] output = add_positional_features ( output ) [EOL] if self . _positional_embedding is not None : [EOL] position_ids = torch . arange ( inputs . size ( [number] ) , dtype = torch . long , device = output . device ) [EOL] position_ids = position_ids . unsqueeze ( [number] ) . expand ( inputs . shape [ : - [number] ] ) [EOL] output = output + self . _positional_embedding ( position_ids ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] output = output . permute ( [number] , [number] , [number] ) [EOL] [comment] [EOL] mask = ~ mask [EOL] output = self . _transformer ( output , src_key_padding_mask = mask ) [EOL] output = output . permute ( [number] , [number] , [number] ) [EOL] [EOL] return output [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
[docstring] [EOL] [EOL] from allennlp . modules . seq2seq_encoders . compose_encoder import ComposeEncoder [EOL] from allennlp . modules . seq2seq_encoders . feedforward_encoder import FeedForwardEncoder [EOL] from allennlp . modules . seq2seq_encoders . gated_cnn_encoder import GatedCnnEncoder [EOL] from allennlp . modules . seq2seq_encoders . pass_through_encoder import PassThroughEncoder [EOL] from allennlp . modules . seq2seq_encoders . pytorch_seq2seq_wrapper import ( AugmentedLstmSeq2SeqEncoder , GruSeq2SeqEncoder , LstmSeq2SeqEncoder , PytorchSeq2SeqWrapper , RnnSeq2SeqEncoder , StackedAlternatingLstmSeq2SeqEncoder , StackedBidirectionalLstmSeq2SeqEncoder , ) [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] from allennlp . modules . seq2seq_encoders . pytorch_transformer_wrapper import PytorchTransformer [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from overrides import overrides [EOL] import torch [EOL] from torch . nn . utils . rnn import pad_packed_sequence [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules . augmented_lstm import AugmentedLstm [EOL] from allennlp . modules . seq2seq_encoders . seq2seq_encoder import Seq2SeqEncoder [EOL] from allennlp . modules . stacked_alternating_lstm import StackedAlternatingLstm [EOL] from allennlp . modules . stacked_bidirectional_lstm import StackedBidirectionalLstm [EOL] [EOL] [EOL] class PytorchSeq2SeqWrapper ( Seq2SeqEncoder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , module , stateful = False ) : [EOL] super ( ) . __init__ ( stateful ) [EOL] self . _module = module [EOL] try : [EOL] if not self . _module . batch_first : [EOL] raise ConfigurationError ( [string] ) [EOL] except AttributeError : [EOL] pass [EOL] [EOL] try : [EOL] self . _is_bidirectional = self . _module . bidirectional [EOL] except AttributeError : [EOL] self . _is_bidirectional = False [EOL] if self . _is_bidirectional : [EOL] self . _num_directions = [number] [EOL] else : [EOL] self . _num_directions = [number] [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . _module . input_size [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . _module . hidden_size * self . _num_directions [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return self . _is_bidirectional [EOL] [EOL] @ overrides def forward ( self , inputs , mask , hidden_state = None ) : [EOL] [EOL] if self . stateful and mask is None : [EOL] raise ValueError ( [string] ) [EOL] if self . stateful and hidden_state is not None : [EOL] raise ValueError ( [string] ) [EOL] [EOL] if mask is None : [EOL] return self . _module ( inputs , hidden_state ) [ [number] ] [EOL] [EOL] batch_size , total_sequence_length = mask . size ( ) [EOL] [EOL] packed_sequence_output , final_states , restoration_indices = self . sort_and_run_forward ( self . _module , inputs , mask , hidden_state ) [EOL] [EOL] unpacked_sequence_tensor , _ = pad_packed_sequence ( packed_sequence_output , batch_first = True ) [EOL] [EOL] num_valid = unpacked_sequence_tensor . size ( [number] ) [EOL] [comment] [EOL] [comment] [EOL] if not isinstance ( final_states , ( list , tuple ) ) and self . stateful : [EOL] final_states = [ final_states ] [EOL] [EOL] [comment] [EOL] if num_valid < batch_size : [EOL] _ , length , output_dim = unpacked_sequence_tensor . size ( ) [EOL] zeros = unpacked_sequence_tensor . new_zeros ( batch_size - num_valid , length , output_dim ) [EOL] unpacked_sequence_tensor = torch . cat ( [ unpacked_sequence_tensor , zeros ] , [number] ) [EOL] [EOL] [comment] [EOL] if self . stateful : [EOL] new_states = [ ] [EOL] for state in final_states : [EOL] num_layers , _ , state_dim = state . size ( ) [EOL] zeros = state . new_zeros ( num_layers , batch_size - num_valid , state_dim ) [EOL] new_states . append ( torch . cat ( [ state , zeros ] , [number] ) ) [EOL] final_states = new_states [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] sequence_length_difference = total_sequence_length - unpacked_sequence_tensor . size ( [number] ) [EOL] if sequence_length_difference > [number] : [EOL] zeros = unpacked_sequence_tensor . new_zeros ( batch_size , sequence_length_difference , unpacked_sequence_tensor . size ( - [number] ) ) [EOL] unpacked_sequence_tensor = torch . cat ( [ unpacked_sequence_tensor , zeros ] , [number] ) [EOL] [EOL] if self . stateful : [EOL] self . _update_states ( final_states , restoration_indices ) [EOL] [EOL] [comment] [EOL] return unpacked_sequence_tensor . index_select ( [number] , restoration_indices ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class GruSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , bias = True , dropout = [number] , bidirectional = False , stateful = False , ) : [EOL] module = torch . nn . GRU ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class LstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , bias = True , dropout = [number] , bidirectional = False , stateful = False , ) : [EOL] module = torch . nn . LSTM ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class RnnSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers = [number] , nonlinearity = [string] , bias = True , dropout = [number] , bidirectional = False , stateful = False , ) : [EOL] module = torch . nn . RNN ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , nonlinearity = nonlinearity , bias = bias , batch_first = True , dropout = dropout , bidirectional = bidirectional , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class AugmentedLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , go_forward = True , recurrent_dropout_probability = [number] , use_highway = True , use_input_projection_bias = True , stateful = False , ) : [EOL] module = AugmentedLstm ( input_size = input_size , hidden_size = hidden_size , go_forward = go_forward , recurrent_dropout_probability = recurrent_dropout_probability , use_highway = use_highway , use_input_projection_bias = use_input_projection_bias , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class StackedAlternatingLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers , recurrent_dropout_probability = [number] , use_highway = True , use_input_projection_bias = True , stateful = False , ) : [EOL] module = StackedAlternatingLstm ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , recurrent_dropout_probability = recurrent_dropout_probability , use_highway = use_highway , use_input_projection_bias = use_input_projection_bias , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL] [EOL] [EOL] @ Seq2SeqEncoder . register ( [string] ) class StackedBidirectionalLstmSeq2SeqEncoder ( PytorchSeq2SeqWrapper ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , input_size , hidden_size , num_layers , recurrent_dropout_probability = [number] , layer_dropout_probability = [number] , use_highway = True , stateful = False , ) : [EOL] module = StackedBidirectionalLstm ( input_size = input_size , hidden_size = hidden_size , num_layers = num_layers , recurrent_dropout_probability = recurrent_dropout_probability , layer_dropout_probability = layer_dropout_probability , use_highway = use_highway , ) [EOL] super ( ) . __init__ ( module = module , stateful = stateful ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.Tensor$ 0 0 0 0 0 0 0 0 0 0 $torch.BoolTensor$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 $torch.BoolTensor$ 0 $torch.Tensor$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.float$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0
from typing import Set , Any , Dict , List , Mapping [EOL] import allennlp [EOL] import typing [EOL] import inspect [EOL] import torch [EOL] import builtins [EOL] from typing import Dict [EOL] import inspect [EOL] [EOL] import torch [EOL] from overrides import overrides [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import TextFieldTensors [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] [EOL] [EOL] @ TextFieldEmbedder . register ( [string] ) class BasicTextFieldEmbedder ( TextFieldEmbedder ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , token_embedders ) : [EOL] super ( ) . __init__ ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . _token_embedders = token_embedders [EOL] for key , embedder in token_embedders . items ( ) : [EOL] name = [string] % key [EOL] self . add_module ( name , embedder ) [EOL] self . _ordered_embedder_keys = sorted ( self . _token_embedders . keys ( ) ) [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] output_dim = [number] [EOL] for embedder in self . _token_embedders . values ( ) : [EOL] output_dim += embedder . get_output_dim ( ) [EOL] return output_dim [EOL] [EOL] def forward ( self , text_field_input , num_wrapping_dims = [number] , ** kwargs ) : [EOL] if self . _token_embedders . keys ( ) != text_field_input . keys ( ) : [EOL] message = [string] % ( str ( self . _token_embedders . keys ( ) ) , str ( text_field_input . keys ( ) ) , ) [EOL] raise ConfigurationError ( message ) [EOL] [EOL] embedded_representations = [ ] [EOL] for key in self . _ordered_embedder_keys : [EOL] [comment] [EOL] [comment] [EOL] embedder = getattr ( self , [string] . format ( key ) ) [EOL] forward_params = inspect . signature ( embedder . forward ) . parameters [EOL] forward_params_values = { } [EOL] missing_tensor_args = set ( ) [EOL] for param in forward_params . keys ( ) : [EOL] if param in kwargs : [EOL] forward_params_values [ param ] = kwargs [ param ] [EOL] else : [EOL] missing_tensor_args . add ( param ) [EOL] [EOL] for _ in range ( num_wrapping_dims ) : [EOL] embedder = TimeDistributed ( embedder ) [EOL] [EOL] tensors = text_field_input [ key ] [EOL] if len ( tensors ) == [number] and len ( missing_tensor_args ) == [number] : [EOL] [comment] [EOL] [comment] [EOL] token_vectors = embedder ( list ( tensors . values ( ) ) [ [number] ] , ** forward_params_values ) [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] token_vectors = embedder ( ** tensors , ** forward_params_values ) [EOL] if token_vectors is not None : [EOL] [comment] [EOL] [comment] [EOL] embedded_representations . append ( token_vectors ) [EOL] return torch . cat ( embedded_representations , dim = - [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.modules.token_embedders.token_embedder.TokenEmbedder]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $torch.Tensor$ 0 0 0 $allennlp.data.TextFieldTensors$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.TextFieldTensors$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.TextFieldTensors$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Mapping[builtins.str,inspect.Parameter]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Mapping[builtins.str,inspect.Parameter]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 $allennlp.data.TextFieldTensors$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0
import builtins [EOL] import allennlp [EOL] import torch [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . data import TextFieldTensors [EOL] [EOL] [EOL] class TextFieldEmbedder ( torch . nn . Module , Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def forward ( self , text_field_input , num_wrapping_dims = [number] , ** kwargs ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_output_dim ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $torch.Tensor$ 0 0 0 $allennlp.data.TextFieldTensors$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . text_field_embedders . basic_text_field_embedder import BasicTextFieldEmbedder [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] from allennlp . predictors . predictor import Predictor [EOL] from allennlp . predictors . sentence_tagger import SentenceTaggerPredictor [EOL] from allennlp . predictors . text_classifier import TextClassifierPredictor [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import numpy [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List , Dict [EOL] [EOL] from overrides import overrides [EOL] import numpy [EOL] [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . data . fields import FlagField , TextField , SequenceLabelField [EOL] from allennlp . data . tokenizers . spacy_tokenizer import SpacyTokenizer [EOL] from allennlp . models import Model [EOL] from allennlp . predictors . predictor import Predictor [EOL] [EOL] [EOL] @ Predictor . register ( [string] ) class SentenceTaggerPredictor ( Predictor ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model , dataset_reader , language = [string] ) : [EOL] super ( ) . __init__ ( model , dataset_reader ) [EOL] self . _tokenizer = SpacyTokenizer ( language = language , pos_tags = True ) [EOL] [EOL] def predict ( self , sentence ) : [EOL] return self . predict_json ( { [string] : sentence } ) [EOL] [EOL] @ overrides def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] sentence = json_dict [ [string] ] [EOL] tokens = self . _tokenizer . tokenize ( sentence ) [EOL] return self . _dataset_reader . text_to_instance ( tokens ) [EOL] [EOL] @ overrides def predictions_to_labeled_instances ( self , instance , outputs ) : [EOL] [docstring] [EOL] predicted_tags = outputs [ [string] ] [EOL] predicted_spans = [ ] [EOL] [EOL] i = [number] [EOL] while i < len ( predicted_tags ) : [EOL] tag = predicted_tags [ i ] [EOL] [comment] [EOL] if tag [ [number] ] == [string] : [EOL] current_tags = [ t if idx == i else [string] for idx , t in enumerate ( predicted_tags ) ] [EOL] predicted_spans . append ( current_tags ) [EOL] [comment] [EOL] elif tag [ [number] ] == [string] : [EOL] begin_idx = i [EOL] while tag [ [number] ] != [string] : [EOL] i += [number] [EOL] tag = predicted_tags [ i ] [EOL] end_idx = i [EOL] current_tags = [ t if begin_idx <= idx <= end_idx else [string] for idx , t in enumerate ( predicted_tags ) ] [EOL] predicted_spans . append ( current_tags ) [EOL] i += [number] [EOL] [EOL] [comment] [EOL] instances = [ ] [EOL] for labels in predicted_spans : [EOL] new_instance = instance . duplicate ( ) [EOL] text_field = instance [ [string] ] [comment] [EOL] new_instance . add_field ( [string] , SequenceLabelField ( labels , text_field ) , self . _model . vocab ) [EOL] new_instance . add_field ( [string] , FlagField ( True ) ) [EOL] instances . append ( new_instance ) [EOL] [EOL] return instances [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.models.Model$ 0 $allennlp.data.DatasetReader$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.Model$ 0 $allennlp.data.DatasetReader$ 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.common.util.JsonDict$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $allennlp.data.Instance$ 0 $typing.Dict[builtins.str,numpy.ndarray]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,numpy.ndarray]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 $allennlp.data.Instance$ 0 0 0 0 0 $allennlp.data.fields.TextField$ 0 $allennlp.data.Instance$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.data.fields.TextField$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0
from typing import Any , Dict , Pattern , List , Iterator , Type , Tuple [EOL] import allennlp [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] import builtins [EOL] from typing import List , Iterator , Dict , Tuple , Any , Type [EOL] import json [EOL] import re [EOL] from contextlib import contextmanager [EOL] [EOL] import numpy [EOL] from torch . utils . hooks import RemovableHandle [EOL] from torch import Tensor [EOL] from torch import backends [EOL] [EOL] from allennlp . common import Registrable , plugins [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . data import DatasetReader , Instance [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . models import Model [EOL] from allennlp . models . archival import Archive , load_archive [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] class Predictor ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model , dataset_reader , frozen = True ) : [EOL] if frozen : [EOL] model . eval ( ) [EOL] self . _model = model [EOL] self . _dataset_reader = dataset_reader [EOL] self . cuda_device = next ( self . _model . named_parameters ( ) ) [ [number] ] . get_device ( ) [EOL] [EOL] def load_line ( self , line ) : [EOL] [docstring] [EOL] return json . loads ( line ) [EOL] [EOL] def dump_line ( self , outputs ) : [EOL] [docstring] [EOL] return json . dumps ( outputs ) + [string] [EOL] [EOL] def predict_json ( self , inputs ) : [EOL] instance = self . _json_to_instance ( inputs ) [EOL] return self . predict_instance ( instance ) [EOL] [EOL] def json_to_labeled_instances ( self , inputs ) : [EOL] [docstring] [EOL] [EOL] instance = self . _json_to_instance ( inputs ) [EOL] outputs = self . _model . forward_on_instance ( instance ) [EOL] new_instances = self . predictions_to_labeled_instances ( instance , outputs ) [EOL] return new_instances [EOL] [EOL] def get_gradients ( self , instances ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] original_param_name_to_requires_grad_dict = { } [EOL] for param_name , param in self . _model . named_parameters ( ) : [EOL] original_param_name_to_requires_grad_dict [ param_name ] = param . requires_grad [EOL] param . requires_grad = True [EOL] [EOL] embedding_gradients = [ ] [EOL] hooks = self . _register_embedding_gradient_hooks ( embedding_gradients ) [EOL] [EOL] dataset = Batch ( instances ) [EOL] dataset . index_instances ( self . _model . vocab ) [EOL] dataset_tensor_dict = util . move_to_device ( dataset . as_tensor_dict ( ) , self . cuda_device ) [EOL] [comment] [EOL] with backends . cudnn . flags ( enabled = False ) : [EOL] outputs = self . _model . make_output_human_readable ( self . _model . forward ( ** dataset_tensor_dict ) ) [EOL] [EOL] loss = outputs [ [string] ] [EOL] self . _model . zero_grad ( ) [EOL] loss . backward ( ) [EOL] [EOL] for hook in hooks : [EOL] hook . remove ( ) [EOL] [EOL] grad_dict = dict ( ) [EOL] for idx , grad in enumerate ( embedding_gradients ) : [EOL] key = [string] + str ( idx + [number] ) [EOL] grad_dict [ key ] = grad . detach ( ) . cpu ( ) . numpy ( ) [EOL] [EOL] [comment] [EOL] for param_name , param in self . _model . named_parameters ( ) : [EOL] param . requires_grad = original_param_name_to_requires_grad_dict [ param_name ] [EOL] [EOL] return grad_dict , outputs [EOL] [EOL] def _register_embedding_gradient_hooks ( self , embedding_gradients ) : [EOL] [docstring] [EOL] [EOL] def hook_layers ( module , grad_in , grad_out ) : [EOL] embedding_gradients . append ( grad_out [ [number] ] ) [EOL] [EOL] backward_hooks = [ ] [EOL] embedding_layer = util . find_embedding_layer ( self . _model ) [EOL] backward_hooks . append ( embedding_layer . register_backward_hook ( hook_layers ) ) [EOL] return backward_hooks [EOL] [EOL] @ contextmanager def capture_model_internals ( self , module_regex = [string] ) : [EOL] [docstring] [EOL] results = { } [EOL] hooks = [ ] [EOL] [EOL] [comment] [EOL] def add_output ( idx ) : [EOL] def _add_output ( mod , _ , outputs ) : [EOL] results [ idx ] = { [string] : str ( mod ) , [string] : sanitize ( outputs ) } [EOL] [EOL] return _add_output [EOL] [EOL] regex = re . compile ( module_regex ) [EOL] for idx , ( name , module ) in enumerate ( self . _model . named_modules ( ) ) : [EOL] if regex . fullmatch ( name ) and module != self . _model : [EOL] hook = module . register_forward_hook ( add_output ( idx ) ) [EOL] hooks . append ( hook ) [EOL] [EOL] [comment] [EOL] yield results [EOL] [EOL] [comment] [EOL] for hook in hooks : [EOL] hook . remove ( ) [EOL] [EOL] def predict_instance ( self , instance ) : [EOL] outputs = self . _model . forward_on_instance ( instance ) [EOL] return sanitize ( outputs ) [EOL] [EOL] def predictions_to_labeled_instances ( self , instance , outputs ) : [EOL] [docstring] [EOL] [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] def _json_to_instance ( self , json_dict ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def predict_batch_json ( self , inputs ) : [EOL] instances = self . _batch_json_to_instances ( inputs ) [EOL] return self . predict_batch_instance ( instances ) [EOL] [EOL] def predict_batch_instance ( self , instances ) : [EOL] outputs = self . _model . forward_on_instances ( instances ) [EOL] return sanitize ( outputs ) [EOL] [EOL] def _batch_json_to_instances ( self , json_dicts ) : [EOL] [docstring] [EOL] instances = [ ] [EOL] for json_dict in json_dicts : [EOL] instances . append ( self . _json_to_instance ( json_dict ) ) [EOL] return instances [EOL] [EOL] @ classmethod def from_path ( cls , archive_path , predictor_name = None , cuda_device = - [number] , dataset_reader_to_load = [string] , frozen = True , import_plugins = True , ) : [EOL] [docstring] [EOL] if import_plugins : [EOL] plugins . import_plugins ( ) [EOL] return Predictor . from_archive ( load_archive ( archive_path , cuda_device = cuda_device ) , predictor_name , dataset_reader_to_load = dataset_reader_to_load , frozen = frozen , ) [EOL] [EOL] @ classmethod def from_archive ( cls , archive , predictor_name = None , dataset_reader_to_load = [string] , frozen = True , ) : [EOL] [docstring] [EOL] [comment] [EOL] config = archive . config . duplicate ( ) [EOL] [EOL] if not predictor_name : [EOL] model_type = config . get ( [string] ) . get ( [string] ) [EOL] model_class , _ = Model . resolve_class_name ( model_type ) [EOL] predictor_name = model_class . default_predictor [EOL] predictor_class = ( Predictor . by_name ( predictor_name ) if predictor_name is not None else cls ) [EOL] [EOL] if dataset_reader_to_load == [string] and [string] in config : [EOL] dataset_reader_params = config [ [string] ] [EOL] else : [EOL] dataset_reader_params = config [ [string] ] [EOL] dataset_reader = DatasetReader . from_params ( dataset_reader_params ) [EOL] [EOL] model = archive . model [EOL] if frozen : [EOL] model . eval ( ) [EOL] [EOL] return predictor_class ( model , dataset_reader ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.models.Model$ 0 $allennlp.data.DatasetReader$ 0 $builtins.bool$ 0 0 0 0 0 0 $builtins.bool$ 0 0 $allennlp.models.Model$ 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 $allennlp.models.Model$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 $allennlp.data.DatasetReader$ 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.Dict[builtins.str,typing.Any],typing.Dict[builtins.str,typing.Any]]$ 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 0 $typing.List[torch.utils.hooks.RemovableHandle]$ 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[torch.utils.hooks.RemovableHandle]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[torch.Tensor]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Iterator[builtins.dict]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.data.Instance$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $allennlp.data.Instance$ 0 $typing.Dict[builtins.str,numpy.ndarray]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.Instance$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.Instance]$ 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[allennlp.common.util.JsonDict]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $"Predictor"$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 $builtins.int$ 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $"Predictor"$ 0 0 0 $allennlp.models.archival.Archive$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.models.archival.Archive$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 $typing.Type[Predictor]$ 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.models.archival.Archive$ 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Type[Predictor]$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0
from allennlp . data . dataloader import DataLoader , PyTorchDataLoader , allennlp_collate [EOL] from allennlp . data . dataset_readers . dataset_reader import ( DatasetReader , AllennlpDataset , AllennlpLazyDataset , ) [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] from allennlp . data . fields . text_field import TextFieldTensors [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . samplers import BatchSampler , Sampler [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer , IndexedTokenList [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . batch import Batch [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . token_indexers . single_id_token_indexer import SingleIdTokenIndexer [EOL] from allennlp . data . token_indexers . token_characters_indexer import TokenCharactersIndexer [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] from allennlp . data . token_indexers . elmo_indexer import ELMoTokenCharactersIndexer [EOL] from allennlp . data . token_indexers . spacy_indexer import SpacyTokenIndexer [EOL] from allennlp . data . token_indexers . pretrained_transformer_indexer import PretrainedTransformerIndexer [EOL] from allennlp . data . token_indexers . pretrained_transformer_mismatched_indexer import ( PretrainedTransformerMismatchedIndexer , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Dict , List [EOL] import logging [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . token_indexers import PretrainedTransformerIndexer , TokenIndexer [EOL] from allennlp . data . token_indexers . token_indexer import IndexedTokenList [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ TokenIndexer . register ( [string] ) class PretrainedTransformerMismatchedIndexer ( TokenIndexer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_name , namespace = [string] , max_length = None , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] [comment] [EOL] self . _matched_indexer = PretrainedTransformerIndexer ( model_name , namespace , max_length , ** kwargs ) [EOL] self . _allennlp_tokenizer = self . _matched_indexer . _allennlp_tokenizer [EOL] self . _tokenizer = self . _matched_indexer . _tokenizer [EOL] self . _num_added_start_tokens = self . _matched_indexer . _num_added_start_tokens [EOL] self . _num_added_end_tokens = self . _matched_indexer . _num_added_end_tokens [EOL] [EOL] @ overrides def count_vocab_items ( self , token , counter ) : [EOL] return self . _matched_indexer . count_vocab_items ( token , counter ) [EOL] [EOL] @ overrides def tokens_to_indices ( self , tokens , vocabulary ) : [EOL] self . _matched_indexer . _add_encoding_to_vocabulary_if_needed ( vocabulary ) [EOL] [EOL] wordpieces , offsets = self . _allennlp_tokenizer . intra_word_tokenize ( [ t . text for t in tokens ] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] offsets = [ x if x is not None else ( - [number] , - [number] ) for x in offsets ] [EOL] [EOL] output = { [string] : [ t . text_id for t in wordpieces ] , [string] : [ True ] * len ( tokens ) , [string] : [ t . type_id for t in wordpieces ] , [string] : offsets , [string] : [ True ] * len ( wordpieces ) , } [EOL] [EOL] return self . _matched_indexer . _postprocess_output ( output ) [EOL] [EOL] @ overrides def get_empty_token_list ( self ) : [EOL] output = self . _matched_indexer . get_empty_token_list ( ) [EOL] output [ [string] ] = [ ] [EOL] output [ [string] ] = [ ] [EOL] return output [EOL] [EOL] @ overrides def as_padded_tensor_dict ( self , tokens , padding_lengths ) : [EOL] tokens = tokens . copy ( ) [EOL] padding_lengths = padding_lengths . copy ( ) [EOL] [EOL] offsets_tokens = tokens . pop ( [string] ) [EOL] offsets_padding_lengths = padding_lengths . pop ( [string] ) [EOL] [EOL] tensor_dict = self . _matched_indexer . as_padded_tensor_dict ( tokens , padding_lengths ) [EOL] tensor_dict [ [string] ] = torch . LongTensor ( pad_sequence_to_length ( offsets_tokens , offsets_padding_lengths , default_value = lambda : ( [number] , [number] ) ) ) [EOL] return tensor_dict [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( other , PretrainedTransformerMismatchedIndexer ) : [EOL] for key in self . __dict__ : [EOL] if key == [string] : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] if self . __dict__ [ key ] != other . __dict__ [ key ] : [EOL] return False [EOL] return True [EOL] return NotImplemented [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 $allennlp.data.token_indexers.token_indexer.IndexedTokenList$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $allennlp.data.token_indexers.token_indexer.IndexedTokenList$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.token_indexer.IndexedTokenList$ 0 0 0 0 0 0 $allennlp.data.token_indexers.token_indexer.IndexedTokenList$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 0 0 $builtins.int$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Type , Any , List [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List [EOL] [EOL] import torch [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] IndexedTokenList = Dict [ str , List [ Any ] ] [EOL] [EOL] [EOL] class TokenIndexer ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] has_warned_for_as_padded_tensor = False [EOL] [EOL] def __init__ ( self , token_min_padding_length = [number] ) : [EOL] self . _token_min_padding_length = token_min_padding_length [EOL] [EOL] def count_vocab_items ( self , token , counter ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def tokens_to_indices ( self , tokens , vocabulary ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def indices_to_tokens ( self , indexed_tokens , vocabulary ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_empty_token_list ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def get_padding_lengths ( self , indexed_tokens ) : [EOL] [docstring] [EOL] padding_lengths = { } [EOL] for key , token_list in indexed_tokens . items ( ) : [EOL] padding_lengths [ key ] = max ( len ( token_list ) , self . _token_min_padding_length ) [EOL] return padding_lengths [EOL] [EOL] def as_padded_tensor_dict ( self , tokens , padding_lengths ) : [EOL] [docstring] [EOL] tensor_dict = { } [EOL] for key , val in tokens . items ( ) : [EOL] if val and isinstance ( val [ [number] ] , bool ) : [EOL] tensor = torch . BoolTensor ( pad_sequence_to_length ( val , padding_lengths [ key ] , default_value = lambda : False ) ) [EOL] else : [EOL] tensor = torch . LongTensor ( pad_sequence_to_length ( val , padding_lengths [ key ] ) ) [EOL] tensor_dict [ key ] = tensor [EOL] return tensor_dict [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( self , other . __class__ ) : [EOL] return self . __dict__ == other . __dict__ [EOL] return NotImplemented [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $IndexedTokenList$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $IndexedTokenList$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 $IndexedTokenList$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $IndexedTokenList$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $IndexedTokenList$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 $IndexedTokenList$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $IndexedTokenList$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Optional , List [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from typing import Dict , List , Optional [EOL] import itertools [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer , IndexedTokenList [EOL] [EOL] [EOL] _DEFAULT_VALUE = [string] [EOL] [EOL] [EOL] @ TokenIndexer . register ( [string] ) class SingleIdTokenIndexer ( TokenIndexer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , namespace = [string] , lowercase_tokens = False , start_tokens = None , end_tokens = None , feature_name = [string] , default_value = _DEFAULT_VALUE , token_min_padding_length = [number] , ) : [EOL] super ( ) . __init__ ( token_min_padding_length ) [EOL] self . namespace = namespace [EOL] self . lowercase_tokens = lowercase_tokens [EOL] [EOL] self . _start_tokens = [ Token ( st ) for st in ( start_tokens or [ ] ) ] [EOL] self . _end_tokens = [ Token ( et ) for et in ( end_tokens or [ ] ) ] [EOL] self . _feature_name = feature_name [EOL] self . _default_value = default_value [EOL] [EOL] @ overrides def count_vocab_items ( self , token , counter ) : [EOL] if self . namespace is not None : [EOL] text = self . _get_feature_value ( token ) [EOL] if self . lowercase_tokens : [EOL] text = text . lower ( ) [EOL] counter [ self . namespace ] [ text ] += [number] [EOL] [EOL] @ overrides def tokens_to_indices ( self , tokens , vocabulary ) : [EOL] indices = [ ] [EOL] [EOL] for token in itertools . chain ( self . _start_tokens , tokens , self . _end_tokens ) : [EOL] text = self . _get_feature_value ( token ) [EOL] if self . namespace is None : [EOL] [comment] [EOL] indices . append ( text ) [comment] [EOL] else : [EOL] if self . lowercase_tokens : [EOL] text = text . lower ( ) [EOL] indices . append ( vocabulary . get_token_index ( text , self . namespace ) ) [EOL] [EOL] return { [string] : indices } [EOL] [EOL] @ overrides def get_empty_token_list ( self ) : [EOL] return { [string] : [ ] } [EOL] [EOL] def _get_feature_value ( self , token ) : [EOL] text = getattr ( token , self . _feature_name ) [EOL] if text is None : [EOL] if self . _default_value is not _DEFAULT_VALUE : [EOL] text = self . _default_value [EOL] else : [EOL] raise ValueError ( f"{ token } [string] { self . _feature_name } [string] " [string] [string] ) [EOL] return text [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.int]]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $allennlp.data.token_indexers.token_indexer.IndexedTokenList$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.token.Token$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0
from typing import Optional , Any , Dict , List , Generator , Tuple [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Dict , List , Optional , Tuple [EOL] import logging [EOL] import torch [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . data . tokenizers import PretrainedTransformerTokenizer [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer , IndexedTokenList [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ TokenIndexer . register ( [string] ) class PretrainedTransformerIndexer ( TokenIndexer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , model_name , namespace = [string] , max_length = None , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . _namespace = namespace [EOL] self . _allennlp_tokenizer = PretrainedTransformerTokenizer ( model_name ) [EOL] self . _tokenizer = self . _allennlp_tokenizer . tokenizer [EOL] self . _added_to_vocabulary = False [EOL] [EOL] self . _num_added_start_tokens = len ( self . _allennlp_tokenizer . single_sequence_start_tokens ) [EOL] self . _num_added_end_tokens = len ( self . _allennlp_tokenizer . single_sequence_end_tokens ) [EOL] [EOL] self . _max_length = max_length [EOL] if self . _max_length is not None : [EOL] num_added_tokens = len ( self . _allennlp_tokenizer . tokenize ( [string] ) ) - [number] [EOL] self . _effective_max_length = ( self . _max_length - num_added_tokens ) [EOL] if self . _effective_max_length <= [number] : [EOL] raise ValueError ( [string] ) [EOL] [EOL] def _add_encoding_to_vocabulary_if_needed ( self , vocab ) : [EOL] [docstring] [EOL] if self . _added_to_vocabulary : [EOL] return [EOL] [EOL] try : [EOL] vocab_items = self . _tokenizer . get_vocab ( ) . items ( ) [EOL] except NotImplementedError : [EOL] vocab_items = ( ( self . _tokenizer . convert_ids_to_tokens ( idx ) , idx ) for idx in range ( self . _tokenizer . vocab_size ) ) [EOL] for word , idx in vocab_items : [EOL] vocab . _token_to_index [ self . _namespace ] [ word ] = idx [EOL] vocab . _index_to_token [ self . _namespace ] [ idx ] = word [EOL] [EOL] self . _added_to_vocabulary = True [EOL] [EOL] @ overrides def count_vocab_items ( self , token , counter ) : [EOL] [comment] [EOL] pass [EOL] [EOL] @ overrides def tokens_to_indices ( self , tokens , vocabulary ) : [EOL] self . _add_encoding_to_vocabulary_if_needed ( vocabulary ) [EOL] [EOL] indices , type_ids = self . _extract_token_and_type_ids ( tokens ) [EOL] [comment] [EOL] output = { [string] : indices , [string] : [ True ] * len ( indices ) , [string] : type_ids , } [EOL] [EOL] return self . _postprocess_output ( output ) [EOL] [EOL] @ overrides def indices_to_tokens ( self , indexed_tokens , vocabulary ) : [EOL] self . _add_encoding_to_vocabulary_if_needed ( vocabulary ) [EOL] [EOL] token_ids = indexed_tokens [ [string] ] [EOL] type_ids = indexed_tokens . get ( [string] ) [EOL] [EOL] return [ Token ( text = vocabulary . get_token_from_index ( token_ids [ i ] , self . _namespace ) , text_id = token_ids [ i ] , type_id = type_ids [ i ] if type_ids is not None else None , ) for i in range ( len ( token_ids ) ) ] [EOL] [EOL] def _extract_token_and_type_ids ( self , tokens ) : [EOL] [docstring] [EOL] indices = [ ] [EOL] type_ids = [ ] [EOL] for token in tokens : [EOL] if getattr ( token , [string] , None ) is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] indices . append ( token . text_id ) [EOL] else : [EOL] raise KeyError ( [string] f" [string] { token . text }" ) [EOL] [EOL] if type_ids is not None and getattr ( token , [string] , None ) is not None : [EOL] type_ids . append ( token . type_id ) [EOL] else : [EOL] type_ids . append ( [number] ) [EOL] [EOL] return indices , type_ids [EOL] [EOL] def _postprocess_output ( self , output ) : [EOL] [docstring] [EOL] if self . _max_length is not None : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] indices = output [ [string] ] [EOL] [comment] [EOL] indices = indices [ self . _num_added_start_tokens : - self . _num_added_end_tokens ] [EOL] [comment] [EOL] folded_indices = [ indices [ i : i + self . _effective_max_length ] for i in range ( [number] , len ( indices ) , self . _effective_max_length ) ] [EOL] [comment] [EOL] folded_indices = [ self . _tokenizer . build_inputs_with_special_tokens ( segment ) for segment in folded_indices ] [EOL] [comment] [EOL] indices = [ i for segment in folded_indices for i in segment ] [EOL] [EOL] output [ [string] ] = indices [EOL] output [ [string] ] = [ [number] ] * len ( indices ) [EOL] output [ [string] ] = [ True ] * len ( indices ) [EOL] [EOL] return output [EOL] [EOL] @ overrides def get_empty_token_list ( self ) : [EOL] output = { [string] : [ ] , [string] : [ ] , [string] : [ ] } [EOL] if self . _max_length is not None : [EOL] output [ [string] ] = [ ] [EOL] return output [EOL] [EOL] @ overrides def as_padded_tensor_dict ( self , tokens , padding_lengths ) : [EOL] tensor_dict = { } [EOL] for key , val in tokens . items ( ) : [EOL] if key == [string] : [EOL] padding_value = [number] [EOL] mktensor = torch . LongTensor [EOL] elif key == [string] or key == [string] : [EOL] padding_value = False [EOL] mktensor = torch . BoolTensor [EOL] elif len ( val ) > [number] and isinstance ( val [ [number] ] , bool ) : [EOL] padding_value = False [EOL] mktensor = torch . BoolTensor [EOL] else : [EOL] padding_value = self . _tokenizer . pad_token_id [EOL] if padding_value is None : [EOL] padding_value = [number] [EOL] mktensor = torch . LongTensor [EOL] [EOL] tensor = mktensor ( pad_sequence_to_length ( val , padding_lengths [ key ] , default_value = lambda : padding_value ) ) [EOL] [EOL] tensor_dict [ key ] = tensor [EOL] return tensor_dict [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( other , PretrainedTransformerIndexer ) : [EOL] for key in self . __dict__ : [EOL] if key == [string] : [EOL] [comment] [EOL] [comment] [EOL] continue [EOL] if self . __dict__ [ key ] != other . __dict__ [ key ] : [EOL] return False [EOL] return True [EOL] return NotImplemented [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Literal , Any , List , Union [EOL] import builtins [EOL] import typing [EOL] import typing_extensions [EOL] from typing import List [EOL] from overrides import overrides [EOL] [EOL] import spacy [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . common . util import get_spacy_model [EOL] [EOL] [EOL] class SentenceSplitter ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def split_sentences ( self , text ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def batch_split_sentences ( self , texts ) : [EOL] [docstring] [EOL] return [ self . split_sentences ( text ) for text in texts ] [EOL] [EOL] [EOL] @ SentenceSplitter . register ( [string] ) class SpacySentenceSplitter ( SentenceSplitter ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , language = [string] , rule_based = False ) : [EOL] [comment] [EOL] self . spacy = get_spacy_model ( language , parse = not rule_based , ner = False , pos_tags = False ) [EOL] if rule_based : [EOL] [comment] [EOL] [comment] [EOL] sbd_name = [string] if spacy . __version__ < [string] else [string] [EOL] if not self . spacy . has_pipe ( sbd_name ) : [EOL] sbd = self . spacy . create_pipe ( sbd_name ) [EOL] self . spacy . add_pipe ( sbd ) [EOL] [EOL] @ overrides def split_sentences ( self , text ) : [EOL] return [ sent . string . strip ( ) for sent in self . spacy ( text ) . sents ] [EOL] [EOL] @ overrides def batch_split_sentences ( self , texts ) : [EOL] [docstring] [EOL] return [ [ sentence . string . strip ( ) for sentence in doc . sents ] for doc in self . spacy . pipe ( texts ) ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0
from typing import Optional , Type [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from dataclasses import dataclass [EOL] from typing import Optional [EOL] [EOL] [EOL] @ dataclass ( init = False , repr = False ) class Token : [EOL] [docstring] [EOL] [EOL] __slots__ = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] text = ... [EOL] idx = ... [EOL] idx_end = ... [EOL] lemma_ = ... [EOL] pos_ = ... [EOL] tag_ = ... [EOL] dep_ = ... [EOL] ent_type_ = ... [EOL] text_id = ... [EOL] type_id = ... [EOL] [EOL] def __init__ ( self , text = None , idx = None , idx_end = None , lemma_ = None , pos_ = None , tag_ = None , dep_ = None , ent_type_ = None , text_id = None , type_id = None , ) : [EOL] assert text is None or isinstance ( text , str ) [comment] [EOL] self . text = text [EOL] self . idx = idx [EOL] self . idx_end = idx_end [EOL] self . lemma_ = lemma_ [EOL] self . pos_ = pos_ [EOL] self . tag_ = tag_ [EOL] self . dep_ = dep_ [EOL] self . ent_type_ = ent_type_ [EOL] self . text_id = text_id [EOL] self . type_id = type_id [EOL] [EOL] def __str__ ( self ) : [EOL] return self . text [EOL] [EOL] def __repr__ ( self ) : [EOL] return self . __str__ ( ) [EOL] [EOL] [EOL] def show_token ( token ) : [EOL] return ( f"{ token . text } [string] " f" [string] { token . idx } [string] " f" [string] { token . idx_end } [string] " f" [string] { token . lemma_ } [string] " f" [string] { token . pos_ } [string] " f" [string] { token . tag_ } [string] " f" [string] { token . dep_ } [string] " f" [string] { token . ent_type_ } [string] " f" [string] { token . text_id } [string] " f" [string] { token . type_id } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 $None$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import re [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] [EOL] [EOL] @ Tokenizer . register ( [string] ) class LettersDigitsTokenizer ( Tokenizer ) : [EOL] [docstring] [EOL] [EOL] @ overrides def tokenize ( self , text ) : [EOL] [comment] [EOL] tokens = [ Token ( m . group ( ) , idx = m . start ( ) ) for m in re . finditer ( [string] , text ) ] [EOL] return tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0
from typing import Optional , List [EOL] import logging [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from typing import List , Optional [EOL] import logging [EOL] [EOL] from allennlp . common import Registrable [EOL] from allennlp . data . tokenizers . token import Token [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class Tokenizer ( Registrable ) : [EOL] [docstring] [EOL] [EOL] default_implementation = [string] [EOL] [EOL] def batch_tokenize ( self , texts ) : [EOL] [docstring] [EOL] return [ self . tokenize ( text ) for text in texts ] [EOL] [EOL] def tokenize ( self , text ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def add_special_tokens ( self , tokens1 , tokens2 = None ) : [EOL] [docstring] [EOL] return tokens1 + ( tokens2 or [ ] ) [EOL] [EOL] def num_special_tokens_for_sequence ( self ) : [EOL] [docstring] [EOL] return [number] [EOL] [EOL] def num_special_tokens_for_pair ( self ) : [EOL] [docstring] [EOL] return [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0
[docstring] [EOL] [EOL] from allennlp . data . tokenizers . tokenizer import Token , Tokenizer [EOL] from allennlp . data . tokenizers . spacy_tokenizer import SpacyTokenizer [EOL] from allennlp . data . tokenizers . letters_digits_tokenizer import LettersDigitsTokenizer [EOL] from allennlp . data . tokenizers . pretrained_transformer_tokenizer import PretrainedTransformerTokenizer [EOL] from allennlp . data . tokenizers . character_tokenizer import CharacterTokenizer [EOL] from allennlp . data . tokenizers . sentence_splitter import SentenceSplitter [EOL] from allennlp . data . tokenizers . whitespace_tokenizer import WhitespaceTokenizer [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , List [EOL] import spacy [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List , Optional [EOL] [EOL] from overrides import overrides [EOL] import spacy [EOL] from spacy . tokens import Doc [EOL] [EOL] from allennlp . common . util import get_spacy_model [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] [EOL] [EOL] @ Tokenizer . register ( [string] ) class SpacyTokenizer ( Tokenizer ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , language = [string] , pos_tags = False , parse = False , ner = False , keep_spacy_tokens = False , split_on_spaces = False , start_tokens = None , end_tokens = None , ) : [EOL] self . spacy = get_spacy_model ( language , pos_tags , parse , ner ) [EOL] if split_on_spaces : [EOL] self . spacy . tokenizer = _WhitespaceSpacyTokenizer ( self . spacy . vocab ) [EOL] [EOL] self . _keep_spacy_tokens = keep_spacy_tokens [EOL] self . _start_tokens = start_tokens or [ ] [EOL] [comment] [EOL] [comment] [EOL] self . _start_tokens . reverse ( ) [EOL] self . _end_tokens = end_tokens or [ ] [EOL] [EOL] def _sanitize ( self , tokens ) : [EOL] [docstring] [EOL] if not self . _keep_spacy_tokens : [EOL] tokens = [ Token ( token . text , token . idx , token . idx + len ( token . text ) , token . lemma_ , token . pos_ , token . tag_ , token . dep_ , token . ent_type_ , ) for token in tokens ] [EOL] for start_token in self . _start_tokens : [EOL] tokens . insert ( [number] , Token ( start_token , [number] ) ) [EOL] for end_token in self . _end_tokens : [EOL] tokens . append ( Token ( end_token , - [number] ) ) [EOL] return tokens [EOL] [EOL] @ overrides def batch_tokenize ( self , texts ) : [EOL] return [ self . _sanitize ( _remove_spaces ( tokens ) ) for tokens in self . spacy . pipe ( texts , n_threads = - [number] ) ] [EOL] [EOL] @ overrides def tokenize ( self , text ) : [EOL] [comment] [EOL] return self . _sanitize ( _remove_spaces ( self . spacy ( text ) ) ) [EOL] [EOL] [EOL] class _WhitespaceSpacyTokenizer : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , vocab ) : [EOL] self . vocab = vocab [EOL] [EOL] def __call__ ( self , text ) : [EOL] words = text . split ( [string] ) [EOL] spaces = [ True ] * len ( words ) [EOL] return Doc ( self . vocab , words = words , spaces = spaces ) [EOL] [EOL] [EOL] def _remove_spaces ( tokens ) : [EOL] return [ token for token in tokens if not token . is_space ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $typing.Optional[typing.List[builtins.str]]$ 0 0 0 $typing.Optional[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $typing.Optional[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[typing.List[builtins.str]]$ 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 $typing.List[typing.List[allennlp.data.tokenizers.token.Token]]$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[builtins.bool]$ 0 $typing.List[builtins.bool]$ 0 0 0 0 0 $typing.List[spacy.tokens.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] [EOL] [EOL] @ Tokenizer . register ( [string] ) @ Tokenizer . register ( [string] ) class WhitespaceTokenizer ( Tokenizer ) : [EOL] [docstring] [EOL] [EOL] @ overrides def tokenize ( self , text ) : [EOL] return [ Token ( t ) for t in text . split ( ) ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0
from allennlp . data . samplers . samplers import ( Sampler , BatchSampler , SequentialSampler , SubsetRandomSampler , WeightedRandomSampler , RandomSampler , BasicBatchSampler , ) [EOL] from allennlp . data . samplers . bucket_batch_sampler import BucketBatchSampler [EOL] from allennlp . data . samplers . max_tokens_batch_sampler import MaxTokensBatchSampler [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Iterable , Any , List , Tuple [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import logging [EOL] from typing import List , Iterable , Tuple [EOL] import random [EOL] import math [EOL] [EOL] from torch . utils import data [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import lazy_groups_of [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . samplers import BatchSampler [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def add_noise_to_value ( value , noise_param ) : [EOL] noise_value = value * noise_param [EOL] noise = random . uniform ( - noise_value , noise_value ) [EOL] return value + noise [EOL] [EOL] [EOL] @ BatchSampler . register ( [string] ) class BucketBatchSampler ( BatchSampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_source , batch_size , sorting_keys = None , padding_noise = [number] , drop_last = False , ) : [EOL] [EOL] self . vocab = data_source . vocab [EOL] self . sorting_keys = sorting_keys [EOL] self . padding_noise = padding_noise [EOL] self . batch_size = batch_size [EOL] self . data_source = data_source [EOL] self . drop_last = drop_last [EOL] [EOL] def _argsort_by_padding ( self , instances ) : [EOL] [docstring] [EOL] if not self . sorting_keys : [EOL] logger . info ( [string] ) [EOL] self . _guess_sorting_keys ( instances ) [EOL] logger . info ( f" [string] { self . sorting_keys } [string] " ) [EOL] instances_with_lengths = [ ] [EOL] for instance in instances : [EOL] [comment] [EOL] lengths = [ ] [EOL] noisy_lengths = [ ] [EOL] for field_name in self . sorting_keys : [EOL] if field_name not in instance . fields : [EOL] raise ConfigurationError ( f' [string] { field_name } [string] ' f" [string] { list ( instance . fields . keys ( ) ) } [string] " ) [EOL] lengths . append ( len ( instance . fields [ field_name ] ) ) [EOL] [EOL] noisy_lengths . append ( add_noise_to_value ( lengths [ - [number] ] , self . padding_noise ) ) [EOL] instances_with_lengths . append ( ( noisy_lengths , lengths , instance ) ) [EOL] with_indices = [ ( x , i ) for i , x in enumerate ( instances_with_lengths ) ] [EOL] with_indices . sort ( key = lambda x : x [ [number] ] [ [number] ] ) [EOL] return ( [ instance_with_index [ - [number] ] for instance_with_index in with_indices ] , [ instance_with_index [ [number] ] [ [number] ] for instance_with_index in with_indices ] , ) [EOL] [EOL] def __iter__ ( self ) : [EOL] indices , _ = self . _argsort_by_padding ( self . data_source ) [EOL] batches = [ ] [EOL] for group in lazy_groups_of ( indices , self . batch_size ) : [EOL] batch_indices = list ( group ) [EOL] if self . drop_last and len ( batch_indices ) < self . batch_size : [EOL] continue [EOL] batches . append ( batch_indices ) [EOL] random . shuffle ( batches ) [EOL] for batch in batches : [EOL] yield batch [EOL] [EOL] def _guess_sorting_keys ( self , instances , num_instances = [number] ) : [EOL] [docstring] [EOL] max_length = [number] [EOL] longest_field = None [EOL] for i , instance in enumerate ( instances ) : [EOL] instance . index_fields ( self . vocab ) [EOL] for field_name , field in instance . fields . items ( ) : [EOL] length = len ( field ) [EOL] if length > max_length : [EOL] max_length = length [EOL] longest_field = field_name [EOL] if i > num_instances : [EOL] [comment] [EOL] break [EOL] [EOL] if not longest_field : [EOL] [comment] [EOL] [comment] [EOL] raise AssertionError ( [string] [string] ) [EOL] self . sorting_keys = [ longest_field ] [EOL] [EOL] def __len__ ( self ) : [EOL] batch_count_float = len ( self . data_source ) / self . batch_size [EOL] if self . drop_last : [EOL] return math . floor ( batch_count_float ) [EOL] else : [EOL] return math . ceil ( batch_count_float ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0
from typing import Iterable , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from typing import List , Iterable [EOL] from torch . utils import data [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] [docstring] [EOL] [EOL] [EOL] class Sampler ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __iter__ ( self ) : [EOL] [EOL] raise NotImplementedError [EOL] [EOL] [EOL] class BatchSampler ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __iter__ ( self ) : [EOL] [EOL] raise NotImplementedError [EOL] [EOL] [EOL] @ Sampler . register ( [string] ) class SequentialSampler ( data . SequentialSampler , Sampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_source ) : [EOL] super ( ) . __init__ ( data_source ) [EOL] [EOL] [EOL] @ Sampler . register ( [string] ) class RandomSampler ( data . RandomSampler , Sampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_source , replacement = False , num_samples = None ) : [EOL] super ( ) . __init__ ( data_source , replacement , num_samples ) [EOL] [EOL] [EOL] @ Sampler . register ( [string] ) class SubsetRandomSampler ( data . SubsetRandomSampler , Sampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , indices ) : [EOL] super ( ) . __init__ ( indices ) [EOL] [EOL] [EOL] @ Sampler . register ( [string] ) class WeightedRandomSampler ( data . WeightedRandomSampler , Sampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , weights , num_samples , replacement = True ) : [EOL] super ( ) . __init__ ( weights , num_samples , replacement ) [EOL] [EOL] [EOL] @ BatchSampler . register ( [string] ) class BasicBatchSampler ( data . BatchSampler , BatchSampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , sampler , batch_size , drop_last ) : [EOL] super ( ) . __init__ ( sampler , batch_size , drop_last ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $Sampler$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $Sampler$ 0 $builtins.int$ 0 $builtins.bool$ 0 0
from typing import Optional , List , Iterator , TypeVar , Iterable [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] import logging [EOL] import random [EOL] from typing import List , Iterable , Optional , Iterator , TypeVar [EOL] [EOL] from allennlp . data . samplers import BatchSampler , BucketBatchSampler [EOL] from torch . utils import data [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] A = TypeVar ( [string] ) [EOL] [EOL] [EOL] @ BatchSampler . register ( [string] ) class MaxTokensBatchSampler ( BucketBatchSampler ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , data_source , max_tokens = None , sorting_keys = None , padding_noise = [number] , ) : [EOL] super ( ) . __init__ ( data_source , - [number] , sorting_keys , padding_noise , False ) [EOL] [EOL] self . max_tokens = max_tokens [EOL] [EOL] def _lazy_groups_of_max_size ( self , iterable , sizes , ) : [EOL] [docstring] [EOL] cur_max_size = [number] [EOL] group = [ ] [EOL] [EOL] iterator = iter ( iterable ) [EOL] size_iter = iter ( sizes ) [EOL] [EOL] for item , size in zip ( iterator , size_iter ) : [EOL] if size > self . max_tokens : [EOL] logger . warning ( [string] , size , self . max_tokens , ) [EOL] group_size = max ( size , cur_max_size ) * ( len ( group ) + [number] ) [EOL] [EOL] if group_size > self . max_tokens : [EOL] yield group [EOL] cur_max_size = [number] [EOL] group = [ ] [EOL] [EOL] group . append ( item ) [EOL] cur_max_size = max ( cur_max_size , size ) [EOL] [EOL] if len ( group ) != [number] : [EOL] yield group [EOL] [EOL] def __iter__ ( self ) : [EOL] indices , lengths = self . _argsort_by_padding ( self . data_source ) [EOL] [EOL] max_lengths = [ max ( length ) for length in lengths ] [EOL] group_iterator = self . _lazy_groups_of_max_size ( indices , max_lengths ) [EOL] [EOL] batches = [ list ( group ) for group in group_iterator ] [EOL] random . shuffle ( batches ) [EOL] for batch in batches : [EOL] yield batch [EOL] [EOL] def __len__ ( self ) : [EOL] [comment] [EOL] return sum ( [number] for _ in self ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[A]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Callable , List , Union , Iterator , Iterable [EOL] import logging [EOL] import allennlp [EOL] import filelock [EOL] import typing [EOL] import pathlib [EOL] import builtins [EOL] import itertools [EOL] from typing import Iterable , Iterator , Optional , List , Any , Callable , Union [EOL] import logging [EOL] import os [EOL] from pathlib import Path [EOL] import warnings [EOL] [EOL] from filelock import FileLock , Timeout [EOL] import jsonpickle [EOL] import torch . distributed as dist [EOL] from torch . utils . data import Dataset , IterableDataset , get_worker_info [EOL] [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] from allennlp . common import Tqdm , util [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import CacheFile [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class AllennlpDataset ( Dataset ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , instances , vocab = None ) : [EOL] self . instances = instances [EOL] self . vocab = vocab [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] if self . vocab is not None : [EOL] self . instances [ idx ] . index_fields ( self . vocab ) [EOL] return self . instances [ idx ] [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . instances ) [EOL] [EOL] def __iter__ ( self ) : [EOL] [docstring] [EOL] yield from self . instances [EOL] [EOL] def index_with ( self , vocab ) : [EOL] self . vocab = vocab [EOL] [EOL] [EOL] class AllennlpLazyDataset ( IterableDataset ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , instance_generator , file_path , vocab = None , ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _instance_generator = instance_generator [EOL] self . _file_path = file_path [EOL] self . vocab = vocab [EOL] [EOL] def __iter__ ( self ) : [EOL] for instance in self . _instance_generator ( self . _file_path ) : [EOL] if self . vocab is not None : [EOL] instance . index_fields ( self . vocab ) [EOL] yield instance [EOL] [EOL] def index_with ( self , vocab ) : [EOL] self . vocab = vocab [EOL] [EOL] [EOL] class DatasetReader ( Registrable ) : [EOL] [docstring] [EOL] [EOL] CACHE_FILE_LOCK_TIMEOUT = [number] [EOL] [docstring] [EOL] [EOL] def __init__ ( self , lazy = False , cache_directory = None , max_instances = None , manual_distributed_sharding = False , manual_multi_process_sharding = False , ) : [EOL] self . lazy = lazy [EOL] self . max_instances = max_instances [EOL] self . _cache_directory = None [EOL] if cache_directory : [EOL] self . _cache_directory = Path ( cache_directory ) [EOL] os . makedirs ( self . _cache_directory , exist_ok = True ) [EOL] self . manual_distributed_sharding = manual_distributed_sharding [EOL] self . manual_multi_process_sharding = manual_multi_process_sharding [EOL] [EOL] def read ( self , file_path ) : [EOL] [docstring] [EOL] if not isinstance ( file_path , str ) : [EOL] file_path = str ( file_path ) [EOL] [EOL] lazy = getattr ( self , [string] , None ) [EOL] [EOL] if lazy is None : [EOL] warnings . warn ( [string] [string] , UserWarning , ) [EOL] [EOL] if lazy : [EOL] return AllennlpLazyDataset ( self . _instance_iterator , file_path ) [EOL] else : [EOL] cache_file = None [EOL] if self . _cache_directory : [EOL] cache_file = self . _get_cache_location_for_file_path ( file_path ) [EOL] [EOL] if cache_file is not None and os . path . exists ( cache_file ) : [EOL] try : [EOL] [comment] [EOL] [comment] [EOL] cache_file_lock = FileLock ( cache_file + [string] , timeout = self . CACHE_FILE_LOCK_TIMEOUT ) [EOL] cache_file_lock . acquire ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] cache_file_lock . release ( ) [EOL] logger . info ( [string] , cache_file ) [EOL] instances = self . _instances_from_cache_file ( cache_file ) [EOL] except Timeout : [EOL] logger . warning ( [string] [string] , self . CACHE_FILE_LOCK_TIMEOUT , ) [EOL] instances = self . _multi_worker_islice ( self . _read ( file_path ) ) [EOL] else : [EOL] instances = self . _multi_worker_islice ( self . _read ( file_path ) ) [EOL] [EOL] [comment] [EOL] if not isinstance ( instances , list ) : [EOL] instances = list ( instances ) [EOL] [EOL] if not instances : [EOL] raise ConfigurationError ( [string] [string] . format ( file_path ) ) [EOL] [EOL] [comment] [EOL] if cache_file is not None and not os . path . exists ( cache_file ) : [EOL] if self . max_instances is not None : [EOL] [comment] [EOL] logger . warning ( [string] ) [EOL] elif util . is_distributed ( ) or ( get_worker_info ( ) and get_worker_info ( ) . num_workers ) : [EOL] [comment] [EOL] [comment] [EOL] logger . warning ( [string] ) [EOL] else : [EOL] try : [EOL] with FileLock ( cache_file + [string] , timeout = self . CACHE_FILE_LOCK_TIMEOUT ) : [EOL] self . _instances_to_cache_file ( cache_file , instances ) [EOL] except Timeout : [EOL] logger . warning ( [string] [string] , self . CACHE_FILE_LOCK_TIMEOUT , ) [EOL] [EOL] return AllennlpDataset ( instances ) [EOL] [EOL] def _get_cache_location_for_file_path ( self , file_path ) : [EOL] return str ( self . _cache_directory / util . flatten_filename ( str ( file_path ) ) ) [EOL] [EOL] def _read ( self , file_path ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def _instances_from_cache_file ( self , cache_filename ) : [EOL] with open ( cache_filename , [string] ) as cache_file : [EOL] yield from self . _multi_worker_islice ( cache_file , self . deserialize_instance ) [EOL] [EOL] def _instances_to_cache_file ( self , cache_filename , instances ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] with CacheFile ( cache_filename , mode = [string] ) as cache_handle : [EOL] logger . info ( [string] , cache_handle . name ) [EOL] for instance in Tqdm . tqdm ( instances , desc = [string] ) : [EOL] cache_handle . write ( self . serialize_instance ( instance ) + [string] ) [EOL] [EOL] def text_to_instance ( self , * inputs ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def serialize_instance ( self , instance ) : [EOL] [docstring] [EOL] return jsonpickle . dumps ( instance ) [EOL] [EOL] def deserialize_instance ( self , string ) : [EOL] [docstring] [EOL] return jsonpickle . loads ( string . strip ( ) ) [comment] [EOL] [EOL] def _multi_worker_islice ( self , iterable , transform = None , ensure_lazy = False , ) : [EOL] [docstring] [EOL] if ensure_lazy and isinstance ( iterable , ( list , tuple ) ) : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] wrap_with_tqdm = True [EOL] start_index = [number] [EOL] step_size = [number] [EOL] if not self . manual_distributed_sharding and util . is_distributed ( ) : [EOL] start_index = dist . get_rank ( ) [EOL] step_size = dist . get_world_size ( ) [EOL] worker_info = None if self . manual_multi_process_sharding else get_worker_info ( ) [EOL] if worker_info : [EOL] warnings . warn ( [string] [string] [string] [string] [string] [string] , UserWarning , ) [EOL] [comment] [EOL] start_index *= worker_info . num_workers [EOL] start_index += worker_info . id [EOL] [comment] [EOL] step_size *= worker_info . num_workers [EOL] if worker_info . id > [number] : [EOL] [comment] [EOL] wrap_with_tqdm = False [EOL] [EOL] islice = itertools . islice ( iterable , start_index , self . max_instances , step_size ) [EOL] if wrap_with_tqdm : [EOL] islice = Tqdm . tqdm ( islice , desc = [string] ) [EOL] [EOL] if transform is not None : [EOL] return ( transform ( x ) for x in islice ) [EOL] return islice [EOL] [EOL] def _instance_iterator ( self , file_path ) : [EOL] cache_file = None [EOL] if self . _cache_directory : [EOL] cache_file = self . _get_cache_location_for_file_path ( file_path ) [EOL] [EOL] if cache_file is not None and os . path . exists ( cache_file ) : [EOL] cache_file_lock = FileLock ( cache_file + [string] , timeout = self . CACHE_FILE_LOCK_TIMEOUT ) [EOL] try : [EOL] cache_file_lock . acquire ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] cache_file_lock . release ( ) [EOL] logger . info ( [string] , cache_file ) [EOL] with open ( cache_file ) as data_file : [EOL] yield from self . _multi_worker_islice ( data_file , transform = self . deserialize_instance ) [EOL] except Timeout : [EOL] logger . warning ( [string] [string] , self . CACHE_FILE_LOCK_TIMEOUT , ) [EOL] yield from self . _multi_worker_islice ( self . _read ( file_path ) , ensure_lazy = True ) [EOL] elif cache_file is not None and not os . path . exists ( cache_file ) : [EOL] instances = self . _multi_worker_islice ( self . _read ( file_path ) , ensure_lazy = True ) [EOL] [comment] [EOL] if self . max_instances is not None : [EOL] [comment] [EOL] logger . warning ( [string] ) [EOL] yield from instances [EOL] elif util . is_distributed ( ) or ( get_worker_info ( ) and get_worker_info ( ) . num_workers ) : [EOL] [comment] [EOL] [comment] [EOL] logger . warning ( [string] ) [EOL] yield from instances [EOL] else : [EOL] try : [EOL] with FileLock ( cache_file + [string] , timeout = self . CACHE_FILE_LOCK_TIMEOUT ) : [EOL] with CacheFile ( cache_file , mode = [string] ) as cache_handle : [EOL] logger . info ( [string] , cache_handle . name ) [EOL] for instance in instances : [EOL] cache_handle . write ( self . serialize_instance ( instance ) + [string] ) [EOL] yield instance [EOL] except Timeout : [EOL] logger . warning ( [string] [string] , self . CACHE_FILE_LOCK_TIMEOUT , ) [EOL] yield from instances [EOL] else : [EOL] [comment] [EOL] yield from self . _multi_worker_islice ( self . _read ( file_path ) , ensure_lazy = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[AllennlpDataset,AllennlpLazyDataset]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $filelock.UnixFileLock$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $filelock.UnixFileLock$ 0 0 0 0 0 0 0 0 0 0 0 $filelock.UnixFileLock$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $typing.Iterable[typing.Any]$ 0 $typing.Optional[typing.Callable[[typing.Any],allennlp.data.instance.Instance]]$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Iterable[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Iterable[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Optional[typing.Callable[[typing.Any],allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 $typing.Optional[typing.Callable[[typing.Any],allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $filelock.UnixFileLock$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $filelock.UnixFileLock$ 0 0 0 0 0 0 0 0 0 0 0 $filelock.UnixFileLock$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0
from typing import Iterable , List [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import glob [EOL] import logging [EOL] import os [EOL] import torch [EOL] from typing import Iterable [EOL] [EOL] from allennlp . common import util [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . file_utils import cached_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . instance import Instance [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class ShardedDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , base_reader , ** kwargs ) : [EOL] super ( ) . __init__ ( manual_distributed_sharding = True , ** kwargs ) [EOL] [EOL] if util . is_distributed ( ) : [EOL] self . _rank = torch . distributed . get_rank ( ) [EOL] self . _world_size = torch . distributed . get_world_size ( ) [EOL] else : [EOL] self . _rank = [number] [EOL] self . _world_size = [number] [EOL] [EOL] self . reader = base_reader [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if getattr ( self . reader , [string] , False ) : [EOL] raise ValueError ( [string] [string] ) [EOL] [comment] [EOL] [comment] [EOL] self . reader . manual_distributed_sharding = True [EOL] [EOL] def text_to_instance ( self , * args , ** kwargs ) : [EOL] [docstring] [EOL] return self . reader . text_to_instance ( * args , ** kwargs ) [comment] [EOL] [EOL] def _read ( self , file_path ) : [EOL] try : [EOL] maybe_extracted_archive = cached_path ( file_path , extract_archive = True ) [EOL] if not os . path . isdir ( maybe_extracted_archive ) : [EOL] [comment] [EOL] raise ConfigurationError ( f"{ file_path } [string] " ) [EOL] shards = [ os . path . join ( maybe_extracted_archive , p ) for p in os . listdir ( maybe_extracted_archive ) if not p . startswith ( [string] ) ] [EOL] if not shards : [EOL] raise ConfigurationError ( f" [string] { file_path }" ) [EOL] except FileNotFoundError : [EOL] [comment] [EOL] shards = glob . glob ( file_path ) [EOL] if not shards : [EOL] raise ConfigurationError ( f" [string] { file_path }" ) [EOL] [EOL] [comment] [EOL] shards . sort ( ) [EOL] [EOL] for i , shard in enumerate ( shards ) : [EOL] if i % self . _world_size == self . _rank : [EOL] logger . info ( f" [string] { shard }" ) [EOL] for instance in self . reader . read ( shard ) : [EOL] yield instance [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , Any , Dict , Union , Iterator , Mapping , Iterable [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from typing import Dict , Mapping , Iterable [EOL] import json [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . fields import MetadataField [EOL] from allennlp . data . instance import Instance [EOL] [EOL] _VALID_SCHEMES = { [string] , [string] } [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class InterleavingDatasetReader ( DatasetReader ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , readers , dataset_field_name = [string] , scheme = [string] , ** kwargs , ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . _readers = readers [EOL] self . _dataset_field_name = dataset_field_name [EOL] [EOL] if scheme not in _VALID_SCHEMES : [EOL] raise ConfigurationError ( f" [string] { scheme }" ) [EOL] self . _scheme = scheme [EOL] [EOL] def _read_round_robin ( self , datasets ) : [EOL] remaining = set ( datasets ) [EOL] dataset_iterators = { key : iter ( dataset ) for key , dataset in datasets . items ( ) } [EOL] [EOL] while remaining : [EOL] for key , dataset in dataset_iterators . items ( ) : [EOL] if key in remaining : [EOL] try : [EOL] instance = next ( dataset ) [EOL] instance . fields [ self . _dataset_field_name ] = MetadataField ( key ) [EOL] yield instance [EOL] except StopIteration : [EOL] remaining . remove ( key ) [EOL] [EOL] def _read_all_at_once ( self , datasets ) : [EOL] for key , dataset in datasets . items ( ) : [EOL] for instance in dataset : [EOL] instance . fields [ self . _dataset_field_name ] = MetadataField ( key ) [EOL] yield instance [EOL] [EOL] def _read ( self , file_path ) : [EOL] try : [EOL] file_paths = json . loads ( file_path ) [EOL] except json . JSONDecodeError : [EOL] raise ConfigurationError ( [string] [string] ) [EOL] [EOL] if file_paths . keys ( ) != self . _readers . keys ( ) : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] [comment] [EOL] datasets = { key : reader . read ( file_paths [ key ] ) for key , reader in self . _readers . items ( ) } [EOL] [EOL] if self . _scheme == [string] : [EOL] yield from self . _read_round_robin ( datasets ) [EOL] elif self . _scheme == [string] : [EOL] yield from self . _read_all_at_once ( datasets ) [EOL] else : [EOL] raise RuntimeError ( [string] ) [EOL] [EOL] def text_to_instance ( self ) : [comment] [EOL] [EOL] raise RuntimeError ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.Dict[builtins.str,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 $typing.Dict[builtins.str,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $typing.Mapping[builtins.str,typing.Iterable[allennlp.data.instance.Instance]]$ 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Mapping[builtins.str,typing.Iterable[allennlp.data.instance.Instance]]$ 0 0 $typing.Dict[typing.Any,typing.Iterator[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Mapping[builtins.str,typing.Iterable[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Iterator[typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $typing.Mapping[builtins.str,typing.Iterable[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 0 $typing.Mapping[builtins.str,typing.Iterable[allennlp.data.instance.Instance]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset,allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset]]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset,allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset,allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Type , List , TypeVar [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from copy import deepcopy [EOL] from typing import Dict , Generic , List , TypeVar [EOL] [EOL] import torch [EOL] [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] DataArray = TypeVar ( [string] , torch . Tensor , Dict [ str , torch . Tensor ] , Dict [ str , Dict [ str , torch . Tensor ] ] ) [EOL] [EOL] [EOL] class Field ( Generic [ DataArray ] ) : [EOL] [docstring] [EOL] [EOL] __slots__ = [ ] [comment] [EOL] [EOL] def count_vocab_items ( self , counter ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def index ( self , vocab ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def get_padding_lengths ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def as_tensor ( self , padding_lengths ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def empty_field ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def batch_tensors ( self , tensor_list ) : [comment] [EOL] [docstring] [EOL] [EOL] return torch . stack ( tensor_list ) [EOL] [EOL] def __eq__ ( self , other ) : [EOL] if isinstance ( self , other . __class__ ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for class_ in self . __class__ . mro ( ) : [EOL] for attr in getattr ( class_ , [string] , [ ] ) : [EOL] if getattr ( self , attr ) != getattr ( other , attr ) : [EOL] return False [EOL] [comment] [EOL] [comment] [EOL] if hasattr ( self , [string] ) : [EOL] return self . __dict__ == other . __dict__ [EOL] return True [EOL] return NotImplemented [EOL] [EOL] def __len__ ( self ) : [EOL] raise NotImplementedError [EOL] [EOL] def duplicate ( self ) : [EOL] return deepcopy ( self ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import builtins [EOL] import typing [EOL] from typing import Any , Dict , List [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . data . fields . field import Field [EOL] [EOL] [EOL] class FlagField ( Field [ Any ] ) : [EOL] [docstring] [EOL] [EOL] __slots__ = [ [string] ] [EOL] [EOL] def __init__ ( self , flag_value ) : [EOL] self . flag_value = flag_value [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] return { } [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] return self . flag_value [EOL] [EOL] @ overrides def empty_field ( self ) : [EOL] [comment] [EOL] [comment] [EOL] return FlagField ( self . flag_value ) [EOL] [EOL] def __str__ ( self ) : [EOL] return f" [string] { self . flag_value } [string] " [EOL] [EOL] def __len__ ( self ) : [EOL] return [number] [EOL] [EOL] @ overrides def batch_tensors ( self , tensor_list ) : [EOL] if len ( set ( tensor_list ) ) != [number] : [EOL] raise ValueError ( f" [string] { tensor_list }" ) [EOL] return tensor_list [ [number] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0
from typing import Any , Set , Dict , List , Union , Iterator [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Dict , List , Union , Set , Iterator [EOL] import logging [EOL] import textwrap [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . util import pad_sequence_to_length [EOL] from allennlp . data . fields . field import Field [EOL] from allennlp . data . fields . sequence_field import SequenceField [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class SequenceLabelField ( Field [ torch . Tensor ] ) : [EOL] [docstring] [EOL] [EOL] __slots__ = [ [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] _already_warned_namespaces = set ( ) [EOL] [EOL] def __init__ ( self , labels , sequence_field , label_namespace = [string] , ) : [EOL] self . labels = labels [EOL] self . sequence_field = sequence_field [EOL] self . _label_namespace = label_namespace [EOL] self . _indexed_labels = None [EOL] self . _maybe_warn_for_namespace ( label_namespace ) [EOL] if len ( labels ) != sequence_field . sequence_length ( ) : [EOL] raise ConfigurationError ( [string] [string] % ( len ( labels ) , sequence_field . sequence_length ( ) ) ) [EOL] [EOL] self . _skip_indexing = False [EOL] if all ( isinstance ( x , int ) for x in labels ) : [EOL] self . _indexed_labels = labels [EOL] self . _skip_indexing = True [EOL] [EOL] elif not all ( isinstance ( x , str ) for x in labels ) : [EOL] raise ConfigurationError ( [string] [string] [string] . format ( labels , [ type ( x ) for x in labels ] ) ) [EOL] [EOL] def _maybe_warn_for_namespace ( self , label_namespace ) : [EOL] if not ( self . _label_namespace . endswith ( [string] ) or self . _label_namespace . endswith ( [string] ) ) : [EOL] if label_namespace not in self . _already_warned_namespaces : [EOL] logger . warning ( [string] [string] [string] [string] , self . _label_namespace , ) [EOL] self . _already_warned_namespaces . add ( label_namespace ) [EOL] [EOL] [comment] [EOL] def __iter__ ( self ) : [EOL] return iter ( self . labels ) [EOL] [EOL] def __getitem__ ( self , idx ) : [EOL] return self . labels [ idx ] [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . labels ) [EOL] [EOL] @ overrides def count_vocab_items ( self , counter ) : [EOL] if self . _indexed_labels is None : [EOL] for label in self . labels : [EOL] counter [ self . _label_namespace ] [ label ] += [number] [comment] [EOL] [EOL] @ overrides def index ( self , vocab ) : [EOL] if not self . _skip_indexing : [EOL] self . _indexed_labels = [ vocab . get_token_index ( label , self . _label_namespace ) for label in self . labels ] [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] return { [string] : self . sequence_field . sequence_length ( ) } [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] desired_num_tokens = padding_lengths [ [string] ] [EOL] padded_tags = pad_sequence_to_length ( self . _indexed_labels , desired_num_tokens ) [EOL] tensor = torch . LongTensor ( padded_tags ) [EOL] return tensor [EOL] [EOL] @ overrides def empty_field ( self ) : [EOL] [comment] [EOL] empty_list = [ ] [EOL] sequence_label_field = SequenceLabelField ( empty_list , self . sequence_field . empty_field ( ) ) [EOL] sequence_label_field . _indexed_labels = empty_list [EOL] return sequence_label_field [EOL] [EOL] def __str__ ( self ) : [EOL] length = self . sequence_field . sequence_length ( ) [EOL] formatted_labels = [string] . join ( [string] + labels + [string] for labels in textwrap . wrap ( repr ( self . labels ) , [number] ) ) [EOL] return ( f" [string] { length } [string] " f" [string] { formatted_labels } [string] { self . _label_namespace } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Iterator[typing.Union[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[builtins.str,builtins.int]$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $builtins.int$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $"SequenceLabelField"$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $allennlp.data.fields.sequence_label_field.SequenceLabelField$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.fields.sequence_label_field.SequenceLabelField$ 0 0 0 $typing.List[builtins.str]$ 0 0 $allennlp.data.fields.sequence_label_field.SequenceLabelField$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Type [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from allennlp . data . fields . field import DataArray , Field [EOL] [EOL] [EOL] class SequenceField ( Field [ DataArray ] ) : [EOL] [docstring] [EOL] [EOL] __slots__ = [ ] [comment] [EOL] [EOL] def sequence_length ( self ) : [EOL] [docstring] [EOL] raise NotImplementedError [EOL] [EOL] def empty_field ( self ) : [EOL] raise NotImplementedError [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.fields.sequence_field.SequenceField]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $"SequenceField"$ 0 0 0 0 0 0 0 0
from typing import Set , Any , Dict , List , Tuple [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] import builtins [EOL] from typing import Dict , List , Set , Tuple [EOL] import logging [EOL] import textwrap [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data . fields . field import Field [EOL] from allennlp . data . fields . sequence_field import SequenceField [EOL] from allennlp . data . vocabulary import Vocabulary [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class AdjacencyField ( Field [ torch . Tensor ] ) : [EOL] [docstring] [EOL] [EOL] __slots__ = [ [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] _already_warned_namespaces = set ( ) [EOL] [EOL] def __init__ ( self , indices , sequence_field , labels = None , label_namespace = [string] , padding_value = - [number] , ) : [EOL] self . indices = indices [EOL] self . labels = labels [EOL] self . sequence_field = sequence_field [EOL] self . _label_namespace = label_namespace [EOL] self . _padding_value = padding_value [EOL] self . _indexed_labels = None [EOL] [EOL] self . _maybe_warn_for_namespace ( label_namespace ) [EOL] field_length = sequence_field . sequence_length ( ) [EOL] [EOL] if len ( set ( indices ) ) != len ( indices ) : [EOL] raise ConfigurationError ( f" [string] { indices }" ) [EOL] [EOL] if not all ( [number] <= index [ [number] ] < field_length and [number] <= index [ [number] ] < field_length for index in indices ) : [EOL] raise ConfigurationError ( f" [string] " f" [string] { indices } [string] { field_length }" ) [EOL] [EOL] if labels is not None and len ( indices ) != len ( labels ) : [EOL] raise ConfigurationError ( f" [string] " f" [string] { labels } [string] { indices }" ) [EOL] [EOL] def _maybe_warn_for_namespace ( self , label_namespace ) : [EOL] if not ( self . _label_namespace . endswith ( [string] ) or self . _label_namespace . endswith ( [string] ) ) : [EOL] if label_namespace not in self . _already_warned_namespaces : [EOL] logger . warning ( [string] [string] [string] [string] , self . _label_namespace , ) [EOL] self . _already_warned_namespaces . add ( label_namespace ) [EOL] [EOL] @ overrides def count_vocab_items ( self , counter ) : [EOL] if self . _indexed_labels is None and self . labels is not None : [EOL] for label in self . labels : [EOL] counter [ self . _label_namespace ] [ label ] += [number] [comment] [EOL] [EOL] @ overrides def index ( self , vocab ) : [EOL] if self . labels is not None : [EOL] self . _indexed_labels = [ vocab . get_token_index ( label , self . _label_namespace ) for label in self . labels ] [EOL] [EOL] @ overrides def get_padding_lengths ( self ) : [EOL] return { [string] : self . sequence_field . sequence_length ( ) } [EOL] [EOL] @ overrides def as_tensor ( self , padding_lengths ) : [EOL] desired_num_tokens = padding_lengths [ [string] ] [EOL] tensor = torch . ones ( desired_num_tokens , desired_num_tokens ) * self . _padding_value [EOL] labels = self . _indexed_labels or [ [number] for _ in range ( len ( self . indices ) ) ] [EOL] [EOL] for index , label in zip ( self . indices , labels ) : [EOL] tensor [ index ] = label [EOL] return tensor [EOL] [EOL] @ overrides def empty_field ( self ) : [EOL] [EOL] [comment] [EOL] empty_list = [ ] [EOL] adjacency_field = AdjacencyField ( empty_list , self . sequence_field . empty_field ( ) , padding_value = self . _padding_value ) [EOL] return adjacency_field [EOL] [EOL] def __str__ ( self ) : [EOL] length = self . sequence_field . sequence_length ( ) [EOL] formatted_labels = [string] . join ( [string] + labels + [string] for labels in textwrap . wrap ( repr ( self . labels ) , [number] ) ) [EOL] formatted_indices = [string] . join ( [string] + index + [string] for index in textwrap . wrap ( repr ( self . indices ) , [number] ) ) [EOL] return ( f" [string] { length } [string] " f" [string] { formatted_indices } [string] " f" [string] { formatted_labels } [string] { self . _label_namespace } [string] " ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . sequence_field ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.vocabulary.Vocabulary$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.Tensor$ 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 $builtins.int$ 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $"AdjacencyField"$ 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 $allennlp.data.fields.adjacency_field.AdjacencyField$ 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.fields.adjacency_field.AdjacencyField$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from allennlp . interpret . attackers . attacker import Attacker [EOL] from allennlp . interpret . saliency_interpreters . saliency_interpreter import SaliencyInterpreter [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from allennlp . interpret . attackers . attacker import Attacker [EOL] from allennlp . interpret . attackers . input_reduction import InputReduction [EOL] from allennlp . interpret . attackers . hotflip import Hotflip [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Set , Any , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . data import Instance [EOL] [EOL] [EOL] def get_fields_to_compare ( inputs , instance , input_field_to_attack ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] fields_to_compare = { key : instance [ key ] for key in instance . fields if key not in inputs [EOL] and key != input_field_to_attack [EOL] and key != [string] [EOL] and key != [string] } [EOL] return fields_to_compare [EOL] [EOL] [EOL] def instance_has_changed ( instance , fields_to_compare ) : [EOL] if [string] in fields_to_compare : [EOL] [comment] [EOL] [comment] [EOL] original_clusters = set ( tuple ( x ) for x in fields_to_compare [ [string] ] ) [EOL] new_clusters = set ( tuple ( x ) for x in instance [ [string] ] ) [comment] [EOL] return original_clusters != new_clusters [EOL] if any ( instance [ field ] != fields_to_compare [ field ] for field in fields_to_compare ) : [EOL] return True [EOL] return False [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import numpy [EOL] import allennlp [EOL] import typing [EOL] import math [EOL] [EOL] from typing import List [EOL] import numpy [EOL] [EOL] from allennlp . common . util import JsonDict , sanitize [EOL] from allennlp . interpret . saliency_interpreters . saliency_interpreter import SaliencyInterpreter [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] @ SaliencyInterpreter . register ( [string] ) class SimpleGradient ( SaliencyInterpreter ) : [EOL] [docstring] [EOL] [EOL] def saliency_interpret_from_json ( self , inputs ) : [EOL] [docstring] [EOL] labeled_instances = self . predictor . json_to_labeled_instances ( inputs ) [EOL] [EOL] [comment] [EOL] embeddings_list = [ ] [EOL] [EOL] instances_with_grads = dict ( ) [EOL] for idx , instance in enumerate ( labeled_instances ) : [EOL] [comment] [EOL] handle = self . _register_forward_hook ( embeddings_list ) [EOL] grads = self . predictor . get_gradients ( [ instance ] ) [ [number] ] [EOL] handle . remove ( ) [EOL] [EOL] [comment] [EOL] embeddings_list . reverse ( ) [EOL] for key , grad in grads . items ( ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] input_idx = int ( key [ - [number] ] ) - [number] [EOL] [comment] [EOL] emb_grad = numpy . sum ( grad [ [number] ] * embeddings_list [ input_idx ] , axis = [number] ) [EOL] norm = numpy . linalg . norm ( emb_grad , ord = [number] ) [EOL] normalized_grad = [ math . fabs ( e ) / norm for e in emb_grad ] [EOL] grads [ key ] = normalized_grad [EOL] [EOL] instances_with_grads [ [string] + str ( idx + [number] ) ] = grads [EOL] return sanitize ( instances_with_grads ) [EOL] [EOL] def _register_forward_hook ( self , embeddings_list ) : [EOL] [docstring] [EOL] [EOL] def forward_hook ( module , inputs , output ) : [EOL] embeddings_list . append ( output . squeeze ( [number] ) . clone ( ) . detach ( ) . numpy ( ) ) [EOL] [EOL] embedding_layer = util . find_embedding_layer ( self . predictor . _model ) [EOL] handle = embedding_layer . register_forward_hook ( forward_hook ) [EOL] [EOL] return handle [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $typing.List[numpy.ndarray]$ 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[numpy.ndarray]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[numpy.ndarray]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[numpy.ndarray]$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.List[builtins.float]$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0
from allennlp . interpret . saliency_interpreters . saliency_interpreter import SaliencyInterpreter [EOL] from allennlp . interpret . saliency_interpreters . simple_gradient import SimpleGradient [EOL] from allennlp . interpret . saliency_interpreters . integrated_gradient import IntegratedGradient [EOL] from allennlp . interpret . saliency_interpreters . smooth_gradient import SmoothGradient [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import allennlp [EOL] from allennlp . common import Registrable [EOL] from allennlp . common . util import JsonDict [EOL] from allennlp . predictors import Predictor [EOL] [EOL] [EOL] class SaliencyInterpreter ( Registrable ) : [EOL] [docstring] [EOL] [EOL] def __init__ ( self , predictor ) : [EOL] self . predictor = predictor [EOL] [EOL] def saliency_interpret_from_json ( self , inputs ) : [EOL] [docstring] [EOL] raise NotImplementedError ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] [docstring] [EOL] from typing import Dict , Any , Optional , Union , Tuple , List [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] import pytest [EOL] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . common . testing . model_test_case import ModelTestCase [EOL] from allennlp . common . testing . distributed_test import run_distributed_test [EOL] [EOL] from allennlp . training . metrics import Metric [EOL] [EOL] [EOL] _available_devices = [ [string] ] + ( [ [string] ] if torch . cuda . is_available ( ) else [ ] ) [EOL] [EOL] [EOL] def multi_device ( test_method ) : [EOL] [docstring] [EOL] return pytest . mark . parametrize ( [string] , _available_devices ) ( pytest . mark . gpu ( test_method ) ) [EOL] [EOL] [EOL] def requires_gpu ( test_method ) : [EOL] [docstring] [EOL] return pytest . mark . gpu ( pytest . mark . skipif ( not torch . cuda . is_available ( ) , reason = [string] ) ( test_method ) ) [EOL] [EOL] [EOL] def requires_multi_gpu ( test_method ) : [EOL] [docstring] [EOL] return pytest . mark . gpu ( pytest . mark . skipif ( torch . cuda . device_count ( ) < [number] , reason = [string] ) ( test_method ) ) [EOL] [EOL] [EOL] def cpu_or_gpu ( test_method ) : [EOL] [docstring] [EOL] return pytest . mark . gpu ( test_method ) [EOL] [EOL] [EOL] [comment] [EOL] [EOL] [EOL] def assert_metrics_values ( metrics , desired_values , rtol = [number] , atol = [number] , ) : [EOL] for key in metrics : [EOL] assert_allclose ( metrics [ key ] , desired_values [ key ] , rtol = rtol , atol = atol ) [EOL] [EOL] [EOL] def global_distributed_metric ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] kwargs = { } [EOL] [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] metric ( ** kwargs ) [EOL] [EOL] metrics = metric . get_metric ( False ) [EOL] if not isinstance ( metrics , Dict ) and not isinstance ( desired_values , Dict ) : [EOL] metrics = { [string] : metrics } [EOL] desired_values = { [string] : desired_values } [EOL] [EOL] [comment] [EOL] if isinstance ( exact , bool ) : [EOL] if exact : [EOL] rtol = [number] [EOL] atol = [number] [EOL] else : [EOL] rtol = [number] [EOL] atol = [number] [EOL] else : [EOL] rtol = exact [ [number] ] [EOL] atol = exact [ [number] ] [EOL] [EOL] assert_metrics_values ( metrics , desired_values , rtol , atol ) [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Callable , Dict , Literal , List , Union , Tuple [EOL] import builtins [EOL] import typing [EOL] import typing_extensions [EOL] import datetime [EOL] from typing import List , Dict , Any , Tuple , Callable [EOL] import torch [EOL] import torch . distributed as dist [EOL] import torch . multiprocessing as mp [EOL] [EOL] from allennlp . common . checks import check_for_gpu [EOL] [EOL] [EOL] def init_process ( process_rank , distributed_device_ids = None , world_size = [number] , func = None , func_args = None , func_kwargs = None , master_addr = [string] , master_port = [number] , ) : [EOL] assert world_size > [number] [EOL] [EOL] global_rank = process_rank [EOL] [EOL] gpu_id = distributed_device_ids [ process_rank ] [comment] [EOL] [EOL] if gpu_id >= [number] : [EOL] torch . cuda . set_device ( int ( gpu_id ) ) [EOL] dist . init_process_group ( backend = [string] , init_method = f" [string] { master_addr } [string] { master_port }" , world_size = world_size , rank = global_rank , ) [EOL] else : [EOL] dist . init_process_group ( backend = [string] , init_method = f" [string] { master_addr } [string] { master_port }" , world_size = world_size , rank = global_rank , timeout = datetime . timedelta ( seconds = [number] ) , ) [EOL] [EOL] func ( global_rank , world_size , gpu_id , * func_args , ** func_kwargs ) [EOL] [EOL] dist . barrier ( ) [EOL] [EOL] [EOL] def run_distributed_test ( device_ids = [ - [number] , - [number] ] , func = None , * args , ** kwargs , ) : [EOL] [docstring] [EOL] check_for_gpu ( device_ids ) [EOL] [comment] [EOL] [comment] [EOL] start_method = [string] if any ( x >= [number] for x in device_ids ) else [string] [EOL] nprocs = world_size = len ( device_ids ) [EOL] mp . start_processes ( init_process , args = ( device_ids , world_size , func , args , kwargs ) , nprocs = nprocs , start_method = start_method , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Type [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import logging [EOL] import os [EOL] import pathlib [EOL] import shutil [EOL] import tempfile [EOL] from unittest import mock [EOL] [EOL] from allennlp . common . checks import log_pytorch_version_info [EOL] [EOL] TEST_DIR = tempfile . mkdtemp ( prefix = [string] ) [EOL] [EOL] [EOL] class AllenNlpTestCase : [EOL] [docstring] [EOL] [EOL] PROJECT_ROOT = ( pathlib . Path ( __file__ ) . parent / [string] / [string] / [string] ) . resolve ( ) [EOL] MODULE_ROOT = PROJECT_ROOT / [string] [EOL] TOOLS_ROOT = MODULE_ROOT / [string] [EOL] TESTS_ROOT = PROJECT_ROOT / [string] [EOL] FIXTURES_ROOT = PROJECT_ROOT / [string] [EOL] [EOL] def setup_method ( self ) : [EOL] logging . basicConfig ( format = [string] , level = logging . DEBUG ) [EOL] [comment] [EOL] [comment] [EOL] logging . getLogger ( [string] ) . disabled = True [EOL] logging . getLogger ( [string] ) . disabled = True [EOL] logging . getLogger ( [string] ) . setLevel ( logging . INFO ) [EOL] logging . getLogger ( [string] ) . disabled = True [EOL] log_pytorch_version_info ( ) [EOL] [EOL] self . TEST_DIR = pathlib . Path ( TEST_DIR ) [EOL] [EOL] os . makedirs ( self . TEST_DIR , exist_ok = True ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] def _cleanup_archive_dir_without_logging ( path ) : [EOL] if os . path . exists ( path ) : [EOL] shutil . rmtree ( path ) [EOL] [EOL] self . patcher = mock . patch ( [string] , _cleanup_archive_dir_without_logging ) [EOL] self . mock_cleanup_archive_dir = self . patcher . start ( ) [EOL] [EOL] def teardown_method ( self ) : [EOL] shutil . rmtree ( self . TEST_DIR ) [EOL] self . patcher . stop ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
import logging [EOL] import argparse [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import logging [EOL] import pathlib [EOL] [EOL] from overrides import overrides [EOL] import torch [EOL] [EOL] import allennlp [EOL] from allennlp . common . util import import_module_and_submodules [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . version import VERSION [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Subcommand . register ( [string] ) class TestInstall ( Subcommand ) : [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] description = [string] [EOL] subparser = parser . add_parser ( self . name , description = description , help = [string] ) [EOL] subparser . set_defaults ( func = _run_test ) [EOL] return subparser [EOL] [EOL] [EOL] def _get_module_root ( ) : [EOL] return pathlib . Path ( allennlp . __file__ ) . parent [EOL] [EOL] [EOL] def _run_test ( args ) : [EOL] [comment] [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] import_module_and_submodules ( [string] ) [EOL] logger . info ( [string] , VERSION , _get_module_root ( ) ) [EOL] logger . info ( [string] , torch . cuda . device_count ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse._SubParsersAction$ 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Set , Dict , List , Union , Tuple [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import itertools [EOL] import builtins [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import logging [EOL] import math [EOL] import os [EOL] import re [EOL] from typing import List , Tuple [EOL] import itertools [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . common import Params , Tqdm [EOL] from allennlp . common import logging as common_logging [EOL] from allennlp . common . checks import check_for_gpu , ConfigurationError [EOL] from allennlp . common . util import prepare_environment [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . data import DataLoader [EOL] from allennlp . models import Model [EOL] from allennlp . training import GradientDescentTrainer , Trainer [EOL] from allennlp . training . util import create_serialization_dir , datasets_from_params [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Subcommand . register ( [string] ) class FindLearningRate ( Subcommand ) : [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] [EOL] description = [string] [EOL] subparser = parser . add_parser ( self . name , description = description , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] subparser . add_argument ( [string] , [string] , required = True , type = str , help = [string] , ) [EOL] subparser . add_argument ( [string] , [string] , type = str , default = [string] , help = ( [string] [string] [string] ) , ) [EOL] subparser . add_argument ( [string] , type = float , default = [number] , help = [string] ) [EOL] subparser . add_argument ( [string] , type = float , default = [number] , help = [string] ) [EOL] subparser . add_argument ( [string] , type = int , default = [number] , help = [string] , ) [EOL] subparser . add_argument ( [string] , type = float , default = None , help = [string] [string] , ) [EOL] subparser . add_argument ( [string] , action = [string] , help = [string] , ) [EOL] subparser . add_argument ( [string] , [string] , action = [string] , required = False , help = [string] , ) [EOL] subparser . add_argument ( [string] , action = [string] , default = False , help = [string] , ) [EOL] [EOL] subparser . set_defaults ( func = find_learning_rate_from_args ) [EOL] [EOL] return subparser [EOL] [EOL] [EOL] def find_learning_rate_from_args ( args ) : [EOL] [docstring] [EOL] common_logging . FILE_FRIENDLY_LOGGING = args . file_friendly_logging [EOL] params = Params . from_file ( args . param_path , args . overrides ) [EOL] find_learning_rate_model ( params , args . serialization_dir , start_lr = args . start_lr , end_lr = args . end_lr , num_batches = args . num_batches , linear_steps = args . linear , stopping_factor = args . stopping_factor , force = args . force , ) [EOL] [EOL] [EOL] def find_learning_rate_model ( params , serialization_dir , start_lr = [number] , end_lr = [number] , num_batches = [number] , linear_steps = False , stopping_factor = None , force = False , ) : [EOL] [docstring] [EOL] create_serialization_dir ( params , serialization_dir , recover = False , force = force ) [EOL] [EOL] prepare_environment ( params ) [EOL] [EOL] cuda_device = params . params . get ( [string] ) . get ( [string] , - [number] ) [EOL] check_for_gpu ( cuda_device ) [EOL] distributed_params = params . params . get ( [string] ) [EOL] [comment] [EOL] assert not distributed_params , [string] [EOL] [EOL] all_datasets = datasets_from_params ( params ) [EOL] datasets_for_vocab_creation = set ( params . pop ( [string] , all_datasets ) ) [EOL] [EOL] for dataset in datasets_for_vocab_creation : [EOL] if dataset not in all_datasets : [EOL] raise ConfigurationError ( f" [string] { dataset }" ) [EOL] [EOL] logger . info ( [string] , [string] . join ( datasets_for_vocab_creation ) , ) [EOL] vocab = Vocabulary . from_params ( params . pop ( [string] , { } ) , instances = ( instance for key , dataset in all_datasets . items ( ) for instance in dataset if key in datasets_for_vocab_creation ) , ) [EOL] [EOL] train_data = all_datasets [ [string] ] [EOL] train_data . index_with ( vocab ) [EOL] model = Model . from_params ( vocab = vocab , params = params . pop ( [string] ) ) [EOL] data_loader = DataLoader . from_params ( dataset = train_data , params = params . pop ( [string] ) ) [EOL] [EOL] trainer_params = params . pop ( [string] ) [EOL] [EOL] no_grad_regexes = trainer_params . pop ( [string] , ( ) ) [EOL] for name , parameter in model . named_parameters ( ) : [EOL] if any ( re . search ( regex , name ) for regex in no_grad_regexes ) : [EOL] parameter . requires_grad_ ( False ) [EOL] [EOL] trainer_choice = trainer_params . pop ( [string] , [string] ) [EOL] if trainer_choice != [string] : [EOL] raise ConfigurationError ( [string] ) [EOL] trainer = Trainer . from_params ( model = model , serialization_dir = serialization_dir , data_loader = data_loader , params = trainer_params , ) [EOL] [EOL] logger . info ( f" [string] { start_lr } [string] { end_lr } [string] { num_batches } [string] " ) [EOL] learning_rates , losses = search_learning_rate ( trainer , start_lr = start_lr , end_lr = end_lr , num_batches = num_batches , linear_steps = linear_steps , stopping_factor = stopping_factor , ) [EOL] logger . info ( [string] ) [EOL] losses = _smooth ( losses , [number] ) [EOL] [EOL] _save_plot ( learning_rates , losses , os . path . join ( serialization_dir , [string] ) ) [EOL] [EOL] [EOL] def search_learning_rate ( trainer , start_lr = [number] , end_lr = [number] , num_batches = [number] , linear_steps = False , stopping_factor = None , ) : [EOL] [docstring] [EOL] if num_batches <= [number] : [EOL] raise ConfigurationError ( [string] ) [EOL] [EOL] trainer . model . train ( ) [EOL] [EOL] infinite_generator = itertools . cycle ( trainer . data_loader ) [EOL] train_generator_tqdm = Tqdm . tqdm ( infinite_generator , total = num_batches ) [EOL] [EOL] learning_rates = [ ] [EOL] losses = [ ] [EOL] best = [number] [EOL] if linear_steps : [EOL] lr_update_factor = ( end_lr - start_lr ) / num_batches [EOL] else : [EOL] lr_update_factor = ( end_lr / start_lr ) ** ( [number] / num_batches ) [EOL] [EOL] for i , batch in enumerate ( train_generator_tqdm ) : [EOL] [EOL] if linear_steps : [EOL] current_lr = start_lr + ( lr_update_factor * i ) [EOL] else : [EOL] current_lr = start_lr * ( lr_update_factor ** i ) [EOL] [EOL] for param_group in trainer . optimizer . param_groups : [EOL] param_group [ [string] ] = current_lr [EOL] [EOL] trainer . optimizer . zero_grad ( ) [EOL] loss = trainer . batch_outputs ( batch , for_training = True ) [ [string] ] [EOL] loss . backward ( ) [EOL] loss = loss . detach ( ) . cpu ( ) . item ( ) [EOL] [EOL] if stopping_factor is not None and ( math . isnan ( loss ) or loss > stopping_factor * best ) : [EOL] logger . info ( f" [string] { loss } [string] " ) [EOL] break [EOL] [EOL] trainer . rescale_gradients ( ) [EOL] trainer . optimizer . step ( ) [EOL] [EOL] learning_rates . append ( current_lr ) [EOL] losses . append ( loss ) [EOL] [EOL] if loss < best and i > [number] : [EOL] best = loss [EOL] [EOL] if i == num_batches : [EOL] break [EOL] [EOL] return learning_rates , losses [EOL] [EOL] [EOL] def _smooth ( values , beta ) : [EOL] [docstring] [EOL] avg_value = [number] [EOL] smoothed = [ ] [EOL] for i , value in enumerate ( values ) : [EOL] avg_value = beta * avg_value + ( [number] - beta ) * value [EOL] smoothed . append ( avg_value / ( [number] - beta ** ( i + [number] ) ) ) [EOL] return smoothed [EOL] [EOL] [EOL] def _save_plot ( learning_rates , losses , save_path ) : [EOL] [EOL] try : [EOL] import matplotlib [EOL] [EOL] matplotlib . use ( [string] ) [comment] [EOL] import matplotlib . pyplot as plt [EOL] [EOL] except ModuleNotFoundError as error : [EOL] [EOL] logger . warn ( [string] ) [EOL] raise error [EOL] [EOL] plt . ylabel ( [string] ) [EOL] plt . xlabel ( [string] ) [EOL] plt . xscale ( [string] ) [EOL] plt . plot ( learning_rates , losses ) [EOL] logger . info ( f" [string] { save_path } [string] " ) [EOL] plt . savefig ( save_path ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse._SubParsersAction$ 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Tuple[typing.List[builtins.float],typing.List[builtins.float]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , Union [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import builtins [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import json [EOL] import logging [EOL] from typing import Any , Dict [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . common import logging as common_logging [EOL] from allennlp . common . util import prepare_environment [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data import DataLoader [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . training . util import evaluate [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Subcommand . register ( [string] ) class Evaluate ( Subcommand ) : [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] description = [string] [EOL] subparser = parser . add_parser ( self . name , description = description , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] , ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , help = [string] ) [EOL] [EOL] cuda_device = subparser . add_mutually_exclusive_group ( required = False ) [EOL] cuda_device . add_argument ( [string] , type = int , default = - [number] , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , [string] , type = str , default = [string] , help = ( [string] [string] [string] ) , ) [EOL] [EOL] subparser . add_argument ( [string] , type = int , help = [string] ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , default = [string] , help = [string] , ) [EOL] [EOL] subparser . add_argument ( [string] , action = [string] , default = False , help = [string] [string] [string] , ) [EOL] [EOL] subparser . add_argument ( [string] , type = str , default = [string] , help = [string] [string] [string] [string] , ) [EOL] subparser . add_argument ( [string] , action = [string] , default = False , help = [string] , ) [EOL] [EOL] subparser . set_defaults ( func = evaluate_from_args ) [EOL] [EOL] return subparser [EOL] [EOL] [EOL] def evaluate_from_args ( args ) : [EOL] common_logging . FILE_FRIENDLY_LOGGING = args . file_friendly_logging [EOL] [EOL] [comment] [EOL] logging . getLogger ( [string] ) . disabled = True [EOL] logging . getLogger ( [string] ) . disabled = True [EOL] logging . getLogger ( [string] ) . setLevel ( logging . INFO ) [EOL] [EOL] [comment] [EOL] archive = load_archive ( args . archive_file , weights_file = args . weights_file , cuda_device = args . cuda_device , overrides = args . overrides , ) [EOL] config = archive . config [EOL] prepare_environment ( config ) [EOL] model = archive . model [EOL] model . eval ( ) [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] validation_dataset_reader_params = config . pop ( [string] , None ) [EOL] if validation_dataset_reader_params is not None : [EOL] dataset_reader = DatasetReader . from_params ( validation_dataset_reader_params ) [EOL] else : [EOL] dataset_reader = DatasetReader . from_params ( config . pop ( [string] ) ) [EOL] evaluation_data_path = args . input_file [EOL] logger . info ( [string] , evaluation_data_path ) [EOL] instances = dataset_reader . read ( evaluation_data_path ) [EOL] [EOL] embedding_sources = ( json . loads ( args . embedding_sources_mapping ) if args . embedding_sources_mapping else { } ) [EOL] [EOL] if args . extend_vocab : [EOL] logger . info ( [string] ) [EOL] model . vocab . extend_from_instances ( instances = instances ) [EOL] model . extend_embedder_vocab ( embedding_sources ) [EOL] [EOL] instances . index_with ( model . vocab ) [EOL] data_loader_params = config . pop ( [string] , None ) [EOL] if data_loader_params is None : [EOL] data_loader_params = config . pop ( [string] ) [EOL] if args . batch_size : [EOL] data_loader_params [ [string] ] = args . batch_size [EOL] data_loader = DataLoader . from_params ( dataset = instances , params = data_loader_params ) [EOL] [EOL] metrics = evaluate ( model , data_loader , args . cuda_device , args . batch_weight_key , output_file = args . output_file , predictions_output_file = args . predictions_output_file , ) [EOL] [EOL] logger . info ( [string] ) [EOL] [EOL] return metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse._SubParsersAction$ 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._MutuallyExclusiveGroup$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 $argparse._MutuallyExclusiveGroup$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Generator , Any , List [EOL] import logging [EOL] import typing [EOL] import argparse [EOL] [docstring] [EOL] [EOL] import argparse [EOL] import json [EOL] import logging [EOL] import os [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands . subcommand import Subcommand [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ Subcommand . register ( [string] ) class PrintResults ( Subcommand ) : [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] [EOL] description = [string] [EOL] subparser = parser . add_parser ( self . name , description = description , help = [string] , ) [EOL] subparser . add_argument ( [string] , type = str , help = [string] , ) [EOL] [EOL] subparser . add_argument ( [string] , [string] , type = str , nargs = [string] , help = [string] [string] , default = None , required = False , ) [EOL] subparser . add_argument ( [string] , [string] , type = str , help = [string] , default = [string] , required = False , ) [EOL] [EOL] subparser . set_defaults ( func = print_results_from_args ) [EOL] return subparser [EOL] [EOL] [EOL] def print_results_from_args ( args ) : [EOL] [docstring] [EOL] path = args . path [EOL] metrics_name = args . metrics_filename [EOL] keys = args . keys [EOL] [EOL] results_dict = { } [EOL] for root , _ , files in os . walk ( path ) : [EOL] if metrics_name in files : [EOL] full_name = os . path . join ( root , metrics_name ) [EOL] with open ( full_name ) as file_ : [EOL] metrics = json . load ( file_ ) [EOL] results_dict [ full_name ] = metrics [EOL] [EOL] sorted_keys = sorted ( list ( results_dict . keys ( ) ) ) [EOL] print ( f" [string] { [string] . join ( keys ) }" ) [EOL] for name in sorted_keys : [EOL] results = results_dict [ name ] [EOL] keys_to_print = ( str ( results . get ( key , [string] ) ) for key in keys ) [EOL] print ( f"{ name } [string] { [string] . join ( keys_to_print ) }" ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 $argparse._SubParsersAction$ 0 0 0 0 $builtins.str$ 0 0 0 $argparse.ArgumentParser$ 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , Set [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import builtins [EOL] import argparse [EOL] import logging [EOL] from typing import Any , Optional [EOL] [EOL] from overrides import overrides [EOL] [EOL] from allennlp import __version__ [EOL] from allennlp . commands . evaluate import Evaluate [EOL] from allennlp . commands . find_learning_rate import FindLearningRate [EOL] from allennlp . commands . predict import Predict [EOL] from allennlp . commands . print_results import PrintResults [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . commands . test_install import TestInstall [EOL] from allennlp . commands . train import Train [EOL] from allennlp . common . plugins import import_plugins [EOL] from allennlp . common . util import import_module_and_submodules [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] class ArgumentParserWithDefaults ( argparse . ArgumentParser ) : [EOL] [docstring] [EOL] [EOL] _action_defaults_to_ignore = { [string] , [string] , [string] , [string] } [EOL] [EOL] @ staticmethod def _is_empty_default ( default ) : [EOL] if default is None : [EOL] return True [EOL] if isinstance ( default , ( str , list , tuple , set ) ) : [EOL] return not bool ( default ) [EOL] return False [EOL] [EOL] @ overrides def add_argument ( self , * args , ** kwargs ) : [EOL] [comment] [EOL] default = kwargs . get ( [string] ) [EOL] if kwargs . get ( [string] ) not in self . _action_defaults_to_ignore and not self . _is_empty_default ( default ) : [EOL] description = kwargs . get ( [string] , [string] ) [EOL] kwargs [ [string] ] = f"{ description } [string] { default } [string] " [EOL] super ( ) . add_argument ( * args , ** kwargs ) [EOL] [EOL] [EOL] def create_parser ( prog = None ) : [EOL] [docstring] [EOL] parser = ArgumentParserWithDefaults ( description = [string] , prog = prog ) [EOL] parser . add_argument ( [string] , action = [string] , version = f" [string] { __version__ }" ) [EOL] [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] [EOL] for subcommand_name in sorted ( Subcommand . list_available ( ) ) : [EOL] subcommand_class = Subcommand . by_name ( subcommand_name ) [EOL] subcommand = subcommand_class ( ) [EOL] subparser = subcommand . add_subparser ( subparsers ) [EOL] subparser . add_argument ( [string] , type = str , action = [string] , default = [ ] , help = [string] , ) [EOL] [EOL] return parser [EOL] [EOL] [EOL] def main ( prog = None ) : [EOL] [docstring] [EOL] import_plugins ( ) [EOL] [EOL] parser = create_parser ( prog ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] if [string] in dir ( args ) : [EOL] [comment] [EOL] for package_name in args . include_package : [EOL] import_module_and_submodules ( package_name ) [EOL] args . func ( args ) [EOL] else : [EOL] parser . print_help ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any [EOL] import logging [EOL] import builtins [EOL] import typing [EOL] import glob [EOL] import logging [EOL] import os [EOL] import re [EOL] import shutil [EOL] import sys [EOL] import tempfile [EOL] [EOL] sys . path . insert ( [number] , os . path . dirname ( os . path . abspath ( os . path . join ( __file__ , os . pardir ) ) ) ) [EOL] from allennlp . commands . test_install import _get_module_root [EOL] from allennlp . commands . train import train_model_from_file , train_model [EOL] from allennlp . common import Params [EOL] from allennlp . common . util import pushd [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] def train_fixture ( config_prefix , config_filename = [string] ) : [EOL] config_file = config_prefix + config_filename [EOL] serialization_dir = config_prefix + [string] [EOL] [comment] [EOL] [comment] [EOL] if os . path . exists ( serialization_dir ) : [EOL] shutil . rmtree ( serialization_dir ) [EOL] [EOL] [comment] [EOL] train_model_from_file ( config_file , serialization_dir ) [EOL] [EOL] [comment] [EOL] shutil . rmtree ( os . path . join ( serialization_dir , [string] ) ) [EOL] [EOL] for filename in glob . glob ( os . path . join ( serialization_dir , [string] ) ) : [EOL] if ( filename . endswith ( [string] ) or filename . endswith ( [string] ) or re . search ( [string] , filename ) ) : [EOL] os . remove ( filename ) [EOL] [EOL] [EOL] def train_fixture_gpu ( config_prefix ) : [EOL] config_file = config_prefix + [string] [EOL] serialization_dir = config_prefix + [string] [EOL] params = Params . from_file ( config_file ) [EOL] params [ [string] ] [ [string] ] = [number] [EOL] [EOL] [comment] [EOL] tempdir = tempfile . gettempdir ( ) [EOL] train_model ( params , tempdir ) [EOL] [EOL] [comment] [EOL] shutil . copy ( os . path . join ( tempdir , [string] ) , os . path . join ( serialization_dir , [string] ) ) [EOL] shutil . copy ( os . path . join ( tempdir , [string] ) , os . path . join ( serialization_dir , [string] ) ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] module_root = _get_module_root ( ) . parent [EOL] with pushd ( module_root , verbose = True ) : [EOL] models = [ ( [string] , [string] ) , [string] , [string] , [string] , ] [EOL] for model in models : [EOL] if isinstance ( model , tuple ) : [EOL] model , config_filename = model [EOL] train_fixture ( f" [string] { model } [string] " , config_filename ) [EOL] else : [EOL] train_fixture ( f" [string] { model } [string] " ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] from datetime import datetime as dt [EOL] import os [EOL] [EOL] from github import Github [EOL] [EOL] [EOL] LABELS_TO_EXEMPT = [ [string] , [string] , [string] , [string] ] [EOL] [EOL] [EOL] def main ( ) : [EOL] g = Github ( os . environ [ [string] ] ) [EOL] repo = g . get_repo ( [string] ) [EOL] open_issues = repo . get_issues ( state = [string] ) [EOL] [EOL] for issue in open_issues : [EOL] if ( issue . milestone is None [EOL] and not issue . assignees [EOL] and issue . pull_request is None [EOL] and ( dt . utcnow ( ) - issue . updated_at ) . days > [number] [EOL] and ( dt . utcnow ( ) - issue . created_at ) . days >= [number] [EOL] and not any ( label . name . lower ( ) in LABELS_TO_EXEMPT for label in issue . get_labels ( ) ) ) : [EOL] print ( [string] , issue ) [EOL] issue . create_comment ( [string] [string] ) [EOL] issue . add_to_labels ( [string] ) [EOL] issue . edit ( state = [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Dict , Any [EOL] import builtins [EOL] import typing [EOL] import requests [EOL] import argparse [EOL] import argparse [EOL] from typing import Dict [EOL] [EOL] import requests [EOL] [EOL] [EOL] def parse_args ( ) : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , choices = [ [string] , [string] , [string] ] ) [EOL] return parser . parse_args ( ) [EOL] [EOL] [EOL] def get_current_version ( ) : [EOL] VERSION = { } [EOL] with open ( [string] , [string] ) as version_file : [EOL] exec ( version_file . read ( ) , VERSION ) [EOL] return [string] + VERSION [ [string] ] [EOL] [EOL] [EOL] def get_latest_version ( ) : [EOL] resp = requests . get ( [string] ) [EOL] return resp . json ( ) [ [number] ] [ [string] ] [EOL] [EOL] [EOL] def get_stable_version ( ) : [EOL] resp = requests . get ( [string] ) [EOL] return resp . json ( ) [ [string] ] [EOL] [EOL] [EOL] def main ( ) : [EOL] opts = parse_args ( ) [EOL] if opts . version_type == [string] : [EOL] print ( get_stable_version ( ) ) [EOL] elif opts . version_type == [string] : [EOL] print ( get_latest_version ( ) ) [EOL] elif opts . version_type == [string] : [EOL] print ( get_current_version ( ) ) [EOL] else : [EOL] raise NotImplementedError [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any , List [EOL] import pathlib [EOL] import typing [EOL] import argparse [EOL] [docstring] [EOL] [EOL] import argparse [EOL] from pathlib import Path [EOL] from typing import Any , List [EOL] [EOL] from ruamel . yaml import YAML [EOL] [EOL] from allennlp . version import VERSION [EOL] [EOL] [EOL] API_TOC_KEY = [string] [EOL] [EOL] [EOL] def parse_args ( ) : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , help = [string] ) [EOL] parser . add_argument ( [string] , help = [string] ) [EOL] parser . add_argument ( [string] , help = [string] ) [EOL] parser . add_argument ( [string] , help = [string] ) [EOL] parser . add_argument ( [string] , type = str , default = f" [string] { VERSION }" ) [EOL] return parser . parse_args ( ) [EOL] [EOL] [EOL] def build_api_toc ( source_path , docs_root ) : [EOL] nav_entries = [ ] [EOL] [EOL] for child in source_path . iterdir ( ) : [EOL] if child . is_dir ( ) : [EOL] nav_subsection = build_api_toc ( child , docs_root ) [EOL] elif child . suffix == [string] : [EOL] nav_subsection = str ( child . relative_to ( docs_root ) ) [EOL] nav_entries . append ( { child . stem : nav_subsection } ) [EOL] [EOL] nav_entries . sort ( key = lambda x : list ( x ) [ [number] ] , reverse = False ) [EOL] return nav_entries [EOL] [EOL] [EOL] def main ( ) : [EOL] yaml = YAML ( ) [EOL] opts = parse_args ( ) [EOL] [EOL] source_yaml = yaml . load ( Path ( opts . source_yaml ) ) [EOL] [EOL] nav_entries = build_api_toc ( Path ( opts . api_docs_path ) , Path ( opts . docs_root ) ) [EOL] [EOL] [comment] [EOL] source_yaml [ [string] ] = f" [string] { opts . docs_version }" [EOL] [EOL] [comment] [EOL] site_nav = source_yaml [ [string] ] [EOL] for nav_obj in site_nav : [EOL] if API_TOC_KEY in nav_obj : [EOL] break [EOL] nav_obj [ API_TOC_KEY ] = nav_entries [EOL] [EOL] with open ( opts . target_yaml , [string] ) as f : [EOL] yaml . dump ( source_yaml , f ) [EOL] [EOL] print ( f"{ opts . target_yaml } [string] " ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] from datetime import datetime as dt [EOL] import os [EOL] [EOL] from github import Github [EOL] [EOL] [EOL] def main ( ) : [EOL] g = Github ( os . environ [ [string] ] ) [EOL] repo = g . get_repo ( [string] ) [EOL] open_issues = repo . get_issues ( state = [string] ) [EOL] [EOL] for issue in open_issues : [EOL] if ( issue . milestone is None [EOL] and issue . assignees [EOL] and issue . pull_request is None [EOL] and ( dt . utcnow ( ) - issue . updated_at ) . days >= [number] ) : [EOL] assignees = [string] . join ( [ f" [string] { user . login }" for user in issue . assignees ] ) [EOL] print ( f" [string] { assignees } [string] { issue }" ) [EOL] issue . create_comment ( f"{ assignees } [string] " [string] ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] main ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] from typing import Any , List [EOL] import logging [EOL] import scripts [EOL] import sqlite3 [EOL] import typing [EOL] import argparse [EOL] import subprocess [EOL] import builtins [EOL] import argparse [EOL] import json [EOL] import logging [EOL] import os [EOL] import random [EOL] import sqlite3 [EOL] import subprocess [EOL] import time [EOL] from enum import Enum [EOL] from logging . handlers import RotatingFileHandler [EOL] from sqlite3 import Connection [EOL] from subprocess import PIPE [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] logger . setLevel ( logging . DEBUG ) [EOL] formatter = logging . Formatter ( fmt = [string] , datefmt = [string] ) [EOL] dot_allennlp_dir = f"{ os . environ [ [string] ] } [string] " [EOL] [comment] [EOL] if not os . path . exists ( dot_allennlp_dir ) : [EOL] os . mkdir ( dot_allennlp_dir ) [EOL] handler = RotatingFileHandler ( f"{ dot_allennlp_dir } [string] " , maxBytes = [number] * [number] , backupCount = [number] ) [EOL] handler . setFormatter ( formatter ) [EOL] logger . addHandler ( handler ) [EOL] [EOL] BEAKER_QUERY_INTERVAL_SECONDS = [number] [EOL] [EOL] [EOL] [comment] [EOL] class BeakerStatus ( Enum ) : [EOL] submitted = [string] [EOL] provisioning = [string] [EOL] initializing = [string] [EOL] running = [string] [EOL] terminating = [string] [EOL] preempted = [string] [EOL] succeeded = [string] [EOL] skipped = [string] [EOL] stopped = [string] [EOL] failed = [string] [EOL] [EOL] def __str__ ( self ) : [EOL] return self . name [EOL] [EOL] def is_end_state ( self ) : [EOL] if self is BeakerStatus . preempted : [EOL] return True [EOL] elif self is BeakerStatus . succeeded : [EOL] return True [EOL] elif self is BeakerStatus . skipped : [EOL] return True [EOL] elif self is BeakerStatus . stopped : [EOL] return True [EOL] elif self is BeakerStatus . failed : [EOL] return True [EOL] else : [EOL] return False [EOL] [EOL] [EOL] class BeakerWrapper : [EOL] def get_status ( self , experiment_id ) : [EOL] command = [ [string] , [string] , [string] , experiment_id ] [EOL] experiment_json = subprocess . check_output ( command ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] experiment_data = json . loads ( experiment_json ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert len ( experiment_data ) == [number] , [string] [EOL] assert ( len ( experiment_data [ [number] ] [ [string] ] ) == [number] ) , [string] [EOL] status = BeakerStatus ( experiment_data [ [number] ] [ [string] ] [ [number] ] [ [string] ] ) [EOL] [comment] [EOL] time . sleep ( BEAKER_QUERY_INTERVAL_SECONDS ) [EOL] return status [EOL] [EOL] def resume ( self , experiment_id ) : [EOL] command = [ [string] , [string] , [string] , f" [string] { experiment_id }" ] [EOL] [comment] [EOL] time . sleep ( BEAKER_QUERY_INTERVAL_SECONDS ) [EOL] return subprocess . check_output ( command , universal_newlines = True ) . strip ( ) [EOL] [EOL] [EOL] def create_table ( connection ) : [EOL] cursor = connection . cursor ( ) [EOL] create_table_statement = [string] [EOL] cursor . execute ( create_table_statement ) [EOL] connection . commit ( ) [EOL] [EOL] [EOL] def start_autoresume ( connection , experiment_id , max_resumes ) : [EOL] cursor = connection . cursor ( ) [EOL] cursor . execute ( [string] , ( experiment_id , experiment_id , max_resumes , [number] ) , ) [EOL] connection . commit ( ) [EOL] [EOL] [EOL] def stop_autoresume ( connection , experiment_id ) : [EOL] cursor = connection . cursor ( ) [EOL] cursor . execute ( [string] , ( experiment_id , ) ) [EOL] result = cursor . fetchall ( ) [EOL] assert result , f" [string] { experiment_id } [string] " [EOL] cursor . execute ( [string] , ( experiment_id , ) ) [EOL] connection . commit ( ) [EOL] [EOL] [EOL] def resume ( connection , beaker ) : [EOL] logger . info ( [string] ) [EOL] [EOL] cursor = connection . cursor ( ) [EOL] cursor . execute ( [string] ) [EOL] experiments = cursor . fetchall ( ) [EOL] [EOL] for experiment_row in experiments : [EOL] experiment_id , original_id , max_resumes , current_resume = experiment_row [EOL] status = beaker . get_status ( experiment_id ) [EOL] if status . is_end_state ( ) : [EOL] stop_autoresume ( connection , experiment_id ) [EOL] if status is BeakerStatus . preempted : [EOL] if current_resume >= max_resumes : [EOL] logger . info ( f" [string] { experiment_id } [string] " f" [string] { max_resumes } [string] { original_id }" ) [EOL] else : [EOL] new_experiment_id = beaker . resume ( experiment_id ) [EOL] logger . info ( f" [string] { experiment_id } [string] " f" [string] { current_resume } [string] { max_resumes } [string] " f"{ new_experiment_id } [string] { original_id }" ) [EOL] cursor . execute ( [string] , ( new_experiment_id , original_id , max_resumes , current_resume + [number] ) , ) [EOL] connection . commit ( ) [EOL] else : [EOL] logger . info ( f" [string] { experiment_id } [string] " f"{ status } [string] { original_id }" ) [EOL] [EOL] [EOL] class Action ( Enum ) : [EOL] start = [string] [EOL] stop = [string] [EOL] resume = [string] [EOL] [EOL] def __str__ ( self ) : [EOL] return self . name [EOL] [EOL] [EOL] def main ( args ) : [EOL] [comment] [EOL] time . sleep ( random . randint ( [number] , args . random_delay_seconds ) ) [EOL] [EOL] db_path = f"{ dot_allennlp_dir } [string] " [EOL] connection = sqlite3 . connect ( db_path ) [EOL] [EOL] [comment] [EOL] cursor = connection . cursor ( ) [EOL] cursor . execute ( [string] ) [EOL] tables = cursor . fetchall ( ) [EOL] if not tables : [EOL] create_table ( connection ) [EOL] [EOL] [comment] [EOL] crontab_l_result = subprocess . run ( [ [string] , [string] ] , universal_newlines = True , stdout = PIPE , stderr = PIPE ) [EOL] if crontab_l_result . returncode == [number] : [EOL] current_crontab = crontab_l_result . stdout [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert [string] in crontab_l_result . stderr , f" [string] { crontab_l_result . stderr }" [EOL] current_crontab = [string] [EOL] [EOL] full_path = os . path . abspath ( __file__ ) [EOL] if full_path not in current_crontab : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] cron_line = ( f" [string] { os . environ [ [string] ] } [string] " f" [string] { full_path } [string] " ) [EOL] new_crontab = current_crontab + cron_line [EOL] subprocess . run ( [ [string] , [string] ] , input = new_crontab , encoding = [string] ) [EOL] [EOL] if args . action is Action . start : [EOL] assert args . experiment_id [EOL] start_autoresume ( connection , args . experiment_id , args . max_resumes ) [EOL] elif args . action is Action . stop : [EOL] assert args . experiment_id [EOL] stop_autoresume ( connection , args . experiment_id ) [EOL] elif args . action is Action . resume : [EOL] beaker = BeakerWrapper ( ) [EOL] resume ( connection , beaker ) [EOL] else : [EOL] raise Exception ( f" [string] { args . action }" ) [EOL] connection . close ( ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] parser = argparse . ArgumentParser ( ) [EOL] parser . add_argument ( [string] , type = Action , choices = list ( Action ) , required = True ) [EOL] parser . add_argument ( [string] , type = str ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] parser . add_argument ( [string] , type = int , default = [number] ) [EOL] args = parser . parse_args ( ) [EOL] [EOL] try : [EOL] main ( args ) [EOL] except Exception : [EOL] [comment] [EOL] [comment] [EOL] logger . exception ( [string] ) [EOL] raise [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 $logging.Formatter$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $logging.handlers.RotatingFileHandler$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.handlers.RotatingFileHandler$ 0 0 0 $logging.Formatter$ 0 0 $logging.Logger$ 0 0 0 $logging.handlers.RotatingFileHandler$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $BeakerStatus$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.bytes$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.bytes$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0
import builtins [EOL] [docstring] [EOL] [EOL] SOME_GLOBAL_VAR = [string] [EOL] [docstring] [EOL] [EOL] [EOL] def func_with_no_args ( ) : [EOL] [docstring] [EOL] return None [EOL] [EOL] [EOL] def func_with_args ( a , b , c = [number] ) : [EOL] [docstring] [EOL] return a + b * c [EOL] [EOL] [EOL] class SomeClass : [EOL] [docstring] [EOL] [EOL] some_class_level_variable = [number] [EOL] [docstring] [EOL] [EOL] some_class_level_var_with_type = [number] [EOL] [EOL] def __init__ ( self ) : [EOL] self . x = [number] [EOL] [EOL] def _private_method ( self ) : [EOL] [docstring] [EOL] pass [EOL] [EOL] def some_method ( self ) : [EOL] [docstring] [EOL] return None [EOL] [EOL] def method_with_alternative_return_section ( self ) : [EOL] [docstring] [EOL] return [number] [EOL] [EOL] def method_with_alternative_return_section3 ( self ) : [EOL] [docstring] [EOL] return [number] [EOL] [EOL] [EOL] class AnotherClassWithReallyLongConstructor : [EOL] def __init__ ( self , a_really_long_argument_name = [number] , another_long_name = [number] , these_variable_names_are_terrible = [string] , ** kwargs , ) : [EOL] self . a = a_really_long_argument_name [EOL] self . b = another_long_name [EOL] self . c = these_variable_names_are_terrible [EOL] self . other = kwargs [EOL] [EOL] [EOL] class _PrivateClass : [EOL] def public_method_on_private_class ( self ) : [EOL] [docstring] [EOL] pass [EOL]	0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 $builtins.float$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.float$ 0 $builtins.float$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any [EOL] import builtins [EOL] import scripts [EOL] import typing [EOL] from typing import Optional [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from scripts . py2md import py2md , Param , DocstringError [EOL] [EOL] [EOL] class TestPy2md ( AllenNlpTestCase ) : [EOL] def test_basic_example ( self , capsys ) : [EOL] py2md ( [string] ) [EOL] captured = capsys . readouterr ( ) [EOL] [EOL] with open ( self . PROJECT_ROOT / [string] / [string] / [string] / [string] ) as f : [EOL] expected = f . read ( ) [EOL] [EOL] assert captured . out . split ( [string] ) == expected . split ( [string] ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , [string] , ) , ( [string] , [string] , ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] , ) def test_param_from_and_to_line ( line_in , line_out ) : [EOL] param = Param . from_line ( line_in ) [EOL] assert param is not None [EOL] assert param . to_line ( ) == line_out [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ [string] , [string] , [string] , [string] , [string] , [string] , ] , ) def test_param_from_bad_line_raises ( line ) : [EOL] with pytest . raises ( DocstringError ) : [EOL] Param . from_line ( line ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import scripts [EOL] import typing [EOL] import unittest [EOL] import pytest [EOL] import sqlite3 [EOL] from unittest . mock import call , Mock [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] from scripts . ai2_internal . resume_daemon import ( BeakerStatus , create_table , handler , logger , resume , start_autoresume , ) [EOL] [EOL] [comment] [EOL] logger . removeHandler ( handler ) [EOL] [EOL] [EOL] class ResumeDaemonTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . connection = sqlite3 . connect ( [string] ) [EOL] create_table ( self . connection ) [EOL] [EOL] def test_create_beaker_status_works ( self ) : [EOL] status = BeakerStatus ( [string] ) [EOL] assert status . name == [string] [EOL] [EOL] def test_create_beaker_status_throws ( self ) : [EOL] with pytest . raises ( ValueError ) : [EOL] status = BeakerStatus ( [string] ) [EOL] assert status . name == [string] [EOL] [EOL] def test_does_nothing_on_empty_db ( self ) : [EOL] beaker = Mock ( ) [EOL] resume ( self . connection , beaker ) [EOL] assert not beaker . method_calls [EOL] [EOL] def test_does_not_resume_a_running_experiment ( self ) : [EOL] beaker = Mock ( ) [EOL] experiment_id = [string] [EOL] start_autoresume ( self . connection , experiment_id , [number] ) [EOL] beaker . get_status . return_value = BeakerStatus . running [EOL] resume ( self . connection , beaker ) [EOL] beaker . get_status . assert_called ( ) [EOL] assert len ( beaker . method_calls ) == [number] [EOL] [EOL] def test_does_not_resume_a_finished_experiment ( self ) : [EOL] beaker = Mock ( ) [EOL] experiment_id = [string] [EOL] start_autoresume ( self . connection , experiment_id , [number] ) [EOL] beaker . get_status . return_value = BeakerStatus . succeeded [EOL] resume ( self . connection , beaker ) [EOL] beaker . get_status . assert_called ( ) [EOL] assert len ( beaker . method_calls ) == [number] [EOL] [EOL] def test_does_resume_a_preempted_experiment ( self ) : [EOL] beaker = Mock ( ) [EOL] experiment_id = [string] [EOL] start_autoresume ( self . connection , experiment_id , [number] ) [EOL] beaker . get_status . return_value = BeakerStatus . preempted [EOL] beaker . resume . return_value = [string] [EOL] resume ( self . connection , beaker ) [EOL] beaker . get_status . assert_called ( ) [EOL] beaker . resume . assert_called ( ) [EOL] assert len ( beaker . method_calls ) == [number] [EOL] [EOL] def test_respects_upper_bound_on_resumes ( self ) : [EOL] beaker = Mock ( ) [EOL] experiment_id = [string] [EOL] start_autoresume ( self . connection , experiment_id , [number] ) [EOL] beaker . get_status . return_value = BeakerStatus . preempted [EOL] for i in range ( [number] ) : [EOL] beaker . resume . return_value = f" [string] { i }" [EOL] resume ( self . connection , beaker ) [EOL] calls = [ call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , ] [EOL] beaker . assert_has_calls ( calls ) [EOL] [EOL] def test_handles_a_realistic_scenario ( self ) : [EOL] beaker = Mock ( ) [EOL] experiment_id = [string] [EOL] start_autoresume ( self . connection , experiment_id , [number] ) [EOL] beaker . get_status . return_value = BeakerStatus . preempted [EOL] for i in range ( [number] ) : [EOL] beaker . resume . return_value = f" [string] { i }" [EOL] if i == [number] : [EOL] beaker . get_status . return_value = BeakerStatus . succeeded [EOL] resume ( self . connection , beaker ) [EOL] calls = [ call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , call . resume ( [string] ) , call . get_status ( [string] ) , ] [EOL] beaker . assert_has_calls ( calls ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 0 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 0 0 0 0 $scripts.ai2_internal.resume_daemon.BeakerStatus$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $unittest.mock.Mock$ 0 0 0 $typing.List[typing.Any]$ 0 0
from typing import Pattern [EOL] import builtins [EOL] import typing [EOL] import re [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . version import VERSION [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] VALID_VERSION_RE = re . compile ( [string] [string] [string] [string] [string] [string] [string] [string] ) [EOL] [EOL] [EOL] def is_valid ( version ) : [EOL] return VALID_VERSION_RE . match ( version ) is not None [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( [string] , True ) , ( [string] , True ) , ( [string] , True ) , ( [string] , True ) , ( [string] , True ) , ( [string] , True ) , ( [string] , False ) , ( [string] , False ) , ( [string] , False ) , ] , ) def test_is_valid_helper ( version , valid ) : [EOL] assert is_valid ( version ) is valid [EOL] [EOL] [EOL] def test_version ( ) : [EOL] [docstring] [EOL] assert is_valid ( VERSION ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Optional , Any , Set , Dict , List , Union , Mapping , Iterable , Type , Tuple [EOL] import tests [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from typing import Dict , Iterable , List , Mapping , Optional , Set , Tuple , Union [EOL] [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import Lazy , Params , Registrable [EOL] from allennlp . common . from_params import FromParams , takes_arg , remove_optional , create_kwargs [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import DataLoader , DatasetReader , Tokenizer [EOL] from allennlp . models import Model [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . common . checks import ConfigurationError [EOL] [EOL] [EOL] class MyClass ( FromParams ) : [EOL] def __init__ ( self , my_int , my_bool = False ) : [EOL] self . my_int = my_int [EOL] self . my_bool = my_bool [EOL] [EOL] [EOL] class TestFromParams ( AllenNlpTestCase ) : [EOL] def test_takes_arg ( self ) : [EOL] def bare_function ( some_input ) : [EOL] return some_input + [number] [EOL] [EOL] assert takes_arg ( bare_function , [string] ) [EOL] assert not takes_arg ( bare_function , [string] ) [EOL] [EOL] class SomeClass : [EOL] total = [number] [EOL] [EOL] def __init__ ( self , constructor_param ) : [EOL] self . constructor_param = constructor_param [EOL] [EOL] def check_param ( self , check ) : [EOL] return self . constructor_param == check [EOL] [EOL] @ classmethod def set_total ( cls , new_total ) : [EOL] cls . total = new_total [EOL] [EOL] assert takes_arg ( SomeClass , [string] ) [EOL] assert takes_arg ( SomeClass , [string] ) [EOL] assert not takes_arg ( SomeClass , [string] ) [EOL] [EOL] assert takes_arg ( SomeClass . check_param , [string] ) [EOL] assert not takes_arg ( SomeClass . check_param , [string] ) [EOL] [EOL] assert takes_arg ( SomeClass . set_total , [string] ) [EOL] assert not takes_arg ( SomeClass . set_total , [string] ) [EOL] [EOL] def test_remove_optional ( self ) : [EOL] optional_type = Optional [ Dict [ str , str ] ] [EOL] bare_type = remove_optional ( optional_type ) [comment] [EOL] bare_bare_type = remove_optional ( bare_type ) [EOL] [EOL] assert bare_type == Dict [ str , str ] [EOL] assert bare_bare_type == Dict [ str , str ] [EOL] [EOL] assert remove_optional ( Optional [ str ] ) == str [EOL] assert remove_optional ( str ) == str [EOL] [EOL] def test_from_params ( self ) : [EOL] my_class = MyClass . from_params ( Params ( { [string] : [number] } ) , my_bool = True ) [EOL] [EOL] assert isinstance ( my_class , MyClass ) [EOL] assert my_class . my_int == [number] [EOL] assert my_class . my_bool [EOL] [EOL] def test_good_error_message_when_passing_non_params ( self ) : [EOL] from allennlp . nn import InitializerApplicator [EOL] [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : [ [ [string] , [string] ] , [ [string] , [string] ] ] } ) [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] InitializerApplicator . from_params ( params = params . pop ( [string] ) ) [EOL] [EOL] def test_create_kwargs ( self ) : [EOL] kwargs = create_kwargs ( MyClass , MyClass , Params ( { [string] : [number] } ) , my_bool = True , my_float = [number] ) [EOL] [EOL] [comment] [EOL] assert kwargs == { [string] : [number] , [string] : True } [EOL] [EOL] def test_extras ( self ) : [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size , name ) : [EOL] self . size = size [EOL] self . name = name [EOL] [EOL] @ A . register ( [string] ) class C ( A ) : [EOL] def __init__ ( self , size , name ) : [EOL] self . size = size [EOL] self . name = name [EOL] [EOL] [comment] [EOL] @ classmethod def from_params ( cls , params , size , ** extras ) : [comment] [EOL] name = params . pop ( [string] ) [EOL] return cls ( size = size , name = name ) [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : [string] , [string] : [number] } ) [EOL] b = A . from_params ( params , name = [string] ) [EOL] [EOL] assert b . name == [string] [EOL] assert b . size == [number] [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : [string] , [string] : [number] } ) [EOL] b = A . from_params ( params , name = [string] , unwanted = True ) [EOL] [EOL] assert b . name == [string] [EOL] assert b . size == [number] [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : [string] , [string] : [string] } ) [EOL] c = A . from_params ( params , size = [number] ) [EOL] assert c . name == [string] [EOL] assert c . size == [number] [EOL] [EOL] [comment] [EOL] params = Params ( { [string] : [string] , [string] : [string] } ) [EOL] c = A . from_params ( params , size = [number] , unwanted = True ) [EOL] [EOL] assert c . name == [string] [EOL] assert c . size == [number] [EOL] [EOL] def test_extras_for_custom_classes ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class BaseClass ( Registrable ) : [EOL] pass [EOL] [EOL] class BaseClass2 ( Registrable ) : [EOL] pass [EOL] [EOL] @ BaseClass . register ( [string] ) class A ( BaseClass ) : [EOL] def __init__ ( self , a , b , val ) : [EOL] self . a = a [EOL] self . b = b [EOL] self . val = val [EOL] [EOL] def __hash__ ( self ) : [EOL] return self . b [EOL] [EOL] def __eq__ ( self , other ) : [EOL] return self . b == other . b [EOL] [EOL] @ classmethod def from_params ( cls , params , a , ** extras ) : [comment] [EOL] [comment] [EOL] b = params . pop_int ( [string] ) [EOL] val = params . pop ( [string] , [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( a = a , b = b , val = val ) [EOL] [EOL] @ BaseClass2 . register ( [string] ) class B ( BaseClass2 ) : [EOL] def __init__ ( self , c , b ) : [EOL] self . c = c [EOL] self . b = b [EOL] [EOL] @ classmethod def from_params ( cls , params , c , ** extras ) : [comment] [EOL] b = params . pop_int ( [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] return cls ( c = c , b = b ) [EOL] [EOL] @ BaseClass . register ( [string] ) class E ( BaseClass ) : [EOL] def __init__ ( self , m , n ) : [EOL] self . m = m [EOL] self . n = n [EOL] [EOL] @ classmethod def from_params ( cls , params , ** extras2 ) : [comment] [EOL] m = params . pop_int ( [string] ) [EOL] params . assert_empty ( cls . __name__ ) [EOL] n = extras2 [ [string] ] [EOL] return cls ( m = m , n = n ) [EOL] [EOL] class C : [EOL] pass [EOL] [EOL] @ BaseClass . register ( [string] ) class D ( BaseClass ) : [EOL] def __init__ ( self , arg1 , arg2 , arg3 , arg4 , arg5 , ) : [EOL] self . arg1 = arg1 [EOL] self . arg2 = arg2 [EOL] self . arg3 = arg3 [EOL] self . arg4 = arg4 [EOL] self . arg5 = arg5 [EOL] [EOL] vals = [ [number] , [number] , [number] ] [EOL] params = Params ( { [string] : [string] , [string] : [ { [string] : [string] , [string] : vals [ [number] ] } , { [string] : [string] , [string] : vals [ [number] ] } , { [string] : [string] , [string] : vals [ [number] ] } , ] , [string] : [ { [string] : [string] , [string] : vals [ [number] ] } , { [string] : [string] , [string] : vals [ [number] ] } ] , [string] : { [string] : { [string] : [string] , [string] : vals [ [number] ] } , [string] : { [string] : [string] , [string] : vals [ [number] ] } , } , [string] : [ { [string] : [string] , [string] : vals [ [number] ] , [string] : [string] } , { [string] : [string] , [string] : vals [ [number] ] , [string] : [string] } , { [string] : [string] , [string] : vals [ [number] ] , [string] : [string] } , ] , [string] : [ { [string] : [string] , [string] : [number] } ] , } ) [EOL] extra = C ( ) [EOL] tval1 = [number] [EOL] tval2 = [number] [EOL] d = BaseClass . from_params ( params = params , extra = extra , a = tval1 , c = tval2 , n = [number] ) [EOL] [EOL] [comment] [EOL] assert len ( d . arg1 ) == len ( vals ) [EOL] assert isinstance ( d . arg1 , list ) [EOL] assert isinstance ( d . arg1 [ [number] ] , A ) [EOL] assert all ( x . b == y for x , y in zip ( d . arg1 , vals ) ) [EOL] assert all ( x . a == tval1 for x in d . arg1 ) [EOL] [EOL] [comment] [EOL] assert isinstance ( d . arg2 , tuple ) [EOL] assert isinstance ( d . arg2 [ [number] ] , A ) [EOL] assert isinstance ( d . arg2 [ [number] ] , B ) [EOL] assert d . arg2 [ [number] ] . a == tval1 [EOL] assert d . arg2 [ [number] ] . c == tval2 [EOL] assert d . arg2 [ [number] ] . b == d . arg2 [ [number] ] . b == vals [ [number] ] [EOL] [EOL] [comment] [EOL] assert isinstance ( d . arg3 , dict ) [EOL] assert isinstance ( d . arg3 [ [string] ] , A ) [EOL] assert d . arg3 [ [string] ] . a == d . arg3 [ [string] ] . a == tval1 [EOL] assert d . arg3 [ [string] ] . b == vals [ [number] ] [EOL] assert d . arg3 [ [string] ] . b == vals [ [number] ] [EOL] [EOL] [comment] [EOL] assert isinstance ( d . arg4 , set ) [EOL] assert len ( d . arg4 ) == [number] [EOL] assert any ( x . val == [string] for x in d . arg4 ) [EOL] assert any ( x . val == [string] for x in d . arg4 ) [EOL] [EOL] [comment] [EOL] assert isinstance ( d . arg5 , list ) [EOL] assert isinstance ( d . arg5 [ [number] ] , E ) [EOL] assert d . arg5 [ [number] ] . m == [number] [EOL] assert d . arg5 [ [number] ] . n == [number] [EOL] [EOL] def test_no_constructor ( self ) : [EOL] params = Params ( { [string] : [string] } ) [EOL] [EOL] Tokenizer . from_params ( params ) [EOL] [EOL] def test_union ( self ) : [EOL] class A ( FromParams ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] class B ( FromParams ) : [EOL] def __init__ ( self , b ) : [EOL] [comment] [EOL] [comment] [EOL] self . b = b [EOL] [EOL] params = Params ( { [string] : [number] } ) [EOL] a = A . from_params ( params ) [EOL] assert a . a == [number] [EOL] [EOL] params = Params ( { [string] : [ [number] , [number] , [number] ] } ) [EOL] a = A . from_params ( params ) [EOL] assert a . a == [ [number] , [number] , [number] ] [EOL] [EOL] params = Params ( { [string] : { [string] : [number] } } ) [EOL] b = B . from_params ( params ) [EOL] assert isinstance ( b . b , A ) [EOL] assert b . b . a == [number] [EOL] [EOL] params = Params ( { [string] : [ { [string] : [number] } , { [string] : [ [number] , [number] ] } ] } ) [EOL] b = B . from_params ( params ) [EOL] assert isinstance ( b . b , list ) [EOL] assert b . b [ [number] ] . a == [number] [EOL] assert b . b [ [number] ] . a == [ [number] , [number] ] [EOL] [EOL] def test_crazy_nested_union ( self ) : [EOL] class A ( FromParams ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] class B ( FromParams ) : [EOL] def __init__ ( self , b ) : [EOL] [comment] [EOL] [comment] [EOL] self . b = b [EOL] [EOL] class C ( FromParams ) : [EOL] def __init__ ( self , c ) : [EOL] [comment] [EOL] [comment] [EOL] self . c = c [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : { [string] : { [string] : [number] } , [string] : { [string] : [ [number] , [number] ] } } } ) [EOL] c = C . from_params ( params ) [EOL] assert isinstance ( c . c , dict ) [EOL] assert c . c [ [string] ] . a == [number] [EOL] assert c . c [ [string] ] . a == [ [number] , [number] ] [EOL] [EOL] def test_union_of_castable_types ( self ) : [EOL] class IntFloat ( FromParams ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] class FloatInt ( FromParams ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] float_param_str = [string] [EOL] int_param_str = [string] [EOL] import json [EOL] [EOL] for expected_type , param_str in [ ( int , int_param_str ) , ( float , float_param_str ) ] : [EOL] for cls in [ IntFloat , FloatInt ] : [EOL] c = cls . from_params ( Params ( json . loads ( param_str ) ) ) [EOL] assert type ( c . a ) == expected_type [EOL] [EOL] def test_invalid_type_conversions ( self ) : [EOL] class A ( FromParams ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] with pytest . raises ( TypeError ) : [EOL] A . from_params ( Params ( { [string] : [string] } ) ) [EOL] with pytest . raises ( TypeError ) : [EOL] A . from_params ( Params ( { [string] : [number] } ) ) [EOL] [EOL] def test_dict ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size ) : [EOL] self . size = size [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : { [string] : { [string] : [string] , [string] : [number] } , [string] : { [string] : [string] , [string] : [number] } } , } ) [EOL] d = C . from_params ( params ) [EOL] [EOL] assert isinstance ( d . items , dict ) [EOL] assert len ( d . items ) == [number] [EOL] assert all ( isinstance ( key , str ) for key in d . items . keys ( ) ) [EOL] assert all ( isinstance ( value , B ) for value in d . items . values ( ) ) [EOL] assert d . items [ [string] ] . size == [number] [EOL] assert d . items [ [string] ] . size == [number] [EOL] [EOL] def test_dict_not_params ( self ) : [EOL] class A ( FromParams ) : [EOL] def __init__ ( self , counts ) : [EOL] self . counts = counts [EOL] [EOL] params = Params ( { [string] : { [string] : [number] , [string] : [number] } } ) [EOL] a = A . from_params ( params ) [EOL] [EOL] assert isinstance ( a . counts , dict ) [EOL] assert not isinstance ( a . counts , Params ) [EOL] [EOL] def test_list ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size ) : [EOL] self . size = size [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [ { [string] : [string] , [string] : [number] } , { [string] : [string] , [string] : [number] } ] } ) [EOL] d = C . from_params ( params ) [EOL] [EOL] assert isinstance ( d . items , list ) [EOL] assert len ( d . items ) == [number] [EOL] assert all ( isinstance ( item , B ) for item in d . items ) [EOL] assert d . items [ [number] ] . size == [number] [EOL] assert d . items [ [number] ] . size == [number] [EOL] [EOL] def test_tuple ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size ) : [EOL] self . size = size [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , name ) : [EOL] self . name = name [EOL] [EOL] class E ( Registrable ) : [EOL] pass [EOL] [EOL] @ E . register ( [string] ) class F ( E ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [ { [string] : [string] , [string] : [number] } , { [string] : [string] , [string] : [string] } ] } ) [EOL] f = E . from_params ( params ) [EOL] [EOL] assert isinstance ( f . items , tuple ) [EOL] assert len ( f . items ) == [number] [EOL] assert isinstance ( f . items [ [number] ] , B ) [EOL] assert isinstance ( f . items [ [number] ] , D ) [EOL] assert f . items [ [number] ] . size == [number] [EOL] assert f . items [ [number] ] . name == [string] [EOL] [EOL] def test_set ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] def __init__ ( self , name ) : [EOL] self . name = name [EOL] [EOL] def __eq__ ( self , other ) : [EOL] return self . name == other . name [EOL] [EOL] def __hash__ ( self ) : [EOL] return hash ( self . name ) [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] pass [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [ { [string] : [string] , [string] : [string] } , { [string] : [string] , [string] : [string] } , { [string] : [string] , [string] : [string] } , ] , } ) [EOL] d = C . from_params ( params ) [EOL] [EOL] assert isinstance ( d . items , set ) [EOL] assert len ( d . items ) == [number] [EOL] assert all ( isinstance ( item , B ) for item in d . items ) [EOL] assert any ( item . name == [string] for item in d . items ) [EOL] assert any ( item . name == [string] for item in d . items ) [EOL] [EOL] def test_transferring_of_modules ( self ) : [EOL] [EOL] model_archive = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] trained_model = load_archive ( model_archive ) . model [EOL] [EOL] config_file = str ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] model_params = Params . from_file ( config_file ) . pop ( [string] ) . as_dict ( quiet = True ) [EOL] [EOL] [comment] [EOL] model_params [ [string] ] = { [string] : { [string] : model_archive , [string] : [string] , [string] : True , } } [EOL] model_params [ [string] ] = { [string] : { [string] : model_archive , [string] : [string] , [string] : False , } } [EOL] [EOL] transfer_model = Model . from_params ( vocab = trained_model . vocab , params = Params ( model_params ) ) [EOL] [EOL] [comment] [EOL] for trained_parameter , transfer_parameter in zip ( trained_model . _text_field_embedder . parameters ( ) , transfer_model . _text_field_embedder . parameters ( ) , ) : [EOL] assert torch . all ( trained_parameter == transfer_parameter ) [EOL] for trained_parameter , transfer_parameter in zip ( trained_model . _seq2seq_encoder . parameters ( ) , transfer_model . _seq2seq_encoder . parameters ( ) , ) : [EOL] assert torch . all ( trained_parameter == transfer_parameter ) [EOL] [comment] [EOL] for trained_parameter , transfer_parameter in zip ( trained_model . _feedforward . parameters ( ) , transfer_model . _feedforward . parameters ( ) , ) : [EOL] assert torch . all ( trained_parameter != transfer_parameter ) [EOL] [EOL] [comment] [EOL] for parameter in transfer_model . _text_field_embedder . parameters ( ) : [EOL] assert not parameter . requires_grad [EOL] [EOL] [comment] [EOL] for parameter in transfer_model . _seq2seq_encoder . parameters ( ) : [EOL] assert parameter . requires_grad [EOL] [EOL] def test_transferring_of_modules_ensures_type_consistency ( self ) : [EOL] [EOL] model_archive = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] trained_model = load_archive ( model_archive ) . model [EOL] [EOL] config_file = str ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] model_params = Params . from_file ( config_file ) . pop ( [string] ) . as_dict ( quiet = True ) [EOL] [EOL] [comment] [EOL] model_params [ [string] ] = { [string] : { [string] : model_archive , [string] : [string] , } } [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Model . from_params ( vocab = trained_model . vocab , params = Params ( model_params ) ) [EOL] [EOL] def test_bare_string_params ( self ) : [EOL] dataset = [ [number] ] [EOL] [EOL] class TestLoader ( Registrable ) : [EOL] @ classmethod def from_partial_objects ( cls , data_loader ) : [EOL] return data_loader . construct ( dataset = dataset ) [EOL] [EOL] TestLoader . register ( [string] , constructor = [string] ) ( TestLoader ) [EOL] [EOL] data_loader = TestLoader . from_params ( Params ( { [string] : [string] , [string] : { [string] : { [string] : [string] , [string] : [number] , [string] : True , [string] : [string] , } } , } ) ) [EOL] assert data_loader . batch_sampler . sampler . __class__ . __name__ == [string] [EOL] assert data_loader . batch_sampler . sampler . data_source is dataset [EOL] [EOL] def test_kwargs_are_passed_to_superclass ( self ) : [EOL] params = Params ( { [string] : [string] , [string] : True , [string] : [string] } ) [EOL] reader = DatasetReader . from_params ( params ) [EOL] assert reader . lazy is True [EOL] assert str ( reader . _cache_directory ) == [string] [EOL] [EOL] def test_kwargs_with_multiple_inheritance ( self ) : [EOL] [comment] [EOL] [comment] [EOL] class A ( Registrable ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] from numbers import Number [EOL] [EOL] @ A . register ( [string] ) class B1 ( A , Number ) : [EOL] def __init__ ( self , b , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . b = b [EOL] [EOL] @ A . register ( [string] ) class B2 ( Number , A ) : [EOL] def __init__ ( self , b , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . b = b [EOL] [EOL] b = B1 . from_params ( params = Params ( { [string] : [number] , [string] : [number] } ) ) [EOL] assert b . b == [number] [EOL] assert b . a == [number] [EOL] [EOL] b = B2 . from_params ( params = Params ( { [string] : [number] , [string] : [number] } ) ) [EOL] assert b . b == [number] [EOL] assert b . a == [number] [EOL] [EOL] def test_only_infer_superclass_params_if_unknown ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class BaseClass ( Registrable ) : [EOL] def __init__ ( self ) : [EOL] self . x = None [EOL] self . a = None [EOL] self . rest = None [EOL] [EOL] @ BaseClass . register ( [string] ) class A ( BaseClass ) : [EOL] def __init__ ( self , a , x , ** kwargs ) : [EOL] super ( ) . __init__ ( ) [EOL] self . x = x [EOL] self . a = a [EOL] self . rest = kwargs [EOL] [EOL] @ BaseClass . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , a , x = [number] , ** kwargs ) : [EOL] super ( ) . __init__ ( x = x , a = - [number] , raw_a = a , ** kwargs ) [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [string] } ) [EOL] [comment] [EOL] [comment] [EOL] instance = BaseClass . from_params ( params ) [EOL] assert instance . x == [number] [EOL] assert instance . a == - [number] [EOL] assert len ( instance . rest ) == [number] [EOL] assert type ( instance . rest [ [string] ] ) == str [EOL] assert instance . rest [ [string] ] == [string] [EOL] [EOL] def test_kwargs_are_passed_to_deeper_superclasses ( self ) : [EOL] [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class BaseClass ( Registrable ) : [EOL] def __init__ ( self ) : [EOL] self . a = None [EOL] self . b = None [EOL] self . c = None [EOL] [EOL] @ BaseClass . register ( [string] ) class A ( BaseClass ) : [EOL] def __init__ ( self , a ) : [EOL] super ( ) . __init__ ( ) [EOL] self . a = a [EOL] [EOL] @ BaseClass . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , b , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . b = b [EOL] [EOL] @ BaseClass . register ( [string] ) class C ( B ) : [EOL] def __init__ ( self , c , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . c = c [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [string] , [string] : [string] , [string] : [string] } ) [EOL] [EOL] instance = BaseClass . from_params ( params ) [EOL] assert instance . a == [string] [EOL] assert instance . b == [string] [EOL] assert instance . c == [string] [EOL] [EOL] def test_lazy_construction_can_happen_multiple_times ( self ) : [EOL] test_string = [string] [EOL] extra_string = [string] [EOL] [EOL] class ConstructedObject ( FromParams ) : [EOL] def __init__ ( self , string , extra ) : [EOL] self . string = string [EOL] self . extra = extra [EOL] [EOL] class Testing ( FromParams ) : [EOL] def __init__ ( self , lazy_object ) : [EOL] first_time = lazy_object . construct ( extra = extra_string ) [EOL] second_time = lazy_object . construct ( extra = extra_string ) [EOL] assert first_time . string == test_string [EOL] assert first_time . extra == extra_string [EOL] assert second_time . string == test_string [EOL] assert second_time . extra == extra_string [EOL] [EOL] Testing . from_params ( Params ( { [string] : { [string] : test_string } } ) ) [EOL] [EOL] def test_iterable ( self ) : [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size ) : [EOL] self . size = size [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : [ { [string] : [string] , [string] : [number] } , { [string] : [string] , [string] : [number] } ] } ) [EOL] d = C . from_params ( params ) [EOL] [EOL] assert isinstance ( d . items , Iterable ) [EOL] items = list ( d . items ) [EOL] assert len ( items ) == [number] [EOL] assert all ( isinstance ( item , B ) for item in items ) [EOL] assert items [ [number] ] . size == [number] [EOL] assert items [ [number] ] . size == [number] [EOL] [EOL] def test_mapping ( self ) : [EOL] from allennlp . common . registrable import Registrable [EOL] [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] @ A . register ( [string] ) class B ( A ) : [EOL] def __init__ ( self , size ) : [EOL] self . size = size [EOL] [EOL] class C ( Registrable ) : [EOL] pass [EOL] [EOL] @ C . register ( [string] ) class D ( C ) : [EOL] def __init__ ( self , items ) : [EOL] self . items = items [EOL] [EOL] params = Params ( { [string] : [string] , [string] : { [string] : { [string] : [string] , [string] : [number] } , [string] : { [string] : [string] , [string] : [number] } } , } ) [EOL] d = C . from_params ( params ) [EOL] [EOL] assert isinstance ( d . items , Mapping ) [EOL] assert len ( d . items ) == [number] [EOL] assert all ( isinstance ( key , str ) for key in d . items . keys ( ) ) [EOL] assert all ( isinstance ( value , B ) for value in d . items . values ( ) ) [EOL] assert d . items [ [string] ] . size == [number] [EOL] assert d . items [ [string] ] . size == [number] [EOL] [EOL] def test_extra_parameters_are_not_allowed_when_there_is_no_constructor ( self ) : [EOL] class A ( FromParams ) : [EOL] pass [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] A . from_params ( Params ( { [string] : [string] , [string] : [string] } ) ) [EOL] [EOL] def test_raises_when_there_are_no_implementations ( self ) : [EOL] class A ( Registrable ) : [EOL] pass [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] A . from_params ( [string] ) [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] A . from_params ( Params ( { [string] : [string] , [string] : [string] } ) ) [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] A . from_params ( Params ( { } ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] class B ( Registrable ) : [EOL] def __init__ ( self ) : [EOL] pass [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] B . from_params ( [string] ) [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] B . from_params ( Params ( { [string] : [string] , [string] : [string] } ) ) [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] B . from_params ( Params ( { } ) ) [EOL] [EOL] def test_from_params_raises_error_on_wrong_parameter_name_in_optional_union ( self ) : [EOL] class NestedClass ( FromParams ) : [EOL] def __init__ ( self , varname = None ) : [EOL] self . varname = varname [EOL] [EOL] class WrapperClass ( FromParams ) : [EOL] def __init__ ( self , nested_class = None ) : [EOL] if isinstance ( nested_class , str ) : [EOL] nested_class = NestedClass ( varname = nested_class ) [EOL] self . nested_class = nested_class [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] WrapperClass . from_params ( params = Params ( { [string] : { [string] : [string] } } ) ) [EOL] [EOL] def test_from_params_handles_base_class_kwargs ( self ) : [EOL] class Foo ( FromParams ) : [EOL] def __init__ ( self , a , b = None , ** kwargs ) : [EOL] self . a = a [EOL] self . b = b [EOL] for key , value in kwargs . items ( ) : [EOL] setattr ( self , key , value ) [EOL] [EOL] foo = Foo . from_params ( Params ( { [string] : [number] , [string] : [string] } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] [EOL] foo = Foo . from_params ( Params ( { [string] : [number] , [string] : [string] , [string] : { [string] : [string] } } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] assert foo . c == { [string] : [string] } [EOL] [EOL] class Bar ( Foo ) : [EOL] def __init__ ( self , a , b , d , ** kwargs ) : [EOL] super ( ) . __init__ ( a , b = b , ** kwargs ) [EOL] self . d = d [EOL] [EOL] bar = Bar . from_params ( Params ( { [string] : [number] , [string] : [string] , [string] : { [string] : [string] } , [string] : [number] } ) ) [EOL] assert bar . a == [number] [EOL] assert bar . b == [string] [EOL] assert bar . c == { [string] : [string] } [EOL] assert bar . d == [number] [EOL] [EOL] def test_from_params_base_class_kwargs_crashes_if_params_not_handled ( self ) : [EOL] class Bar ( FromParams ) : [EOL] def __init__ ( self , c = None ) : [EOL] self . c = c [EOL] [EOL] class Foo ( Bar ) : [EOL] def __init__ ( self , a , b = None , ** kwargs ) : [EOL] super ( ) . __init__ ( ** kwargs ) [EOL] self . a = a [EOL] self . b = b [EOL] [EOL] foo = Foo . from_params ( Params ( { [string] : [number] , [string] : [string] , [string] : [string] } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] assert foo . c == [string] [EOL] [EOL] with pytest . raises ( TypeError , match = [string] ) : [EOL] Foo . from_params ( Params ( { [string] : [number] , [string] : [string] , [string] : [string] } ) ) [EOL] [EOL] def test_from_params_handles_kwargs_in_non_from_params_registered_class ( self ) : [EOL] class Bar ( Registrable ) : [EOL] pass [EOL] [EOL] class Baz : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] @ Bar . register ( [string] ) class Foo ( Baz ) : [EOL] def __init__ ( self , a , b = None , ** kwargs ) : [EOL] super ( ) . __init__ ( a ) [EOL] self . b = b [EOL] for key , value in kwargs . items ( ) : [EOL] setattr ( self , key , value ) [EOL] [EOL] foo = Bar . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [string] } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] [EOL] foo = Bar . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [string] , [string] : { [string] : [string] } } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] assert foo . c == { [string] : [string] } [EOL] [EOL] def test_from_params_does_not_pass_extras_to_non_from_params_registered_class ( self ) : [EOL] class Bar ( Registrable ) : [EOL] pass [EOL] [EOL] class Baz : [EOL] def __init__ ( self , a , c = None ) : [EOL] self . a = a [EOL] self . c = c [EOL] [EOL] @ Bar . register ( [string] ) class Foo ( Baz ) : [EOL] def __init__ ( self , a , b = None , ** kwargs ) : [EOL] super ( ) . __init__ ( a , ** kwargs ) [EOL] self . b = b [EOL] [EOL] foo = Bar . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [string] } ) ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] assert foo . c is None [EOL] [EOL] foo = Bar . from_params ( params = Params ( { [string] : [string] , [string] : [number] , [string] : [string] , [string] : { [string] : [string] } } ) , extra = [string] ) [EOL] assert foo . a == [number] [EOL] assert foo . b == [string] [EOL] assert foo . c == { [string] : [string] } [EOL] [EOL] def test_from_params_child_has_kwargs_base_implicit_constructor ( self ) : [EOL] class Foo ( FromParams ) : [EOL] pass [EOL] [EOL] class Bar ( Foo ) : [EOL] def __init__ ( self , a , ** kwargs ) : [EOL] self . a = a [EOL] [EOL] bar = Bar . from_params ( Params ( { [string] : [number] } ) ) [EOL] assert bar . a == [number] [EOL] [EOL] def test_from_params_has_args ( self ) : [EOL] class Foo ( FromParams ) : [EOL] def __init__ ( self , a , * args ) : [EOL] self . a = a [EOL] [EOL] foo = Foo . from_params ( Params ( { [string] : [number] } ) ) [EOL] assert foo . a == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $None$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Type[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.MyClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.MyClass$ 0 0 0 0 0 $tests.common.from_params_test.MyClass$ 0 0 0 0 0 0 $tests.common.from_params_test.MyClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"C"$ 0 0 0 $allennlp.common.Params$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras.A$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"A"$ 0 0 0 $allennlp.common.Params$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"B"$ 0 0 0 $allennlp.common.Params$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $"E"$ 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 $allennlp.common.Params$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.C$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.C$ 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.C$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_extras_for_custom_classes.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 $tests.common.from_params_test.TestFromParams.test_union.B$ 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_union.A$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 $tests.common.from_params_test.TestFromParams.test_crazy_nested_union.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[tests.common.from_params_test.TestFromParams.test_union_of_castable_types.FloatInt,tests.common.from_params_test.TestFromParams.test_union_of_castable_types.IntFloat]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Union[tests.common.from_params_test.TestFromParams.test_union_of_castable_types.FloatInt,tests.common.from_params_test.TestFromParams.test_union_of_castable_types.IntFloat]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict_not_params.A$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict_not_params.A$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_dict_not_params.A$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_list.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_tuple.E$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_set.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.DataLoader$ 0 0 0 $allennlp.common.Lazy[allennlp.data.DataLoader]$ 0 0 0 0 $allennlp.common.Lazy[allennlp.data.DataLoader]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.DatasetReader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 $typing.Any$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_only_infer_superclass_params_if_unknown.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_kwargs_are_passed_to_deeper_superclasses.BaseClass$ 0 0 0 0 0 $typing.Any$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_kwargs_are_passed_to_deeper_superclasses.BaseClass$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_kwargs_are_passed_to_deeper_superclasses.BaseClass$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_kwargs_are_passed_to_deeper_superclasses.BaseClass$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Lazy[ConstructedObject]$ 0 0 0 0 0 $allennlp.common.Lazy[ConstructedObject]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.common.Lazy[ConstructedObject]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_iterable.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_iterable.C$ 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $tests.common.from_params_test.TestFromParams.test_iterable.C$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_mapping.C$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Bar$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Bar$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Bar$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Bar$ 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_handles_base_class_kwargs.Bar$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_base_class_kwargs_crashes_if_params_not_handled.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_base_class_kwargs_crashes_if_params_not_handled.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_base_class_kwargs_crashes_if_params_not_handled.Foo$ 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_base_class_kwargs_crashes_if_params_not_handled.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_child_has_kwargs_base_implicit_constructor.Bar$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_child_has_kwargs_base_implicit_constructor.Bar$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_has_args.Foo$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.common.from_params_test.TestFromParams.test_from_params_has_args.Foo$ 0 0 0 0 0
	0
from typing import Set , Any [EOL] import typing [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands import Subcommand [EOL] from allennlp . common . plugins import ( discover_plugins , import_plugins , ) [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import pushd [EOL] [EOL] [EOL] class TestPlugins ( AllenNlpTestCase ) : [EOL] @ overrides def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . plugins_root = self . FIXTURES_ROOT / [string] [EOL] [EOL] def test_no_plugins ( self ) : [EOL] available_plugins = set ( discover_plugins ( ) ) [EOL] assert available_plugins == set ( ) [EOL] [EOL] def test_file_plugin ( self ) : [EOL] available_plugins = set ( discover_plugins ( ) ) [EOL] assert available_plugins == set ( ) [EOL] [EOL] with pushd ( self . plugins_root ) : [EOL] available_plugins = set ( discover_plugins ( ) ) [EOL] assert available_plugins == { [string] } [EOL] [EOL] import_plugins ( ) [EOL] subcommands_available = Subcommand . list_available ( ) [EOL] assert [string] in subcommands_available [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
import logging [EOL] import os [EOL] import logging [EOL] import random [EOL] [EOL] from allennlp . common . logging import AllenNlpLogger [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestLogging ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] logger = logging . getLogger ( str ( random . random ( ) ) ) [EOL] self . test_log_file = os . path . join ( self . TEST_DIR , [string] ) [EOL] logger . addHandler ( logging . FileHandler ( self . test_log_file ) ) [EOL] logger . setLevel ( logging . DEBUG ) [EOL] self . logger = logger [EOL] self . _msg = [string] [EOL] [EOL] def test_debug_once ( self ) : [EOL] self . logger . debug_once ( self . _msg ) [EOL] self . logger . debug_once ( self . _msg ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] def test_info_once ( self ) : [EOL] self . logger . info_once ( self . _msg ) [EOL] self . logger . info_once ( self . _msg ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] def test_warning_once ( self ) : [EOL] self . logger . warning_once ( self . _msg ) [EOL] self . logger . warning_once ( self . _msg ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] def test_error_once ( self ) : [EOL] self . logger . error_once ( self . _msg ) [EOL] self . logger . error_once ( self . _msg ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] def test_critical_once ( self ) : [EOL] self . logger . critical_once ( self . _msg ) [EOL] self . logger . critical_once ( self . _msg ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] def test_debug_once_different_args ( self ) : [EOL] self . logger . debug_once ( [string] , [number] ) [EOL] self . logger . debug_once ( [string] , [number] ) [EOL] [EOL] with open ( self . test_log_file , [string] ) as f : [EOL] assert len ( f . readlines ( ) ) == [number] [EOL] [EOL] assert len ( self . logger . _seen_msgs ) == [number] [EOL] [EOL] def test_getLogger ( self ) : [EOL] logger = logging . getLogger ( [string] ) [EOL] [EOL] assert isinstance ( logger , AllenNlpLogger ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0
from typing import Set , Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase , multi_device [EOL] [EOL] actual_devices = set ( ) [EOL] [EOL] [EOL] class TestTesting ( AllenNlpTestCase ) : [EOL] @ multi_device def test_multi_device ( self , device ) : [EOL] actual_devices . add ( device ) [EOL] [EOL] def test_devices_accounted_for ( self ) : [EOL] expected_devices = { [string] , [string] } if torch . cuda . is_available ( ) else { [string] } [EOL] assert expected_devices == actual_devices [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 $typing.Set[typing.Any]$ 0
from typing import Set , Any , Dict , List , Iterator [EOL] import typing [EOL] import sys [EOL] from collections import OrderedDict [EOL] [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import util [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import push_python_path [EOL] [EOL] [EOL] class Unsanitizable : [EOL] pass [EOL] [EOL] [EOL] class Sanitizable : [EOL] def to_json ( self ) : [EOL] return { [string] : True } [EOL] [EOL] [EOL] class TestCommonUtils ( AllenNlpTestCase ) : [EOL] def test_group_by_count ( self ) : [EOL] assert util . group_by_count ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [number] , [number] ) == [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , ] [EOL] [EOL] def test_lazy_groups_of ( self ) : [EOL] xs = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] groups = util . lazy_groups_of ( iter ( xs ) , group_size = [number] ) [EOL] assert next ( groups ) == [ [number] , [number] , [number] ] [EOL] assert next ( groups ) == [ [number] , [number] , [number] ] [EOL] assert next ( groups ) == [ [number] ] [EOL] with pytest . raises ( StopIteration ) : [EOL] _ = next ( groups ) [EOL] [EOL] def test_pad_sequence_to_length ( self ) : [EOL] assert util . pad_sequence_to_length ( [ [number] , [number] , [number] ] , [number] ) == [ [number] , [number] , [number] , [number] , [number] ] [EOL] assert util . pad_sequence_to_length ( [ [number] , [number] , [number] ] , [number] , default_value = lambda : [number] ) == [ [number] , [number] , [number] , [number] , [number] ] [EOL] assert util . pad_sequence_to_length ( [ [number] , [number] , [number] ] , [number] , padding_on_right = False ) == [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] def test_namespace_match ( self ) : [EOL] assert util . namespace_match ( [string] , [string] ) [EOL] assert util . namespace_match ( [string] , [string] ) [EOL] assert util . namespace_match ( [string] , [string] ) [EOL] assert util . namespace_match ( [string] , [string] ) [EOL] assert not util . namespace_match ( [string] , [string] ) [EOL] [EOL] def test_sanitize ( self ) : [EOL] assert util . sanitize ( torch . Tensor ( [ [number] , [number] ] ) ) == [ [number] , [number] ] [EOL] assert util . sanitize ( torch . LongTensor ( [ [number] , [number] ] ) ) == [ [number] , [number] ] [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] util . sanitize ( Unsanitizable ( ) ) [EOL] [EOL] assert util . sanitize ( Sanitizable ( ) ) == { [string] : True } [EOL] [EOL] def test_import_submodules ( self ) : [EOL] ( self . TEST_DIR / [string] ) . mkdir ( ) [EOL] ( self . TEST_DIR / [string] / [string] ) . touch ( ) [EOL] ( self . TEST_DIR / [string] / [string] ) . mkdir ( ) [EOL] ( self . TEST_DIR / [string] / [string] / [string] ) . touch ( ) [EOL] ( self . TEST_DIR / [string] / [string] / [string] ) . touch ( ) [EOL] [EOL] with push_python_path ( self . TEST_DIR ) : [EOL] assert [string] not in sys . modules [EOL] assert [string] not in sys . modules [EOL] [EOL] util . import_module_and_submodules ( [string] ) [EOL] [EOL] assert [string] in sys . modules [EOL] assert [string] in sys . modules [EOL] assert [string] in sys . modules [EOL] [EOL] def test_get_frozen_and_tunable_parameter_names ( self ) : [EOL] model = torch . nn . Sequential ( OrderedDict ( [ ( [string] , torch . nn . Conv1d ( [number] , [number] , [number] ) ) , ( [string] , torch . nn . Linear ( [number] , [number] ) ) ] ) ) [EOL] named_parameters = dict ( model . named_parameters ( ) ) [EOL] named_parameters [ [string] ] . requires_grad_ ( False ) [EOL] named_parameters [ [string] ] . requires_grad_ ( False ) [EOL] ( frozen_parameter_names , tunable_parameter_names , ) = util . get_frozen_and_tunable_parameter_names ( model ) [EOL] assert set ( frozen_parameter_names ) == { [string] , [string] } [EOL] assert set ( tunable_parameter_names ) == { [string] , [string] } [EOL] [EOL] def test_sanitize_ptb_tokenized_string ( self ) : [EOL] def create_surrounding_test_case ( start_ptb_token , end_ptb_token , start_token , end_token ) : [EOL] return ( [string] . format ( start_ptb_token , end_ptb_token ) , [string] . format ( start_token , end_token ) , ) [EOL] [EOL] def create_fwd_token_test_case ( fwd_token ) : [EOL] return [string] . format ( fwd_token ) , [string] . format ( fwd_token ) [EOL] [EOL] def create_backward_token_test_case ( backward_token ) : [EOL] return [string] . format ( backward_token ) , [string] . format ( backward_token ) [EOL] [EOL] punct_forward = { [string] , [string] , [string] } [EOL] punct_backward = { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] test_cases = [ create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , create_surrounding_test_case ( [string] , [string] , [string] , [string] ) , * [ create_fwd_token_test_case ( t ) for t in punct_forward ] , * [ create_backward_token_test_case ( t ) for t in punct_backward ] , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ( [string] , [string] ) , ] [EOL] [EOL] for ptb_string , expected in test_cases : [EOL] actual = util . sanitize_ptb_tokenized_string ( ptb_string ) [EOL] assert actual == expected [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 $typing.Iterator[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0
from typing import Type , Any , Callable [EOL] import allennlp [EOL] import typing [EOL] import inspect [EOL] import os [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . registrable import Registrable [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import push_python_path [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] from allennlp . data . samplers import Sampler , BatchSampler [EOL] from allennlp . data . token_indexers . token_indexer import TokenIndexer [EOL] from allennlp . data . tokenizers . tokenizer import Tokenizer [EOL] from allennlp . modules . text_field_embedders . text_field_embedder import TextFieldEmbedder [EOL] from allennlp . modules . token_embedders . token_embedder import TokenEmbedder [EOL] from allennlp . nn . regularizers . regularizer import Regularizer [EOL] [EOL] [EOL] class TestRegistrable ( AllenNlpTestCase ) : [EOL] def test_registrable_functionality_works ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] base_class = Tokenizer [EOL] assert [string] not in base_class . list_available ( ) [EOL] [EOL] @ base_class . register ( [string] ) class Fake ( base_class ) : [EOL] [EOL] pass [EOL] [EOL] assert base_class . by_name ( [string] ) == Fake [EOL] [EOL] default = base_class . default_implementation [EOL] if default is not None : [EOL] assert base_class . list_available ( ) [ [number] ] == default [EOL] base_class . default_implementation = [string] [EOL] assert base_class . list_available ( ) [ [number] ] == [string] [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] base_class . default_implementation = [string] [EOL] base_class . list_available ( ) [EOL] base_class . default_implementation = default [EOL] [EOL] [comment] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] [EOL] @ base_class . register ( [string] ) class FakeAlternate ( base_class ) : [EOL] [EOL] pass [EOL] [EOL] [comment] [EOL] [comment] [EOL] @ base_class . register ( [string] , exist_ok = True ) class FakeAlternate ( base_class ) : [EOL] [EOL] pass [EOL] [EOL] assert base_class . by_name ( [string] ) == FakeAlternate [EOL] [EOL] del Registrable . _registry [ base_class ] [ [string] ] [EOL] [EOL] [comment] [EOL] [EOL] def test_registry_has_builtin_samplers ( self ) : [EOL] assert Sampler . by_name ( [string] ) . __name__ == [string] [EOL] assert Sampler . by_name ( [string] ) . __name__ == [string] [EOL] assert BatchSampler . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_registry_has_builtin_tokenizers ( self ) : [EOL] assert Tokenizer . by_name ( [string] ) . __name__ == [string] [EOL] assert Tokenizer . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_registry_has_builtin_token_indexers ( self ) : [EOL] assert TokenIndexer . by_name ( [string] ) . __name__ == [string] [EOL] assert TokenIndexer . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_registry_has_builtin_regularizers ( self ) : [EOL] assert Regularizer . by_name ( [string] ) . __name__ == [string] [EOL] assert Regularizer . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_registry_has_builtin_token_embedders ( self ) : [EOL] assert TokenEmbedder . by_name ( [string] ) . __name__ == [string] [EOL] assert TokenEmbedder . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_registry_has_builtin_text_field_embedders ( self ) : [EOL] assert TextFieldEmbedder . by_name ( [string] ) . __name__ == [string] [EOL] [EOL] def test_implicit_include_package ( self ) : [EOL] [comment] [EOL] packagedir = self . TEST_DIR / [string] [EOL] packagedir . mkdir ( ) [EOL] ( packagedir / [string] ) . touch ( ) [EOL] [EOL] [comment] [EOL] with push_python_path ( self . TEST_DIR ) : [EOL] [comment] [EOL] reader = DatasetReader . by_name ( [string] ) [EOL] [EOL] with open ( inspect . getabsfile ( reader ) ) as f : [EOL] code = f . read ( ) . replace ( [string] , [string] , ) [EOL] [EOL] with open ( os . path . join ( packagedir , [string] ) , [string] ) as f : [EOL] f . write ( code ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) as exc : [EOL] DatasetReader . by_name ( [string] ) [EOL] assert [string] in str ( exc . value ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) as exc : [EOL] DatasetReader . by_name ( [string] ) [EOL] assert [string] in str ( exc . value ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] DatasetReader . by_name ( [string] ) [EOL] assert [string] in str ( exc . value ) [EOL] [EOL] [comment] [EOL] duplicate_reader = DatasetReader . by_name ( [string] ) [EOL] assert duplicate_reader . __name__ == [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 $builtins.str$ 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 $builtins.str$ 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[allennlp.data.tokenizers.tokenizer.Tokenizer]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[...,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[...,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[...,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 0 0 0 0 0 0 0 0 $typing.Callable[...,allennlp.data.dataset_readers.dataset_reader.DatasetReader]$ 0 0 0 0 0
from typing import Optional , Any , List [EOL] import _csv [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import builtins [EOL] import argparse [EOL] import csv [EOL] import io [EOL] import json [EOL] import os [EOL] import pathlib [EOL] import shutil [EOL] import sys [EOL] import tempfile [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . commands import main [EOL] from allennlp . commands . predict import Predict [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import JsonDict , push_python_path [EOL] from allennlp . data . dataset_readers import DatasetReader , TextClassificationJsonReader [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor , TextClassifierPredictor [EOL] [EOL] [EOL] class TestPredict ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . classifier_model_path = ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] self . classifier_data_path = ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] self . tempdir = pathlib . Path ( tempfile . mkdtemp ( ) ) [EOL] self . infile = self . tempdir / [string] [EOL] self . outfile = self . tempdir / [string] [EOL] [EOL] def test_add_predict_subparser ( self ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Predict ( ) . add_subparser ( subparsers ) [EOL] [EOL] kebab_args = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] args = parser . parse_args ( kebab_args ) [EOL] [EOL] assert args . func . __name__ == [string] [EOL] assert args . archive_file == [string] [EOL] assert args . output_file == [string] [EOL] assert args . batch_size == [number] [EOL] assert args . cuda_device == [number] [EOL] assert args . silent [EOL] [EOL] def test_works_with_known_model ( self ) : [EOL] with open ( self . infile , [string] ) as f : [EOL] f . write ( [string] ) [EOL] f . write ( [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . infile ) , [string] , str ( self . outfile ) , [string] , ] [EOL] [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( self . outfile ) [EOL] [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] shutil . rmtree ( self . tempdir ) [EOL] [EOL] def test_using_dataset_reader_works_with_known_model ( self ) : [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , ] [EOL] [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( self . outfile ) [EOL] [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] shutil . rmtree ( self . tempdir ) [EOL] [EOL] def test_uses_correct_dataset_reader ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] @ Predictor . register ( [string] ) class _TestPredictor ( Predictor ) : [EOL] def dump_line ( self , outputs ) : [EOL] data = { [string] : type ( self . _dataset_reader ) . __name__ } [comment] [EOL] return json . dumps ( data ) + [string] [EOL] [EOL] def load_line ( self , line ) : [EOL] raise NotImplementedError [EOL] [EOL] @ DatasetReader . register ( [string] ) class FakeDatasetReader ( TextClassificationJsonReader ) : [EOL] pass [EOL] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] assert results [ [number] ] [ [string] ] == [string] [EOL] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] assert results [ [number] ] [ [string] ] == [string] [EOL] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] assert results [ [number] ] [ [string] ] == [string] [EOL] [EOL] [comment] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , [string] , [string] , ] [EOL] with pytest . raises ( NotImplementedError ) : [EOL] main ( ) [EOL] [EOL] def test_base_predictor ( self ) : [EOL] [comment] [EOL] model_path = str ( self . classifier_model_path ) [EOL] archive = load_archive ( model_path ) [EOL] model_type = archive . config . get ( [string] ) . get ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] from allennlp . models import Model [EOL] [EOL] model_class , _ = Model . resolve_class_name ( model_type ) [EOL] saved_default_predictor = model_class . default_predictor [EOL] model_class . default_predictor = None [EOL] try : [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , model_path , str ( self . classifier_data_path ) , [string] , str ( self . outfile ) , [string] , [string] , ] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] finally : [EOL] model_class . default_predictor = saved_default_predictor [EOL] [EOL] def test_batch_prediction_works_with_known_model ( self ) : [EOL] with open ( self . infile , [string] ) as f : [EOL] f . write ( [string] ) [EOL] f . write ( [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . infile ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , ] [EOL] [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( self . outfile ) [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] shutil . rmtree ( self . tempdir ) [EOL] [EOL] def test_fails_without_required_args ( self ) : [EOL] sys . argv = [ [string] , [string] , [string] , ] [comment] [EOL] [EOL] with pytest . raises ( SystemExit ) as cm : [EOL] main ( ) [EOL] [EOL] assert cm . value . code == [number] [comment] [EOL] [EOL] def test_can_specify_predictor ( self ) : [EOL] @ Predictor . register ( [string] ) class ExplicitPredictor ( TextClassifierPredictor ) : [EOL] [docstring] [EOL] [EOL] def predict_json ( self , inputs ) : [EOL] result = super ( ) . predict_json ( inputs ) [EOL] result [ [string] ] = True [EOL] return result [EOL] [EOL] with open ( self . infile , [string] ) as f : [EOL] f . write ( [string] ) [EOL] f . write ( [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . infile ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , ] [EOL] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] [comment] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] shutil . rmtree ( self . tempdir ) [EOL] [EOL] def test_other_modules ( self ) : [EOL] [comment] [EOL] packagedir = self . TEST_DIR / [string] [EOL] packagedir . mkdir ( ) [EOL] ( packagedir / [string] ) . touch ( ) [EOL] [EOL] [comment] [EOL] with push_python_path ( self . TEST_DIR ) : [EOL] [comment] [EOL] from allennlp . predictors import text_classifier [EOL] [EOL] with open ( text_classifier . __file__ ) as f : [EOL] code = f . read ( ) . replace ( [string] , [string] , ) [EOL] [EOL] with open ( os . path . join ( packagedir , [string] ) , [string] ) as f : [EOL] f . write ( code ) [EOL] [EOL] self . infile = os . path . join ( self . TEST_DIR , [string] ) [EOL] self . outfile = os . path . join ( self . TEST_DIR , [string] ) [EOL] [EOL] with open ( self . infile , [string] ) as f : [EOL] f . write ( [string] ) [EOL] f . write ( [string] ) [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . infile ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , ] [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] main ( ) [EOL] [EOL] [comment] [EOL] sys . argv . extend ( [ [string] , [string] ] ) [EOL] main ( ) [EOL] [EOL] assert os . path . exists ( self . outfile ) [EOL] [EOL] with open ( self . outfile , [string] ) as f : [EOL] results = [ json . loads ( line ) for line in f ] [EOL] [EOL] assert len ( results ) == [number] [EOL] [comment] [EOL] for result in results : [EOL] assert set ( result . keys ( ) ) == { [string] , [string] , [string] , [string] , [string] } [EOL] [EOL] def test_alternative_file_formats ( self ) : [EOL] @ Predictor . register ( [string] ) class CsvPredictor ( TextClassifierPredictor ) : [EOL] [docstring] [EOL] [EOL] def load_line ( self , line ) : [EOL] reader = csv . reader ( [ line ] ) [EOL] sentence , label = next ( reader ) [EOL] return { [string] : sentence , [string] : label } [EOL] [EOL] def dump_line ( self , outputs ) : [EOL] output = io . StringIO ( ) [EOL] writer = csv . writer ( output ) [EOL] row = [ outputs [ [string] ] , * outputs [ [string] ] ] [EOL] [EOL] writer . writerow ( row ) [EOL] return output . getvalue ( ) [EOL] [EOL] with open ( self . infile , [string] ) as f : [EOL] writer = csv . writer ( f ) [EOL] writer . writerow ( [ [string] , [string] ] ) [EOL] writer . writerow ( [ [string] , [string] ] ) [EOL] [EOL] sys . argv = [ [string] , [string] , str ( self . classifier_model_path ) , str ( self . infile ) , [string] , str ( self . outfile ) , [string] , [string] , [string] , ] [EOL] [EOL] main ( ) [EOL] assert os . path . exists ( self . outfile ) [EOL] [EOL] with open ( self . outfile ) as f : [EOL] reader = csv . reader ( f ) [EOL] results = [ row for row in reader ] [EOL] [EOL] assert len ( results ) == [number] [EOL] for row in results : [EOL] assert len ( row ) == [number] [comment] [EOL] label , * probs = row [EOL] for prob in probs : [EOL] assert [number] <= float ( prob ) <= [number] [EOL] assert label != [string] [EOL] [EOL] shutil . rmtree ( self . tempdir ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[builtins.str]$ 0 $typing.Optional[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 $allennlp.common.util.JsonDict$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $_csv._writer$ 0 0 0 $_csv._writer$ 0 0 0 0 $_csv._writer$ 0 0 0 0 0 0 0 0 0 0 $_csv._writer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $_csv._reader$ 0 0 0 $_csv._reader$ 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 $_csv._reader$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.str]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , List , Any [EOL] import typing [EOL] import shutil [EOL] import sys [EOL] [EOL] import pytest [EOL] from overrides import overrides [EOL] [EOL] from allennlp . commands import main [EOL] from allennlp . commands . subcommand import Subcommand [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . plugins import discover_plugins [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . util import push_python_path , pushd [EOL] [EOL] [EOL] class TestMain ( AllenNlpTestCase ) : [EOL] def test_fails_on_unknown_command ( self ) : [EOL] sys . argv = [ [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] with pytest . raises ( SystemExit ) as cm : [EOL] main ( ) [EOL] [EOL] assert cm . value . code == [number] [comment] [EOL] [EOL] def test_subcommand_overrides ( self ) : [EOL] called = False [EOL] [EOL] def do_nothing ( _ ) : [EOL] nonlocal called [EOL] called = True [EOL] [EOL] @ Subcommand . register ( [string] , exist_ok = True ) class FakeEvaluate ( Subcommand ) : [comment] [EOL] @ overrides def add_subparser ( self , parser ) : [EOL] subparser = parser . add_parser ( self . name , description = [string] , help = [string] ) [EOL] subparser . set_defaults ( func = do_nothing ) [EOL] return subparser [EOL] [EOL] sys . argv = [ [string] , [string] ] [EOL] main ( ) [EOL] assert called [EOL] [EOL] def test_other_modules ( self ) : [EOL] [comment] [EOL] packagedir = self . TEST_DIR / [string] [EOL] packagedir . mkdir ( ) [EOL] ( packagedir / [string] ) . touch ( ) [EOL] [EOL] [comment] [EOL] with push_python_path ( self . TEST_DIR ) : [EOL] [comment] [EOL] from allennlp . models import simple_tagger [EOL] [EOL] with open ( simple_tagger . __file__ ) as model_file : [EOL] code = model_file . read ( ) . replace ( [string] , [string] , ) [EOL] [EOL] with open ( packagedir / [string] , [string] ) as new_model_file : [EOL] new_model_file . write ( code ) [EOL] [EOL] [comment] [EOL] shutil . copy ( self . FIXTURES_ROOT / [string] / [string] , self . TEST_DIR ) [EOL] data_path = str ( self . TEST_DIR / [string] ) [EOL] [EOL] [comment] [EOL] config_path = self . TEST_DIR / [string] [EOL] config_json = [string] . replace ( [string] , data_path ) [EOL] with open ( config_path , [string] ) as config_file : [EOL] config_file . write ( config_json ) [EOL] [EOL] serialization_dir = self . TEST_DIR / [string] [EOL] [EOL] [comment] [EOL] sys . argv = [ [string] , [string] , str ( config_path ) , [string] , str ( serialization_dir ) ] [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] main ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] sys . argv . extend ( [ [string] , [string] , [string] ] ) [EOL] [EOL] main ( ) [EOL] [EOL] [comment] [EOL] with open ( config_path , [string] ) as new_config_file : [EOL] new_config_file . write ( config_json . replace ( [string] , [string] ) ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] main ( ) [EOL] [EOL] def test_file_plugin_loaded ( self ) : [EOL] plugins_root = self . FIXTURES_ROOT / [string] [EOL] [EOL] sys . argv = [ [string] ] [EOL] [EOL] available_plugins = set ( discover_plugins ( ) ) [EOL] assert available_plugins == set ( ) [EOL] [EOL] with pushd ( plugins_root ) : [EOL] main ( ) [EOL] subcommands_available = Subcommand . list_available ( ) [EOL] assert [string] in subcommands_available [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Set[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0
	0
from typing import Optional , Any , Set , Callable , Dict , OrderedDict , Pattern , List , Union , Type , Counter , Iterable [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import argparse [EOL] import tests [EOL] import torch [EOL] import collections [EOL] import builtins [EOL] import argparse [EOL] import copy [EOL] import json [EOL] [EOL] import logging [EOL] import math [EOL] import os [EOL] import re [EOL] import shutil [EOL] from collections import OrderedDict , Counter [EOL] from typing import Iterable , Optional , List , Dict , Any [EOL] [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . commands . train import Train , train_model , train_model_from_args , TrainModel [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase , cpu_or_gpu [EOL] from allennlp . data import DatasetReader , Instance , Vocabulary [EOL] from allennlp . data . dataloader import TensorDict [EOL] from allennlp . models import load_archive , Model [EOL] from allennlp . models . archival import CONFIG_NAME [EOL] from allennlp . training import BatchCallback , GradientDescentTrainer [EOL] from allennlp . training . learning_rate_schedulers import ( ExponentialLearningRateScheduler , LearningRateScheduler , ) [EOL] [EOL] SEQUENCE_TAGGING_DATA_PATH = str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] SEQUENCE_TAGGING_SHARDS_PATH = str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] [EOL] [EOL] @ BatchCallback . register ( [string] ) class TrainingDataLoggerBatchCallback ( BatchCallback ) : [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] if is_training : [EOL] logger = logging . getLogger ( __name__ ) [EOL] for batch in batch_inputs : [EOL] for metadata in batch [ [string] ] : [EOL] logger . info ( f" [string] { metadata [ [string] ] [ [number] ] } [string] " ) [comment] [EOL] [EOL] [EOL] _seen_training_devices = set ( ) [EOL] [EOL] [EOL] @ BatchCallback . register ( [string] ) class TrainingDeviceLoggerBatchCallback ( BatchCallback ) : [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] global _seen_training_devices [EOL] for tensor in trainer . model . parameters ( ) : [EOL] _seen_training_devices . add ( tensor . device ) [EOL] [EOL] [EOL] class TestTrain ( AllenNlpTestCase ) : [EOL] DEFAULT_PARAMS = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] def test_train_model ( self ) : [EOL] params = lambda : copy . deepcopy ( self . DEFAULT_PARAMS ) [EOL] [EOL] train_model ( params ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] [comment] [EOL] serialization_dir2 = os . path . join ( self . TEST_DIR , [string] ) [EOL] assert not os . path . exists ( serialization_dir2 ) [EOL] os . makedirs ( serialization_dir2 ) [EOL] train_model ( params ( ) , serialization_dir = serialization_dir2 ) [EOL] [EOL] [comment] [EOL] serialization_dir3 = os . path . join ( self . TEST_DIR , [string] ) [EOL] assert not os . path . exists ( serialization_dir3 ) [EOL] os . makedirs ( serialization_dir3 ) [EOL] with open ( os . path . join ( serialization_dir3 , [string] ) , [string] ) as f : [EOL] f . write ( [string] ) [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( params ( ) , serialization_dir = serialization_dir3 ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( params ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] [comment] [EOL] train_model ( params ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) , recover = True , ) [EOL] [EOL] [comment] [EOL] train_model ( params ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) , force = True ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( params ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) , force = True , recover = True , ) [EOL] [EOL] @ cpu_or_gpu def test_detect_gpu ( self ) : [EOL] import copy [EOL] [EOL] params = copy . deepcopy ( self . DEFAULT_PARAMS ) [EOL] params [ [string] ] [ [string] ] = [ [string] ] [EOL] [EOL] global _seen_training_devices [EOL] _seen_training_devices . clear ( ) [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert len ( _seen_training_devices ) == [number] [EOL] seen_training_device = next ( iter ( _seen_training_devices ) ) [EOL] if torch . cuda . device_count ( ) == [number] : [EOL] assert seen_training_device . type == [string] [EOL] else : [EOL] assert seen_training_device . type == [string] [EOL] [EOL] @ cpu_or_gpu def test_force_gpu ( self ) : [EOL] import copy [EOL] [EOL] params = copy . deepcopy ( self . DEFAULT_PARAMS ) [EOL] params [ [string] ] [ [string] ] = [ [string] ] [EOL] params [ [string] ] [ [string] ] = [number] [EOL] [EOL] global _seen_training_devices [EOL] _seen_training_devices . clear ( ) [EOL] if torch . cuda . device_count ( ) == [number] : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] else : [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert len ( _seen_training_devices ) == [number] [EOL] seen_training_device = next ( iter ( _seen_training_devices ) ) [EOL] assert seen_training_device . type == [string] [EOL] [EOL] @ cpu_or_gpu def test_force_cpu ( self ) : [EOL] import copy [EOL] [EOL] params = copy . deepcopy ( self . DEFAULT_PARAMS ) [EOL] params [ [string] ] [ [string] ] = [ [string] ] [EOL] params [ [string] ] [ [string] ] = - [number] [EOL] [EOL] global _seen_training_devices [EOL] _seen_training_devices . clear ( ) [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert len ( _seen_training_devices ) == [number] [EOL] seen_training_device = next ( iter ( _seen_training_devices ) ) [EOL] assert seen_training_device . type == [string] [EOL] [EOL] @ cpu_or_gpu def test_train_model_distributed ( self ) : [EOL] if torch . cuda . device_count ( ) >= [number] : [EOL] devices = [ [number] , [number] ] [EOL] else : [EOL] devices = [ - [number] , - [number] ] [EOL] [EOL] params = lambda : Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , [string] : { [string] : devices } , } ) [EOL] [EOL] out_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] train_model ( params ( ) , serialization_dir = out_dir ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] serialized_files = os . listdir ( out_dir ) [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] [EOL] [comment] [EOL] assert load_archive ( out_dir ) . model [EOL] [EOL] @ cpu_or_gpu @ pytest . mark . parametrize ( [string] , [ True , False ] ) def test_train_model_distributed_with_sharded_reader ( self , lazy ) : [EOL] if torch . cuda . device_count ( ) >= [number] : [EOL] devices = [ [number] , [number] ] [EOL] else : [EOL] devices = [ - [number] , - [number] ] [EOL] [EOL] params = lambda : Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] , [string] : { [string] : [string] } , [string] : lazy , } , [string] : SEQUENCE_TAGGING_SHARDS_PATH , [string] : SEQUENCE_TAGGING_SHARDS_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , [string] : { [string] : devices } , } ) [EOL] [EOL] out_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] train_model ( params ( ) , serialization_dir = out_dir ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] serialized_files = os . listdir ( out_dir ) [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] [EOL] [comment] [EOL] archive = load_archive ( out_dir ) [EOL] assert archive . model [EOL] [EOL] [comment] [EOL] tokens = archive . model . vocab . _token_to_index [ [string] ] . keys ( ) [EOL] assert tokens == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] [comment] [EOL] train_early = [string] [EOL] validation_early = [string] [EOL] train_complete = [string] [EOL] validation_complete = [string] [EOL] [EOL] [comment] [EOL] with open ( os . path . join ( out_dir , [string] ) ) as f : [EOL] worker0_log = f . read ( ) [EOL] assert train_early in worker0_log [EOL] assert validation_early in worker0_log [EOL] assert train_complete not in worker0_log [EOL] assert validation_complete not in worker0_log [EOL] [EOL] with open ( os . path . join ( out_dir , [string] ) ) as f : [EOL] worker1_log = f . read ( ) [EOL] assert train_early not in worker1_log [EOL] assert validation_early not in worker1_log [EOL] assert train_complete in worker1_log [EOL] assert validation_complete in worker1_log [EOL] [EOL] @ cpu_or_gpu @ pytest . mark . parametrize ( [string] , [ True , False ] ) def test_train_model_distributed_without_sharded_reader ( self , lazy ) : [EOL] if torch . cuda . device_count ( ) >= [number] : [EOL] devices = [ [number] , [number] ] [EOL] else : [EOL] devices = [ - [number] , - [number] ] [EOL] [EOL] num_epochs = [number] [EOL] params = lambda : Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] , [string] : lazy } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : num_epochs , [string] : [string] , [string] : [ [string] ] , } , [string] : { [string] : devices } , } ) [EOL] [EOL] out_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] train_model ( params ( ) , serialization_dir = out_dir ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] serialized_files = os . listdir ( out_dir ) [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] assert [string] in serialized_files [EOL] [EOL] [comment] [EOL] archive = load_archive ( out_dir ) [EOL] assert archive . model [EOL] [EOL] [comment] [EOL] tokens = set ( archive . model . vocab . _token_to_index [ [string] ] . keys ( ) ) [EOL] assert tokens == { [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] train_complete = [string] [EOL] validation_complete = [string] [EOL] [EOL] import re [EOL] [EOL] pattern = re . compile ( [string] ) [EOL] first_word_counts = Counter ( ) [comment] [EOL] with open ( os . path . join ( out_dir , [string] ) ) as f : [EOL] worker0_log = f . read ( ) [EOL] assert train_complete in worker0_log [EOL] assert validation_complete in worker0_log [EOL] for first_word in pattern . findall ( worker0_log ) : [EOL] first_word_counts [ first_word ] += [number] [EOL] [EOL] with open ( os . path . join ( out_dir , [string] ) ) as f : [EOL] worker1_log = f . read ( ) [EOL] assert train_complete in worker1_log [EOL] assert validation_complete in worker1_log [EOL] for first_word in pattern . findall ( worker1_log ) : [EOL] first_word_counts [ first_word ] += [number] [EOL] [EOL] assert first_word_counts == { [string] : num_epochs , [string] : num_epochs , [string] : num_epochs , [string] : num_epochs , } [EOL] [EOL] def test_distributed_raises_error_with_no_gpus ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , [string] : { } , } ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] def test_train_saves_all_keys_in_config ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : [number] , [string] : [number] , [string] : [number] , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] serialization_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] params_as_dict = ( params . as_ordered_dict ( ) ) [comment] [EOL] train_model ( params , serialization_dir = serialization_dir ) [EOL] [EOL] config_path = os . path . join ( serialization_dir , CONFIG_NAME ) [EOL] with open ( config_path ) as config : [EOL] saved_config_as_dict = OrderedDict ( json . load ( config ) ) [EOL] assert params_as_dict == saved_config_as_dict [EOL] [EOL] def test_error_is_throw_when_cuda_device_is_not_available ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : torch . cuda . device_count ( ) , [string] : [string] , } , } ) [EOL] [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] def test_train_with_test_set ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : True , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] def test_train_number_of_steps ( self ) : [EOL] number_of_epochs = [number] [EOL] [EOL] last_num_steps_per_epoch = None [EOL] [EOL] @ LearningRateScheduler . register ( [string] ) class MockLRScheduler ( ExponentialLearningRateScheduler ) : [EOL] def __init__ ( self , optimizer , num_steps_per_epoch ) : [EOL] super ( ) . __init__ ( optimizer ) [EOL] nonlocal last_num_steps_per_epoch [EOL] last_num_steps_per_epoch = num_steps_per_epoch [EOL] [EOL] batch_callback_counter = [number] [EOL] [EOL] @ BatchCallback . register ( [string] ) class CounterBatchCallback ( BatchCallback ) : [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] nonlocal batch_callback_counter [EOL] if is_training : [EOL] batch_callback_counter += [number] [EOL] [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : True , [string] : { [string] : [number] } , [string] : { [string] : number_of_epochs , [string] : [string] , [string] : { [string] : [string] } , [string] : [ [string] ] , } , } ) [EOL] train_model ( params . duplicate ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs [EOL] batch_callback_counter = [number] [EOL] normal_steps_per_epoch = last_num_steps_per_epoch [EOL] [EOL] original_batch_size = params [ [string] ] [ [string] ] [EOL] params [ [string] ] [ [string] ] = [number] [EOL] train_model ( params . duplicate ( ) , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs [EOL] batch_callback_counter = [number] [EOL] assert normal_steps_per_epoch == math . ceil ( last_num_steps_per_epoch / original_batch_size ) [EOL] [EOL] params [ [string] ] [ [string] ] = original_batch_size [EOL] params [ [string] ] [ [string] ] = [number] [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] assert batch_callback_counter == last_num_steps_per_epoch * number_of_epochs [EOL] batch_callback_counter = [number] [EOL] assert math . ceil ( normal_steps_per_epoch / [number] ) == last_num_steps_per_epoch [EOL] [EOL] def test_train_args ( self ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Train ( ) . add_subparser ( subparsers ) [EOL] [EOL] for serialization_arg in [ [string] , [string] ] : [EOL] raw_args = [ [string] , [string] , serialization_arg , [string] ] [EOL] [EOL] args = parser . parse_args ( raw_args ) [EOL] [EOL] assert args . func == train_model_from_args [EOL] assert args . param_path == [string] [EOL] assert args . serialization_dir == [string] [EOL] [EOL] [comment] [EOL] with pytest . raises ( SystemExit ) as cm : [EOL] args = parser . parse_args ( [ [string] , [string] , [string] ] ) [EOL] assert cm . exception . code == [number] [comment] [EOL] [EOL] [comment] [EOL] with pytest . raises ( SystemExit ) as cm : [EOL] args = parser . parse_args ( [ [string] , [string] ] ) [EOL] assert cm . exception . code == [number] [comment] [EOL] [EOL] def test_train_model_can_instantiate_from_params ( self ) : [EOL] params = Params . from_file ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] [comment] [EOL] TrainModel . from_params ( params = params , serialization_dir = self . TEST_DIR , local_rank = [number] , batch_weight_key = [string] ) [EOL] [EOL] def test_train_can_fine_tune_model_from_archive ( self ) : [EOL] params = Params . from_file ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] train_loop = TrainModel . from_params ( params = params , serialization_dir = self . TEST_DIR , local_rank = [number] , batch_weight_key = [string] ) [EOL] train_loop . run ( ) [EOL] [EOL] model = Model . from_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert train_loop . model . vocab . get_vocab_size ( ) > model . vocab . get_vocab_size ( ) [EOL] [EOL] [EOL] @ DatasetReader . register ( [string] ) class LazyFakeReader ( DatasetReader ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( lazy = True ) [EOL] self . reader = DatasetReader . from_params ( Params ( { [string] : [string] , [string] : True } ) ) [EOL] [EOL] def _read ( self , file_path ) : [EOL] [docstring] [EOL] return self . reader . read ( file_path ) [EOL] [EOL] [EOL] class TestTrainOnLazyDataset ( AllenNlpTestCase ) : [EOL] def test_train_model ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] def test_train_with_test_set ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : True , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] train_model ( params , serialization_dir = os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] def test_train_nograd_regex ( self ) : [EOL] params_get = lambda : Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : SEQUENCE_TAGGING_DATA_PATH , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] serialization_dir = os . path . join ( self . TEST_DIR , [string] ) [EOL] regex_lists = [ [ ] , [ [string] ] , [ [string] , [string] ] ] [EOL] for regex_list in regex_lists : [EOL] params = params_get ( ) [EOL] params [ [string] ] [ [string] ] = regex_list [EOL] shutil . rmtree ( serialization_dir , ignore_errors = True ) [EOL] model = train_model ( params , serialization_dir = serialization_dir ) [EOL] [comment] [EOL] [comment] [EOL] for name , parameter in model . named_parameters ( ) : [EOL] if any ( re . search ( regex , name ) for regex in regex_list ) : [EOL] assert not parameter . requires_grad [EOL] else : [EOL] assert parameter . requires_grad [EOL] [comment] [EOL] params = params_get ( ) [EOL] params [ [string] ] [ [string] ] = [ [string] ] [EOL] shutil . rmtree ( serialization_dir , ignore_errors = True ) [EOL] with pytest . raises ( Exception ) : [EOL] train_model ( params , serialization_dir = serialization_dir ) [EOL] [EOL] [EOL] class TestDryRun ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] [EOL] self . params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] def test_dry_run_doesnt_overwrite_vocab ( self ) : [EOL] vocab_path = self . TEST_DIR / [string] [EOL] os . mkdir ( vocab_path ) [EOL] [comment] [EOL] with open ( vocab_path / [string] , [string] ) as open_file : [EOL] open_file . write ( [string] ) [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] train_model ( self . params , self . TEST_DIR , dry_run = True ) [EOL] [EOL] def test_dry_run_without_vocabulary_key ( self ) : [EOL] train_model ( self . params , self . TEST_DIR , dry_run = True ) [EOL] [EOL] def test_dry_run_makes_vocab ( self ) : [EOL] vocab_path = self . TEST_DIR / [string] [EOL] [EOL] train_model ( self . params , self . TEST_DIR , dry_run = True ) [EOL] [EOL] vocab_files = os . listdir ( vocab_path ) [EOL] assert set ( vocab_files ) == { [string] , [string] , [string] , [string] , } [EOL] [EOL] with open ( vocab_path / [string] ) as f : [EOL] tokens = [ line . strip ( ) for line in f ] [EOL] [EOL] tokens . sort ( ) [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] with open ( vocab_path / [string] ) as f : [EOL] labels = [ line . strip ( ) for line in f ] [EOL] [EOL] labels . sort ( ) [EOL] assert labels == [ [string] , [string] ] [EOL] [EOL] def test_dry_run_with_extension ( self ) : [EOL] existing_serialization_dir = self . TEST_DIR / [string] [EOL] extended_serialization_dir = self . TEST_DIR / [string] [EOL] existing_vocab_path = existing_serialization_dir / [string] [EOL] extended_vocab_path = extended_serialization_dir / [string] [EOL] [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] os . makedirs ( existing_serialization_dir , exist_ok = True ) [EOL] vocab . save_to_files ( existing_vocab_path ) [EOL] [EOL] self . params [ [string] ] = { } [EOL] self . params [ [string] ] [ [string] ] = [string] [EOL] self . params [ [string] ] [ [string] ] = str ( existing_vocab_path ) [EOL] self . params [ [string] ] [ [string] ] = { [string] : [number] } [EOL] train_model ( self . params , extended_serialization_dir , dry_run = True ) [EOL] [EOL] vocab_files = os . listdir ( extended_vocab_path ) [EOL] assert set ( vocab_files ) == { [string] , [string] , [string] , [string] , } [EOL] [EOL] with open ( extended_vocab_path / [string] ) as f : [EOL] tokens = [ line . strip ( ) for line in f ] [EOL] [EOL] assert tokens [ [number] ] == [string] [EOL] assert tokens [ [number] ] == [string] [EOL] assert tokens [ [number] ] == [string] [EOL] [EOL] tokens . sort ( ) [EOL] assert tokens == [ [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] [EOL] with open ( extended_vocab_path / [string] ) as f : [EOL] labels = [ line . strip ( ) for line in f ] [EOL] [EOL] labels . sort ( ) [EOL] assert labels == [ [string] , [string] ] [EOL] [EOL] def test_dry_run_without_extension ( self ) : [EOL] existing_serialization_dir = self . TEST_DIR / [string] [EOL] extended_serialization_dir = self . TEST_DIR / [string] [EOL] existing_vocab_path = existing_serialization_dir / [string] [EOL] extended_vocab_path = extended_serialization_dir / [string] [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] os . makedirs ( existing_serialization_dir , exist_ok = True ) [EOL] vocab . save_to_files ( existing_vocab_path ) [EOL] [EOL] self . params [ [string] ] = { } [EOL] self . params [ [string] ] [ [string] ] = [string] [EOL] self . params [ [string] ] [ [string] ] = str ( existing_vocab_path ) [EOL] train_model ( self . params , extended_serialization_dir , dry_run = True ) [EOL] [EOL] with open ( extended_vocab_path / [string] ) as f : [EOL] tokens = [ line . strip ( ) for line in f ] [EOL] [EOL] assert tokens [ [number] ] == [string] [EOL] assert tokens [ [number] ] == [string] [EOL] assert tokens [ [number] ] == [string] [EOL] assert len ( tokens ) == [number] [EOL] [EOL] def test_make_vocab_args ( self ) : [EOL] parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Train ( ) . add_subparser ( subparsers ) [EOL] for serialization_arg in [ [string] , [string] ] : [EOL] raw_args = [ [string] , [string] , serialization_arg , [string] , [string] , ] [EOL] args = parser . parse_args ( raw_args ) [EOL] assert args . func == train_model_from_args [EOL] assert args . param_path == [string] [EOL] assert args . serialization_dir == [string] [EOL] assert args . dry_run [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $"GradientDescentTrainer"$ 0 $typing.List[allennlp.data.dataloader.TensorDict]$ 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $builtins.bool$ 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.dataloader.TensorDict]$ 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $"GradientDescentTrainer"$ 0 $typing.List[allennlp.data.dataloader.TensorDict]$ 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[tests.commands.train_test.TestTrain]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 0 0 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Pattern[builtins.str]$ 0 0 0 $builtins.str$ 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 0 0 0 0 $collections.Counter[typing.Any]$ 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $collections.OrderedDict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $collections.OrderedDict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $allennlp.training.GradientDescentTrainer$ 0 $typing.List[typing.List[allennlp.data.dataloader.TensorDict]]$ 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Optional[builtins.int]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $None$ 0 $typing.Optional[builtins.int]$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Optional[builtins.int]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $None$ 0 0 0 0 0 $typing.Optional[builtins.int]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Optional[builtins.int]$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 $typing.Optional[builtins.int]$ 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 $allennlp.models.model.Model$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Callable[[],typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Union[typing.List[typing.Any],typing.List[builtins.str]]]$ 0 0 $typing.Any$ 0 $typing.Callable[[],typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 $typing.Optional[allennlp.models.model.Model]$ 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Optional[allennlp.models.model.Model]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Callable[[],typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 $argparse.ArgumentParser$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse._SubParsersAction$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $argparse.Namespace$ 0 $argparse.ArgumentParser$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0 0 0 0 $argparse.Namespace$ 0 0 0
from typing import Any , Dict , List , Iterator , Tuple [EOL] import allennlp [EOL] import typing [EOL] import tests [EOL] import torch [EOL] import builtins [EOL] import argparse [EOL] import json [EOL] from typing import Iterator , List , Dict [EOL] [EOL] import torch [EOL] from flaky import flaky [EOL] import pytest [EOL] [EOL] from allennlp . commands . evaluate import evaluate_from_args , Evaluate , evaluate [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataloader import TensorDict [EOL] from allennlp . models import Model [EOL] [EOL] [EOL] class DummyDataLoader : [EOL] def __init__ ( self , outputs ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _outputs = outputs [EOL] [EOL] def __iter__ ( self ) : [EOL] yield from self . _outputs [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . _outputs ) [EOL] [EOL] [EOL] class DummyModel ( Model ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( None ) [comment] [EOL] [EOL] def forward ( self , ** kwargs ) : [comment] [EOL] return kwargs [EOL] [EOL] [EOL] class TestEvaluate ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] [EOL] self . parser = argparse . ArgumentParser ( description = [string] ) [EOL] subparsers = self . parser . add_subparsers ( title = [string] , metavar = [string] ) [EOL] Evaluate ( ) . add_subparser ( subparsers ) [EOL] [EOL] def test_evaluate_calculates_average_loss ( self ) : [EOL] losses = [ [number] , [number] , [number] ] [EOL] outputs = [ { [string] : torch . Tensor ( [ loss ] ) } for loss in losses ] [EOL] data_loader = DummyDataLoader ( outputs ) [EOL] metrics = evaluate ( DummyModel ( ) , data_loader , - [number] , [string] ) [EOL] assert metrics [ [string] ] == pytest . approx ( [number] ) [EOL] [EOL] def test_evaluate_calculates_average_loss_with_weights ( self ) : [EOL] losses = [ [number] , [number] , [number] ] [EOL] weights = [ [number] , [number] , [number] ] [EOL] inputs = zip ( losses , weights ) [EOL] outputs = [ { [string] : torch . Tensor ( [ loss ] ) , [string] : torch . Tensor ( [ weight ] ) } for loss , weight in inputs ] [EOL] data_loader = DummyDataLoader ( outputs ) [EOL] metrics = evaluate ( DummyModel ( ) , data_loader , - [number] , [string] ) [EOL] assert metrics [ [string] ] == pytest . approx ( ( [number] + [number] + [number] ) / [number] ) [EOL] [EOL] @ flaky def test_evaluate_from_args ( self ) : [EOL] kebab_args = [ [string] , str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) , str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] , [string] , ] [EOL] [EOL] args = self . parser . parse_args ( kebab_args ) [EOL] metrics = evaluate_from_args ( args ) [EOL] assert metrics . keys ( ) == { [string] , [string] , [string] , [string] , [string] , [string] , } [EOL] [EOL] def test_output_file_evaluate_from_args ( self ) : [EOL] output_file = str ( self . TEST_DIR / [string] ) [EOL] predictions_output_file = str ( self . TEST_DIR / [string] ) [EOL] kebab_args = [ [string] , str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) , str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] , [string] , [string] , output_file , [string] , predictions_output_file , ] [EOL] args = self . parser . parse_args ( kebab_args ) [EOL] computed_metrics = evaluate_from_args ( args ) [EOL] [EOL] with open ( output_file , [string] ) as file : [EOL] saved_metrics = json . load ( file ) [EOL] assert computed_metrics == saved_metrics [EOL] [EOL] with open ( predictions_output_file , [string] ) as file : [EOL] for line in file : [EOL] prediction = json . loads ( line . strip ( ) ) [EOL] assert [string] in prediction [EOL] [EOL] def test_evaluate_works_with_vocab_expansion ( self ) : [EOL] archive_path = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] [comment] [EOL] evaluate_data_path = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] embeddings_filename = str ( self . FIXTURES_ROOT / [string] / [string] ) [comment] [EOL] embedding_sources_mapping = json . dumps ( { [string] : embeddings_filename } ) [EOL] kebab_args = [ [string] , archive_path , evaluate_data_path , [string] , [string] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] metrics_1 = evaluate_from_args ( self . parser . parse_args ( kebab_args ) ) [EOL] metrics_2 = evaluate_from_args ( self . parser . parse_args ( kebab_args + [ [string] ] ) ) [EOL] metrics_3 = evaluate_from_args ( self . parser . parse_args ( kebab_args + [ [string] , embedding_sources_mapping ] ) ) [EOL] assert metrics_1 != metrics_2 [EOL] assert metrics_2 != metrics_3 [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $typing.List[allennlp.data.dataloader.TensorDict]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.dataloader.TensorDict]$ 0 0 0 $typing.Iterator[allennlp.data.dataloader.TensorDict]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,torch.Tensor]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 $tests.commands.evaluate_test.DummyDataLoader$ 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $tests.commands.evaluate_test.DummyDataLoader$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Tuple[builtins.float,builtins.float]]$ 0 0 0 $typing.List[builtins.float]$ 0 $typing.List[builtins.float]$ 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterator[typing.Tuple[builtins.float,builtins.float]]$ 0 0 $tests.commands.evaluate_test.DummyDataLoader$ 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $tests.commands.evaluate_test.DummyDataLoader$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $typing.Dict[builtins.str,typing.Any]$ 0
	0
from typing import Dict , Any , List , Tuple [EOL] import allennlp [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] import builtins [EOL] from typing import Dict , Tuple [EOL] [EOL] import numpy as np [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . nn . beam_search import BeamSearch [EOL] [EOL] [EOL] transition_probabilities = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , ] ) [EOL] [EOL] [EOL] def take_step ( last_predictions , state , step ) : [EOL] [docstring] [EOL] log_probs_list = [ ] [EOL] for last_token in last_predictions : [EOL] log_probs = torch . log ( transition_probabilities [ last_token . item ( ) ] ) [EOL] log_probs_list . append ( log_probs ) [EOL] [EOL] return torch . stack ( log_probs_list ) , state [EOL] [EOL] [EOL] class BeamSearchTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . end_index = transition_probabilities . size ( ) [ [number] ] - [number] [EOL] self . beam_search = BeamSearch ( self . end_index , max_steps = [number] , beam_size = [number] ) [EOL] [EOL] [comment] [EOL] self . expected_top_k = np . array ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] [comment] [EOL] self . expected_log_probs = np . log ( np . array ( [ [number] , [number] , [number] ] ) ) [EOL] [EOL] def _check_results ( self , batch_size = [number] , expected_top_k = None , expected_log_probs = None , beam_search = None , state = None , ) : [EOL] expected_top_k = expected_top_k if expected_top_k is not None else self . expected_top_k [EOL] expected_log_probs = ( expected_log_probs if expected_log_probs is not None else self . expected_log_probs ) [EOL] state = state or { } [EOL] [EOL] beam_search = beam_search or self . beam_search [EOL] beam_size = beam_search . beam_size [EOL] [EOL] initial_predictions = torch . tensor ( [ [number] ] * batch_size ) [EOL] top_k , log_probs = beam_search . search ( initial_predictions , state , take_step ) [comment] [EOL] [EOL] [comment] [EOL] assert list ( top_k . size ( ) ) [ : - [number] ] == [ batch_size , beam_size ] [EOL] np . testing . assert_array_equal ( top_k [ [number] ] . numpy ( ) , expected_top_k ) [EOL] [EOL] [comment] [EOL] assert list ( log_probs . size ( ) ) == [ batch_size , beam_size ] [EOL] np . testing . assert_allclose ( log_probs [ [number] ] . numpy ( ) , expected_log_probs ) [EOL] [EOL] def test_search ( self ) : [EOL] self . _check_results ( ) [EOL] [EOL] def test_finished_state ( self ) : [EOL] state = { } [EOL] state [ [string] ] = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [comment] [EOL] [EOL] expected_finished_state = { } [EOL] expected_finished_state [ [string] ] = np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , ] ) [EOL] [comment] [EOL] [EOL] self . _check_results ( state = state ) [EOL] [EOL] [comment] [EOL] for key , array in expected_finished_state . items ( ) : [EOL] np . testing . assert_allclose ( state [ key ] . numpy ( ) , array ) [EOL] [EOL] def test_diff_shape_state ( self ) : [EOL] state = { } [EOL] state [ [string] ] = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] state [ [string] ] = state [ [string] ] . unsqueeze ( [number] ) . repeat ( [number] , [number] , [number] ) [EOL] [comment] [EOL] [EOL] seq = [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , ] [EOL] seq = [ seq ] * [number] [EOL] expected_finished_state = { } [EOL] expected_finished_state [ [string] ] = np . array ( seq ) [EOL] [comment] [EOL] [EOL] self . _check_results ( state = state ) [EOL] [EOL] [comment] [EOL] for key , array in expected_finished_state . items ( ) : [EOL] np . testing . assert_allclose ( state [ key ] . numpy ( ) , array ) [EOL] [EOL] def test_batch_size_of_one ( self ) : [EOL] self . _check_results ( batch_size = [number] ) [EOL] [EOL] def test_greedy_search ( self ) : [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] ) [EOL] expected_top_k = np . array ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] expected_log_probs = np . log ( np . array ( [ [number] ] ) ) [EOL] self . _check_results ( expected_top_k = expected_top_k , expected_log_probs = expected_log_probs , beam_search = beam_search , ) [EOL] [EOL] def test_early_stopping ( self ) : [EOL] [docstring] [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] , max_steps = [number] ) [EOL] expected_top_k = np . array ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] expected_log_probs = np . log ( np . array ( [ [number] , [number] , [number] ] ) ) [EOL] self . _check_results ( expected_top_k = expected_top_k , expected_log_probs = expected_log_probs , beam_search = beam_search , ) [EOL] [EOL] def test_different_per_node_beam_size ( self ) : [EOL] [comment] [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] , per_node_beam_size = [number] ) [EOL] self . _check_results ( beam_search = beam_search ) [EOL] [EOL] [comment] [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] , per_node_beam_size = [number] ) [EOL] self . _check_results ( beam_search = beam_search ) [EOL] [EOL] def test_catch_bad_config ( self ) : [EOL] [docstring] [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] self . _check_results ( beam_search = beam_search ) [EOL] [EOL] def test_warn_for_bad_log_probs ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] initial_predictions = torch . LongTensor ( [ self . end_index - [number] , self . end_index - [number] ] ) [EOL] with pytest . warns ( RuntimeWarning , match = [string] ) : [EOL] self . beam_search . search ( initial_predictions , { } , take_step ) [EOL] [EOL] def test_empty_sequences ( self ) : [EOL] initial_predictions = torch . LongTensor ( [ self . end_index - [number] , self . end_index - [number] ] ) [EOL] beam_search = BeamSearch ( self . end_index , beam_size = [number] ) [EOL] with pytest . warns ( RuntimeWarning , match = [string] ) : [EOL] predictions , log_probs = beam_search . search ( initial_predictions , { } , take_step ) [EOL] [comment] [EOL] assert list ( predictions . size ( ) ) == [ [number] , [number] , [number] ] [EOL] [comment] [EOL] assert list ( log_probs . size ( ) ) == [ [number] , [number] ] [EOL] assert ( predictions == self . end_index ) . all ( ) [EOL] assert ( log_probs == [number] ) . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.int]]]$ 0 0 $typing.List[typing.List[typing.List[builtins.int]]]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.beam_search.BeamSearch$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] class TestPredictor ( AllenNlpTestCase ) : [EOL] def test_from_archive_does_not_consume_params ( self ) : [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] Predictor . from_archive ( archive , [string] ) [EOL] [EOL] [comment] [EOL] Predictor . from_archive ( archive , [string] ) [EOL] [EOL] def test_loads_correct_dataset_reader ( self ) : [EOL] [comment] [EOL] [comment] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] assert predictor . _dataset_reader . _token_indexers [ [string] ] . namespace == [string] [EOL] [EOL] predictor = Predictor . from_archive ( archive , [string] , dataset_reader_to_load = [string] ) [EOL] assert predictor . _dataset_reader . _token_indexers [ [string] ] . namespace == [string] [EOL] [EOL] predictor = Predictor . from_archive ( archive , [string] , dataset_reader_to_load = [string] ) [EOL] assert predictor . _dataset_reader . _token_indexers [ [string] ] . namespace == [string] [EOL] [EOL] def test_get_gradients ( self ) : [EOL] inputs = { [string] : [string] , } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] [EOL] instance = predictor . _json_to_instance ( inputs ) [EOL] outputs = predictor . _model . forward_on_instance ( instance ) [EOL] labeled_instances = predictor . predictions_to_labeled_instances ( instance , outputs ) [EOL] for instance in labeled_instances : [EOL] grads = predictor . get_gradients ( [ instance ] ) [ [number] ] [EOL] assert [string] in grads [EOL] assert grads [ [string] ] is not None [EOL] assert len ( grads [ [string] ] [ [number] ] ) == [number] [comment] [EOL] [EOL] def test_get_gradients_when_requires_grad_is_false ( self ) : [EOL] inputs = { [string] : [string] , } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] [EOL] [comment] [EOL] embedding_layer = util . find_embedding_layer ( predictor . _model ) [EOL] assert not embedding_layer . weight . requires_grad [EOL] instance = predictor . _json_to_instance ( inputs ) [EOL] outputs = predictor . _model . forward_on_instance ( instance ) [EOL] labeled_instances = predictor . predictions_to_labeled_instances ( instance , outputs ) [EOL] [comment] [EOL] for instance in labeled_instances : [EOL] grads = predictor . get_gradients ( [ instance ] ) [ [number] ] [EOL] assert bool ( grads ) [EOL] [comment] [EOL] assert not embedding_layer . weight . requires_grad [EOL] [EOL] def test_captures_model_internals ( self ) : [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] [EOL] with predictor . capture_model_internals ( ) as internals : [EOL] predictor . predict_json ( inputs ) [EOL] [EOL] assert len ( internals ) == [number] [EOL] [EOL] with predictor . capture_model_internals ( [string] ) as internals : [EOL] predictor . predict_json ( inputs ) [EOL] assert len ( internals ) == [number] [EOL] [EOL] def test_predicts_batch_json ( self ) : [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] results = predictor . predict_batch_json ( [ inputs ] * [number] ) [EOL] assert len ( results ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0
	0
	0
from typing import Dict , Any [EOL] import allennlp [EOL] import typing [EOL] from pytest import approx [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . interpret . saliency_interpreters import SimpleGradient [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] [EOL] [EOL] class TestSimpleGradient ( AllenNlpTestCase ) : [EOL] def test_simple_gradient_basic_text ( self ) : [EOL] inputs = { [string] : [string] } [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] interpreter = SimpleGradient ( predictor ) [EOL] interpretation = interpreter . saliency_interpret_from_json ( inputs ) [EOL] assert interpretation is not None [EOL] assert [string] in interpretation [EOL] assert [string] in interpretation [ [string] ] [EOL] grad_input_1 = interpretation [ [string] ] [ [string] ] [EOL] assert len ( grad_input_1 ) == [number] [comment] [EOL] [EOL] [comment] [EOL] repeat_interpretation = interpreter . saliency_interpret_from_json ( inputs ) [EOL] repeat_grad_input_1 = repeat_interpretation [ [string] ] [ [string] ] [EOL] for grad , repeat_grad in zip ( grad_input_1 , repeat_grad_input_1 ) : [EOL] assert grad == approx ( repeat_grad ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.interpret.saliency_interpreters.simple_gradient.SimpleGradient$ 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.interpret.saliency_interpreters.simple_gradient.SimpleGradient$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.interpret.saliency_interpreters.simple_gradient.SimpleGradient$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . predictors import Predictor [EOL] from allennlp . interpret . saliency_interpreters import SmoothGradient [EOL] [EOL] [EOL] class TestSmoothGradient ( AllenNlpTestCase ) : [EOL] def test_smooth_gradient ( self ) : [EOL] inputs = { [string] : [string] } [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive , [string] ) [EOL] [EOL] interpreter = SmoothGradient ( predictor ) [EOL] interpretation = interpreter . saliency_interpret_from_json ( inputs ) [EOL] assert interpretation is not None [EOL] assert [string] in interpretation [EOL] assert [string] in interpretation [ [string] ] [EOL] assert len ( interpretation [ [string] ] [ [string] ] ) == [number] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . token_indexers import TokenCharactersIndexer [EOL] from allennlp . interpret . attackers import Hotflip [EOL] from allennlp . models . archival import load_archive [EOL] from allennlp . modules . token_embedders import EmptyEmbedder [EOL] from allennlp . predictors import Predictor [EOL] [EOL] [EOL] class TestHotflip ( AllenNlpTestCase ) : [EOL] def test_hotflip ( self ) : [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] [EOL] hotflipper = Hotflip ( predictor ) [EOL] hotflipper . initialize ( ) [EOL] attack = hotflipper . attack_from_json ( inputs , [string] , [string] ) [EOL] assert attack is not None [EOL] assert [string] in attack [EOL] assert [string] in attack [EOL] assert [string] in attack [EOL] assert len ( attack [ [string] ] [ [number] ] ) == len ( attack [ [string] ] ) [comment] [EOL] [EOL] def test_with_token_characters_indexer ( self ) : [EOL] [EOL] inputs = { [string] : [string] } [EOL] [EOL] archive = load_archive ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] predictor = Predictor . from_archive ( archive ) [EOL] predictor . _dataset_reader . _token_indexers [ [string] ] = TokenCharactersIndexer ( min_padding_length = [number] ) [EOL] predictor . _model . _text_field_embedder . _token_embedders [ [string] ] = EmptyEmbedder ( ) [EOL] [EOL] hotflipper = Hotflip ( predictor ) [EOL] hotflipper . initialize ( ) [EOL] attack = hotflipper . attack_from_json ( inputs , [string] , [string] ) [EOL] assert attack is not None [EOL] assert [string] in attack [EOL] assert [string] in attack [EOL] assert [string] in attack [EOL] assert len ( attack [ [string] ] [ [number] ] ) == len ( attack [ [string] ] ) [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 $typing.Any$ 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.predictors.predictor.Predictor$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0
	0
import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] @ pytest . mark . skip ( [string] ) class TestBasicAllenNlp ( AllenNlpTestCase ) : [EOL] @ classmethod def test_run_as_script ( cls ) : [EOL] [comment] [EOL] [EOL] import tutorials . tagger . basic_allennlp [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List [EOL] import typing [EOL] import torch [EOL] [EOL] import pytest [EOL] import numpy [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules import ScalarMix [EOL] from allennlp . nn import util [EOL] [EOL] [EOL] class TestScalarMix ( AllenNlpTestCase ) : [EOL] def test_scalar_mix_can_run_forward ( self ) : [EOL] mixture = ScalarMix ( [number] ) [EOL] tensors = [ torch . randn ( [ [number] , [number] , [number] ] ) for _ in range ( [number] ) ] [EOL] for k in range ( [number] ) : [EOL] mixture . scalar_parameters [ k ] . data [ [number] ] = [number] * ( k + [number] ) [EOL] mixture . gamma . data [ [number] ] = [number] [EOL] result = mixture ( tensors ) [EOL] [EOL] weights = [ [number] , [number] , [number] ] [EOL] normed_weights = numpy . exp ( weights ) / numpy . sum ( numpy . exp ( weights ) ) [EOL] expected_result = sum ( normed_weights [ k ] * tensors [ k ] . data . numpy ( ) for k in range ( [number] ) ) [EOL] expected_result *= [number] [EOL] numpy . testing . assert_almost_equal ( expected_result , result . data . numpy ( ) ) [EOL] [EOL] def test_scalar_mix_throws_error_on_incorrect_number_of_inputs ( self ) : [EOL] mixture = ScalarMix ( [number] ) [EOL] tensors = [ torch . randn ( [ [number] , [number] , [number] ] ) for _ in range ( [number] ) ] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = mixture ( tensors ) [EOL] [EOL] def test_scalar_mix_throws_error_on_incorrect_initial_scalar_parameters_length ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] ScalarMix ( [number] , initial_scalar_parameters = [ [number] , [number] ] ) [EOL] [EOL] def test_scalar_mix_trainable_with_initial_scalar_parameters ( self ) : [EOL] initial_scalar_parameters = [ [number] , [number] , [number] ] [EOL] mixture = ScalarMix ( [number] , initial_scalar_parameters = initial_scalar_parameters , trainable = False ) [EOL] for i , scalar_mix_parameter in enumerate ( mixture . scalar_parameters ) : [EOL] assert scalar_mix_parameter . requires_grad is False [EOL] assert scalar_mix_parameter . item ( ) == initial_scalar_parameters [ i ] [EOL] [EOL] def test_scalar_mix_layer_norm ( self ) : [EOL] mixture = ScalarMix ( [number] , do_layer_norm = [string] ) [EOL] [EOL] tensors = [ torch . randn ( [ [number] , [number] , [number] ] ) for _ in range ( [number] ) ] [EOL] numpy_mask = numpy . ones ( ( [number] , [number] ) , dtype = [string] ) [EOL] numpy_mask [ [number] , [number] : ] = [number] [EOL] mask = torch . from_numpy ( numpy_mask ) . bool ( ) [EOL] [EOL] weights = [ [number] , [number] , [number] ] [EOL] for k in range ( [number] ) : [EOL] mixture . scalar_parameters [ k ] . data [ [number] ] = weights [ k ] [EOL] mixture . gamma . data [ [number] ] = [number] [EOL] result = mixture ( tensors , mask ) [EOL] [EOL] normed_weights = numpy . exp ( weights ) / numpy . sum ( numpy . exp ( weights ) ) [EOL] expected_result = numpy . zeros ( ( [number] , [number] , [number] ) ) [EOL] for k in range ( [number] ) : [EOL] mean = numpy . mean ( tensors [ k ] . data . numpy ( ) [ numpy_mask == [number] ] ) [EOL] std = numpy . std ( tensors [ k ] . data . numpy ( ) [ numpy_mask == [number] ] ) [EOL] normed_tensor = ( tensors [ k ] . data . numpy ( ) - mean ) / ( std + util . tiny_value_of_dtype ( torch . float ) ) [EOL] expected_result += normed_tensor * normed_weights [ k ] [EOL] expected_result *= [number] [EOL] [EOL] numpy . testing . assert_almost_equal ( expected_result , result . data . numpy ( ) , decimal = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import pytest [EOL] import torch [EOL] [EOL] import numpy [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules import GatedSum [EOL] [EOL] [EOL] class TestGatedSum ( AllenNlpTestCase ) : [EOL] def test_gated_sum_can_run_forward ( self ) : [EOL] a = torch . FloatTensor ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] b = - a + [number] [EOL] weight_value = [number] [EOL] gate_value = torch . sigmoid ( torch . FloatTensor ( [ [number] ] ) ) [EOL] expected = gate_value * a + ( [number] - gate_value ) * b [EOL] [EOL] with torch . no_grad ( ) : [comment] [EOL] gated_sum = GatedSum ( a . size ( - [number] ) ) [EOL] gated_sum . _gate . weight *= [number] [EOL] gated_sum . _gate . weight += weight_value [EOL] gated_sum . _gate . bias *= [number] [EOL] [EOL] out = gated_sum ( a , b ) [EOL] numpy . testing . assert_almost_equal ( expected . data . numpy ( ) , out . data . numpy ( ) , decimal = [number] ) [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] GatedSum ( a . size ( - [number] ) ) ( a , b . unsqueeze ( [number] ) ) [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] GatedSum ( [number] ) ( a , b ) [EOL] [EOL] def test_input_output_dim ( self ) : [EOL] dim = [number] [EOL] gated_sum = GatedSum ( dim ) [EOL] numpy . testing . assert_equal ( gated_sum . get_input_dim ( ) , dim ) [EOL] numpy . testing . assert_equal ( gated_sum . get_output_dim ( ) , dim ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 0 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 0 0 $builtins.int$ 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.modules.gated_sum.GatedSum$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $allennlp.modules.gated_sum.GatedSum$ 0 0 0 0 0 $builtins.int$ 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] from numpy . testing import assert_almost_equal [EOL] import inspect [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules import FeedForward [EOL] from allennlp . nn import InitializerApplicator , Initializer , Activation [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestFeedForward ( AllenNlpTestCase ) : [EOL] def test_can_construct_from_params ( self ) : [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] } ) [EOL] feedforward = FeedForward . from_params ( params ) [EOL] assert len ( feedforward . _activations ) == [number] [EOL] assert [ isinstance ( a , torch . nn . ReLU ) for a in feedforward . _activations ] [EOL] assert len ( feedforward . _linear_layers ) == [number] [EOL] assert [ layer . weight . size ( - [number] ) == [number] for layer in feedforward . _linear_layers ] [EOL] [EOL] params = Params ( { [string] : [number] , [string] : [ [number] , [number] , [number] ] , [string] : [ [string] , [string] , [string] ] , [string] : [number] , [string] : [number] , } ) [EOL] feedforward = FeedForward . from_params ( params ) [EOL] assert len ( feedforward . _activations ) == [number] [EOL] assert isinstance ( feedforward . _activations [ [number] ] , torch . nn . ReLU ) [EOL] assert isinstance ( feedforward . _activations [ [number] ] , torch . nn . ReLU ) [EOL] [comment] [EOL] [comment] [EOL] assert not isinstance ( feedforward . _activations [ [number] ] , torch . nn . ReLU ) [EOL] [EOL] assert len ( feedforward . _linear_layers ) == [number] [EOL] assert feedforward . _linear_layers [ [number] ] . weight . size ( [number] ) == [number] [EOL] assert feedforward . _linear_layers [ [number] ] . weight . size ( [number] ) == [number] [EOL] assert feedforward . _linear_layers [ [number] ] . weight . size ( [number] ) == [number] [EOL] [EOL] assert len ( feedforward . _dropout ) == [number] [EOL] assert [ d . p == [number] for d in feedforward . _dropout ] [EOL] [EOL] def test_init_checks_hidden_dim_consistency ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] FeedForward ( [number] , [number] , [ [number] , [number] ] , Activation . by_name ( [string] ) ( ) ) [EOL] [EOL] def test_init_checks_activation_consistency ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] FeedForward ( [number] , [number] , [number] , [ Activation . by_name ( [string] ) ( ) , Activation . by_name ( [string] ) ( ) ] ) [EOL] [EOL] def test_forward_gives_correct_output ( self ) : [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [string] , [string] : [number] } ) [EOL] feedforward = FeedForward . from_params ( params ) [EOL] [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( feedforward ) [EOL] [EOL] input_tensor = torch . FloatTensor ( [ [ - [number] , [number] ] ] ) [EOL] output = feedforward ( input_tensor ) . data . numpy ( ) [EOL] assert output . shape == ( [number] , [number] ) [EOL] [comment] [EOL] [comment] [EOL] assert_almost_equal ( output , [ [ [number] , [number] , [number] ] ] ) [EOL] [EOL] def test_textual_representation_contains_activations ( self ) : [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [ [string] , [string] , [string] ] , [string] : [number] , } ) [EOL] feedforward = FeedForward . from_params ( params ) [EOL] expected_text_representation = inspect . cleandoc ( [string] ) [EOL] actual_text_representation = str ( feedforward ) [EOL] [EOL] assert actual_text_representation == expected_text_representation [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $builtins.str$ 0
from typing import Set , Any , List , Tuple [EOL] import typing [EOL] import itertools [EOL] import math [EOL] [EOL] from pytest import approx , raises [EOL] import torch [EOL] from numpy . testing import assert_allclose [EOL] [EOL] from allennlp . modules import ConditionalRandomField [EOL] from allennlp . modules . conditional_random_field import allowed_transitions [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestConditionalRandomField ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . logits = torch . Tensor ( [ [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , ] ) [EOL] self . tags = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] [EOL] self . transitions = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ - [number] , [number] , - [number] , [number] , [number] ] , [ [number] , [number] , [number] , - [number] , - [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) [EOL] [EOL] self . transitions_from_start = torch . Tensor ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] self . transitions_to_end = torch . Tensor ( [ - [number] , - [number] , [number] , - [number] , - [number] ] ) [EOL] [EOL] [comment] [EOL] self . crf = ConditionalRandomField ( [number] ) [EOL] self . crf . transitions = torch . nn . Parameter ( self . transitions ) [EOL] self . crf . start_transitions = torch . nn . Parameter ( self . transitions_from_start ) [EOL] self . crf . end_transitions = torch . nn . Parameter ( self . transitions_to_end ) [EOL] [EOL] def score ( self , logits , tags ) : [EOL] [docstring] [EOL] [comment] [EOL] total = self . transitions_from_start [ tags [ [number] ] ] + self . transitions_to_end [ tags [ - [number] ] ] [EOL] [comment] [EOL] for tag , next_tag in zip ( tags , tags [ [number] : ] ) : [EOL] total += self . transitions [ tag , next_tag ] [EOL] [comment] [EOL] for logit , tag in zip ( logits , tags ) : [EOL] total += logit [ tag ] [EOL] return total [EOL] [EOL] def naive_most_likely_sequence ( self , logits , mask ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] most_likely_tags = [ ] [EOL] best_scores = [ ] [EOL] [EOL] for logit , mas in zip ( logits , mask ) : [EOL] mask_indices = mas . nonzero ( as_tuple = False ) . squeeze ( ) [EOL] logit = torch . index_select ( logit , [number] , mask_indices ) [EOL] sequence_length = logit . shape [ [number] ] [EOL] most_likely , most_likelihood = None , - float ( [string] ) [EOL] for tags in itertools . product ( range ( [number] ) , repeat = sequence_length ) : [EOL] score = self . score ( logit . data , tags ) [EOL] if score > most_likelihood : [EOL] most_likely , most_likelihood = tags , score [EOL] [comment] [EOL] most_likely_tags . append ( list ( most_likely ) ) [EOL] best_scores . append ( most_likelihood ) [EOL] [EOL] return most_likely_tags , best_scores [EOL] [EOL] def test_forward_works_without_mask ( self ) : [EOL] log_likelihood = self . crf ( self . logits , self . tags ) . item ( ) [EOL] [EOL] [comment] [EOL] manual_log_likelihood = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for logits_i , tags_i in zip ( self . logits , self . tags ) : [EOL] numerator = self . score ( logits_i . detach ( ) , tags_i . detach ( ) ) [EOL] all_scores = [ self . score ( logits_i . detach ( ) , tags_j ) for tags_j in itertools . product ( range ( [number] ) , repeat = [number] ) ] [EOL] denominator = math . log ( sum ( math . exp ( score ) for score in all_scores ) ) [EOL] [comment] [EOL] manual_log_likelihood += numerator - denominator [EOL] [EOL] [comment] [EOL] assert manual_log_likelihood . item ( ) == approx ( log_likelihood ) [EOL] [EOL] def test_forward_works_with_mask ( self ) : [EOL] [comment] [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , True , False ] ] ) [EOL] [EOL] log_likelihood = self . crf ( self . logits , self . tags , mask ) . item ( ) [EOL] [EOL] [comment] [EOL] manual_log_likelihood = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for logits_i , tags_i , mask_i in zip ( self . logits , self . tags , mask ) : [EOL] [comment] [EOL] sequence_length = torch . sum ( mask_i . detach ( ) ) [EOL] logits_i = logits_i . data [ : sequence_length ] [EOL] tags_i = tags_i . data [ : sequence_length ] [EOL] [EOL] numerator = self . score ( logits_i , tags_i ) [EOL] all_scores = [ self . score ( logits_i , tags_j ) for tags_j in itertools . product ( range ( [number] ) , repeat = sequence_length ) ] [EOL] denominator = math . log ( sum ( math . exp ( score ) for score in all_scores ) ) [EOL] [comment] [EOL] manual_log_likelihood += numerator - denominator [EOL] [EOL] [comment] [EOL] assert manual_log_likelihood . item ( ) == approx ( log_likelihood ) [EOL] [EOL] def test_viterbi_tags ( self ) : [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , False , True ] ] ) [EOL] [EOL] viterbi_path = self . crf . viterbi_tags ( self . logits , mask ) [EOL] [EOL] [comment] [EOL] viterbi_tags = [ x for x , y in viterbi_path ] [EOL] viterbi_scores = [ y for x , y in viterbi_path ] [EOL] [EOL] most_likely_tags , best_scores = self . naive_most_likely_sequence ( self . logits , mask ) [EOL] [EOL] assert viterbi_tags == most_likely_tags [EOL] assert_allclose ( viterbi_scores , best_scores , rtol = [number] ) [EOL] [EOL] def test_viterbi_tags_no_mask ( self ) : [EOL] viterbi_path = self . crf . viterbi_tags ( self . logits ) [EOL] [EOL] [comment] [EOL] viterbi_tags = [ x for x , y in viterbi_path ] [EOL] viterbi_scores = [ y for x , y in viterbi_path ] [EOL] [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , True , True ] ] ) [EOL] most_likely_tags , best_scores = self . naive_most_likely_sequence ( self . logits , mask ) [EOL] [EOL] assert viterbi_tags == most_likely_tags [EOL] assert_allclose ( viterbi_scores , best_scores , rtol = [number] ) [EOL] [EOL] def test_viterbi_tags_top_k ( self ) : [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , True , False ] ] ) [EOL] [EOL] best_paths = self . crf . viterbi_tags ( self . logits , mask , top_k = [number] ) [EOL] [EOL] [comment] [EOL] top_path_and_score = [ top_k_paths [ [number] ] for top_k_paths in best_paths ] [EOL] assert top_path_and_score == self . crf . viterbi_tags ( self . logits , mask ) [EOL] [EOL] next_path_and_score = [ top_k_paths [ [number] ] for top_k_paths in best_paths ] [EOL] next_viterbi_tags = [ x for x , _ in next_path_and_score ] [EOL] [EOL] [comment] [EOL] assert next_viterbi_tags == [ [ [number] , [number] , [number] ] , [ [number] , [number] ] ] [EOL] [EOL] def test_constrained_viterbi_tags ( self ) : [EOL] constraints = { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , } [EOL] [EOL] [comment] [EOL] [comment] [EOL] for i in range ( [number] ) : [EOL] constraints . add ( ( [number] , i ) ) [EOL] constraints . add ( ( i , [number] ) ) [EOL] [EOL] crf = ConditionalRandomField ( num_tags = [number] , constraints = constraints ) [EOL] crf . transitions = torch . nn . Parameter ( self . transitions ) [EOL] crf . start_transitions = torch . nn . Parameter ( self . transitions_from_start ) [EOL] crf . end_transitions = torch . nn . Parameter ( self . transitions_to_end ) [EOL] [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , True , False ] ] ) [EOL] [EOL] viterbi_path = crf . viterbi_tags ( self . logits , mask ) [EOL] [EOL] [comment] [EOL] viterbi_tags = [ x for x , y in viterbi_path ] [EOL] [EOL] [comment] [EOL] assert viterbi_tags == [ [ [number] , [number] , [number] ] , [ [number] , [number] ] ] [EOL] [EOL] def test_allowed_transitions ( self ) : [EOL] [EOL] bio_labels = [ [string] , [string] , [string] , [string] , [string] ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( bio_labels ) ) ) [EOL] [EOL] [comment] [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , } [EOL] [EOL] bioul_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( bioul_labels ) ) ) [EOL] [EOL] [comment] [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , } [EOL] [EOL] iob1_labels = [ [string] , [string] , [string] , [string] , [string] ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( iob1_labels ) ) ) [EOL] [EOL] [comment] [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , } [EOL] with raises ( ConfigurationError ) : [EOL] allowed_transitions ( [string] , { } ) [EOL] [EOL] bmes_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [comment] [EOL] [comment] [EOL] allowed = allowed_transitions ( [string] , dict ( enumerate ( bmes_labels ) ) ) [EOL] assert set ( allowed ) == { ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.float$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 $typing.Set[typing.Tuple[builtins.int,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import pytest [EOL] import numpy [EOL] import torch [EOL] import torch . nn . init [EOL] from torch . nn . modules . rnn import LSTM [EOL] from torch . nn . utils . rnn import pad_packed_sequence , pack_padded_sequence [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . augmented_lstm import AugmentedLstm , AugmentedLSTMCell , BiAugmentedLstm [EOL] from allennlp . nn import InitializerApplicator , Initializer [EOL] from allennlp . nn . util import sort_batch_by_length [EOL] [EOL] [EOL] class TestAugmentedLSTM ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] tensor [ [number] , [number] : , : ] = [number] [EOL] sequence_lengths = torch . LongTensor ( [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] self . random_tensor = tensor [EOL] self . sequence_lengths = sequence_lengths [EOL] [EOL] def test_variable_length_sequences_return_correctly_padded_outputs ( self ) : [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] tensor = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] lstm = AugmentedLstm ( [number] , [number] ) [EOL] output , _ = lstm ( tensor ) [EOL] output_sequence , _ = pad_packed_sequence ( output , batch_first = True ) [EOL] [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] [EOL] def test_variable_length_sequences_run_backward_return_correctly_padded_outputs ( self ) : [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] tensor = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] lstm = AugmentedLstm ( [number] , [number] , go_forward = False ) [EOL] output , _ = lstm ( tensor ) [EOL] output_sequence , _ = pad_packed_sequence ( output , batch_first = True ) [EOL] [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] numpy . testing . assert_array_equal ( output_sequence . data [ [number] , [number] : , : ] . numpy ( ) , [number] ) [EOL] [EOL] def test_augmented_lstm_computes_same_function_as_pytorch_lstm ( self ) : [EOL] augmented_lstm = AugmentedLstm ( [number] , [number] ) [EOL] pytorch_lstm = LSTM ( [number] , [number] , num_layers = [number] , batch_first = True ) [EOL] [comment] [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( augmented_lstm ) [EOL] initializer ( pytorch_lstm ) [EOL] [EOL] initial_state = torch . zeros ( [ [number] , [number] , [number] ] ) [EOL] initial_memory = torch . zeros ( [ [number] , [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor * [number] , self . sequence_lengths ) [EOL] lstm_input = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] [EOL] augmented_output , augmented_state = augmented_lstm ( lstm_input , ( initial_state , initial_memory ) ) [EOL] pytorch_output , pytorch_state = pytorch_lstm ( lstm_input , ( initial_state , initial_memory ) ) [EOL] pytorch_output_sequence , _ = pad_packed_sequence ( pytorch_output , batch_first = True ) [EOL] augmented_output_sequence , _ = pad_packed_sequence ( augmented_output , batch_first = True ) [EOL] [EOL] numpy . testing . assert_array_almost_equal ( pytorch_output_sequence . data . numpy ( ) , augmented_output_sequence . data . numpy ( ) , decimal = [number] ) [EOL] numpy . testing . assert_array_almost_equal ( pytorch_state [ [number] ] . data . numpy ( ) , augmented_state [ [number] ] . data . numpy ( ) , decimal = [number] ) [EOL] numpy . testing . assert_array_almost_equal ( pytorch_state [ [number] ] . data . numpy ( ) , augmented_state [ [number] ] . data . numpy ( ) , decimal = [number] ) [EOL] [EOL] def test_augmented_lstm_works_with_highway_connections ( self ) : [EOL] augmented_lstm = AugmentedLstm ( [number] , [number] , use_highway = True ) [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] lstm_input = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] augmented_lstm ( lstm_input ) [EOL] [EOL] def test_augmented_lstm_throws_error_on_non_packed_sequence_input ( self ) : [EOL] lstm = AugmentedLstm ( [number] , [number] ) [EOL] tensor = torch . rand ( [ [number] , [number] , [number] ] ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] lstm ( tensor ) [EOL] [EOL] def test_augmented_lstm_is_initialized_with_correct_biases ( self ) : [EOL] lstm = AugmentedLSTMCell ( [number] , [number] ) [EOL] true_state_bias = numpy . array ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] numpy . testing . assert_array_equal ( lstm . state_linearity . bias . data . numpy ( ) , true_state_bias ) [EOL] [EOL] [comment] [EOL] lstm = AugmentedLSTMCell ( [number] , [number] , use_highway = False ) [EOL] true_state_bias = numpy . array ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] numpy . testing . assert_array_equal ( lstm . state_linearity . bias . data . numpy ( ) , true_state_bias ) [EOL] [EOL] def test_dropout_is_not_applied_to_output_or_returned_hidden_states ( self ) : [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] tensor = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] lstm = AugmentedLstm ( [number] , [number] , recurrent_dropout_probability = [number] ) [EOL] [EOL] output , ( hidden_state , _ ) = lstm ( tensor ) [EOL] output_sequence , _ = pad_packed_sequence ( output , batch_first = True ) [EOL] [comment] [EOL] num_hidden_dims_zero_across_timesteps = ( ( output_sequence . sum ( [number] ) == [number] ) . sum ( ) ) . item ( ) [EOL] [comment] [EOL] assert not num_hidden_dims_zero_across_timesteps [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] hidden_state = hidden_state . squeeze ( ) [EOL] num_hidden_dims_zero_across_timesteps = ( ( hidden_state == [number] ) . sum ( ) ) . item ( ) [EOL] assert not num_hidden_dims_zero_across_timesteps [EOL] [EOL] def test_dropout_version_is_different_to_no_dropout ( self ) : [EOL] augmented_lstm = AugmentedLstm ( [number] , [number] ) [EOL] dropped_augmented_lstm = AugmentedLstm ( [number] , [number] , recurrent_dropout_probability = [number] ) [EOL] [comment] [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( augmented_lstm ) [EOL] initializer ( dropped_augmented_lstm ) [EOL] [EOL] initial_state = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] initial_memory = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] [EOL] [comment] [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] lstm_input = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] [EOL] augmented_output , augmented_state = augmented_lstm ( lstm_input , ( initial_state , initial_memory ) ) [EOL] dropped_output , dropped_state = dropped_augmented_lstm ( lstm_input , ( initial_state , initial_memory ) ) [EOL] dropped_output_sequence , _ = pad_packed_sequence ( dropped_output , batch_first = True ) [EOL] augmented_output_sequence , _ = pad_packed_sequence ( augmented_output , batch_first = True ) [EOL] with pytest . raises ( AssertionError ) : [EOL] numpy . testing . assert_array_almost_equal ( dropped_output_sequence . data . numpy ( ) , augmented_output_sequence . data . numpy ( ) , decimal = [number] , ) [EOL] with pytest . raises ( AssertionError ) : [EOL] numpy . testing . assert_array_almost_equal ( dropped_state [ [number] ] . data . numpy ( ) , augmented_state [ [number] ] . data . numpy ( ) , decimal = [number] ) [EOL] with pytest . raises ( AssertionError ) : [EOL] numpy . testing . assert_array_almost_equal ( dropped_state [ [number] ] . data . numpy ( ) , augmented_state [ [number] ] . data . numpy ( ) , decimal = [number] ) [EOL] [EOL] def test_biaugmented_lstm ( self ) : [EOL] for bidirectional in [ True , False ] : [EOL] bi_augmented_lstm = BiAugmentedLstm ( [number] , [number] , [number] , recurrent_dropout_probability = [number] , bidirectional = bidirectional ) [EOL] sorted_tensor , sorted_sequence , _ , _ = sort_batch_by_length ( self . random_tensor , self . sequence_lengths ) [EOL] lstm_input = pack_padded_sequence ( sorted_tensor , sorted_sequence . data . tolist ( ) , batch_first = True ) [EOL] bi_augmented_lstm ( lstm_input ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLSTMCell$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLSTMCell$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLSTMCell$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLSTMCell$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.AugmentedLstm$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.BiAugmentedLstm$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.augmented_lstm.BiAugmentedLstm$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] from numpy . testing import assert_almost_equal [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . modules import Maxout [EOL] from allennlp . nn import InitializerApplicator , Initializer [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestMaxout ( AllenNlpTestCase ) : [EOL] def test_init_checks_output_dims_consistency ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Maxout ( input_dim = [number] , num_layers = [number] , output_dims = [ [number] , [number] , [number] ] , pool_sizes = [number] , dropout = [number] ) [EOL] [EOL] def test_init_checks_pool_sizes_consistency ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Maxout ( input_dim = [number] , num_layers = [number] , output_dims = [number] , pool_sizes = [ [number] , [number] , [number] ] , dropout = [number] ) [EOL] [EOL] def test_init_checks_dropout_consistency ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Maxout ( input_dim = [number] , num_layers = [number] , output_dims = [number] , pool_sizes = [number] , dropout = [ [number] , [number] ] ) [EOL] [EOL] def test_forward_gives_correct_output ( self ) : [EOL] params = Params ( { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } ) [EOL] maxout = Maxout . from_params ( params ) [EOL] [EOL] constant_init = Initializer . from_params ( Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] initializer = InitializerApplicator ( [ ( [string] , constant_init ) ] ) [EOL] initializer ( maxout ) [EOL] [EOL] input_tensor = torch . FloatTensor ( [ [ - [number] , [number] ] ] ) [EOL] output = maxout ( input_tensor ) . data . numpy ( ) [EOL] assert output . shape == ( [number] , [number] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert_almost_equal ( output , [ [ - [number] , - [number] , - [number] ] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.nn.initializers.InitializerApplicator$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
	0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] import numpy [EOL] from overrides import overrides [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2seq_encoders import ComposeEncoder , FeedForwardEncoder , Seq2SeqEncoder [EOL] from allennlp . modules import FeedForward [EOL] [EOL] [EOL] class MockSeq2SeqEncoder ( Seq2SeqEncoder ) : [EOL] def __init__ ( self , input_dim , output_dim , bidirectional = False ) : [EOL] super ( ) . __init__ ( ) [EOL] self . input_dim = input_dim [EOL] self . output_dim = output_dim [EOL] self . bidirectional = bidirectional [EOL] [EOL] @ overrides def forward ( self , inputs , mask ) : [EOL] pass [EOL] [EOL] @ overrides def get_input_dim ( self ) : [EOL] return self . input_dim [EOL] [EOL] @ overrides def get_output_dim ( self ) : [EOL] return self . output_dim [EOL] [EOL] @ overrides def is_bidirectional ( self ) : [EOL] return self . bidirectional [EOL] [EOL] [EOL] def _make_feedforward ( input_dim , output_dim ) : [EOL] return FeedForwardEncoder ( FeedForward ( input_dim = input_dim , num_layers = [number] , activations = torch . nn . ReLU ( ) , hidden_dims = output_dim ) ) [EOL] [EOL] [EOL] class TestPassThroughEncoder ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . encoder = ComposeEncoder ( [ _make_feedforward ( [number] , [number] ) , _make_feedforward ( [number] , [number] ) , _make_feedforward ( [number] , [number] ) ] ) [EOL] [EOL] def test_get_dimension_is_correct ( self ) : [EOL] assert self . encoder . get_input_dim ( ) == [number] [EOL] assert self . encoder . get_output_dim ( ) == [number] [EOL] [EOL] def test_composes ( self ) : [EOL] tensor = torch . zeros ( [number] , [number] , [number] ) [EOL] output = self . encoder ( tensor ) [EOL] [EOL] for encoder in self . encoder . encoders : [EOL] tensor = encoder ( tensor ) [EOL] [EOL] numpy . testing . assert_array_almost_equal ( output . detach ( ) . cpu ( ) . numpy ( ) , tensor . detach ( ) . cpu ( ) . numpy ( ) ) [EOL] [EOL] def test_pass_through_encoder_with_mask ( self ) : [EOL] tensor = torch . randn ( [ [number] , [number] , [number] ] ) [EOL] mask = torch . tensor ( [ [ True , True , True ] , [ True , False , False ] ] ) [EOL] output = self . encoder ( tensor , mask ) [EOL] [EOL] for encoder in self . encoder . encoders : [EOL] tensor = encoder ( tensor , mask ) [EOL] [EOL] numpy . testing . assert_array_almost_equal ( output . detach ( ) . cpu ( ) . numpy ( ) , tensor . detach ( ) . cpu ( ) . numpy ( ) ) [EOL] [EOL] def test_empty ( self ) : [EOL] with pytest . raises ( ValueError ) : [EOL] ComposeEncoder ( [ ] ) [EOL] [EOL] def test_mismatched_size ( self ) : [EOL] with pytest . raises ( ValueError ) : [EOL] ComposeEncoder ( [ MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] ) , MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] ) , ] ) [EOL] [EOL] def test_mismatched_bidirectionality ( self ) : [EOL] with pytest . raises ( ValueError ) : [EOL] ComposeEncoder ( [ MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] ) , MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] , bidirectional = True ) , ] ) [EOL] [EOL] def test_all_bidirectional ( self ) : [EOL] ComposeEncoder ( [ MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] , bidirectional = True ) , MockSeq2SeqEncoder ( input_dim = [number] , output_dim = [number] , bidirectional = True ) , ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Optional , Any [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Optional [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . modules . seq2seq_encoders import PytorchTransformer [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ None , [string] , [string] ] ) def test_positional_embeddings ( positional_encoding ) : [EOL] [comment] [EOL] batch_size = [number] [EOL] max_seq_len = [number] [EOL] n_head = [number] [EOL] dims = [number] * n_head [EOL] transformer = PytorchTransformer ( dims , [number] , positional_encoding = positional_encoding , num_attention_heads = n_head ) [EOL] transformer . eval ( ) [EOL] [EOL] with torch . no_grad ( ) : [EOL] inputs = torch . randn ( batch_size , max_seq_len , dims ) [EOL] mask = torch . ones ( batch_size , max_seq_len , dtype = torch . bool ) [EOL] for b in range ( batch_size ) : [EOL] mask [ b , max_seq_len - b : ] = False [EOL] [EOL] assert not torch . isnan ( inputs ) . any ( ) [EOL] assert torch . isfinite ( inputs ) . all ( ) [EOL] outputs = transformer ( inputs , mask ) [EOL] assert outputs . size ( ) == inputs . size ( ) [EOL] assert not torch . isnan ( outputs ) . any ( ) [EOL] assert torch . isfinite ( outputs ) . all ( ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ None , [string] , [string] ] ) def test_mask_works ( positional_encoding ) : [EOL] [comment] [EOL] batch_size = [number] [EOL] max_seq_len = [number] [EOL] n_head = [number] [EOL] dims = [number] * n_head [EOL] transformer = PytorchTransformer ( dims , [number] , positional_encoding = positional_encoding , num_attention_heads = n_head ) [EOL] transformer . eval ( ) [EOL] [EOL] with torch . no_grad ( ) : [EOL] [comment] [EOL] inputs = torch . randn ( batch_size , max_seq_len , dims ) [EOL] all_ones_mask = torch . ones ( batch_size , max_seq_len , dtype = torch . bool ) [EOL] mask = all_ones_mask . clone ( ) [EOL] for b in range ( batch_size ) : [EOL] mask [ b , max_seq_len - b : ] = False [EOL] altered_inputs = inputs + ( ~ mask ) . unsqueeze ( [number] ) * [number] [EOL] [EOL] [comment] [EOL] assert not torch . allclose ( transformer ( inputs , all_ones_mask ) , transformer ( altered_inputs , all_ones_mask ) ) [EOL] [EOL] [comment] [EOL] assert torch . allclose ( torch . masked_select ( transformer ( inputs , mask ) , mask . unsqueeze ( [number] ) ) , torch . masked_select ( transformer ( altered_inputs , mask ) , mask . unsqueeze ( [number] ) ) , ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ None , [string] , [string] ] ) def test_positional_encodings ( positional_encoding ) : [EOL] [comment] [EOL] batch_size = [number] [EOL] max_seq_len = [number] [EOL] n_head = [number] [EOL] dims = [number] * n_head [EOL] transformer = PytorchTransformer ( dims , [number] , positional_encoding = positional_encoding , num_attention_heads = n_head ) [EOL] transformer . eval ( ) [EOL] [EOL] with torch . no_grad ( ) : [EOL] [comment] [EOL] [comment] [EOL] inputs = torch . randn ( batch_size , max_seq_len , dims ) [EOL] mask = torch . ones ( batch_size , max_seq_len , dtype = torch . bool ) [EOL] for b in range ( batch_size ) : [EOL] mask [ b , max_seq_len - b : ] = False [EOL] unshuffled_output = transformer ( inputs , mask ) [EOL] [EOL] shuffle = torch . arange ( [number] , max_seq_len ) . unsqueeze ( [number] ) . expand_as ( mask ) . clone ( ) [EOL] for b in range ( batch_size ) : [EOL] [comment] [EOL] perm = torch . randperm ( max_seq_len - b ) [EOL] shuffle [ b , : max_seq_len - b ] = shuffle [ b , perm ] [EOL] shuffle = shuffle . unsqueeze ( [number] ) . expand_as ( inputs ) [EOL] shuffled_input = torch . gather ( inputs , [number] , shuffle ) [EOL] shuffled_output = transformer ( shuffled_input , mask ) [EOL] [EOL] if positional_encoding is None : [EOL] assert torch . allclose ( torch . gather ( unshuffled_output , [number] , shuffle ) , shuffled_output , atol = [number] ) [EOL] else : [EOL] assert not torch . allclose ( torch . gather ( unshuffled_output , [number] , shuffle ) , shuffled_output , atol = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import pytest [EOL] import torch [EOL] from numpy . testing import assert_almost_equal [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . modules . token_embedders import BagOfWordCountsTokenEmbedder [EOL] [EOL] [EOL] class TestBagOfWordCountsTokenEmbedder ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . vocab = Vocabulary ( ) [EOL] self . vocab . add_token_to_namespace ( [string] ) [EOL] self . vocab . add_token_to_namespace ( [string] ) [EOL] self . vocab . add_token_to_namespace ( [string] ) [EOL] self . vocab . add_token_to_namespace ( [string] ) [EOL] self . non_padded_vocab = Vocabulary ( non_padded_namespaces = [ [string] ] ) [EOL] [EOL] def test_forward_calculates_bow_properly ( self ) : [EOL] embedder = BagOfWordCountsTokenEmbedder ( self . vocab ) [EOL] numpy_tensor = np . array ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] inputs = torch . from_numpy ( numpy_tensor ) [EOL] embedder_output = embedder ( inputs ) [EOL] numpy_tensor = np . array ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] manual_output = torch . from_numpy ( numpy_tensor ) . float ( ) [EOL] assert_almost_equal ( embedder_output . data . numpy ( ) , manual_output . data . numpy ( ) ) [EOL] [EOL] def test_zeros_out_unknown_tokens ( self ) : [EOL] embedder = BagOfWordCountsTokenEmbedder ( self . vocab , ignore_oov = True ) [EOL] numpy_tensor = np . array ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] inputs = torch . from_numpy ( numpy_tensor ) [EOL] embedder_output = embedder ( inputs ) [EOL] numpy_tensor = np . array ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] manual_output = torch . from_numpy ( numpy_tensor ) . float ( ) [EOL] assert_almost_equal ( embedder_output . data . numpy ( ) , manual_output . data . numpy ( ) ) [EOL] [EOL] def test_ignore_oov_should_fail_on_non_padded_vocab ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] BagOfWordCountsTokenEmbedder ( self . non_padded_vocab , ignore_oov = True ) [EOL] [EOL] def test_projects_properly ( self ) : [EOL] embedder = BagOfWordCountsTokenEmbedder ( vocab = self . vocab , projection_dim = [number] ) [EOL] numpy_tensor = np . array ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) [EOL] inputs = torch . from_numpy ( numpy_tensor ) [EOL] embedder_output = embedder ( inputs ) [EOL] assert embedder_output . shape [ [number] ] == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0
	0
from typing import Any , List [EOL] import typing [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . modules . token_embedders import ElmoTokenEmbedder [EOL] [EOL] [EOL] class TestElmoTokenEmbedder ( ModelTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] , ) [EOL] [EOL] def test_tagger_with_elmo_token_embedder_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_tagger_with_elmo_token_embedder_forward_pass_runs_correctly ( self ) : [EOL] dataset = Batch ( self . instances ) [EOL] dataset . index_instances ( self . vocab ) [EOL] training_tensors = dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] probs = output_dict [ [string] ] [EOL] assert probs . size ( ) == ( [number] , [number] , self . model . vocab . get_vocab_size ( [string] ) ) [EOL] [EOL] def test_forward_works_with_projection_layer ( self ) : [EOL] params = Params ( { [string] : self . FIXTURES_ROOT / [string] / [string] , [string] : self . FIXTURES_ROOT / [string] / [string] , [string] : [number] , } ) [EOL] word1 = [ [number] ] * [number] [EOL] word2 = [ [number] ] * [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word1 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] word2 [ [number] ] = [number] [EOL] embedding_layer = ElmoTokenEmbedder . from_params ( vocab = None , params = params ) [EOL] assert embedding_layer . get_output_dim ( ) == [number] [EOL] [EOL] input_tensor = torch . LongTensor ( [ [ word1 , word2 ] ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] , [number] ) [EOL] [EOL] input_tensor = torch . LongTensor ( [ [ [ word1 ] ] ] ) [EOL] embedded = embedding_layer ( input_tensor ) . data . numpy ( ) [EOL] assert embedded . shape == ( [number] , [number] , [number] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import PretrainedTransformerMismatchedIndexer [EOL] from allennlp . modules . text_field_embedders import BasicTextFieldEmbedder [EOL] from allennlp . modules . token_embedders import PretrainedTransformerMismatchedEmbedder [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] [EOL] [EOL] class TestPretrainedTransformerMismatchedEmbedder ( AllenNlpTestCase ) : [EOL] @ pytest . mark . parametrize ( [string] , [ True , False ] ) def test_end_to_end ( self , train_parameters ) : [EOL] token_indexer = PretrainedTransformerMismatchedIndexer ( [string] ) [EOL] [EOL] sentence1 = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sentence2 = [ [string] , [string] , [string] ] [EOL] tokens1 = [ Token ( word ) for word in sentence1 ] [EOL] tokens2 = [ Token ( word ) for word in sentence2 ] [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] params = Params ( { [string] : { [string] : { [string] : [string] , [string] : [string] , [string] : train_parameters , } } } ) [EOL] token_embedder = BasicTextFieldEmbedder . from_params ( vocab = vocab , params = params ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , ] [EOL] [EOL] [comment] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] assert bert_vectors . size ( ) == ( [number] , max ( len ( sentence1 ) , len ( sentence2 ) ) , [number] ) [EOL] assert not torch . isnan ( bert_vectors ) . any ( ) [EOL] assert bert_vectors . requires_grad == train_parameters [EOL] [EOL] def test_long_sequence_splitting_end_to_end ( self ) : [EOL] token_indexer = PretrainedTransformerMismatchedIndexer ( [string] , max_length = [number] ) [EOL] [EOL] sentence1 = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sentence2 = [ [string] , [string] , [string] ] [EOL] tokens1 = [ Token ( word ) for word in sentence1 ] [EOL] tokens2 = [ Token ( word ) for word in sentence2 ] [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] params = Params ( { [string] : { [string] : { [string] : [string] , [string] : [string] , [string] : [number] , } } } ) [EOL] token_embedder = BasicTextFieldEmbedder . from_params ( vocab = vocab , params = params ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ True , True , True , True , True ] , [ True , True , True , False , False ] , ] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , ] [EOL] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] assert bert_vectors . size ( ) == ( [number] , max ( len ( sentence1 ) , len ( sentence2 ) ) , [number] ) [EOL] assert not torch . isnan ( bert_vectors ) . any ( ) [EOL] [EOL] def test_token_without_wordpieces ( self ) : [EOL] token_indexer = PretrainedTransformerMismatchedIndexer ( [string] ) [EOL] [EOL] sentence1 = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sentence2 = [ [string] , [string] , [string] ] [EOL] tokens1 = [ Token ( word ) for word in sentence1 ] [EOL] tokens2 = [ Token ( word ) for word in sentence2 ] [EOL] vocab = Vocabulary ( ) [EOL] params = Params ( { [string] : { [string] : { [string] : [string] , [string] : [string] , } } } ) [EOL] token_embedder = BasicTextFieldEmbedder . from_params ( vocab = vocab , params = params ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ [ [number] , [number] ] , [ - [number] , - [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ - [number] , - [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , ] [EOL] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] assert bert_vectors . size ( ) == ( [number] , max ( len ( sentence1 ) , len ( sentence2 ) ) , [number] ) [EOL] assert not torch . isnan ( bert_vectors ) . any ( ) [EOL] assert all ( bert_vectors [ [number] , [number] ] == [number] ) [EOL] assert all ( bert_vectors [ [number] , [number] ] == [number] ) [EOL] [EOL] def test_exotic_tokens_no_nan_grads ( self ) : [EOL] token_indexer = PretrainedTransformerMismatchedIndexer ( [string] ) [EOL] [EOL] sentence1 = [ [string] , [string] , [string] , [string] , [string] ] [EOL] sentence2 = [ [string] , [string] , [string] , [string] , [string] ] [EOL] [EOL] tokens1 = [ Token ( word ) for word in sentence1 ] [EOL] tokens2 = [ Token ( word ) for word in sentence2 ] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] token_embedder = BasicTextFieldEmbedder ( { [string] : PretrainedTransformerMismatchedEmbedder ( [string] ) } ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] test_loss = bert_vectors . mean ( ) [EOL] [EOL] test_loss . backward ( ) [EOL] [EOL] for name , param in token_embedder . named_parameters ( ) : [EOL] grad = param . grad [EOL] assert ( grad is None ) or ( not torch . any ( torch . isnan ( grad ) ) . item ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.modules.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.modules.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import math [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import PretrainedTransformerIndexer [EOL] from allennlp . data . tokenizers import PretrainedTransformerTokenizer [EOL] from allennlp . modules . text_field_embedders import BasicTextFieldEmbedder [EOL] from allennlp . modules . token_embedders import PretrainedTransformerEmbedder [EOL] [EOL] [EOL] class TestPretrainedTransformerEmbedder ( AllenNlpTestCase ) : [EOL] def test_forward_runs_when_initialized_from_params ( self ) : [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : [string] } ) [EOL] embedder = PretrainedTransformerEmbedder . from_params ( params ) [EOL] token_ids = torch . randint ( [number] , [number] , ( [number] , [number] ) ) [EOL] mask = torch . randint ( [number] , [number] , ( [number] , [number] ) ) . bool ( ) [EOL] output = embedder ( token_ids = token_ids , mask = mask ) [EOL] assert tuple ( output . size ( ) ) == ( [number] , [number] , [number] ) [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( True , True , False ) , ( False , True , False ) , ( True , False , False ) , ( False , False , False ) , ( True , False , True , ) , ] , ) def test_end_to_end ( self , train_parameters , last_layer_only , gradient_checkpointing ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( model_name = [string] ) [EOL] token_indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] [EOL] sentence1 = [string] [EOL] tokens1 = tokenizer . tokenize ( sentence1 ) [EOL] expected_tokens1 = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in tokens1 ] == expected_tokens1 [EOL] [EOL] sentence2 = [string] [EOL] tokens2 = tokenizer . tokenize ( sentence2 ) [EOL] expected_tokens2 = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] assert [ t . text for t in tokens2 ] == expected_tokens2 [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] params = Params ( { [string] : { [string] : { [string] : [string] , [string] : [string] , [string] : train_parameters , [string] : last_layer_only , [string] : gradient_checkpointing , } } } ) [EOL] token_embedder = BasicTextFieldEmbedder . from_params ( vocab = vocab , params = params ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] max_length = max ( len ( tokens1 ) , len ( tokens2 ) ) [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . shape == ( [number] , max_length ) [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ True , True , True , True , True , True , True , True , True ] , [ True , True , True , True , True , True , True , False , False ] , ] [EOL] [EOL] [comment] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] assert bert_vectors . size ( ) == ( [number] , [number] , [number] ) [EOL] assert bert_vectors . requires_grad == ( train_parameters or not last_layer_only ) [EOL] [EOL] def test_big_token_type_ids ( self ) : [EOL] token_embedder = PretrainedTransformerEmbedder ( [string] ) [EOL] token_ids = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] mask = torch . ones_like ( token_ids ) . bool ( ) [EOL] type_ids = torch . zeros_like ( token_ids ) [EOL] type_ids [ [number] , [number] ] = [number] [EOL] with pytest . raises ( ValueError ) : [EOL] token_embedder ( token_ids , mask , type_ids ) [EOL] [EOL] def test_xlnet_token_type_ids ( self ) : [EOL] token_embedder = PretrainedTransformerEmbedder ( [string] ) [EOL] token_ids = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] mask = torch . ones_like ( token_ids ) . bool ( ) [EOL] type_ids = torch . zeros_like ( token_ids ) [EOL] type_ids [ [number] , [number] ] = [number] [EOL] token_embedder ( token_ids , mask , type_ids ) [EOL] [EOL] def test_long_sequence_splitting_end_to_end ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [EOL] tokenizer = PretrainedTransformerTokenizer ( model_name = [string] ) [EOL] token_indexer = PretrainedTransformerIndexer ( model_name = [string] , max_length = [number] ) [EOL] [EOL] sentence1 = [string] [EOL] tokens1 = tokenizer . tokenize ( sentence1 ) [EOL] sentence2 = [string] [EOL] tokens2 = tokenizer . tokenize ( sentence2 ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] [EOL] params = Params ( { [string] : { [string] : { [string] : [string] , [string] : [string] , [string] : [number] , } } } ) [EOL] token_embedder = BasicTextFieldEmbedder . from_params ( vocab = vocab , params = params ) [EOL] [EOL] instance1 = Instance ( { [string] : TextField ( tokens1 , { [string] : token_indexer } ) } ) [EOL] instance2 = Instance ( { [string] : TextField ( tokens2 , { [string] : token_indexer } ) } ) [EOL] [EOL] batch = Batch ( [ instance1 , instance2 ] ) [EOL] batch . index_instances ( vocab ) [EOL] [EOL] padding_lengths = batch . get_padding_lengths ( ) [EOL] tensor_dict = batch . as_tensor_dict ( padding_lengths ) [EOL] tokens = tensor_dict [ [string] ] [EOL] max_length = max ( len ( tokens1 ) , len ( tokens2 ) ) [EOL] [EOL] [comment] [EOL] segment_concat_length = int ( math . ceil ( max_length / [number] ) ) * [number] + max_length [EOL] assert tokens [ [string] ] [ [string] ] . shape == ( [number] , segment_concat_length ) [EOL] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ True , True , True , True , True , True , True , True , True ] , [ True , True , True , True , True , True , True , False , False ] , ] [EOL] assert tokens [ [string] ] [ [string] ] . tolist ( ) == [ [ True ] * segment_concat_length , [ True ] * ( segment_concat_length - [number] ) + [ False ] * [number] , ] [EOL] [EOL] [comment] [EOL] bert_vectors = token_embedder ( tokens ) [EOL] assert bert_vectors . size ( ) == ( [number] , [number] , [number] ) [EOL] [EOL] def test_fold_long_sequences ( self ) : [EOL] [comment] [EOL] token_ids = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , ] ) [comment] [EOL] segment_concat_mask = ( token_ids > [number] ) . long ( ) [EOL] [EOL] folded_token_ids = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , ] ) [EOL] folded_segment_concat_mask = ( folded_token_ids > [number] ) . long ( ) [EOL] [EOL] token_embedder = PretrainedTransformerEmbedder ( [string] , max_length = [number] ) [EOL] [EOL] ( folded_token_ids_out , folded_segment_concat_mask_out , _ , ) = token_embedder . _fold_long_sequences ( token_ids , segment_concat_mask ) [EOL] assert ( folded_token_ids_out == folded_token_ids ) . all ( ) [EOL] assert ( folded_segment_concat_mask_out == folded_segment_concat_mask ) . all ( ) [EOL] [EOL] def test_unfold_long_sequences ( self ) : [EOL] [comment] [EOL] [comment] [EOL] embeddings = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] , ] ) . unsqueeze ( - [number] ) [EOL] mask = ( embeddings > [number] ) . long ( ) [EOL] [EOL] unfolded_embeddings = torch . LongTensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , ] ) . unsqueeze ( - [number] ) [EOL] [EOL] token_embedder = PretrainedTransformerEmbedder ( [string] , max_length = [number] ) [EOL] [EOL] unfolded_embeddings_out = token_embedder . _unfold_long_sequences ( embeddings , mask , unfolded_embeddings . size ( [number] ) , [number] ) [EOL] assert ( unfolded_embeddings_out == unfolded_embeddings ) . all ( ) [EOL] [EOL] def test_encoder_decoder_model ( self ) : [EOL] token_embedder = PretrainedTransformerEmbedder ( [string] , sub_module = [string] ) [EOL] token_ids = torch . LongTensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] mask = torch . ones_like ( token_ids ) . bool ( ) [EOL] token_embedder ( token_ids , mask ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[builtins.str]$ 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.bool$ 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0
	0
	0
	0
from typing import Any [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2vec_encoders import BertPooler [EOL] [EOL] [EOL] class TestBertPooler ( AllenNlpTestCase ) : [EOL] def test_encoder ( self ) : [EOL] encoder = BertPooler ( [string] ) [EOL] assert encoder . get_input_dim ( ) == encoder . get_output_dim ( ) [EOL] embedding = torch . rand ( [number] , [number] , encoder . get_input_dim ( ) ) [EOL] [EOL] pooled1 = encoder ( embedding ) [EOL] assert pooled1 . size ( ) == ( [number] , encoder . get_input_dim ( ) ) [EOL] [EOL] embedding [ : , [number] : , : ] = [number] [EOL] pooled2 = encoder ( embedding ) [EOL] numpy . testing . assert_array_almost_equal ( pooled1 . detach ( ) . numpy ( ) , pooled2 . detach ( ) . numpy ( ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] import numpy [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2vec_encoders . cls_pooler import ClsPooler [EOL] [EOL] [EOL] class TestClsPooler ( AllenNlpTestCase ) : [EOL] def test_encoder ( self ) : [EOL] embedding = torch . rand ( [number] , [number] , [number] ) [EOL] encoder = ClsPooler ( embedding_dim = [number] ) [EOL] pooled = encoder ( embedding , mask = None ) [EOL] [EOL] assert list ( pooled . size ( ) ) == [ [number] , [number] ] [EOL] numpy . testing . assert_array_almost_equal ( embedding [ : , [number] ] , pooled ) [EOL] [EOL] def test_cls_at_end ( self ) : [EOL] embedding = torch . arange ( [number] ) . reshape ( [number] , [number] ) . unsqueeze ( - [number] ) . expand ( [number] , [number] , [number] ) [EOL] mask = torch . tensor ( [ [ True , True , True , True ] , [ True , True , True , False ] , [ True , True , True , True ] , [ True , False , False , False ] , [ True , True , False , False ] , ] ) [EOL] expected = torch . LongTensor ( [ [number] , [number] , [number] , [number] , [number] ] ) . unsqueeze ( - [number] ) . expand ( [number] , [number] ) [EOL] [EOL] encoder = ClsPooler ( embedding_dim = [number] , cls_is_last_token = True ) [EOL] pooled = encoder ( embedding , mask = mask ) [EOL] [EOL] assert list ( pooled . size ( ) ) == [ [number] , [number] ] [EOL] numpy . testing . assert_array_almost_equal ( expected , pooled ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.seq2vec_encoders.cls_pooler.ClsPooler$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.modules.seq2vec_encoders.cls_pooler.ClsPooler$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.modules.seq2vec_encoders.cls_pooler.ClsPooler$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.modules.seq2vec_encoders.cls_pooler.ClsPooler$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import Any [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . modules . seq2vec_encoders . cnn_highway_encoder import CnnHighwayEncoder [EOL] from allennlp . modules . time_distributed import TimeDistributed [EOL] [EOL] [EOL] class TestCnnHighwayEncoder ( AllenNlpTestCase ) : [EOL] def run_encoder_against_random_embeddings ( self , do_layer_norm ) : [EOL] encoder = CnnHighwayEncoder ( activation = [string] , embedding_dim = [number] , filters = [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , num_highway = [number] , projection_dim = [number] , projection_location = [string] , do_layer_norm = do_layer_norm , ) [EOL] encoder = TimeDistributed ( encoder ) [EOL] [EOL] embedding = torch . from_numpy ( np . random . randn ( [number] , [number] , [number] , [number] ) ) . float ( ) [EOL] mask = torch . ones ( [number] , [number] , [number] ) . bool ( ) [EOL] token_embedding = encoder ( embedding , mask ) [EOL] [EOL] assert list ( token_embedding . size ( ) ) == [ [number] , [number] , [number] ] [EOL] [EOL] def test_cnn_highway_encoder ( self ) : [EOL] self . run_encoder_against_random_embeddings ( do_layer_norm = False ) [EOL] [EOL] def test_cnn_highway_encoder_with_layer_norm ( self ) : [EOL] self . run_encoder_against_random_embeddings ( do_layer_norm = True ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Type , Tuple [EOL] import tests [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import copy [EOL] import glob [EOL] import json [EOL] import os [EOL] import re [EOL] import time [EOL] from typing import Any , Dict , List [EOL] [EOL] import math [EOL] import pytest [EOL] [EOL] import torch [EOL] from torch . utils . data import DataLoader [EOL] from torch . nn . utils import clip_grad_norm_ [EOL] from allennlp . data . dataloader import PyTorchDataLoader [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . testing import AllenNlpTestCase , requires_gpu , requires_multi_gpu [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . data . dataloader import TensorDict [EOL] from allennlp . data . dataset_readers import SequenceTaggingDatasetReader [EOL] from allennlp . models . model import Model [EOL] from allennlp . models . simple_tagger import SimpleTagger [EOL] from allennlp . training import ( GradientDescentTrainer , Checkpointer , TensorboardWriter , BatchCallback , EpochCallback , TrackEpochCallback , ) [EOL] from allennlp . training . learning_rate_schedulers import CosineWithRestarts [EOL] from allennlp . training . learning_rate_schedulers import ExponentialLearningRateScheduler [EOL] from allennlp . training . momentum_schedulers import MomentumScheduler [EOL] from allennlp . training . moving_average import ExponentialMovingAverage [EOL] from allennlp . data import allennlp_collate [EOL] [EOL] [EOL] class TrainerTestBase ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . instances = SequenceTaggingDatasetReader ( ) . read ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] self . instances_lazy = SequenceTaggingDatasetReader ( lazy = True ) . read ( self . FIXTURES_ROOT / [string] / [string] ) [EOL] vocab = Vocabulary . from_instances ( self . instances ) [EOL] self . vocab = vocab [EOL] self . model_params = Params ( { [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } ) [EOL] self . model = SimpleTagger . from_params ( vocab = self . vocab , params = self . model_params ) [EOL] self . optimizer = torch . optim . SGD ( self . model . parameters ( ) , [number] , momentum = [number] ) [EOL] self . data_loader = DataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] self . data_loader_lazy = DataLoader ( self . instances_lazy , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] self . validation_data_loader = DataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] self . instances . index_with ( vocab ) [EOL] self . instances_lazy . index_with ( vocab ) [EOL] [EOL] [EOL] class TestTrainer ( TrainerTestBase ) : [EOL] def test_trainer_can_run ( self ) : [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] [EOL] [comment] [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert metrics [ [string] ] > [number] [EOL] [EOL] def test_trainer_can_run_exponential_moving_average ( self ) : [EOL] moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) , decay = [number] ) [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , moving_average = moving_average , ) [EOL] trainer . train ( ) [EOL] [EOL] @ requires_gpu def test_trainer_can_run_cuda ( self ) : [EOL] self . model . cuda ( ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , cuda_device = [number] ) [EOL] metrics = trainer . train ( ) [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , float ) [EOL] assert metrics [ [string] ] > [number] [EOL] assert [string] in metrics [EOL] assert isinstance ( metrics [ [string] ] , int ) [EOL] [EOL] @ requires_multi_gpu def test_passing_trainer_multiple_gpus_raises_error ( self ) : [EOL] self . model . cuda ( ) [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , cuda_device = [ [number] , [number] ] , ) [EOL] [EOL] def test_data_loader_lazy_epoch_size_correct ( self ) : [EOL] num_epochs = [number] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader_lazy , validation_data_loader = self . validation_data_loader , num_epochs = num_epochs , serialization_dir = self . TEST_DIR , ) [EOL] assert trainer . _batch_num_total == [number] [EOL] metrics = trainer . train ( ) [EOL] epoch = metrics [ [string] ] [EOL] assert epoch == num_epochs - [number] [EOL] assert trainer . _batch_num_total == num_epochs * [number] [EOL] [EOL] def test_data_loader_lazy_epoch_size_correct_custom_epoch_size ( self ) : [EOL] batches_per_epoch = [number] [EOL] num_epochs = [number] [EOL] data_loader_custom_epoch_lazy = PyTorchDataLoader ( self . instances_lazy , batch_size = [number] , collate_fn = allennlp_collate , batches_per_epoch = batches_per_epoch , ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader_custom_epoch_lazy , validation_data_loader = self . validation_data_loader , num_epochs = num_epochs , serialization_dir = self . TEST_DIR , ) [EOL] assert trainer . _batch_num_total == [number] [EOL] metrics = trainer . train ( ) [EOL] epoch = metrics [ [string] ] [EOL] assert epoch == num_epochs - [number] [EOL] assert trainer . _batch_num_total == num_epochs * batches_per_epoch [EOL] [EOL] def test_trainer_respects_epoch_size_equals_total ( self ) : [EOL] batches_per_epoch = [number] [EOL] num_epochs = [number] [EOL] data_loader_equal_epoch = PyTorchDataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate , batches_per_epoch = batches_per_epoch , ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader_equal_epoch , validation_data_loader = self . validation_data_loader , num_epochs = num_epochs , serialization_dir = self . TEST_DIR , ) [EOL] assert trainer . _batch_num_total == [number] [EOL] metrics = trainer . train ( ) [EOL] epoch = metrics [ [string] ] [EOL] assert epoch == num_epochs - [number] [EOL] assert trainer . _batch_num_total == num_epochs * batches_per_epoch [EOL] [EOL] def test_trainer_respects_epoch_size_larger_tnan_total ( self ) : [EOL] batches_per_epoch = [number] [EOL] num_epochs = [number] [EOL] data_loader_larger_epoch = PyTorchDataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate , batches_per_epoch = batches_per_epoch , ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader_larger_epoch , validation_data_loader = self . validation_data_loader , num_epochs = num_epochs , serialization_dir = self . TEST_DIR , ) [EOL] assert trainer . _batch_num_total == [number] [EOL] metrics = trainer . train ( ) [EOL] epoch = metrics [ [string] ] [EOL] assert epoch == num_epochs - [number] [EOL] assert trainer . _batch_num_total == num_epochs * batches_per_epoch [EOL] [EOL] def test_trainer_respects_epoch_size_smaller_tnan_total ( self ) : [EOL] batches_per_epoch = [number] [EOL] num_epochs = [number] [EOL] data_loader_smaller_epoch = PyTorchDataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate , batches_per_epoch = batches_per_epoch , ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader_smaller_epoch , validation_data_loader = self . validation_data_loader , num_epochs = num_epochs , serialization_dir = self . TEST_DIR , ) [EOL] assert trainer . _batch_num_total == [number] [EOL] metrics = trainer . train ( ) [EOL] epoch = metrics [ [string] ] [EOL] assert epoch == num_epochs - [number] [EOL] assert trainer . _batch_num_total == num_epochs * batches_per_epoch [EOL] [EOL] def test_trainer_can_resume_training ( self ) : [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] [EOL] epoch = new_trainer . _restore_checkpoint ( ) [EOL] assert epoch == [number] [EOL] [EOL] tracker = trainer . _metric_tracker [EOL] assert tracker . is_best_so_far ( ) [EOL] assert tracker . _best_so_far is not None [EOL] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_can_resume_training_for_exponential_moving_average ( self ) : [EOL] moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) ) [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , moving_average = moving_average , ) [EOL] trainer . train ( ) [EOL] [EOL] new_moving_average = ExponentialMovingAverage ( self . model . named_parameters ( ) ) [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , moving_average = new_moving_average , ) [EOL] [EOL] epoch = new_trainer . _restore_checkpoint ( ) [EOL] assert epoch == [number] [EOL] [EOL] tracker = trainer . _metric_tracker [EOL] assert tracker . is_best_so_far ( ) [EOL] assert tracker . _best_so_far is not None [EOL] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_increasing_metric ( self , ) : [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) [EOL] tracker = new_trainer . _metric_tracker [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metric ( [number] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] def test_metric_only_considered_best_so_far_when_strictly_better_than_those_before_it_decreasing_metric ( self , ) : [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) [EOL] tracker = new_trainer . _metric_tracker [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metric ( [number] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . is_best_so_far ( ) [EOL] [EOL] [comment] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] def test_should_stop_early_with_increasing_metric ( self ) : [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) [EOL] [EOL] tracker = new_trainer . _metric_tracker [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . should_stop_early ( ) [EOL] [EOL] def test_should_stop_early_with_flat_lining_metric ( self ) : [EOL] flatline = [ [number] ] * [number] [EOL] tracker = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) . _metric_tracker [EOL] tracker . add_metrics ( flatline ) [EOL] assert tracker . should_stop_early [EOL] [EOL] tracker = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) . _metric_tracker [EOL] tracker . add_metrics ( flatline ) [EOL] assert tracker . should_stop_early [EOL] [EOL] def test_should_stop_early_with_decreasing_metric ( self ) : [EOL] new_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , patience = [number] , validation_metric = [string] , ) [EOL] tracker = new_trainer . _metric_tracker [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert not new_tracker . should_stop_early ( ) [EOL] [EOL] new_tracker = copy . deepcopy ( tracker ) [EOL] new_tracker . add_metrics ( [ [number] , [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert new_tracker . should_stop_early ( ) [EOL] [EOL] def test_should_stop_early_with_early_stopping_disabled ( self ) : [EOL] [comment] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , patience = None , validation_metric = [string] , ) [EOL] tracker = trainer . _metric_tracker [EOL] tracker . add_metrics ( [ float ( i ) for i in reversed ( range ( [number] ) ) ] ) [EOL] assert not tracker . should_stop_early ( ) [EOL] [EOL] [comment] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , patience = None , validation_metric = [string] , ) [EOL] tracker = trainer . _metric_tracker [EOL] tracker . add_metrics ( [ float ( i ) for i in range ( [number] ) ] ) [EOL] assert not tracker . should_stop_early ( ) [EOL] [EOL] def test_should_stop_early_with_invalid_patience ( self ) : [EOL] for patience in [ [number] , - [number] , - [number] , [number] , [string] ] : [EOL] with pytest . raises ( ConfigurationError , match = [string] [string] [string] , ) : [EOL] GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , patience = patience , validation_metric = [string] , ) [EOL] [EOL] def test_trainer_can_run_and_resume_with_momentum_scheduler ( self ) : [EOL] scheduler = MomentumScheduler . from_params ( optimizer = self . optimizer , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) , ) [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , momentum_scheduler = scheduler , validation_metric = [string] , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] [EOL] new_scheduler = MomentumScheduler . from_params ( optimizer = self . optimizer , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) , ) [EOL] new_trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , momentum_scheduler = new_scheduler , validation_metric = [string] , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] epoch = new_trainer . _restore_checkpoint ( ) [EOL] assert epoch == [number] [EOL] assert new_trainer . _momentum_scheduler . last_epoch == [number] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_can_run_with_lr_scheduler ( self ) : [EOL] lr_scheduler = ExponentialLearningRateScheduler ( self . optimizer , gamma = [number] ) [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , learning_rate_scheduler = lr_scheduler , validation_metric = [string] , validation_data_loader = self . validation_data_loader , num_epochs = [number] , ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_can_resume_with_lr_scheduler ( self ) : [EOL] lr_scheduler = CosineWithRestarts ( self . optimizer , t_initial = [number] ) [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , learning_rate_scheduler = lr_scheduler , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] [EOL] new_lr_scheduler = CosineWithRestarts ( self . optimizer , t_initial = [number] ) [EOL] new_trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , learning_rate_scheduler = new_lr_scheduler , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] epoch = new_trainer . _restore_checkpoint ( ) [EOL] assert epoch == [number] [EOL] assert new_trainer . _learning_rate_scheduler . last_epoch == [number] [EOL] new_trainer . train ( ) [EOL] [EOL] def test_trainer_raises_on_model_with_no_loss_key ( self ) : [EOL] class FakeModel ( Model ) : [EOL] def forward ( self , ** kwargs ) : [EOL] return { } [EOL] [EOL] with pytest . raises ( RuntimeError ) : [EOL] trainer = GradientDescentTrainer ( FakeModel ( None ) , self . optimizer , self . data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_can_log_histograms ( self ) : [EOL] [comment] [EOL] for module in self . model . modules ( ) : [EOL] module . should_log_activations = True [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , tensorboard_writer = TensorboardWriter ( serialization_dir = self . TEST_DIR , histogram_interval = [number] ) , ) [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_respects_num_serialized_models_to_keep ( self ) : [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] ) , ) [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] for prefix in [ [string] , [string] ] : [EOL] file_names = glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) [EOL] epochs = [ int ( re . search ( [string] , fname ) . group ( [number] ) ) for fname in file_names ] [EOL] assert sorted ( epochs ) == [ [number] , [number] , [number] ] [EOL] [EOL] def test_trainer_saves_metrics_every_epoch ( self ) : [EOL] trainer = GradientDescentTrainer ( model = self . model , optimizer = self . optimizer , data_loader = self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] ) , ) [EOL] trainer . train ( ) [EOL] [EOL] for epoch in range ( [number] ) : [EOL] epoch_file = self . TEST_DIR / f" [string] { epoch } [string] " [EOL] assert epoch_file . exists ( ) [EOL] metrics = json . load ( open ( epoch_file ) ) [EOL] assert [string] in metrics [EOL] assert [string] in metrics [EOL] assert metrics . get ( [string] ) == epoch [EOL] [EOL] def test_trainer_respects_keep_serialized_model_every_num_seconds ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] class SlowDataLoader : [EOL] data_loader = DataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] [EOL] def __iter__ ( self ) : [EOL] time . sleep ( [number] ) [EOL] return iter ( self . data_loader ) [EOL] [EOL] def __len__ ( self ) : [EOL] return len ( self . data_loader ) [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , SlowDataLoader ( ) , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] , keep_serialized_model_every_num_seconds = [number] , ) , ) [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] for prefix in [ [string] , [string] ] : [EOL] file_names = glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) [EOL] epochs = [ int ( re . search ( [string] , fname ) . group ( [number] ) ) for fname in file_names ] [EOL] [comment] [EOL] assert sorted ( epochs ) == [ [number] , [number] , [number] , [number] ] [EOL] [EOL] def test_trainer_can_log_learning_rates_tensorboard ( self ) : [EOL] data_loader = DataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , tensorboard_writer = TensorboardWriter ( serialization_dir = self . TEST_DIR , should_log_learning_rate = True , summary_interval = [number] , ) , ) [EOL] [EOL] trainer . train ( ) [EOL] [EOL] def test_trainer_saves_models_at_specified_interval ( self ) : [EOL] data_loader = DataLoader ( self . instances , batch_size = [number] , collate_fn = allennlp_collate ) [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , model_save_interval = [number] , num_serialized_models_to_keep = [number] , ) , ) [EOL] [EOL] trainer . train ( ) [EOL] [EOL] [comment] [EOL] prefix = [string] [EOL] file_names = sorted ( glob . glob ( os . path . join ( self . TEST_DIR , prefix ) ) ) [EOL] epochs = [ re . search ( [string] , fname ) . group ( [number] ) for fname in file_names ] [EOL] [comment] [EOL] [comment] [EOL] assert len ( epochs ) == [number] [EOL] assert epochs [ [number] ] == [string] [EOL] assert [string] in epochs [ [number] ] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for k in range ( [number] ) : [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] . format ( k ) ) ) [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] . format ( k ) ) ) [EOL] os . remove ( os . path . join ( self . TEST_DIR , [string] ) ) [EOL] [EOL] restore_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , model_save_interval = [number] ) , ) [EOL] epoch = restore_trainer . _restore_checkpoint ( ) [EOL] assert epoch == [number] [EOL] [comment] [EOL] assert restore_trainer . _batch_num_total == [number] [EOL] [EOL] def test_trainer_saves_and_loads_best_validation_metrics_correctly_1 ( self ) : [EOL] [comment] [EOL] [comment] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] _ = trainer . _restore_checkpoint ( ) [EOL] best_epoch_1 = trainer . _metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_1 = trainer . _metric_tracker . best_epoch_metrics [EOL] [comment] [EOL] assert isinstance ( best_validation_metrics_epoch_1 , dict ) [EOL] assert [string] in best_validation_metrics_epoch_1 [EOL] [EOL] [comment] [EOL] restore_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] restore_trainer . train ( ) [EOL] _ = restore_trainer . _restore_checkpoint ( ) [EOL] best_epoch_2 = restore_trainer . _metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_2 = restore_trainer . _metric_tracker . best_epoch_metrics [EOL] [EOL] [comment] [EOL] assert best_epoch_1 == [number] and best_epoch_2 == [number] [EOL] assert best_validation_metrics_epoch_2 != best_validation_metrics_epoch_1 [EOL] [EOL] def test_trainer_saves_and_loads_best_validation_metrics_correctly_2 ( self ) : [EOL] [comment] [EOL] [comment] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] trainer . train ( ) [EOL] [EOL] _ = trainer . _restore_checkpoint ( ) [EOL] best_epoch_1 = trainer . _metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_1 = trainer . _metric_tracker . best_epoch_metrics [EOL] [comment] [EOL] assert isinstance ( best_validation_metrics_epoch_1 , dict ) [EOL] assert [string] in best_validation_metrics_epoch_1 [EOL] [EOL] [comment] [EOL] restore_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] restore_trainer . train ( ) [EOL] _ = restore_trainer . _restore_checkpoint ( ) [EOL] best_epoch_2 = restore_trainer . _metric_tracker . best_epoch [EOL] best_validation_metrics_epoch_2 = restore_trainer . _metric_tracker . best_epoch_metrics [EOL] [EOL] [comment] [EOL] assert best_epoch_1 == best_epoch_2 == [number] [EOL] assert best_validation_metrics_epoch_2 == best_validation_metrics_epoch_1 [EOL] [EOL] def test_restored_training_returns_best_epoch_metrics_even_if_no_better_epoch_is_found_after_restoring ( self , ) : [EOL] [comment] [EOL] [comment] [EOL] original_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] training_metrics = original_trainer . train ( ) [EOL] [EOL] [comment] [EOL] restored_trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , validation_metric = [string] , num_epochs = [number] , serialization_dir = self . TEST_DIR , ) [EOL] restored_metrics = restored_trainer . train ( ) [EOL] [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] assert [string] in restored_metrics [EOL] [EOL] [comment] [EOL] assert training_metrics [ [string] ] == restored_metrics [ [string] ] [EOL] assert training_metrics [ [string] ] == [number] [EOL] assert training_metrics [ [string] ] > restored_metrics [ [string] ] [EOL] [EOL] def test_restoring_works_with_older_checkpointing ( self ) : [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , serialization_dir = self . TEST_DIR , checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] ) , ) [EOL] trainer . train ( ) [EOL] [EOL] for index in range ( [number] ) : [EOL] path = str ( self . TEST_DIR / [string] . format ( index ) ) [EOL] state = torch . load ( path ) [EOL] state . pop ( [string] ) [EOL] state . pop ( [string] ) [EOL] state [ [string] ] = [ [number] , [number] , [number] ] [EOL] torch . save ( state , path ) [EOL] [EOL] next_epoch = trainer . _restore_checkpoint ( ) [EOL] best_epoch = trainer . _metric_tracker . best_epoch [EOL] [EOL] [comment] [EOL] assert next_epoch == [number] [EOL] assert best_epoch == [number] [EOL] assert trainer . _metric_tracker . _best_so_far == [number] [EOL] assert trainer . _metric_tracker . _epochs_with_no_improvement == [number] [EOL] [EOL] def test_trainer_can_run_gradient_accumulation ( self ) : [EOL] instances = list ( self . instances ) [EOL] steps_to_accumulate = [number] [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , validation_data_loader = self . validation_data_loader , num_epochs = [number] , num_gradient_accumulation_steps = steps_to_accumulate , ) [EOL] assert trainer . _num_gradient_accumulation_steps == steps_to_accumulate [EOL] [EOL] metrics = trainer . train ( ) [EOL] [EOL] num_batches_trained_per_epoch = trainer . _batch_num_total // ( metrics [ [string] ] + [number] ) [EOL] num_batches_expected = math . ceil ( math . ceil ( len ( instances ) / self . data_loader . batch_size ) / steps_to_accumulate ) [EOL] [EOL] assert num_batches_trained_per_epoch == num_batches_expected [EOL] [EOL] def test_batch_callback_is_called_at_every_batch ( self ) : [EOL] class FakeBatchCallback ( BatchCallback ) : [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] if not hasattr ( trainer , [string] ) : [EOL] trainer . batch_callback_calls = [ ] [comment] [EOL] trainer . batch_callback_calls . append ( ( epoch , batch_number , is_training ) ) [comment] [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , validation_data_loader = self . validation_data_loader , batch_callbacks = [ FakeBatchCallback ( ) ] , ) [EOL] trainer . train ( ) [EOL] expected_calls = [ ( epoch , batch_number + [number] , is_train ) for epoch in range ( [number] ) for is_train in ( True , False ) for batch_number in range ( len ( self . instances ) // [number] ) ] [EOL] assert trainer . batch_callback_calls == expected_calls [EOL] [EOL] def test_epoch_callback_is_called_at_every_epoch ( self ) : [EOL] class FakeEpochCallback ( EpochCallback ) : [EOL] def __call__ ( self , trainer , metrics , epoch , is_master , ) : [EOL] if not hasattr ( trainer , [string] ) : [EOL] trainer . epoch_callback_calls = [ ] [comment] [EOL] trainer . epoch_callback_calls . append ( epoch ) [comment] [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , validation_data_loader = self . validation_data_loader , epoch_callbacks = [ FakeEpochCallback ( ) ] , ) [EOL] trainer . train ( ) [EOL] expected_calls = [ epoch for epoch in range ( - [number] , [number] ) ] [EOL] assert trainer . epoch_callback_calls == expected_calls [EOL] [EOL] def test_track_epoch_callback ( self ) : [EOL] num_epochs = [number] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = num_epochs , validation_data_loader = self . validation_data_loader , epoch_callbacks = [ TrackEpochCallback ( ) ] , ) [EOL] trainer . train ( ) [EOL] assert trainer . model . epoch == num_epochs [EOL] [EOL] def test_total_loss_is_average_of_batch_loss ( self ) : [EOL] [EOL] batches_per_epoch = [number] [EOL] [EOL] data_loader_custom_epoch_lazy = PyTorchDataLoader ( self . instances_lazy , batch_size = [number] , collate_fn = allennlp_collate , batches_per_epoch = batches_per_epoch , ) [EOL] [EOL] class FakeBatchCallback ( BatchCallback ) : [EOL] def __call__ ( self , trainer , batch_inputs , batch_outputs , epoch , batch_number , is_training , is_master , ) : [EOL] if not hasattr ( trainer , [string] ) : [EOL] trainer . batch_losses = [ ] [comment] [EOL] trainer . batch_losses . append ( batch_outputs [ [number] ] [ [string] ] . item ( ) ) [comment] [EOL] [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , data_loader_custom_epoch_lazy , num_epochs = [number] , batch_callbacks = [ FakeBatchCallback ( ) ] , ) [EOL] metrics = trainer . train ( ) [EOL] [EOL] assert metrics [ [string] ] == float ( sum ( trainer . batch_losses ) / batches_per_epoch ) [EOL] [EOL] [EOL] @ requires_gpu class TestAmpTrainer ( TrainerTestBase ) : [EOL] @ pytest . mark . parametrize ( [string] , [ ( None , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] ) def test_trainer_can_run_amp ( self , grad_norm , num_gradient_accumulation_steps ) : [EOL] self . model . cuda ( ) [EOL] trainer = GradientDescentTrainer ( self . model , self . optimizer , self . data_loader , num_epochs = [number] , cuda_device = [number] , use_amp = True , grad_norm = True , num_gradient_accumulation_steps = num_gradient_accumulation_steps , ) [EOL] _ = trainer . train ( ) [EOL] [EOL] [EOL] class TestSparseClipGrad ( AllenNlpTestCase ) : [EOL] def test_sparse_clip_grad ( self ) : [EOL] [comment] [EOL] embedding = torch . nn . Embedding ( [number] , [number] , sparse = True ) [EOL] embedding . zero_grad ( ) [EOL] ids = ( torch . rand ( [number] ) * [number] ) . long ( ) [EOL] [comment] [EOL] [comment] [EOL] ids [ : [number] ] = [number] [EOL] loss = embedding ( ids ) . sum ( ) [EOL] loss . backward ( ) [EOL] assert embedding . weight . grad . is_sparse [EOL] [EOL] [comment] [EOL] _ = clip_grad_norm_ ( [ embedding . weight ] , [number] ) [EOL] [comment] [EOL] grad = embedding . weight . grad . coalesce ( ) [EOL] assert grad . _values ( ) . norm ( [number] ) . item ( ) == pytest . approx ( [number] , rel = [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.Any$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Type[tests.training.trainer_test.TestTrainer.test_trainer_respects_keep_serialized_model_every_num_seconds.SlowDataLoader]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $builtins.int$ 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $builtins.int$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $"GradientDescentTrainer"$ 0 $typing.List[typing.List[allennlp.data.dataloader.TensorDict]]$ 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.bool]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int,builtins.bool]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $"GradientDescentTrainer"$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 $builtins.int$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 $"GradientDescentTrainer"$ 0 $typing.List[typing.List[allennlp.data.dataloader.TensorDict]]$ 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 0 0 $"GradientDescentTrainer"$ 0 0 0 0 0 $typing.List[typing.Dict[builtins.str,typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List [EOL] import builtins [EOL] import typing [EOL] import os [EOL] import re [EOL] import time [EOL] from contextlib import contextmanager [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common . params import Params [EOL] from allennlp . training import Checkpointer , Trainer [EOL] [EOL] [EOL] class FakeTrainer ( Trainer ) : [EOL] def __init__ ( self , model_state , training_states ) : [EOL] self . _model_state = model_state [EOL] self . _training_states = training_states [EOL] [EOL] @ contextmanager def get_checkpoint_state ( self ) : [EOL] yield self . _model_state , self . _training_states [EOL] [EOL] [EOL] class TestCheckpointer ( AllenNlpTestCase ) : [EOL] def retrieve_and_delete_saved ( self ) : [EOL] [docstring] [EOL] serialization_files = os . listdir ( self . TEST_DIR ) [EOL] model_checkpoints = [ x for x in serialization_files if [string] in x ] [EOL] found_model_epochs = [ int ( re . search ( [string] , x ) . group ( [number] ) ) for x in model_checkpoints ] [EOL] for f in model_checkpoints : [EOL] os . remove ( os . path . join ( self . TEST_DIR , f ) ) [EOL] training_checkpoints = [ x for x in serialization_files if [string] in x ] [EOL] found_training_epochs = [ int ( re . search ( [string] , x ) . group ( [number] ) ) for x in training_checkpoints ] [EOL] for f in training_checkpoints : [EOL] os . remove ( os . path . join ( self . TEST_DIR , f ) ) [EOL] return sorted ( found_model_epochs ) , sorted ( found_training_epochs ) [EOL] [EOL] def test_default ( self ) : [EOL] [docstring] [EOL] default_num_to_keep = [number] [EOL] num_epochs = [number] [EOL] target = list ( range ( num_epochs - default_num_to_keep , num_epochs ) ) [EOL] [EOL] checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR ) [EOL] [EOL] for e in range ( num_epochs ) : [EOL] checkpointer . save_checkpoint ( epoch = e , trainer = FakeTrainer ( model_state = { [string] : e } , training_states = { [string] : e } ) , is_best_so_far = False , ) [EOL] models , training = self . retrieve_and_delete_saved ( ) [EOL] assert models == training == target [EOL] [EOL] def test_keep_zero ( self ) : [EOL] checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = [number] ) [EOL] for e in range ( [number] ) : [EOL] checkpointer . save_checkpoint ( epoch = e , trainer = FakeTrainer ( model_state = { [string] : e } , training_states = { [string] : e } ) , is_best_so_far = True , ) [EOL] files = os . listdir ( self . TEST_DIR ) [EOL] assert [string] not in files [EOL] assert [string] not in files [EOL] [EOL] def test_with_time ( self ) : [EOL] [docstring] [EOL] num_to_keep = [number] [EOL] num_epochs = [number] [EOL] target = list ( range ( num_epochs - num_to_keep , num_epochs ) ) [EOL] pauses = [ [number] , [number] , [number] ] [EOL] target = sorted ( set ( target + pauses ) ) [EOL] checkpointer = Checkpointer ( serialization_dir = self . TEST_DIR , num_serialized_models_to_keep = num_to_keep , keep_serialized_model_every_num_seconds = [number] , ) [EOL] for e in range ( num_epochs ) : [EOL] if e in pauses : [EOL] time . sleep ( [number] ) [EOL] checkpointer . save_checkpoint ( epoch = e , trainer = FakeTrainer ( model_state = { [string] : e } , training_states = { [string] : e } ) , is_best_so_far = False , ) [EOL] models , training = self . retrieve_and_delete_saved ( ) [EOL] assert models == training == target [EOL] [EOL] def test_registered_subclass ( self ) : [EOL] [docstring] [EOL] [EOL] @ Checkpointer . register ( [string] ) class CheckpointerSubclass ( Checkpointer ) : [EOL] def __init__ ( self , x , y ) : [EOL] super ( ) . __init__ ( ) [EOL] self . x = x [EOL] self . y = y [EOL] [EOL] sub_inst = Checkpointer . from_params ( Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) ) [EOL] assert sub_inst . __class__ == CheckpointerSubclass [EOL] assert sub_inst . x == [number] and sub_inst . y == [number] [EOL] [EOL] def test_base_class_from_params ( self ) : [EOL] Checkpointer . from_params ( Params ( { } ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $None$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any [EOL] import logging [EOL] import allennlp [EOL] import typing [EOL] import tests [EOL] import builtins [EOL] import logging [EOL] import os [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . dataset_readers import DatasetReader [EOL] from allennlp . data . fields import LabelField [EOL] from allennlp . models import Model [EOL] from allennlp . training . util import make_vocab_from_params , get_metrics [EOL] [EOL] [EOL] logger = logging . getLogger ( __name__ ) [EOL] [EOL] [EOL] @ pytest . fixture ( scope = [string] , autouse = True ) def train_util_test_reader ( ) : [EOL] @ DatasetReader . register ( [string] ) class TrainUtilTestReader ( DatasetReader ) : [EOL] def _read ( self , data_path ) : [EOL] logger . info ( [string] , data_path ) [EOL] for i in range ( [number] ) : [EOL] yield self . text_to_instance ( i ) [EOL] [EOL] def text_to_instance ( self , index ) : [comment] [EOL] return Instance ( { [string] : LabelField ( index , skip_indexing = True ) } ) [EOL] [EOL] yield TrainUtilTestReader [EOL] [EOL] del DatasetReader . _registry [ DatasetReader ] [ [string] ] [EOL] [EOL] [EOL] class TestMakeVocabFromParams ( AllenNlpTestCase ) : [EOL] @ pytest . mark . parametrize ( [string] , [ Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [ ] , } ) , Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [ ] , } ) , Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [string] , [string] : { [string] : [string] } , } ) , ] , ) def test_no_instances_read_for_vocab ( self , caplog , params ) : [EOL] _ = make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] log_messages = [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert [string] not in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] not in log_messages [EOL] [EOL] def test_only_train_read_for_vocab ( self , caplog ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : [string] , } ) [EOL] _ = make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] log_messages = [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert [string] in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] not in log_messages [EOL] [EOL] def test_all_datasets_read_for_vocab ( self , caplog ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [string] , } ) [EOL] _ = make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] log_messages = [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] [EOL] def test_only_specified_datasets_read_for_vocab ( self , caplog ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [string] , [string] : [ [string] , [string] ] , } ) [EOL] _ = make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] log_messages = [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] not in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] in log_messages [EOL] assert [string] not in log_messages [EOL] [EOL] def test_using_seperate_validation_reader ( self , caplog ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , } ) [EOL] _ = make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] log_messages = [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert [string] in log_messages [EOL] [EOL] def test_invalid_datasets_for_vocab_creation ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , [string] : [ [string] , [string] , [string] ] , } ) [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] [EOL] def test_raise_error_if_directory_non_empty ( self ) : [EOL] params = Params ( { [string] : { [string] : [string] } , [string] : [string] , [string] : [string] , } ) [EOL] os . makedirs ( self . TEST_DIR / [string] ) [EOL] with open ( self . TEST_DIR / [string] / [string] , [string] ) as random_file : [EOL] random_file . write ( [string] ) [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] make_vocab_from_params ( params , str ( self . TEST_DIR ) ) [EOL] [EOL] def test_get_metrics ( self ) : [EOL] class FakeModel ( Model ) : [EOL] def forward ( self , ** kwargs ) : [EOL] return { } [EOL] [EOL] model = FakeModel ( None ) [EOL] total_loss = [number] [EOL] batch_loss = [number] [EOL] num_batches = [number] [EOL] metrics = get_metrics ( model , total_loss , None , batch_loss , None , num_batches ) [EOL] [EOL] assert metrics [ [string] ] == float ( total_loss / num_batches ) [EOL] assert metrics [ [string] ] == batch_loss [EOL] [EOL] metrics = get_metrics ( model , total_loss , None , None , None , num_batches ) [EOL] [EOL] assert metrics [ [string] ] == float ( total_loss / num_batches ) [EOL] assert [string] not in metrics [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $logging.Logger$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 $builtins.str$ 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.training.util_test.TestMakeVocabFromParams.test_get_metrics.FakeModel$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $tests.training.util_test.TestMakeVocabFromParams.test_get_metrics.FakeModel$ 0 $builtins.float$ 0 0 0 $builtins.float$ 0 0 0 $builtins.int$ 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 $builtins.float$ 0 $builtins.int$ 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 $tests.training.util_test.TestMakeVocabFromParams.test_get_metrics.FakeModel$ 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 $builtins.float$ 0 $builtins.int$ 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0
from typing import Any [EOL] import typing [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . training . optimizers import Optimizer [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] from allennlp . common . params import Params [EOL] [EOL] [EOL] class LearningRateSchedulersTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) ) [EOL] [EOL] def test_reduce_on_plateau_error_throw_when_no_metrics_exist ( self ) : [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] } ) ) , params = Params ( { [string] : [string] } ) , ) . step ( None ) [EOL] [EOL] def test_reduce_on_plateau_works_when_metrics_exist ( self ) : [EOL] LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] } ) ) , params = Params ( { [string] : [string] } ) , ) . step ( [number] ) [EOL] [EOL] def test_no_metric_wrapper_can_support_none_for_metrics ( self ) : [EOL] lrs = LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] } ) ) , params = Params ( { [string] : [string] , [string] : [number] } ) , ) [EOL] lrs . lr_scheduler . optimizer . step ( ) [comment] [EOL] lrs . step ( None ) [EOL] [EOL] def test_noam_learning_rate_schedule_does_not_crash ( self ) : [EOL] lrs = LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] } ) ) , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] } ) , ) [EOL] lrs . step ( None ) [EOL] lrs . step_batch ( None ) [EOL] [EOL] def test_polynomial_decay_works_properly ( self ) : [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] , [string] : [number] } ) , ) , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } ) , ) [EOL] optimizer = scheduler . optimizer [EOL] [EOL] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] [EOL] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] [EOL] def test_linear_with_warmup_works_properly ( self ) : [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] , [string] : [number] } ) , ) , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] , } ) , ) [EOL] optimizer = scheduler . optimizer [EOL] [EOL] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [comment] [EOL] [EOL] [comment] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] scheduler . step_batch ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] [EOL] def test_exponential_works_properly ( self ) : [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] , [string] : [number] } ) , ) , params = Params ( { [string] : [string] , [string] : [number] } ) , ) [EOL] optimizer = scheduler . lr_scheduler . optimizer [EOL] optimizer . step ( ) [comment] [EOL] [comment] [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] scheduler . step ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] [EOL] scheduler . step ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] ** [number] [EOL] scheduler . step ( ) [EOL] assert optimizer . param_groups [ [number] ] [ [string] ] == [number] ** [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import builtins [EOL] import typing [EOL] import torch [EOL] from copy import deepcopy [EOL] from typing import Dict , Any [EOL] [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler [EOL] from allennlp . training . optimizers import Optimizer [EOL] [EOL] [EOL] class CosineWithRestartsTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . model = torch . nn . Sequential ( torch . nn . Linear ( [number] , [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] self . cosine_schedule_cases = [ ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ [number] , [number] ] , ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ [number] , [number] ] ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ ] ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ] , [ [number] , [number] ] , ) , ( [number] , { [string] : [number] , [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ ] , ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ ] , ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ] , [ ] , ) , ( [number] , { [string] : [number] , [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ [number] , [number] ] , ) , ( [number] , { [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ] , [ ] , ) , ( [number] , { [string] : [number] , [string] : [number] , [string] : [number] } , [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] , [ ] ) , ] [EOL] [EOL] def _get_optimizer ( self , lr = [number] ) : [EOL] return Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = Params ( { [string] : [string] , [string] : lr } ) ) [EOL] [EOL] def test_from_params ( self ) : [EOL] [docstring] [EOL] optim = self . _get_optimizer ( ) [EOL] sched = LearningRateScheduler . from_params ( optimizer = optim , params = Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] [EOL] assert sched . t_initial == [number] [EOL] assert sched . last_epoch == - [number] [EOL] [EOL] [comment] [EOL] assert optim . param_groups [ [number] ] [ [string] ] == [number] [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] [comment] [EOL] LearningRateScheduler . from_params ( optimizer = optim , params = Params ( { [string] : [string] } ) ) [EOL] [EOL] def test_schedules ( self ) : [EOL] [docstring] [EOL] for epochs , params , lr_checks , _ in self . cosine_schedule_cases : [EOL] optimizer = self . _get_optimizer ( ) [EOL] params [ [string] ] = [string] [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = optimizer , params = Params ( params ) ) [EOL] lrs = [ optimizer . param_groups [ [number] ] [ [string] ] ] [EOL] for _ in range ( epochs ) : [EOL] scheduler . step ( ) [EOL] lrs . append ( optimizer . param_groups [ [number] ] [ [string] ] ) [EOL] [EOL] for it , lr in lr_checks : [EOL] assert lrs [ it ] == lr , f" [string] { it } [string] { lrs [ it ] } [string] { lr }" [EOL] [EOL] def test_schedules_with_save_and_resume ( self ) : [EOL] [docstring] [EOL] [EOL] def init_and_restore_scheduler ( optimizer , params , state_dict = None , ) : [EOL] [docstring] [EOL] params [ [string] ] = [string] [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = optimizer , params = Params ( deepcopy ( params ) ) ) [EOL] if state_dict is not None : [EOL] scheduler . load_state_dict ( state_dict ) [EOL] return scheduler [EOL] [EOL] for epochs , params , lr_checks , checkpoints in self . cosine_schedule_cases : [EOL] optimizer = self . _get_optimizer ( ) [EOL] scheduler = init_and_restore_scheduler ( optimizer , params ) [EOL] state = scheduler . state_dict ( ) [EOL] [EOL] lrs = [ optimizer . param_groups [ [number] ] [ [string] ] ] [EOL] for epoch in range ( epochs ) : [EOL] if epoch in checkpoints : [EOL] [comment] [EOL] scheduler = init_and_restore_scheduler ( optimizer , params , state_dict = state ) [EOL] [EOL] [comment] [EOL] scheduler . step ( [number] ) [EOL] lrs . append ( optimizer . param_groups [ [number] ] [ [string] ] ) [EOL] [EOL] [comment] [EOL] state = scheduler . state_dict ( ) [EOL] [EOL] for it , lr in lr_checks : [EOL] assert lrs [ it ] == lr , f" [string] { it } [string] { lrs [ it ] } [string] { lr }" [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $torch.optim.Optimizer$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $torch.optim.Optimizer$ 0 $torch.optim.Optimizer$ 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0
	0
from typing import Dict , Any , List , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from collections import OrderedDict [EOL] from copy import deepcopy [EOL] from typing import Any , Dict , List , Tuple [EOL] [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . data . dataset_readers . dataset_reader import AllennlpDataset [EOL] from allennlp . common import Lazy , Params [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import PyTorchDataLoader [EOL] from allennlp . training import Trainer [EOL] from allennlp . training . learning_rate_schedulers import LearningRateScheduler , SlantedTriangular [EOL] from allennlp . training . optimizers import Optimizer [EOL] [EOL] [EOL] def is_hat_shaped ( learning_rates ) : [EOL] [docstring] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] has_increasing_segment = False [EOL] has_decreasing_segment = False [EOL] for k in range ( [number] , len ( learning_rates ) ) : [EOL] delta = learning_rates [ k ] - learning_rates [ k - [number] ] [EOL] if delta > [number] : [EOL] has_increasing_segment = True [EOL] if has_decreasing_segment : [EOL] [comment] [EOL] return False [EOL] elif delta < - [number] : [EOL] if not has_increasing_segment : [EOL] [comment] [EOL] return False [EOL] has_decreasing_segment = True [EOL] else : [EOL] [comment] [EOL] pass [EOL] [EOL] return has_increasing_segment and has_decreasing_segment [EOL] [EOL] [EOL] class SlantedTriangularTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . model = torch . nn . Sequential ( OrderedDict ( [ ( [string] , torch . nn . Linear ( [number] , [number] ) ) , ( [string] , torch . nn . Linear ( [number] , [number] ) ) ] ) ) [EOL] [EOL] def _get_optimizer ( self , lr = [number] ) : [EOL] optimizer_params = Params ( { [string] : [string] , [string] : lr } ) [EOL] optimizer_params [ [string] ] = [ [ [ f" [string] { m }" ] , { } ] for m in self . model . _modules ] [EOL] return Optimizer . from_params ( model_parameters = self . model . named_parameters ( ) , params = optimizer_params ) [EOL] [EOL] def _run_scheduler_get_lrs ( self , params , num_steps_per_epoch ) : [EOL] optimizer = self . _get_optimizer ( ) [EOL] params [ [string] ] = [string] [EOL] scheduler = LearningRateScheduler . from_params ( optimizer = optimizer , params = Params ( deepcopy ( params ) ) ) [EOL] lrs = [ ] [EOL] [EOL] batch_num_total = [number] [EOL] for epoch in range ( params [ [string] ] ) : [EOL] for _ in range ( num_steps_per_epoch ) : [EOL] batch_num_total += [number] [EOL] [comment] [EOL] [comment] [EOL] lrs . append ( [ param_group [ [string] ] * float ( param_group [ [string] ] [ [number] ] . requires_grad ) for param_group in optimizer . param_groups [ : [number] ] ] ) [EOL] scheduler . step_batch ( batch_num_total ) [EOL] if params . get ( [string] ) and epoch == [number] : [EOL] assert scheduler . freezing_current [EOL] [comment] [EOL] scheduler . step ( None ) [EOL] [EOL] return lrs [EOL] [EOL] def test_is_hat_shaped ( self ) : [EOL] assert not is_hat_shaped ( [ [number] ] * [number] ) [EOL] assert not is_hat_shaped ( [ float ( k ) for k in range ( [number] ) ] ) [EOL] assert not is_hat_shaped ( [ float ( [number] - k ) for k in range ( [number] ) ] ) [EOL] assert is_hat_shaped ( [ float ( k ) for k in range ( [number] ) ] + [ float ( [number] - k ) for k in range ( [number] ) ] ) [EOL] assert not is_hat_shaped ( [ float ( k ) for k in range ( [number] ) ] + [ float ( [number] - k ) for k in range ( [number] ) ] + [ float ( k ) for k in range ( [number] ) ] ) [EOL] [EOL] def test_from_params_in_trainer ( self ) : [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : [number] , [string] : { [string] : [string] , [string] : True , [string] : True , [string] : [number] , } , } ) [EOL] [comment] [EOL] [comment] [EOL] instances = AllennlpDataset ( [ [number] ] * [number] ) [EOL] optim = self . _get_optimizer ( ) [EOL] trainer = Trainer . from_params ( model = self . model , optimizer = Lazy ( lambda ** kwargs : optim ) , serialization_dir = self . TEST_DIR , params = params , data_loader = PyTorchDataLoader ( instances , batch_size = [number] ) , ) [EOL] assert isinstance ( trainer . _learning_rate_scheduler , SlantedTriangular ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert trainer . _learning_rate_scheduler . num_epochs == [number] [EOL] assert trainer . _learning_rate_scheduler . num_steps_per_epoch == [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] params = Params ( { [string] : [number] , [string] : { [string] : [string] , [string] : [number] , [string] : True , [string] : True , [string] : [number] , } , } ) [EOL] trainer = Trainer . from_params ( model = self . model , optimizer = Lazy ( lambda ** kwargs : optim ) , serialization_dir = self . TEST_DIR , params = params , data_loader = PyTorchDataLoader ( instances , batch_size = [number] ) , ) [EOL] assert trainer . _learning_rate_scheduler . num_epochs == [number] [EOL] [EOL] def test_from_params ( self ) : [EOL] optim = self . _get_optimizer ( ) [EOL] sched = LearningRateScheduler . from_params ( optimizer = optim , params = Params ( { [string] : [string] , [string] : [number] , [string] : [number] , [string] : True , [string] : True , [string] : [number] , } ) , ) [EOL] [EOL] assert sched . num_epochs == [number] [EOL] assert sched . num_steps_per_epoch == [number] [EOL] assert sched . gradual_unfreezing is True [EOL] assert sched . freezing_current is True [EOL] [EOL] assert len ( optim . param_groups ) == [number] [EOL] [comment] [EOL] assert not optim . param_groups [ - [number] ] [ [string] ] [EOL] assert optim . param_groups [ - [number] ] [ [string] ] == [number] / sched . ratio [EOL] assert optim . param_groups [ - [number] ] [ [string] ] == [number] / sched . ratio [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] [comment] [EOL] LearningRateScheduler . from_params ( optimizer = optim , params = Params ( { [string] : [string] , [string] : [number] } ) ) [EOL] LearningRateScheduler . from_params ( optimizer = optim , params = Params ( { [string] : [string] , [string] : [number] } ) , ) [EOL] [EOL] def test_schedules ( self ) : [EOL] slanted_triangular_cases = [ ( { [string] : [number] , [string] : [number] , [string] : True , } , [ ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ] , ) , ( { [string] : [number] , [string] : [number] , [string] : True , [string] : [number] , } , [ ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ] , ) , ( { [string] : [number] , [string] : [number] , [string] : True , [string] : True , [string] : [number] , } , [ ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ( [number] , [number] , [number] ) , ] , ) , ] [EOL] for params , lr_checks in slanted_triangular_cases : [EOL] lrs = self . _run_scheduler_get_lrs ( params , params [ [string] ] ) [EOL] [EOL] for it , layer , lr in lr_checks : [EOL] lr_check = round ( lr , [number] ) [EOL] lr = round ( lrs [ it ] [ layer ] , [number] ) [EOL] assert ( lr == lr_check ) , f" [string] { lr } [string] { it } [string] { layer } [string] { lr_check } [string] " [EOL] [EOL] def test_schedules_num_steps_per_epoch ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for gradual_unfreezing in [ True , False ] : [EOL] for discriminative_fine_tuning in [ True , False ] : [EOL] for num_actual_steps_per_epoch in [ [number] , [number] ] : [EOL] params = { [string] : [number] , [string] : [number] , [string] : gradual_unfreezing , [string] : discriminative_fine_tuning , } [EOL] lrs = self . _run_scheduler_get_lrs ( params , num_actual_steps_per_epoch ) [EOL] first_layer_lrs = [ rates [ [number] ] for rates in lrs ] [EOL] second_layer_lrs = [ rates [ [number] ] for rates in lrs ] [EOL] [EOL] if gradual_unfreezing : [EOL] assert max ( first_layer_lrs [ : num_actual_steps_per_epoch ] ) < [number] [EOL] assert min ( first_layer_lrs [ : num_actual_steps_per_epoch ] ) > - [number] [EOL] assert is_hat_shaped ( first_layer_lrs [ num_actual_steps_per_epoch : ] ) [EOL] assert is_hat_shaped ( second_layer_lrs [ : num_actual_steps_per_epoch ] ) [EOL] assert is_hat_shaped ( second_layer_lrs [ num_actual_steps_per_epoch : ] ) [EOL] else : [EOL] assert is_hat_shaped ( first_layer_lrs ) [EOL] assert is_hat_shaped ( second_layer_lrs ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Dict[builtins.str,typing.Any],typing.List[typing.Tuple[builtins.int,builtins.int,builtins.float]]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Dict[builtins.str,typing.Any],typing.List[typing.Tuple[builtins.int,builtins.int,builtins.float]]]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0
	0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Dict , List , Tuple , Union , Any [EOL] [EOL] import pytest [EOL] import torch [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import ( AllenNlpTestCase , global_distributed_metric , multi_device , run_distributed_test , ) [EOL] from sklearn . metrics import precision_recall_fscore_support [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . training . metrics import FBetaMultiLabelMeasure [EOL] [EOL] [EOL] class FBetaMultiLabelMeasureTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) [EOL] self . targets = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) [EOL] [EOL] [comment] [EOL] self . pred_sum = [ [number] , [number] , [number] , [number] , [number] ] [EOL] self . true_sum = [ [number] , [number] , [number] , [number] , [number] ] [EOL] self . true_positive_sum = [ [number] , [number] , [number] , [number] , [number] ] [EOL] self . true_negative_sum = [ [number] , [number] , [number] , [number] , [number] ] [EOL] self . total_sum = [ [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] [comment] [EOL] desired_precisions = [ [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] ] [EOL] [comment] [EOL] desired_recalls = [ [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] , [number] ] [EOL] desired_fscores = [ ( [number] * p * r ) / ( p + r ) if p + r != [number] else [number] for p , r in zip ( desired_precisions , desired_recalls ) ] [EOL] self . desired_precisions = desired_precisions [EOL] self . desired_recalls = desired_recalls [EOL] self . desired_fscores = desired_fscores [EOL] [EOL] @ multi_device def test_config_errors ( self , device ) : [EOL] [comment] [EOL] pytest . raises ( ConfigurationError , FBetaMultiLabelMeasure , beta = [number] ) [EOL] [EOL] [comment] [EOL] pytest . raises ( ConfigurationError , FBetaMultiLabelMeasure , average = [string] ) [EOL] [EOL] [comment] [EOL] pytest . raises ( ConfigurationError , FBetaMultiLabelMeasure , labels = [ ] ) [EOL] [EOL] @ multi_device def test_runtime_errors ( self , device ) : [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] [comment] [EOL] pytest . raises ( RuntimeError , fbeta . get_metric ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_state ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] [EOL] [comment] [EOL] assert_allclose ( fbeta . _pred_sum . tolist ( ) , self . pred_sum ) [EOL] assert_allclose ( fbeta . _true_sum . tolist ( ) , self . true_sum ) [EOL] assert_allclose ( fbeta . _true_positive_sum . tolist ( ) , self . true_positive_sum ) [EOL] assert_allclose ( fbeta . _true_negative_sum . tolist ( ) , self . true_negative_sum ) [EOL] assert_allclose ( fbeta . _total_sum . tolist ( ) , self . total_sum ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_metric ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] [comment] [EOL] assert_allclose ( precisions , self . desired_precisions ) [EOL] assert_allclose ( recalls , self . desired_recalls ) [EOL] assert_allclose ( fscores , self . desired_fscores ) [EOL] [EOL] [comment] [EOL] assert isinstance ( precisions , List ) [EOL] assert isinstance ( recalls , List ) [EOL] assert isinstance ( fscores , List ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_with_mask ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] mask = torch . tensor ( [ True , True , True , True , True , False ] , device = device ) . unsqueeze ( - [number] ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( self . predictions , self . targets , mask ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] assert_allclose ( fbeta . _pred_sum . tolist ( ) , [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert_allclose ( fbeta . _true_sum . tolist ( ) , [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] assert_allclose ( fbeta . _true_positive_sum . tolist ( ) , [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] [EOL] desired_precisions = [ [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] ] [EOL] desired_recalls = [ [number] / [number] , [number] / [number] , [number] / [number] , [number] / [number] , [number] ] [EOL] desired_fscores = [ ( [number] * p * r ) / ( p + r ) if p + r != [number] else [number] for p , r in zip ( desired_precisions , desired_recalls ) ] [EOL] assert_allclose ( precisions , desired_precisions ) [EOL] assert_allclose ( recalls , desired_recalls ) [EOL] assert_allclose ( fscores , desired_fscores ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_macro_average_metric ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( average = [string] ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] [comment] [EOL] macro_precision = torch . tensor ( self . desired_precisions ) . mean ( ) [EOL] macro_recall = torch . tensor ( self . desired_recalls ) . mean ( ) [EOL] macro_fscore = torch . tensor ( self . desired_fscores ) . mean ( ) [EOL] [comment] [EOL] assert_allclose ( precisions , macro_precision ) [EOL] assert_allclose ( recalls , macro_recall ) [EOL] assert_allclose ( fscores , macro_fscore ) [EOL] [EOL] [comment] [EOL] assert isinstance ( precisions , float ) [EOL] assert isinstance ( recalls , float ) [EOL] assert isinstance ( fscores , float ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_micro_average_metric ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( average = [string] ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] [comment] [EOL] true_positives = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] ] , dtype = torch . float32 ) [EOL] false_positives = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] ] , dtype = torch . float32 ) [EOL] false_negatives = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] ] , dtype = torch . float32 ) [EOL] mean_true_positive = true_positives . mean ( ) [EOL] mean_false_positive = false_positives . mean ( ) [EOL] mean_false_negative = false_negatives . mean ( ) [EOL] [EOL] micro_precision = mean_true_positive / ( mean_true_positive + mean_false_positive ) [EOL] micro_recall = mean_true_positive / ( mean_true_positive + mean_false_negative ) [EOL] micro_fscore = ( [number] * micro_precision * micro_recall ) / ( micro_precision + micro_recall ) [EOL] [comment] [EOL] assert_allclose ( precisions , micro_precision ) [EOL] assert_allclose ( recalls , micro_recall ) [EOL] assert_allclose ( fscores , micro_fscore ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_with_explicit_labels ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] [comment] [EOL] fbeta = FBetaMultiLabelMeasure ( labels = [ [number] , [number] , [number] , [number] , [number] ] ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] desired_precisions = self . desired_precisions [ : : - [number] ] [EOL] desired_recalls = self . desired_recalls [ : : - [number] ] [EOL] desired_fscores = self . desired_fscores [ : : - [number] ] [EOL] [comment] [EOL] assert_allclose ( precisions , desired_precisions ) [EOL] assert_allclose ( recalls , desired_recalls ) [EOL] assert_allclose ( fscores , desired_fscores ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_with_macro_average ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] labels = [ [number] , [number] ] [EOL] fbeta = FBetaMultiLabelMeasure ( average = [string] , labels = labels ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] [comment] [EOL] macro_precision = torch . tensor ( self . desired_precisions ) [ labels ] . mean ( ) [EOL] macro_recall = torch . tensor ( self . desired_recalls ) [ labels ] . mean ( ) [EOL] macro_fscore = torch . tensor ( self . desired_fscores ) [ labels ] . mean ( ) [EOL] [EOL] [comment] [EOL] assert_allclose ( precisions , macro_precision ) [EOL] assert_allclose ( recalls , macro_recall ) [EOL] assert_allclose ( fscores , macro_fscore ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_with_micro_average ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] labels = [ [number] , [number] ] [EOL] fbeta = FBetaMultiLabelMeasure ( average = [string] , labels = labels ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] [comment] [EOL] true_positives = torch . tensor ( [ [number] , [number] ] , dtype = torch . float32 ) [EOL] false_positives = torch . tensor ( [ [number] , [number] ] , dtype = torch . float32 ) [EOL] false_negatives = torch . tensor ( [ [number] , [number] ] , dtype = torch . float32 ) [EOL] mean_true_positive = true_positives . mean ( ) [EOL] mean_false_positive = false_positives . mean ( ) [EOL] mean_false_negative = false_negatives . mean ( ) [EOL] [EOL] micro_precision = mean_true_positive / ( mean_true_positive + mean_false_positive ) [EOL] micro_recall = mean_true_positive / ( mean_true_positive + mean_false_negative ) [EOL] micro_fscore = ( [number] * micro_precision * micro_recall ) / ( micro_precision + micro_recall ) [EOL] [comment] [EOL] assert_allclose ( precisions , micro_precision ) [EOL] assert_allclose ( recalls , micro_recall ) [EOL] assert_allclose ( fscores , micro_fscore ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_with_weighted_average ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . targets = self . targets . to ( device ) [EOL] [EOL] labels = [ [number] , [number] ] [EOL] fbeta = FBetaMultiLabelMeasure ( average = [string] , labels = labels ) [EOL] fbeta ( self . predictions , self . targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] weighted_precision , weighted_recall , weighted_fscore , _ = precision_recall_fscore_support ( self . targets . cpu ( ) . numpy ( ) , torch . where ( self . predictions >= fbeta . _threshold , torch . ones_like ( self . predictions ) , torch . zeros_like ( self . predictions ) , ) . cpu ( ) . numpy ( ) , labels = labels , average = [string] , ) [EOL] [EOL] [comment] [EOL] assert_allclose ( precisions , weighted_precision ) [EOL] assert_allclose ( recalls , weighted_recall ) [EOL] assert_allclose ( fscores , weighted_fscore ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_handles_batch_size_of_one ( self , device ) : [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] mask = torch . tensor ( [ [ True ] ] , device = device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( predictions , targets , mask ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] [EOL] assert_allclose ( precisions , [ [number] , [number] , [number] , [number] ] ) [EOL] assert_allclose ( recalls , [ [number] , [number] , [number] , [number] ] ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_handles_no_prediction_false_last_class ( self , device ) : [EOL] [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [comment] [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( predictions , targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] assert_allclose ( precisions , [ [number] , [number] ] ) [EOL] assert_allclose ( recalls , [ [number] , [number] ] ) [EOL] assert_allclose ( fscores , [ [number] , [number] ] ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_handles_no_prediction_true_last_class ( self , device ) : [EOL] [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [comment] [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( predictions , targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] assert_allclose ( precisions , [ [number] , [number] ] ) [EOL] assert_allclose ( recalls , [ [number] , [number] ] ) [EOL] assert_allclose ( fscores , [ [number] , [number] ] ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_handles_no_prediction_true_other_class ( self , device ) : [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [comment] [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( predictions , targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] assert_allclose ( precisions , [ [number] , [number] ] ) [EOL] assert_allclose ( recalls , [ [number] , [number] ] ) [EOL] assert_allclose ( fscores , [ [number] , [number] ] ) [EOL] [EOL] @ multi_device def test_fbeta_multilabel_handles_no_prediction_true_all_class ( self , device ) : [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [comment] [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [EOL] fbeta = FBetaMultiLabelMeasure ( ) [EOL] fbeta ( predictions , targets ) [EOL] metric = fbeta . get_metric ( ) [EOL] precisions = metric [ [string] ] [EOL] recalls = metric [ [string] ] [EOL] fscores = metric [ [string] ] [EOL] [EOL] assert_allclose ( precisions , [ [number] , [number] ] ) [EOL] assert_allclose ( recalls , [ [number] , [number] ] ) [EOL] assert_allclose ( fscores , [ [number] , [number] ] ) [EOL] [EOL] def test_distributed_fbeta_multilabel_measure ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) , ] [EOL] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_metrics = { [string] : self . desired_precisions , [string] : self . desired_recalls , [string] : self . desired_fscores , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , FBetaMultiLabelMeasure ( ) , metric_kwargs , desired_metrics , exact = False , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] ) , ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_metrics = { [string] : self . desired_precisions , [string] : self . desired_recalls , [string] : self . desired_fscores , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , FBetaMultiLabelMeasure ( ) , metric_kwargs , desired_metrics , exact = False , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] metric_values = metric . get_metric ( ) [EOL] [EOL] for key in desired_values : [EOL] assert_allclose ( desired_values [ key ] , metric_values [ key ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.float]$ 0 $typing.List[builtins.float]$ 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.float]$ 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.float]$ 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.float]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.fbeta_multi_label_measure.FBetaMultiLabelMeasure$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,unknown]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Dict , Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import pytest [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , run_distributed_test , global_distributed_metric , ) [EOL] from allennlp . training . metrics import F1Measure [EOL] [EOL] [EOL] class F1MeasureTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_f1_measure_catches_exceptions ( self , device ) : [EOL] f1_measure = F1Measure ( [number] ) [EOL] predictions = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] out_of_range_labels = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] f1_measure ( predictions , out_of_range_labels ) [EOL] [EOL] @ multi_device def test_f1_measure ( self , device ) : [EOL] f1_measure = F1Measure ( positive_label = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] , device = device , ) [EOL] [comment] [EOL] [comment] [EOL] targets = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] f1_measure ( predictions , targets ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] f1_measure . reset ( ) [EOL] [comment] [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] [comment] [EOL] assert isinstance ( precision , float ) [EOL] assert isinstance ( recall , float ) [EOL] assert isinstance ( f1 , float ) [EOL] [EOL] [comment] [EOL] mask = torch . tensor ( [ True , False , True , True , True , False ] , device = device ) [EOL] f1_measure ( predictions , targets , mask ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] f1_measure . reset ( ) [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] [EOL] @ multi_device def test_f1_measure_other_positive_label ( self , device ) : [EOL] f1_measure = F1Measure ( positive_label = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] , device = device , ) [EOL] [comment] [EOL] [comment] [EOL] targets = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] f1_measure ( predictions , targets ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] f1_measure . reset ( ) [EOL] [comment] [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] [comment] [EOL] assert isinstance ( precision , float ) [EOL] assert isinstance ( recall , float ) [EOL] assert isinstance ( f1 , float ) [EOL] [EOL] @ multi_device def test_f1_measure_accumulates_and_resets_correctly ( self , device ) : [EOL] f1_measure = F1Measure ( positive_label = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] , device = device , ) [EOL] [comment] [EOL] [comment] [EOL] targets = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] f1_measure ( predictions , targets ) [EOL] f1_measure ( predictions , targets ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] f1_measure . reset ( ) [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] [EOL] @ multi_device def test_f1_measure_works_for_sequences ( self , device ) : [EOL] f1_measure = F1Measure ( positive_label = [number] ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , ] , device = device , ) [EOL] [comment] [EOL] [comment] [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] f1_measure ( predictions , targets ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] f1_measure . reset ( ) [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] [EOL] [comment] [EOL] mask = torch . tensor ( [ [ False , True , False ] , [ True , True , True ] ] , device = device ) [EOL] f1_measure ( predictions , targets , mask ) [EOL] metrics = f1_measure . get_metric ( ) [EOL] precision = metrics [ [string] ] [EOL] recall = metrics [ [string] ] [EOL] f1 = metrics [ [string] ] [EOL] assert f1_measure . _true_positives == [number] [EOL] assert f1_measure . _true_negatives == [number] [EOL] assert f1_measure . _false_positives == [number] [EOL] assert f1_measure . _false_negatives == [number] [EOL] assert_allclose ( precision , [number] ) [EOL] assert_allclose ( recall , [number] ) [EOL] assert_allclose ( f1 , [number] ) [EOL] [EOL] def test_distributed_fbeta_measure ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [number] , [number] , [number] ] ) , torch . tensor ( [ [number] , [number] , [number] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_metrics = { [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , F1Measure ( positive_label = [number] ) , metric_kwargs , desired_metrics , exact = False , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 $builtins.float$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 $allennlp.training.metrics.f1_measure.F1Measure$ 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0
from typing import Dict , Any , List , Union [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , Union [EOL] import torch [EOL] [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , run_distributed_test , ) [EOL] from allennlp . training . metrics import ROUGE [EOL] [EOL] [EOL] class RougeTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . metric = ROUGE ( exclude_indices = { [number] } ) [EOL] [EOL] def f1 ( self , r , p ) : [EOL] if r == p == [number] : [EOL] return [number] [EOL] return [number] * r * p / ( r + p ) [EOL] [EOL] @ multi_device def test_rouge ( self , device ) : [EOL] self . metric . reset ( ) [EOL] [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] [EOL] self . metric ( predictions , targets ) [EOL] metrics = self . metric . get_metric ( ) [EOL] [EOL] assert self . metric . _total_sequence_count == [number] [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] unigram_recall = self . metric . _total_rouge_n_recalls [ [number] ] [EOL] assert unigram_recall == [number] / [number] + [number] / [number] + [number] / [number] [EOL] unigram_precision = self . metric . _total_rouge_n_precisions [ [number] ] [EOL] assert unigram_precision == [number] / [number] + [number] / [number] + [number] / [number] [EOL] unigram_f1 = self . metric . _total_rouge_n_f1s [ [number] ] [EOL] assert unigram_f1 == self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) [EOL] [EOL] assert metrics [ [string] ] == unigram_recall / self . metric . _total_sequence_count [EOL] assert metrics [ [string] ] == unigram_precision / self . metric . _total_sequence_count [EOL] assert metrics [ [string] ] == unigram_f1 / self . metric . _total_sequence_count [EOL] [EOL] [comment] [EOL] bigram_recall = self . metric . _total_rouge_n_recalls [ [number] ] [EOL] assert bigram_recall == [number] / [number] + [number] / [number] + [number] / [number] [EOL] bigram_precision = self . metric . _total_rouge_n_precisions [ [number] ] [EOL] assert bigram_precision == [number] / [number] + [number] + [number] / [number] [EOL] bigram_f1 = self . metric . _total_rouge_n_f1s [ [number] ] [EOL] assert bigram_f1 == self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) [EOL] [EOL] assert metrics [ [string] ] == bigram_recall / self . metric . _total_sequence_count [EOL] assert metrics [ [string] ] == bigram_precision / self . metric . _total_sequence_count [EOL] assert metrics [ [string] ] == bigram_f1 / self . metric . _total_sequence_count [EOL] [EOL] [comment] [EOL] [EOL] assert self . metric . _total_rouge_l_f1 == self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) [EOL] [EOL] assert ( metrics [ [string] ] == self . metric . _total_rouge_l_f1 / self . metric . _total_sequence_count ) [EOL] [EOL] def test_rouge_with_zero_counts ( self ) : [EOL] self . metric . reset ( ) [EOL] metrics = self . metric . get_metric ( ) [EOL] for score in metrics . values ( ) : [EOL] assert score == [number] [EOL] [EOL] def test_distributed_rouge ( self ) : [EOL] [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] ] ] ) ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = { } [EOL] desired_values [ [string] ] = [number] / [number] + [number] / [number] + [number] / [number] [EOL] desired_values [ [string] ] = [number] / [number] + [number] / [number] + [number] / [number] [EOL] desired_values [ [string] ] = ( self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) ) [EOL] [EOL] desired_values [ [string] ] = [number] / [number] + [number] / [number] + [number] / [number] [EOL] desired_values [ [string] ] = [number] / [number] + [number] + [number] / [number] [EOL] desired_values [ [string] ] = ( self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) ) [EOL] [EOL] desired_values [ [string] ] = ( self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) + self . f1 ( [number] / [number] , [number] / [number] ) ) [EOL] [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_rouge , ROUGE ( exclude_indices = { [number] } ) , metric_kwargs , desired_values , ) [EOL] [EOL] [EOL] def global_distributed_rouge ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , ) : [EOL] [EOL] kwargs = { } [EOL] [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] metric ( ** kwargs ) [EOL] [EOL] metrics = metric . get_metric ( ) [EOL] [EOL] [comment] [EOL] unigram_recall = metric . _total_rouge_n_recalls [ [number] ] [EOL] assert_allclose ( unigram_recall , desired_values [ [string] ] ) [EOL] unigram_precision = metric . _total_rouge_n_precisions [ [number] ] [EOL] assert_allclose ( unigram_precision , desired_values [ [string] ] ) [EOL] unigram_f1 = metric . _total_rouge_n_f1s [ [number] ] [EOL] assert_allclose ( unigram_f1 , desired_values [ [string] ] ) [EOL] [EOL] assert metrics [ [string] ] == unigram_recall / metric . _total_sequence_count [EOL] assert metrics [ [string] ] == unigram_precision / metric . _total_sequence_count [EOL] assert metrics [ [string] ] == unigram_f1 / metric . _total_sequence_count [EOL] [EOL] [comment] [EOL] bigram_recall = metric . _total_rouge_n_recalls [ [number] ] [EOL] assert_allclose ( bigram_recall , desired_values [ [string] ] ) [EOL] bigram_precision = metric . _total_rouge_n_precisions [ [number] ] [EOL] assert_allclose ( bigram_precision , desired_values [ [string] ] ) [EOL] bigram_f1 = metric . _total_rouge_n_f1s [ [number] ] [EOL] assert_allclose ( bigram_f1 , desired_values [ [string] ] ) [EOL] [EOL] assert metrics [ [string] ] == bigram_recall / metric . _total_sequence_count [EOL] assert metrics [ [string] ] == bigram_precision / metric . _total_sequence_count [EOL] assert metrics [ [string] ] == bigram_f1 / metric . _total_sequence_count [EOL] [EOL] [comment] [EOL] [EOL] assert_allclose ( metric . _total_rouge_l_f1 , desired_values [ [string] ] ) [EOL] [EOL] assert metrics [ [string] ] == metric . _total_rouge_l_f1 / metric . _total_sequence_count [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import math [EOL] import pytest [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import SpearmanCorrelation [EOL] [EOL] [EOL] def spearman_formula ( predictions , labels , mask = None ) : [EOL] [docstring] [EOL] if mask is not None : [EOL] predictions = predictions * mask [EOL] labels = labels * mask [EOL] [EOL] [comment] [EOL] if len ( torch . unique ( predictions ) ) == [number] or len ( torch . unique ( labels ) ) == [number] : [EOL] return float ( [string] ) [EOL] [EOL] len_pre = len ( predictions ) [EOL] [EOL] predictions = [ ( k , v ) for k , v in enumerate ( predictions ) ] [EOL] predictions . sort ( key = lambda x : x [ [number] ] , reverse = True ) [EOL] predictions = [ ( k , v ) for k , v in enumerate ( predictions ) ] [EOL] predictions . sort ( key = lambda x : x [ [number] ] [ [number] ] ) [EOL] [EOL] labels = [ ( k , v ) for k , v in enumerate ( labels ) ] [EOL] labels . sort ( key = lambda x : x [ [number] ] , reverse = True ) [EOL] labels = [ ( k , v ) for k , v in enumerate ( labels ) ] [EOL] labels . sort ( key = lambda x : x [ [number] ] [ [number] ] ) [EOL] [EOL] total = [number] [EOL] for i in range ( len_pre ) : [EOL] total += ( predictions [ i ] [ [number] ] - labels [ i ] [ [number] ] ) ** [number] [EOL] expected_spearman_correlation = [number] - [number] * total / ( len_pre * ( len_pre ** [number] - [number] ) ) [EOL] [EOL] return expected_spearman_correlation [EOL] [EOL] [EOL] class SpearmanCorrelationTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_unmasked_computation ( self , device ) : [EOL] spearman_correlation = SpearmanCorrelation ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions1 = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels1 = [number] * predictions1 + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] predictions2 = torch . randn ( [number] , device = device ) . repeat ( num_labels ) [EOL] predictions2 = predictions2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] labels2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] labels2 = [number] * predictions2 + labels2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] predictions_labels_ = [ ( predictions1 , labels1 ) , ( predictions2 , labels2 ) ] [EOL] [EOL] for predictions , labels in predictions_labels_ : [EOL] spearman_correlation . reset ( ) [EOL] spearman_correlation ( predictions , labels ) [EOL] assert_allclose ( spearman_formula ( predictions . reshape ( - [number] ) , labels . reshape ( - [number] ) ) , spearman_correlation . get_metric ( ) , ) [EOL] [EOL] @ multi_device def test_masked_computation ( self , device ) : [EOL] spearman_correlation = SpearmanCorrelation ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions1 = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels1 = [number] * predictions1 + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] predictions2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] predictions2 = predictions2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] labels2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] labels2 = [number] * predictions2 + labels2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] predictions_labels_ = [ ( predictions1 , labels1 ) , ( predictions2 , labels2 ) ] [EOL] [EOL] [comment] [EOL] mask = torch . randint ( [number] , [number] , size = ( batch_size , num_labels ) , device = device ) . bool ( ) [EOL] [EOL] for predictions , labels in predictions_labels_ : [EOL] spearman_correlation . reset ( ) [EOL] spearman_correlation ( predictions , labels , mask ) [EOL] expected_spearman_correlation = spearman_formula ( predictions . view ( - [number] ) , labels . view ( - [number] ) , mask = mask . view ( - [number] ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] assert ( expected_spearman_correlation * spearman_correlation . get_metric ( ) ) > [number] [EOL] [EOL] @ multi_device def test_reset ( self , device ) : [EOL] spearman_correlation = SpearmanCorrelation ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels = [number] * predictions + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] [comment] [EOL] spearman_correlation . reset ( ) [EOL] spearman_correlation ( predictions , labels ) [EOL] temp = spearman_correlation . get_metric ( ) [EOL] spearman_correlation . reset ( ) [EOL] spearman_correlation ( predictions , labels ) [EOL] assert spearman_correlation . get_metric ( ) == temp [EOL] [EOL] [comment] [EOL] spearman_correlation . reset ( ) [EOL] spearman_correlation ( predictions , labels ) [EOL] [EOL] spearman_correlation . get_metric ( reset = False ) [EOL] assert spearman_correlation . get_metric ( ) != float ( [string] ) [EOL] spearman_correlation . get_metric ( reset = True ) [EOL] assert math . isnan ( spearman_correlation . get_metric ( ) ) [EOL] [EOL] def test_distributed_spearman ( self ) : [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions = torch . randn ( batch_size , num_labels ) [EOL] labels = [number] * predictions + torch . randn ( batch_size , num_labels ) [EOL] desired_spearman = spearman_formula ( predictions . reshape ( - [number] ) , labels . reshape ( - [number] ) ) [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] labels = [ labels [ : [number] ] , labels [ [number] : ] ] [EOL] metric_kwargs = { [string] : predictions , [string] : labels } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , SpearmanCorrelation ( ) , metric_kwargs , desired_spearman , exact = False , ) [EOL] [EOL] def test_distributed_spearman_unequal_batches ( self ) : [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions = torch . randn ( batch_size , num_labels ) [EOL] labels = [number] * predictions + torch . randn ( batch_size , num_labels ) [EOL] desired_spearman = spearman_formula ( predictions . reshape ( - [number] ) , labels . reshape ( - [number] ) ) [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] labels = [ labels [ : [number] ] , labels [ [number] : ] ] [EOL] metric_kwargs = { [string] : predictions , [string] : labels } [EOL] with pytest . raises ( Exception ) as _ : [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , SpearmanCorrelation ( ) , metric_kwargs , desired_spearman , exact = False , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.float]]$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.float]]$ 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 $typing.Any$ 0 $builtins.float$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 $typing.Any$ 0 $builtins.float$ 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.spearman_correlation.SpearmanCorrelation$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Any$ 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import pytest [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import CategoricalAccuracy [EOL] [EOL] [EOL] class CategoricalAccuracyTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_categorical_accuracy ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [number] , [number] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] actual_accuracy = accuracy . get_metric ( ) [EOL] assert actual_accuracy == [number] [EOL] [EOL] @ multi_device def test_top_k_categorical_accuracy ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( top_k = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [number] , [number] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] actual_accuracy = accuracy . get_metric ( ) [EOL] assert actual_accuracy == [number] [EOL] [EOL] @ multi_device def test_top_k_categorical_accuracy_accumulates_and_resets_correctly ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( top_k = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [number] , [number] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] accuracy ( predictions , targets ) [EOL] accuracy ( predictions , torch . tensor ( [ [number] , [number] ] , device = device ) ) [EOL] accuracy ( predictions , torch . tensor ( [ [number] , [number] ] , device = device ) ) [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [EOL] assert actual_accuracy == [number] [EOL] assert accuracy . correct_count == [number] [EOL] assert accuracy . total_count == [number] [EOL] [EOL] @ multi_device def test_top_k_categorical_accuracy_respects_mask ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( top_k = [number] ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , device = device , ) [EOL] targets = torch . tensor ( [ [number] , [number] , [number] ] , device = device ) [EOL] mask = torch . tensor ( [ False , True , True ] , device = device ) [EOL] accuracy ( predictions , targets , mask ) [EOL] actual_accuracy = accuracy . get_metric ( ) [EOL] assert_allclose ( actual_accuracy , [number] ) [EOL] [EOL] @ multi_device def test_top_k_categorical_accuracy_works_for_sequences ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( top_k = [number] ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , ] , device = device , ) [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [EOL] assert_allclose ( actual_accuracy , [number] ) [EOL] [EOL] [comment] [EOL] mask = torch . tensor ( [ [ False , True , True ] , [ True , False , True ] ] , device = device ) [EOL] accuracy ( predictions , targets , mask ) [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [EOL] assert_allclose ( actual_accuracy , [number] ) [EOL] [EOL] @ multi_device def test_top_k_categorical_accuracy_catches_exceptions ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( ) [EOL] predictions = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] out_of_range_labels = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] accuracy ( predictions , out_of_range_labels ) [EOL] [EOL] @ multi_device def test_tie_break_categorical_accuracy ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( tie_break = True ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] , device = device , ) [EOL] [comment] [EOL] targets = torch . tensor ( [ [number] , [number] , [number] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( reset = True ) == ( [number] + [number] + [number] ) / [number] [EOL] [EOL] [comment] [EOL] mask = torch . tensor ( [ True , False , True ] , device = device ) [EOL] targets = torch . tensor ( [ [number] , [number] , [number] ] , device = device ) [EOL] accuracy ( predictions , targets , mask ) [EOL] assert accuracy . get_metric ( reset = True ) == ( [number] + [number] ) / [number] [EOL] [EOL] [comment] [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] , [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] , ] , ] , device = device , ) [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [EOL] assert_allclose ( actual_accuracy , [number] / [number] ) [EOL] [EOL] @ multi_device def test_top_k_and_tie_break_together_catches_exceptions ( self , device ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] CategoricalAccuracy ( top_k = [number] , tie_break = True ) [EOL] [EOL] @ multi_device def test_incorrect_top_k_catches_exceptions ( self , device ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] CategoricalAccuracy ( top_k = [number] ) [EOL] [EOL] @ multi_device def test_does_not_divide_by_zero_with_no_count ( self , device ) : [EOL] accuracy = CategoricalAccuracy ( ) [EOL] assert accuracy . get_metric ( ) == pytest . approx ( [number] ) [EOL] [EOL] def test_distributed_accuracy ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [number] ] ) , torch . tensor ( [ [number] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_accuracy = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , CategoricalAccuracy ( ) , metric_kwargs , desired_accuracy , exact = False , ) [EOL] [EOL] def test_distributed_accuracy_unequal_batches ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [number] , [number] ] ) , torch . tensor ( [ [number] ] ) ] [EOL] mask = [ torch . tensor ( [ False , True ] ) , torch . tensor ( [ True ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets , [string] : mask } [EOL] desired_accuracy = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , CategoricalAccuracy ( top_k = [number] ) , metric_kwargs , desired_accuracy , exact = False , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [number] ] ) , torch . tensor ( [ [number] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_accuracy = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , CategoricalAccuracy ( ) , metric_kwargs , desired_accuracy , exact = True , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] assert desired_values == metric . get_metric ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] import torch [EOL] from nltk import Tree [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import EvalbBracketingScorer [EOL] [EOL] [EOL] class EvalbBracketingScorerTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] EvalbBracketingScorer . compile_evalb ( ) [EOL] [EOL] def tearDown ( self ) : [EOL] EvalbBracketingScorer . clean_evalb ( ) [EOL] super ( ) . tearDown ( ) [EOL] [EOL] def test_evalb_correctly_scores_identical_trees ( self ) : [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] evalb_scorer = EvalbBracketingScorer ( ) [EOL] evalb_scorer ( [ tree1 ] , [ tree2 ] ) [EOL] metrics = evalb_scorer . get_metric ( ) [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] def test_evalb_correctly_scores_imperfect_trees ( self ) : [EOL] [comment] [EOL] [comment] [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] evalb_scorer = EvalbBracketingScorer ( ) [EOL] evalb_scorer ( [ tree1 ] , [ tree2 ] ) [EOL] metrics = evalb_scorer . get_metric ( ) [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] def test_evalb_correctly_calculates_bracketing_metrics_over_multiple_trees ( self ) : [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] evalb_scorer = EvalbBracketingScorer ( ) [EOL] evalb_scorer ( [ tree1 , tree2 ] , [ tree2 , tree2 ] ) [EOL] metrics = evalb_scorer . get_metric ( ) [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] def test_evalb_with_terrible_trees_handles_nan_f1 ( self ) : [EOL] [comment] [EOL] [comment] [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] evalb_scorer = EvalbBracketingScorer ( ) [EOL] evalb_scorer ( [ tree1 ] , [ tree2 ] ) [EOL] metrics = evalb_scorer . get_metric ( ) [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] def test_distributed_evalb ( self ) : [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] predicted_trees = [ [ tree1 ] , [ tree2 ] ] [EOL] gold_trees = [ [ tree2 ] , [ tree2 ] ] [EOL] metric_kwargs = { [string] : predicted_trees , [string] : gold_trees } [EOL] desired_values = { [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , EvalbBracketingScorer ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] tree1 = Tree . fromstring ( [string] ) [EOL] tree2 = Tree . fromstring ( [string] ) [EOL] predicted_trees = [ [ tree1 ] , [ tree2 ] ] [EOL] gold_trees = [ [ tree2 ] , [ tree2 ] ] [EOL] metric_kwargs = { [string] : predicted_trees , [string] : gold_trees } [EOL] desired_values = { [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , EvalbBracketingScorer ( ) , metric_kwargs , desired_values , exact = False , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] metric_values = metric . get_metric ( ) [EOL] [EOL] for key in desired_values : [EOL] assert desired_values [ key ] == metric_values [ key ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.evalb_bracketing_scorer.EvalbBracketingScorer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.List[typing.List[typing.Any]]]$ 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.List[typing.Any]]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Dict[builtins.str,typing.List[typing.List[typing.Any]]]$ 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.List[typing.Any]]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import SequenceAccuracy [EOL] [EOL] [EOL] class SequenceAccuracyTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_sequence_accuracy ( self , device ) : [EOL] accuracy = SequenceAccuracy ( ) [EOL] gold = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ - [number] , - [number] , - [number] ] , [ [number] , [number] , - [number] ] ] ] , device = device , ) [EOL] [EOL] accuracy ( predictions , gold ) [EOL] actual_accuracy = accuracy . get_metric ( ) [ [string] ] [EOL] assert_allclose ( actual_accuracy , [number] / [number] ) [EOL] [EOL] @ multi_device def test_sequence_accuracy_respects_mask ( self , device ) : [EOL] accuracy = SequenceAccuracy ( ) [EOL] gold = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ - [number] , - [number] , - [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , ] , device = device , ) [EOL] mask = torch . tensor ( [ [ False , True , True ] , [ True , True , True ] , [ True , True , False ] , [ True , False , True ] ] , device = device , ) [EOL] [EOL] accuracy ( predictions , gold , mask ) [EOL] actual_accuracy = accuracy . get_metric ( ) [ [string] ] [EOL] assert_allclose ( actual_accuracy , [number] / [number] ) [EOL] [EOL] @ multi_device def test_sequence_accuracy_accumulates_and_resets_correctly ( self , device ) : [EOL] accuracy = SequenceAccuracy ( ) [EOL] gold = torch . tensor ( [ [ [number] , [number] , [number] ] ] , device = device ) [EOL] accuracy ( torch . tensor ( [ [ [ [number] , [number] , [number] ] ] ] , device = device ) , gold ) [EOL] accuracy ( torch . tensor ( [ [ [ [number] , [number] , [number] ] ] ] , device = device ) , gold ) [EOL] [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [ [string] ] [EOL] assert_allclose ( actual_accuracy , [number] / [number] ) [EOL] assert accuracy . correct_count == [number] [EOL] assert accuracy . total_count == [number] [EOL] [EOL] @ multi_device def test_get_metric_on_new_object_works ( self , device ) : [EOL] accuracy = SequenceAccuracy ( ) [EOL] [EOL] actual_accuracy = accuracy . get_metric ( reset = True ) [ [string] ] [EOL] assert_allclose ( actual_accuracy , [number] ) [EOL] [EOL] def test_distributed_sequence_accuracy ( self ) : [EOL] gold = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ - [number] , - [number] , - [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , ] ) [EOL] mask = torch . tensor ( [ [ False , True , True ] , [ True , True , True ] , [ True , True , False ] , [ True , False , True ] ] , ) [EOL] gold = [ gold [ : [number] ] , gold [ [number] : ] ] [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] mask = [ mask [ : [number] ] , mask [ [number] : ] ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : gold , [string] : mask } [EOL] desired_values = { [string] : [number] / [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , SequenceAccuracy ( ) , metric_kwargs , desired_values , exact = False , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] gold = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) [EOL] predictions = torch . tensor ( [ [ [ [number] , [number] , [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , [ [ - [number] , - [number] , - [number] ] , [ [number] , [number] , - [number] ] ] , [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , ] ) [EOL] mask = torch . tensor ( [ [ False , True , True ] , [ True , True , True ] , [ True , True , False ] , [ True , False , True ] ] , ) [EOL] gold = [ gold [ : [number] ] , gold [ [number] : ] ] [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] mask = [ mask [ : [number] ] , mask [ [number] : ] ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : gold , [string] : mask } [EOL] desired_values = { [string] : [number] / [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , SequenceAccuracy ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] assert desired_values [ [string] ] == metric . get_metric ( ) [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import BooleanAccuracy [EOL] [EOL] [EOL] class BooleanAccuracyTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_accuracy_computation ( self , device ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] mask = torch . ones ( [number] , [number] , device = device ) . bool ( ) [EOL] mask [ [number] , [number] ] = [number] [EOL] accuracy ( predictions , targets , mask ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] targets [ [number] , [number] ] = [number] [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] accuracy . reset ( ) [EOL] accuracy ( predictions , targets ) [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] @ multi_device def test_skips_completely_masked_instances ( self , device ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , device = device ) [EOL] [EOL] mask = torch . tensor ( [ [ False , False ] , [ True , False ] , [ True , True ] , [ True , True ] ] , device = device ) [EOL] accuracy ( predictions , targets , mask ) [EOL] [EOL] [comment] [EOL] assert accuracy . get_metric ( ) == [number] / [number] [EOL] [EOL] @ multi_device def test_incorrect_gold_labels_shape_catches_exceptions ( self , device ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] incorrect_shape_labels = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] with pytest . raises ( ValueError ) : [EOL] accuracy ( predictions , incorrect_shape_labels ) [EOL] [EOL] @ multi_device def test_incorrect_mask_shape_catches_exceptions ( self , device ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] predictions = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] labels = torch . rand ( [ [number] , [number] ] , device = device ) [EOL] incorrect_shape_mask = torch . randint ( [number] , [number] , [ [number] , [number] ] , device = device ) . bool ( ) [EOL] with pytest . raises ( ValueError ) : [EOL] accuracy ( predictions , labels , incorrect_shape_mask ) [EOL] [EOL] @ multi_device def test_does_not_divide_by_zero_with_no_count ( self , device ) : [EOL] accuracy = BooleanAccuracy ( ) [EOL] assert accuracy . get_metric ( ) == pytest . approx ( [number] ) [EOL] [EOL] def test_distributed_accuracy ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , BooleanAccuracy ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] def test_distributed_accuracy_unequal_batches ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] ] ) ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , BooleanAccuracy ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] ] , [ [number] , [number] ] ] ) ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = [number] [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , BooleanAccuracy ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] assert desired_values == metric . get_metric ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , List [EOL] import builtins [EOL] import typing [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , run_distributed_test , global_distributed_metric , ) [EOL] from allennlp . training . metrics import Average [EOL] [EOL] [EOL] class AverageTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . metric = Average ( ) [EOL] [EOL] @ multi_device def test_distributed_average ( self , device ) : [EOL] device_ids = [ - [number] , - [number] ] if device == [string] else [ [number] , [number] ] [EOL] metric_kwargs = { [string] : [ [number] , [number] ] , } [EOL] run_distributed_test ( device_ids , global_distributed_metric , self . metric , metric_kwargs , [number] , exact = True , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.float]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[builtins.float]]$ 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] import pytest [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . common . testing import AllenNlpTestCase , multi_device [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . training . metrics import SpanBasedF1Measure , Metric [EOL] [EOL] [EOL] class SpanBasedF1Test ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] [EOL] [comment] [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] vocab . add_token_to_namespace ( [string] , [string] ) [EOL] [EOL] self . vocab = vocab [EOL] [EOL] @ multi_device def test_span_metrics_are_computed_correcly_with_prediction_map ( self , device ) : [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] gold_indices = [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] [EOL] prediction_map_indices = [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] [EOL] [EOL] gold_tensor = torch . tensor ( gold_indices , device = device ) [EOL] prediction_map_tensor = torch . tensor ( prediction_map_indices , device = device ) [EOL] [EOL] prediction_tensor = torch . rand ( [ [number] , [number] , [number] ] , device = device ) [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] [EOL] metric = SpanBasedF1Measure ( self . vocab , [string] ) [EOL] metric ( prediction_tensor , gold_tensor , prediction_map = prediction_map_tensor ) [EOL] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _true_positives . keys ( ) [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_negatives . keys ( ) [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_positives . keys ( ) [EOL] [EOL] [comment] [EOL] metric ( prediction_tensor , gold_tensor , prediction_map = prediction_map_tensor ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _true_positives . keys ( ) [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_negatives . keys ( ) [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_positives . keys ( ) [EOL] [EOL] metric_dict = metric . get_metric ( ) [EOL] [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] @ multi_device def test_span_metrics_are_computed_correctly ( self , device ) : [EOL] gold_labels = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] gold_indices = [ self . vocab . get_token_index ( x , [string] ) for x in gold_labels ] [EOL] [EOL] gold_tensor = torch . tensor ( [ gold_indices ] , device = device ) [EOL] [EOL] prediction_tensor = torch . rand ( [ [number] , [number] , self . vocab . get_vocab_size ( [string] ) ] , device = device ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] mask = torch . tensor ( [ [ True , True , True , True , True , True , True , True , True ] , [ False , False , False , False , False , False , False , False , False ] , ] , device = device , ) [EOL] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ : , [number] , [number] ] = [number] [comment] [EOL] [EOL] metric = SpanBasedF1Measure ( self . vocab , [string] ) [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _true_positives . keys ( ) [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_negatives . keys ( ) [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_positives . keys ( ) [EOL] [EOL] [comment] [EOL] metric ( prediction_tensor , gold_tensor , mask ) [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert metric . _true_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _true_positives . keys ( ) [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert metric . _false_negatives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_negatives . keys ( ) [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert metric . _false_positives [ [string] ] == [number] [EOL] assert [string] not in metric . _false_positives . keys ( ) [EOL] [EOL] metric_dict = metric . get_metric ( ) [EOL] [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] @ multi_device def test_bmes_span_metrics_are_computed_correctly ( self , device ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] gold_indices = [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] ] ] [EOL] gold_tensor = torch . tensor ( gold_indices , device = device ) [EOL] [EOL] prediction_tensor = torch . rand ( [ [number] , [number] , [number] ] , device = device ) [EOL] [comment] [EOL] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] [comment] [EOL] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] prediction_tensor [ [number] , [number] , [number] ] = [number] [comment] [EOL] [EOL] metric = SpanBasedF1Measure ( self . vocab , [string] , label_encoding = [string] ) [EOL] metric ( prediction_tensor , gold_tensor ) [EOL] [EOL] [comment] [EOL] metric_dict = metric . get_metric ( ) [EOL] [EOL] assert_allclose ( metric_dict [ [string] ] , [number] , rtol = [number] , atol = [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] , rtol = [number] , atol = [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] , rtol = [number] , atol = [number] ) [EOL] [EOL] @ multi_device def test_span_f1_can_build_from_params ( self , device ) : [EOL] params = Params ( { [string] : [string] , [string] : [string] , [string] : [ [string] ] } ) [EOL] metric = Metric . from_params ( params = params , vocabulary = self . vocab ) [EOL] assert metric . _ignore_classes == [ [string] ] [comment] [EOL] assert metric . _label_vocabulary == self . vocab . get_index_to_token_vocabulary ( [string] ) [EOL] [EOL] @ multi_device def test_span_f1_accepts_tags_to_spans_function_argument ( self , device ) : [EOL] def mock_tags_to_spans_function ( tag_sequence , classes_to_ignore = None ) : [EOL] return [ ( [string] , ( [number] , [number] ) ) ] [EOL] [EOL] [comment] [EOL] bio_tags = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] gold_indices = [ self . vocab . get_token_index ( x , [string] ) for x in bio_tags ] [EOL] gold_tensor = torch . tensor ( [ gold_indices ] , device = device ) [EOL] prediction_tensor = torch . rand ( [ [number] , [number] , self . vocab . get_vocab_size ( [string] ) ] , device = device ) [EOL] [EOL] metric = SpanBasedF1Measure ( self . vocab , [string] , label_encoding = None , tags_to_spans_function = mock_tags_to_spans_function , ) [EOL] [EOL] metric ( prediction_tensor , gold_tensor ) [EOL] metric_dict = metric . get_metric ( ) [EOL] [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] assert_allclose ( metric_dict [ [string] ] , [number] ) [EOL] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] SpanBasedF1Measure ( self . vocab , label_encoding = [string] ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] SpanBasedF1Measure ( self . vocab , tags_to_spans_function = mock_tags_to_spans_function ) [EOL] with pytest . raises ( ConfigurationError ) : [EOL] SpanBasedF1Measure ( self . vocab , label_encoding = None , tags_to_spans_function = None ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.metric.Metric$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.metric.Metric$ 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.metric.Metric$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Optional , Any , List , Tuple [EOL] import numpy [EOL] import builtins [EOL] import typing [EOL] from typing import Optional [EOL] [EOL] import numpy as np [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , ) [EOL] from allennlp . training . metrics import PearsonCorrelation [EOL] [EOL] [EOL] def pearson_corrcoef ( predictions , labels , fweights = None ) : [EOL] covariance_matrices = np . cov ( predictions , labels , fweights = fweights ) [EOL] denominator = np . sqrt ( covariance_matrices [ [number] , [number] ] * covariance_matrices [ [number] , [number] ] ) [EOL] if np . around ( denominator , decimals = [number] ) == [number] : [EOL] expected_pearson_correlation = [number] [EOL] else : [EOL] expected_pearson_correlation = covariance_matrices [ [number] , [number] ] / denominator [EOL] return expected_pearson_correlation [EOL] [EOL] [EOL] class PearsonCorrelationTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_pearson_correlation_unmasked_computation ( self , device ) : [EOL] pearson_correlation = PearsonCorrelation ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions_1 = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels_1 = [number] * predictions_1 + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] predictions_2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] predictions_2 = predictions_2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] labels_2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] labels_2 = [number] * predictions_2 + labels_2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] predictions_labels = [ ( predictions_1 , labels_1 ) , ( predictions_2 , labels_2 ) ] [EOL] [EOL] stride = [number] [EOL] [EOL] for predictions , labels in predictions_labels : [EOL] pearson_correlation . reset ( ) [EOL] for i in range ( batch_size // stride ) : [EOL] timestep_predictions = predictions [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_labels = labels [ stride * i : stride * ( i + [number] ) , : ] [EOL] expected_pearson_correlation = pearson_corrcoef ( predictions [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , labels [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [EOL] pearson_correlation ( timestep_predictions , timestep_labels ) [EOL] assert_allclose ( expected_pearson_correlation , pearson_correlation . get_metric ( ) ) [EOL] [comment] [EOL] pearson_correlation . reset ( ) [EOL] pearson_correlation ( predictions , labels ) [EOL] assert_allclose ( pearson_corrcoef ( predictions . view ( - [number] ) . cpu ( ) . numpy ( ) , labels . view ( - [number] ) . cpu ( ) . numpy ( ) ) , pearson_correlation . get_metric ( ) , ) [EOL] [EOL] @ multi_device def test_pearson_correlation_masked_computation ( self , device ) : [EOL] pearson_correlation = PearsonCorrelation ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions_1 = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels_1 = [number] * predictions_1 + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] predictions_2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] predictions_2 = predictions_2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] labels_2 = torch . randn ( [number] , device = device ) . expand ( num_labels ) [EOL] labels_2 = [number] * predictions_2 + labels_2 . unsqueeze ( [number] ) . expand ( batch_size , - [number] ) [EOL] [EOL] predictions_labels = [ ( predictions_1 , labels_1 ) , ( predictions_2 , labels_2 ) ] [EOL] [EOL] [comment] [EOL] mask = torch . randint ( [number] , [number] , size = ( batch_size , num_labels ) , device = device ) . bool ( ) [EOL] stride = [number] [EOL] [EOL] for predictions , labels in predictions_labels : [EOL] pearson_correlation . reset ( ) [EOL] for i in range ( batch_size // stride ) : [EOL] timestep_predictions = predictions [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_labels = labels [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_mask = mask [ stride * i : stride * ( i + [number] ) , : ] [EOL] expected_pearson_correlation = pearson_corrcoef ( predictions [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , labels [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , fweights = mask [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [EOL] [EOL] pearson_correlation ( timestep_predictions , timestep_labels , timestep_mask ) [EOL] assert_allclose ( expected_pearson_correlation , pearson_correlation . get_metric ( ) ) [EOL] [comment] [EOL] pearson_correlation . reset ( ) [EOL] pearson_correlation ( predictions , labels , mask ) [EOL] expected_pearson_correlation = pearson_corrcoef ( predictions . view ( - [number] ) . cpu ( ) . numpy ( ) , labels . view ( - [number] ) . cpu ( ) . numpy ( ) , fweights = mask . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [EOL] [EOL] assert_allclose ( expected_pearson_correlation , pearson_correlation . get_metric ( ) ) [EOL] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.float]]$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 $typing.Any$ 0 $builtins.float$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,builtins.float]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] import pytest [EOL] import torch [EOL] from sklearn import metrics [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import Auc [EOL] [EOL] [EOL] class AucTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_auc_computation ( self , device ) : [EOL] auc = Auc ( ) [EOL] all_predictions = [ ] [EOL] all_labels = [ ] [EOL] for _ in range ( [number] ) : [EOL] predictions = torch . randn ( [number] , device = device ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long , device = device ) [EOL] [EOL] auc ( predictions , labels ) [EOL] [EOL] all_predictions . append ( predictions ) [EOL] all_labels . append ( labels ) [EOL] [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( torch . cat ( all_labels , dim = [number] ) . cpu ( ) . numpy ( ) , torch . cat ( all_predictions , dim = [number] ) . cpu ( ) . numpy ( ) , ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_allclose ( real_auc_value , computed_auc_value ) [EOL] [EOL] [comment] [EOL] predictions = torch . randn ( [number] , device = device ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long , device = device ) [EOL] [EOL] auc ( predictions , labels ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . cpu ( ) . numpy ( ) , predictions . cpu ( ) . numpy ( ) ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_allclose ( real_auc_value , computed_auc_value ) [EOL] [EOL] @ multi_device def test_auc_gold_labels_behaviour ( self , device ) : [EOL] [comment] [EOL] auc = Auc ( positive_label = [number] ) [EOL] [EOL] predictions = torch . randn ( [number] , device = device ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long , device = device ) [EOL] [comment] [EOL] labels [ [number] ] = [number] [EOL] auc ( predictions , labels ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . cpu ( ) . numpy ( ) , predictions . cpu ( ) . numpy ( ) , pos_label = [number] ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_allclose ( real_auc_value , computed_auc_value ) [EOL] [EOL] [comment] [EOL] with pytest . raises ( ConfigurationError ) as _ : [EOL] labels = torch . tensor ( [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , device = device ) [EOL] auc ( predictions , labels ) [EOL] [EOL] @ multi_device def test_auc_with_mask ( self , device ) : [EOL] auc = Auc ( ) [EOL] [EOL] predictions = torch . randn ( [number] , device = device ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long , device = device ) [EOL] mask = torch . tensor ( [ True , True , True , True , False , False , False , False ] , device = device ) [EOL] [EOL] auc ( predictions , labels , mask ) [EOL] computed_auc_value = auc . get_metric ( reset = True ) [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels [ : [number] ] . cpu ( ) . numpy ( ) , predictions [ : [number] ] . cpu ( ) . numpy ( ) ) [EOL] real_auc_value = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] assert_allclose ( real_auc_value , computed_auc_value ) [EOL] [EOL] @ multi_device def test_auc_works_without_calling_metric_at_all ( self , device ) : [EOL] auc = Auc ( ) [EOL] auc . get_metric ( ) [EOL] [EOL] def test_distributed_auc ( self ) : [EOL] predictions = torch . randn ( [number] ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long ) [EOL] [comment] [EOL] labels [ [number] ] = [number] [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . cpu ( ) . numpy ( ) , predictions . cpu ( ) . numpy ( ) , pos_label = [number] ) [EOL] [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] labels = [ labels [ : [number] ] , labels [ [number] : ] ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : labels } [EOL] desired_auc = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , Auc ( positive_label = [number] ) , metric_kwargs , desired_auc , exact = False , ) [EOL] [EOL] def test_distributed_auc_unequal_batches ( self ) : [EOL] predictions = torch . randn ( [number] ) [EOL] labels = torch . randint ( [number] , [number] , ( [number] , ) , dtype = torch . long ) [EOL] [comment] [EOL] labels [ [number] ] = [number] [EOL] [EOL] false_positive_rates , true_positive_rates , _ = metrics . roc_curve ( labels . cpu ( ) . numpy ( ) , predictions . cpu ( ) . numpy ( ) , pos_label = [number] ) [EOL] [EOL] predictions = [ predictions [ : [number] ] , predictions [ [number] : ] ] [EOL] labels = [ labels [ : [number] ] , labels [ [number] : ] ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : labels } [EOL] desired_auc = metrics . auc ( false_positive_rates , true_positive_rates ) [EOL] with pytest . raises ( Exception ) as _ : [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , Auc ( positive_label = [number] ) , metric_kwargs , desired_auc , exact = False , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.auc.Auc$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 $allennlp.training.metrics.auc.Auc$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Any$ 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] import torch [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , run_distributed_test , global_distributed_metric , ) [EOL] from allennlp . training . metrics import AttachmentScores [EOL] [EOL] [EOL] class AttachmentScoresTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . scorer = AttachmentScores ( ) [EOL] [EOL] self . predictions = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] self . gold_indices = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] self . label_predictions = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] self . gold_labels = torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) [EOL] [EOL] self . mask = torch . tensor ( [ [ True , True , True , True , True , True ] , [ True , True , True , True , False , False ] ] ) [EOL] [EOL] def _send_tensors_to_device ( self , device ) : [EOL] self . predictions = self . predictions . to ( device ) [EOL] self . gold_indices = self . gold_indices . to ( device ) [EOL] self . label_predictions = self . label_predictions . to ( device ) [EOL] self . gold_labels = self . gold_labels . to ( device ) [EOL] self . mask = self . mask . to ( device ) [EOL] [EOL] @ multi_device def test_perfect_scores ( self , device ) : [EOL] self . _send_tensors_to_device ( device ) [EOL] [EOL] self . scorer ( self . predictions , self . label_predictions , self . gold_indices , self . gold_labels , self . mask ) [EOL] [EOL] for value in self . scorer . get_metric ( ) . values ( ) : [EOL] assert value == [number] [EOL] [EOL] @ multi_device def test_unlabeled_accuracy_ignores_incorrect_labels ( self , device ) : [EOL] self . _send_tensors_to_device ( device ) [EOL] [EOL] label_predictions = self . label_predictions [EOL] [comment] [EOL] label_predictions [ [number] , [number] : ] = [number] [EOL] label_predictions [ [number] , [number] ] = [number] [EOL] self . scorer ( self . predictions , label_predictions , self . gold_indices , self . gold_labels , self . mask ) [EOL] [EOL] metrics = self . scorer . get_metric ( ) [EOL] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert metrics [ [string] ] == [number] [EOL] [comment] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] @ multi_device def test_labeled_accuracy_is_affected_by_incorrect_heads ( self , device ) : [EOL] self . _send_tensors_to_device ( device ) [EOL] [EOL] predictions = self . predictions [EOL] [comment] [EOL] predictions [ [number] , [number] : ] = [number] [EOL] predictions [ [number] , [number] ] = [number] [EOL] [comment] [EOL] predictions [ [number] , [number] ] = [number] [EOL] self . scorer ( predictions , self . label_predictions , self . gold_indices , self . gold_labels , self . mask ) [EOL] [EOL] metrics = self . scorer . get_metric ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert metrics [ [string] ] == [number] [EOL] [comment] [EOL] [comment] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] [comment] [EOL] assert metrics [ [string] ] == [number] [EOL] assert metrics [ [string] ] == [number] [EOL] [EOL] @ multi_device def test_attachment_scores_can_ignore_labels ( self , device ) : [EOL] self . _send_tensors_to_device ( device ) [EOL] [EOL] scorer = AttachmentScores ( ignore_classes = [ [number] ] ) [EOL] [EOL] label_predictions = self . label_predictions [EOL] [comment] [EOL] [comment] [EOL] label_predictions [ [number] , [number] ] = [number] [EOL] scorer ( self . predictions , label_predictions , self . gold_indices , self . gold_labels , self . mask ) [EOL] [EOL] for value in scorer . get_metric ( ) . values ( ) : [EOL] assert value == [number] [EOL] [EOL] def test_distributed_attachment_scores ( self ) : [EOL] predictions = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] gold_indices = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] label_predictions = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] [EOL] gold_labels = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] mask = [ torch . tensor ( [ [ True , True , True , True , True , True ] ] ) , torch . tensor ( [ [ True , True , True , True , False , False ] ] ) , ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : gold_indices , [string] : label_predictions , [string] : gold_labels , [string] : mask , } [EOL] [EOL] desired_metrics = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , AttachmentScores ( ) , metric_kwargs , desired_metrics , exact = True , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] predictions = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] gold_indices = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] label_predictions = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , ] [EOL] [EOL] gold_labels = [ torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) , torch . Tensor ( [ [ [number] , [number] , [number] , [number] , [number] , [number] ] ] ) ] [EOL] [EOL] mask = [ torch . tensor ( [ [ True , True , True , True , True , True ] ] ) , torch . tensor ( [ [ True , True , True , True , False , False ] ] ) , ] [EOL] [EOL] metric_kwargs = { [string] : predictions , [string] : gold_indices , [string] : label_predictions , [string] : gold_labels , [string] : mask , } [EOL] [EOL] desired_metrics = { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , AttachmentScores ( ) , metric_kwargs , desired_metrics , exact = True , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] metrics = metric . get_metric ( ) [EOL] [EOL] for key in metrics : [EOL] assert desired_values [ key ] == metrics [ key ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.attachment_scores.AttachmentScores$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.attachment_scores.AttachmentScores$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.attachment_scores.AttachmentScores$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] import torch [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import MeanAbsoluteError [EOL] [EOL] [EOL] class MeanAbsoluteErrorTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_mean_absolute_error_computation ( self , device ) : [EOL] mae = MeanAbsoluteError ( ) [EOL] predictions = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] targets = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] mae ( predictions , targets ) [EOL] assert mae . get_metric ( ) [ [string] ] == [number] / [number] [EOL] [EOL] mask = torch . tensor ( [ [ True , True , False ] , [ True , True , False ] , [ True , True , False ] , [ True , True , False ] ] , device = device , ) [EOL] mae ( predictions , targets , mask ) [EOL] assert mae . get_metric ( ) [ [string] ] == ( [number] + [number] ) / ( [number] + [number] ) [EOL] [EOL] new_targets = torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] , device = device ) [EOL] mae ( predictions , new_targets ) [EOL] assert mae . get_metric ( ) [ [string] ] == ( [number] + [number] + [number] ) / ( [number] + [number] + [number] ) [EOL] [EOL] mae . reset ( ) [EOL] mae ( predictions , new_targets ) [EOL] assert mae . get_metric ( ) [ [string] ] == [number] / [number] [EOL] [EOL] def test_distributed_accuracy ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = { [string] : [number] / [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , MeanAbsoluteError ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] predictions = [ torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , ] [EOL] targets = [ torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , torch . tensor ( [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] ) , ] [EOL] metric_kwargs = { [string] : predictions , [string] : targets } [EOL] desired_values = { [string] : [number] / [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , MeanAbsoluteError ( ) , metric_kwargs , desired_values , exact = True , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] assert desired_values [ [string] ] == metric . get_metric ( ) [ [string] ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.mean_absolute_error.MeanAbsoluteError$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import builtins [EOL] import typing [EOL] import numpy as np [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , ) [EOL] from allennlp . training . metrics import Covariance [EOL] [EOL] [EOL] class CovarianceTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_covariance_unmasked_computation ( self , device ) : [EOL] covariance = Covariance ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels = [number] * predictions + torch . randn ( batch_size , num_labels , device = device ) [EOL] [EOL] stride = [number] [EOL] [EOL] for i in range ( batch_size // stride ) : [EOL] timestep_predictions = predictions [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_labels = labels [ stride * i : stride * ( i + [number] ) , : ] [EOL] [comment] [EOL] [comment] [EOL] expected_covariance = np . cov ( predictions [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , labels [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [ [number] , [number] ] [EOL] covariance ( timestep_predictions , timestep_labels ) [EOL] assert_allclose ( expected_covariance , covariance . get_metric ( ) ) [EOL] [EOL] [comment] [EOL] covariance . reset ( ) [EOL] covariance ( predictions , labels ) [EOL] assert_allclose ( np . cov ( predictions . view ( - [number] ) . cpu ( ) . numpy ( ) , labels . view ( - [number] ) . cpu ( ) . numpy ( ) ) [ [number] , [number] ] , covariance . get_metric ( ) , ) [EOL] [EOL] @ multi_device def test_covariance_masked_computation ( self , device ) : [EOL] covariance = Covariance ( ) [EOL] batch_size = [number] [EOL] num_labels = [number] [EOL] predictions = torch . randn ( batch_size , num_labels , device = device ) [EOL] labels = [number] * predictions + torch . randn ( batch_size , num_labels , device = device ) [EOL] [comment] [EOL] mask = torch . randint ( [number] , [number] , size = ( batch_size , num_labels ) , device = device ) . bool ( ) [EOL] stride = [number] [EOL] [EOL] for i in range ( batch_size // stride ) : [EOL] timestep_predictions = predictions [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_labels = labels [ stride * i : stride * ( i + [number] ) , : ] [EOL] timestep_mask = mask [ stride * i : stride * ( i + [number] ) , : ] [EOL] [comment] [EOL] [comment] [EOL] expected_covariance = np . cov ( predictions [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , labels [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , fweights = mask [ : stride * ( i + [number] ) , : ] . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [ [number] , [number] ] [EOL] covariance ( timestep_predictions , timestep_labels , timestep_mask ) [EOL] assert_allclose ( expected_covariance , covariance . get_metric ( ) ) [EOL] [EOL] [comment] [EOL] covariance . reset ( ) [EOL] covariance ( predictions , labels , mask ) [EOL] assert_allclose ( np . cov ( predictions . view ( - [number] ) . cpu ( ) . numpy ( ) , labels . view ( - [number] ) . cpu ( ) . numpy ( ) , fweights = mask . view ( - [number] ) . cpu ( ) . numpy ( ) , ) [ [number] , [number] ] , covariance . get_metric ( ) , ) [EOL] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 $builtins.float$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $builtins.float$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $builtins.float$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , Dict , List , Union , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] import torch [EOL] from typing import Any , Dict , List , Tuple , Union [EOL] [EOL] import torch [EOL] from torch . testing import assert_allclose [EOL] [EOL] from allennlp . common . testing import ( AllenNlpTestCase , multi_device , global_distributed_metric , run_distributed_test , ) [EOL] from allennlp . training . metrics import Entropy [EOL] [EOL] [EOL] class EntropyTest ( AllenNlpTestCase ) : [EOL] @ multi_device def test_low_entropy_distribution ( self , device ) : [EOL] metric = Entropy ( ) [EOL] logits = torch . tensor ( [ [ [number] , - [number] , - [number] , - [number] ] , [ [number] , - [number] , - [number] , - [number] ] ] , dtype = torch . float , device = device , ) [EOL] metric ( logits ) [EOL] assert metric . get_metric ( ) [ [string] ] == [number] [EOL] [EOL] @ multi_device def test_entropy_for_uniform_distribution ( self , device ) : [EOL] metric = Entropy ( ) [EOL] logits = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , dtype = torch . float , device = device ) [EOL] metric ( logits ) [EOL] assert_allclose ( metric . get_metric ( ) [ [string] ] , [number] ) [EOL] [comment] [EOL] logits = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , dtype = torch . float , device = device ) [EOL] metric ( logits ) [EOL] assert_allclose ( metric . get_metric ( ) [ [string] ] , [number] ) [EOL] [EOL] metric . reset ( ) [EOL] assert metric . _entropy == [number] [EOL] assert metric . _count == [number] [EOL] [EOL] @ multi_device def test_masked_case ( self , device ) : [EOL] metric = Entropy ( ) [EOL] [comment] [EOL] logits = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , - [number] , - [number] , - [number] ] ] , dtype = torch . float , device = device ) [EOL] mask = torch . tensor ( [ False , True ] , device = device ) [EOL] metric ( logits , mask ) [EOL] assert metric . get_metric ( ) [ [string] ] == [number] [EOL] [EOL] def test_distributed_entropy ( self ) : [EOL] logits = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , dtype = torch . float ) [EOL] logits = [ logits [ [number] ] , logits [ [number] ] ] [EOL] metric_kwargs = { [string] : logits } [EOL] desired_values = { [string] : [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , global_distributed_metric , Entropy ( ) , metric_kwargs , desired_values , exact = False , ) [EOL] [EOL] def test_multiple_distributed_runs ( self ) : [EOL] logits = torch . tensor ( [ [ [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] , dtype = torch . float ) [EOL] logits = [ logits [ [number] ] , logits [ [number] ] ] [EOL] metric_kwargs = { [string] : logits } [EOL] desired_values = { [string] : [number] } [EOL] run_distributed_test ( [ - [number] , - [number] ] , multiple_runs , Entropy ( ) , metric_kwargs , desired_values , exact = False , ) [EOL] [EOL] [EOL] def multiple_runs ( global_rank , world_size , gpu_id , metric , metric_kwargs , desired_values , exact = True , ) : [EOL] [EOL] kwargs = { } [EOL] [comment] [EOL] for argname in metric_kwargs : [EOL] kwargs [ argname ] = metric_kwargs [ argname ] [ global_rank ] [EOL] [EOL] for i in range ( [number] ) : [EOL] metric ( ** kwargs ) [EOL] [EOL] assert_allclose ( desired_values [ [string] ] , metric . get_metric ( ) [ [string] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 $typing.Any$ 0 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 $typing.Any$ 0 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.training.metrics.entropy.Entropy$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.List[typing.Any]]$ 0 $typing.Dict[builtins.str,builtins.float]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Iterable , Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Iterable [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . data . fields import LabelField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . dataloader import PyTorchDataLoader [EOL] from allennlp . data . dataset_readers . dataset_reader import DatasetReader [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_loader_uses_all_instances_when_batches_per_epochs_set ( lazy ) : [EOL] NUM_INSTANCES = [number] [EOL] BATCH_SIZE = [number] [EOL] BATCHES_PER_EPOCH = [number] [EOL] EPOCHS = [number] [EOL] [EOL] class FakeDatasetReader ( DatasetReader ) : [EOL] def _read ( self , filename ) : [EOL] for i in range ( NUM_INSTANCES ) : [EOL] yield Instance ( { [string] : LabelField ( i , skip_indexing = True ) } ) [EOL] [EOL] reader = FakeDatasetReader ( lazy = lazy ) [EOL] dataset = reader . read ( [string] ) [EOL] [EOL] loader = PyTorchDataLoader ( dataset , batch_size = BATCH_SIZE , batches_per_epoch = BATCHES_PER_EPOCH ) [EOL] epoch_batches = [ ] [EOL] for epoch in range ( EPOCHS ) : [EOL] batches = [ ] [EOL] for batch in loader : [EOL] instances = [ ] [EOL] for index in batch [ [string] ] : [EOL] instances . append ( index ) [EOL] batches . append ( instances ) [EOL] epoch_batches . append ( batches ) [EOL] [EOL] assert epoch_batches == [ [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , [ [ [number] , [number] ] , [ [number] , [number] ] , [ [number] , [number] ] ] , ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Type , DefaultDict , Any , List [EOL] import tests [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from collections import defaultdict [EOL] from dataclasses import dataclass [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] from allennlp . data . tokenizers import SpacyTokenizer [EOL] [EOL] [EOL] @ dataclass ( init = False ) class TokenWithStyle ( Token ) : [EOL] __slots__ = [ [string] ] [EOL] [EOL] is_bold = ... [EOL] [EOL] def __init__ ( self , text = None , is_bold = False ) : [EOL] super ( ) . __init__ ( text = text ) [EOL] self . is_bold = is_bold [EOL] [EOL] [EOL] class TestSingleIdTokenIndexer ( AllenNlpTestCase ) : [EOL] def test_count_vocab_items_respects_casing ( self ) : [EOL] indexer = SingleIdTokenIndexer ( [string] ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] assert counter [ [string] ] == { [string] : [number] , [string] : [number] } [EOL] [EOL] indexer = SingleIdTokenIndexer ( [string] , lowercase_tokens = True ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] assert counter [ [string] ] == { [string] : [number] } [EOL] [EOL] def test_as_array_produces_token_sequence ( self ) : [EOL] indexer = SingleIdTokenIndexer ( [string] ) [EOL] padded_tokens = indexer . as_padded_tensor_dict ( { [string] : [ [number] , [number] , [number] , [number] , [number] ] } , { [string] : [number] } ) [EOL] assert padded_tokens [ [string] ] . tolist ( ) == [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] [EOL] [EOL] def test_count_other_features ( self ) : [EOL] indexer = SingleIdTokenIndexer ( [string] , feature_name = [string] ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] token = TokenWithStyle ( [string] ) [EOL] token . is_bold = [string] [EOL] indexer . count_vocab_items ( token , counter ) [EOL] assert counter [ [string] ] == { [string] : [number] } [EOL] [EOL] def test_count_vocab_items_with_non_default_feature_name ( self ) : [EOL] tokenizer = SpacyTokenizer ( parse = True ) [EOL] tokens = tokenizer . tokenize ( [string] ) [EOL] tokens = [ Token ( [string] ) ] + [ t for t in tokens ] + [ Token ( [string] ) ] [EOL] indexer = SingleIdTokenIndexer ( namespace = [string] , feature_name = [string] , default_value = [string] ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] for token in tokens : [EOL] indexer . count_vocab_items ( token , counter ) [EOL] [EOL] assert counter [ [string] ] == { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , } [EOL] [EOL] def test_tokens_to_indices_with_non_default_feature_name ( self ) : [EOL] tokenizer = SpacyTokenizer ( parse = True ) [EOL] tokens = tokenizer . tokenize ( [string] ) [EOL] tokens = [ t for t in tokens ] + [ Token ( [string] ) ] [EOL] vocab = Vocabulary ( ) [EOL] root_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] none_index = vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] indexer = SingleIdTokenIndexer ( namespace = [string] , feature_name = [string] , default_value = [string] ) [EOL] assert indexer . tokens_to_indices ( [ tokens [ [number] ] ] , vocab ) == { [string] : [ root_index ] } [EOL] assert indexer . tokens_to_indices ( [ tokens [ - [number] ] ] , vocab ) == { [string] : [ none_index ] } [EOL] [EOL] def test_crashes_with_empty_feature_value_and_no_default ( self ) : [EOL] tokenizer = SpacyTokenizer ( parse = True ) [EOL] tokens = tokenizer . tokenize ( [string] ) [EOL] tokens = [ t for t in tokens ] + [ Token ( [string] ) ] [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] indexer = SingleIdTokenIndexer ( namespace = [string] , feature_name = [string] ) [EOL] with pytest . raises ( ValueError ) : [EOL] indexer . tokens_to_indices ( [ tokens [ - [number] ] ] , vocab ) [EOL] [EOL] def test_no_namespace_means_no_counting ( self ) : [EOL] tokenizer = SpacyTokenizer ( parse = True ) [EOL] tokens = tokenizer . tokenize ( [string] ) [EOL] tokens = [ Token ( [string] ) ] + [ t for t in tokens ] + [ Token ( [string] ) ] [EOL] indexer = SingleIdTokenIndexer ( namespace = None , feature_name = [string] ) [EOL] [EOL] def fail ( ) : [EOL] assert False [EOL] [EOL] counter = defaultdict ( fail ) [EOL] for token in tokens : [EOL] indexer . count_vocab_items ( token , counter ) [EOL] [EOL] def test_no_namespace_means_no_indexing ( self ) : [EOL] indexer = SingleIdTokenIndexer ( namespace = None , feature_name = [string] ) [EOL] assert indexer . tokens_to_indices ( [ Token ( text_id = [number] ) ] , None ) == { [string] : [ [number] ] } [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $tests.data.token_indexers.single_id_token_indexer_test.TokenWithStyle$ 0 0 0 0 0 0 $tests.data.token_indexers.single_id_token_indexer_test.TokenWithStyle$ 0 $builtins.bool$ 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 $tests.data.token_indexers.single_id_token_indexer_test.TokenWithStyle$ 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List [EOL] import allennlp [EOL] import typing [EOL] import numpy as np [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary , Instance [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . data . token_indexers import ELMoTokenCharactersIndexer [EOL] from allennlp . data . fields import ListField , TextField [EOL] [EOL] [EOL] class TestELMoTokenCharactersIndexer ( AllenNlpTestCase ) : [EOL] def test_bos_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . tokens_to_indices ( [ Token ( [string] ) ] , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] [EOL] assert indices == { [string] : [ expected_indices ] } [EOL] [EOL] def test_eos_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . tokens_to_indices ( [ Token ( [string] ) ] , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] [EOL] assert indices == { [string] : [ expected_indices ] } [EOL] [EOL] def test_unicode_to_char_ids ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] indices = indexer . tokens_to_indices ( [ Token ( chr ( [number] ) + [string] ) ] , Vocabulary ( ) ) [EOL] expected_indices = [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] [EOL] assert indices == { [string] : [ expected_indices ] } [EOL] [EOL] def test_elmo_as_array_produces_token_sequence ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] tokens = [ Token ( [string] ) , Token ( [string] ) ] [EOL] indices = indexer . tokens_to_indices ( tokens , Vocabulary ( ) ) [EOL] padded_tokens = indexer . as_padded_tensor_dict ( indices , padding_lengths = { [string] : [number] } ) [EOL] expected_padded_tokens = [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] , ] [EOL] [EOL] assert padded_tokens [ [string] ] . tolist ( ) == expected_padded_tokens [EOL] [EOL] def test_elmo_indexer_with_additional_tokens ( self ) : [EOL] indexer = ELMoTokenCharactersIndexer ( tokens_to_add = { [string] : [number] } ) [EOL] tokens = [ Token ( [string] ) ] [EOL] indices = indexer . tokens_to_indices ( tokens , Vocabulary ( ) ) [EOL] expected_indices = [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , ] ] [EOL] assert indices [ [string] ] == expected_indices [EOL] [EOL] def test_elmo_empty_token_list ( self ) : [EOL] [comment] [EOL] indexer = ELMoTokenCharactersIndexer ( ) [EOL] assert { [string] : [ ] } == indexer . get_empty_token_list ( ) [EOL] [comment] [EOL] indexer = { [string] : indexer } [EOL] tokens_1 = TextField ( [ Token ( [string] ) ] , indexer ) [EOL] targets_1 = ListField ( [ TextField ( [ Token ( [string] ) ] , indexer ) ] ) [EOL] tokens_2 = TextField ( [ Token ( [string] ) , Token ( [string] ) ] , indexer ) [EOL] targets_2 = ListField ( [ TextField ( [ Token ( [string] ) ] , indexer ) , TextField ( [ Token ( [string] ) ] , indexer ) ] ) [EOL] instance_1 = Instance ( { [string] : tokens_1 , [string] : targets_1 } ) [EOL] instance_2 = Instance ( { [string] : tokens_2 , [string] : targets_2 } ) [EOL] a_batch = Batch ( [ instance_1 , instance_2 ] ) [EOL] a_batch . index_instances ( Vocabulary ( ) ) [EOL] batch_tensor = a_batch . as_tensor_dict ( ) [EOL] elmo_target_token_indices = batch_tensor [ [string] ] [ [string] ] [ [string] ] [EOL] [comment] [EOL] [comment] [EOL] empty_target = elmo_target_token_indices [ [number] ] [ [number] ] . numpy ( ) [EOL] np . testing . assert_array_equal ( np . zeros ( ( [number] , [number] ) ) , empty_target ) [EOL] non_empty_targets = [ elmo_target_token_indices [ [number] ] [ [number] ] , elmo_target_token_indices [ [number] ] [ [number] ] , elmo_target_token_indices [ [number] ] [ [number] ] , ] [EOL] for non_empty_target in non_empty_targets : [EOL] with pytest . raises ( AssertionError ) : [EOL] np . testing . assert_array_equal ( np . zeros ( ( [number] , [number] ) ) , non_empty_target ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.List[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from allennlp . common import cached_transformers [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Vocabulary [EOL] from allennlp . data . token_indexers import PretrainedTransformerIndexer [EOL] from allennlp . data . tokenizers import PretrainedTransformerTokenizer [EOL] [EOL] [EOL] class TestPretrainedTransformerIndexer ( AllenNlpTestCase ) : [EOL] def test_as_array_produces_token_sequence_bert_uncased ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] string_specials = [string] [EOL] string_no_specials = [string] [EOL] tokens = tokenizer . tokenize ( string_specials ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] [comment] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] [EOL] def test_as_array_produces_token_sequence_bert_cased ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] string_specials = [string] [EOL] string_no_specials = [string] [EOL] tokens = tokenizer . tokenize ( string_specials ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] [comment] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] [EOL] def test_as_array_produces_token_sequence_bert_cased_sentence_pair ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] , add_special_tokens = False ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] default_format = [string] [EOL] tokens = tokenizer . tokenize ( default_format ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] allennlp_tokens = allennlp_tokenizer . add_special_tokens ( allennlp_tokenizer . tokenize ( [string] ) , allennlp_tokenizer . tokenize ( [string] ) , ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] [EOL] def test_as_array_produces_token_sequence_roberta ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] string_specials = [string] [EOL] string_no_specials = [string] [EOL] tokens = tokenizer . tokenize ( string_specials ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] [comment] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] [EOL] def test_as_array_produces_token_sequence_roberta_sentence_pair ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] , add_special_tokens = False ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] default_format = [string] [EOL] tokens = tokenizer . tokenize ( default_format ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] allennlp_tokens = allennlp_tokenizer . add_special_tokens ( allennlp_tokenizer . tokenize ( [string] ) , allennlp_tokenizer . tokenize ( [string] ) , ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids , f"{ allennlp_tokens } [string] { tokens }" [EOL] [EOL] def test_transformers_vocab_sizes ( self ) : [EOL] def check_vocab_size ( model_name ) : [EOL] namespace = [string] [EOL] tokenizer = cached_transformers . get_tokenizer ( model_name ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( model_name ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = model_name , namespace = namespace ) [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( [string] ) [EOL] vocab = Vocabulary ( ) [EOL] [comment] [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] del indexed [EOL] assert vocab . get_vocab_size ( namespace = namespace ) == tokenizer . vocab_size [EOL] [EOL] check_vocab_size ( [string] ) [EOL] check_vocab_size ( [string] ) [EOL] check_vocab_size ( [string] ) [EOL] [EOL] def test_transformers_vocabs_added_correctly ( self ) : [EOL] namespace , model_name = [string] , [string] [EOL] tokenizer = cached_transformers . get_tokenizer ( model_name ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( model_name ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = model_name , namespace = namespace ) [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( [string] ) [EOL] vocab = Vocabulary ( ) [EOL] [comment] [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] del indexed [EOL] assert vocab . get_token_to_index_vocabulary ( namespace = namespace ) == tokenizer . encoder [EOL] [EOL] def test_mask ( self ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for model in [ [string] , [string] , [string] ] : [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( model ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = model ) [EOL] string_no_specials = [string] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] expected_masks = [ True ] * len ( indexed [ [string] ] ) [EOL] assert indexed [ [string] ] == expected_masks [EOL] max_length = [number] [EOL] padding_lengths = { key : max_length for key in indexed . keys ( ) } [EOL] padded_tokens = indexer . as_padded_tensor_dict ( indexed , padding_lengths ) [EOL] padding_length = max_length - len ( indexed [ [string] ] ) [EOL] expected_masks = expected_masks + ( [ False ] * padding_length ) [EOL] assert len ( padded_tokens [ [string] ] ) == max_length [EOL] assert padded_tokens [ [string] ] . tolist ( ) == expected_masks [EOL] [EOL] assert len ( padded_tokens [ [string] ] ) == max_length [EOL] pad_token_id = allennlp_tokenizer . tokenizer . pad_token_id [EOL] if pad_token_id is None : [EOL] pad_token_id = [number] [EOL] padding_suffix = [ pad_token_id ] * padding_length [EOL] assert padded_tokens [ [string] ] [ - padding_length : ] . tolist ( ) == padding_suffix [EOL] [EOL] def test_long_sequence_splitting ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] indexer = PretrainedTransformerIndexer ( model_name = [string] , max_length = [number] ) [EOL] string_specials = [string] [EOL] string_no_specials = [string] [EOL] tokens = tokenizer . tokenize ( string_specials ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] assert len ( expected_ids ) == [number] [comment] [EOL] cls_id , sep_id = expected_ids [ [number] ] , expected_ids [ - [number] ] [EOL] expected_ids = ( expected_ids [ : [number] ] + [ sep_id , cls_id ] + expected_ids [ [number] : [number] ] + [ sep_id , cls_id ] + expected_ids [ [number] : ] ) [EOL] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] assert indexed [ [string] ] == [ True ] * len ( expected_ids ) [EOL] assert indexed [ [string] ] == [ True ] * [number] [comment] [EOL] [EOL] @ staticmethod def _assert_tokens_equal ( expected_tokens , actual_tokens ) : [EOL] for expected , actual in zip ( expected_tokens , actual_tokens ) : [EOL] assert expected . text == actual . text [EOL] assert expected . text_id == actual . text_id [EOL] assert expected . type_id == actual . type_id [EOL] [EOL] def test_indices_to_tokens ( self ) : [EOL] allennlp_tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] indexer_max_length = PretrainedTransformerIndexer ( model_name = [string] , max_length = [number] ) [EOL] indexer_no_max_length = PretrainedTransformerIndexer ( model_name = [string] ) [EOL] string_no_specials = [string] [EOL] [EOL] allennlp_tokens = allennlp_tokenizer . tokenize ( string_no_specials ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer_no_max_length . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] tokens_from_indices = indexer_no_max_length . indices_to_tokens ( indexed , vocab ) [EOL] [EOL] self . _assert_tokens_equal ( allennlp_tokens , tokens_from_indices ) [EOL] [EOL] indexed = indexer_max_length . tokens_to_indices ( allennlp_tokens , vocab ) [EOL] tokens_from_indices = indexer_max_length . indices_to_tokens ( indexed , vocab ) [EOL] [EOL] [comment] [EOL] sep_cls = [ allennlp_tokens [ - [number] ] , allennlp_tokens [ [number] ] ] [EOL] expected = ( allennlp_tokens [ : [number] ] + sep_cls + allennlp_tokens [ [number] : [number] ] + sep_cls + allennlp_tokens [ [number] : ] ) [EOL] [EOL] self . _assert_tokens_equal ( expected , tokens_from_indices ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 $builtins.str$ 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[builtins.bool]$ 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[builtins.bool]$ 0 $typing.List[builtins.bool]$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.bool]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.Any]$ 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0
from typing import DefaultDict , Any , List [EOL] import allennlp [EOL] import typing [EOL] from collections import defaultdict [EOL] [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . token_indexers import TokenCharactersIndexer [EOL] from allennlp . data . tokenizers . character_tokenizer import CharacterTokenizer [EOL] [EOL] [EOL] class CharacterTokenIndexerTest ( AllenNlpTestCase ) : [EOL] def test_count_vocab_items_respects_casing ( self ) : [EOL] indexer = TokenCharactersIndexer ( [string] , min_padding_length = [number] ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] assert counter [ [string] ] == { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } [EOL] [EOL] indexer = TokenCharactersIndexer ( [string] , CharacterTokenizer ( lowercase_characters = True ) , min_padding_length = [number] ) [EOL] counter = defaultdict ( lambda : defaultdict ( int ) ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] indexer . count_vocab_items ( Token ( [string] ) , counter ) [EOL] assert counter [ [string] ] == { [string] : [number] , [string] : [number] , [string] : [number] , [string] : [number] } [EOL] [EOL] def test_as_array_produces_token_sequence ( self ) : [EOL] indexer = TokenCharactersIndexer ( [string] , min_padding_length = [number] ) [EOL] padded_tokens = indexer . as_padded_tensor_dict ( { [string] : [ [ [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] ] ] } , padding_lengths = { [string] : [number] , [string] : [number] } , ) [EOL] assert padded_tokens [ [string] ] . tolist ( ) == [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , ] [EOL] [EOL] def test_tokens_to_indices_produces_correct_characters ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] [EOL] indexer = TokenCharactersIndexer ( [string] , min_padding_length = [number] ) [EOL] indices = indexer . tokens_to_indices ( [ Token ( [string] ) ] , vocab ) [EOL] assert indices == { [string] : [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] ] } [EOL] [EOL] def test_start_and_end_tokens ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] indexer = TokenCharactersIndexer ( [string] , start_tokens = [ [string] ] , end_tokens = [ [string] ] , min_padding_length = [number] ) [EOL] indices = indexer . tokens_to_indices ( [ Token ( [string] ) ] , vocab ) [EOL] assert indices == { [string] : [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] ] ] } [EOL] [EOL] def test_min_padding_length ( self ) : [EOL] sentence = [string] [EOL] tokens = [ Token ( token ) for token in sentence . split ( [string] ) ] [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [comment] [EOL] [EOL] indexer = TokenCharactersIndexer ( [string] , min_padding_length = [number] ) [EOL] indices = indexer . tokens_to_indices ( tokens , vocab ) [EOL] padded = indexer . as_padded_tensor_dict ( indices , indexer . get_padding_lengths ( indices ) ) [EOL] assert padded [ [string] ] . tolist ( ) == [ [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , [ [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] ] , ] [EOL] [EOL] def test_warn_min_padding_length ( self ) : [EOL] with pytest . warns ( UserWarning , match = [string] ) : [EOL] TokenCharactersIndexer ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 $typing.DefaultDict[typing.Any,typing.DefaultDict[typing.Any,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common import cached_transformers [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token , Vocabulary [EOL] from allennlp . data . token_indexers import PretrainedTransformerMismatchedIndexer [EOL] [EOL] [EOL] class TestPretrainedTransformerMismatchedIndexer ( AllenNlpTestCase ) : [EOL] def test_bert ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] indexer = PretrainedTransformerMismatchedIndexer ( [string] ) [EOL] text = [ [string] , [string] , [string] ] [EOL] tokens = tokenizer . tokenize ( [string] . join ( [ [string] ] + text + [ [string] ] ) ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( [ Token ( word ) for word in text ] , vocab ) [EOL] assert indexed [ [string] ] == expected_ids [EOL] assert indexed [ [string] ] == [ True ] * len ( text ) [EOL] [comment] [EOL] assert indexed [ [string] ] == [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] assert indexed [ [string] ] == [ True ] * len ( expected_ids ) [EOL] [EOL] keys = indexed . keys ( ) [EOL] assert indexer . get_empty_token_list ( ) == { key : [ ] for key in keys } [EOL] [EOL] max_length = [number] [EOL] padding_lengths = { key : max_length for key in keys } [EOL] padded_tokens = indexer . as_padded_tensor_dict ( indexed , padding_lengths ) [EOL] for key in keys : [EOL] padding_length = max_length - len ( indexed [ key ] ) [EOL] if key == [string] : [EOL] padding = ( [number] , [number] ) [EOL] elif [string] in key : [EOL] padding = False [EOL] else : [EOL] padding = [number] [EOL] expected_value = indexed [ key ] + ( [ padding ] * padding_length ) [EOL] assert len ( padded_tokens [ key ] ) == max_length [EOL] if key == [string] : [EOL] expected_value = [ list ( t ) for t in expected_value ] [EOL] assert padded_tokens [ key ] . tolist ( ) == expected_value [EOL] [EOL] def test_long_sequence_splitting ( self ) : [EOL] tokenizer = cached_transformers . get_tokenizer ( [string] ) [EOL] indexer = PretrainedTransformerMismatchedIndexer ( [string] , max_length = [number] ) [EOL] text = [ [string] , [string] , [string] ] [EOL] tokens = tokenizer . tokenize ( [string] . join ( [ [string] ] + text + [ [string] ] ) ) [EOL] expected_ids = tokenizer . convert_tokens_to_ids ( tokens ) [EOL] assert len ( expected_ids ) == [number] [comment] [EOL] cls_id , sep_id = expected_ids [ [number] ] , expected_ids [ - [number] ] [EOL] expected_ids = ( expected_ids [ : [number] ] + [ sep_id , cls_id ] + expected_ids [ [number] : [number] ] + [ sep_id , cls_id ] + expected_ids [ [number] : ] ) [EOL] [EOL] vocab = Vocabulary ( ) [EOL] indexed = indexer . tokens_to_indices ( [ Token ( word ) for word in text ] , vocab ) [EOL] [EOL] assert indexed [ [string] ] == expected_ids [EOL] [comment] [EOL] assert indexed [ [string] ] == [ True ] * len ( expected_ids ) [EOL] [comment] [EOL] assert indexed [ [string] ] == [ True ] * len ( text ) [EOL] [comment] [EOL] assert indexed [ [string] ] == [ True ] * [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 $typing.Any$ 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Iterable , Any , List [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Iterable [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . dataset_readers import DatasetReader , InterleavingDatasetReader [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . instance import Instance [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] from allennlp . data . tokenizers import SpacyTokenizer [EOL] [EOL] [EOL] class PlainTextReader ( DatasetReader ) : [EOL] def __init__ ( self ) : [EOL] super ( ) . __init__ ( ) [EOL] self . _token_indexers = { [string] : SingleIdTokenIndexer ( ) } [EOL] self . _tokenizer = SpacyTokenizer ( ) [EOL] [EOL] def _read ( self , file_path ) : [EOL] with open ( file_path ) as input_file : [EOL] for line in input_file : [EOL] yield self . text_to_instance ( line ) [EOL] [EOL] def text_to_instance ( self , line ) : [comment] [EOL] [EOL] tokens = self . _tokenizer . tokenize ( line ) [EOL] return Instance ( { [string] : TextField ( tokens , self . _token_indexers ) } ) [EOL] [EOL] [EOL] class TestInterleavingDatasetReader ( AllenNlpTestCase ) : [EOL] def test_round_robin ( self ) : [EOL] readers = { [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) } [EOL] [EOL] reader = InterleavingDatasetReader ( readers ) [EOL] data_dir = self . FIXTURES_ROOT / [string] [EOL] [EOL] file_path = f""" [string] { data_dir / [string] } [string] { data_dir / [string] } [string] { data_dir / [string] } [string] """ [EOL] [EOL] instances = list ( reader . read ( file_path ) ) [EOL] first_three_keys = { instance . fields [ [string] ] . metadata for instance in instances [ : [number] ] } [EOL] assert first_three_keys == { [string] , [string] , [string] } [EOL] [EOL] next_three_keys = { instance . fields [ [string] ] . metadata for instance in instances [ [number] : [number] ] } [EOL] assert next_three_keys == { [string] , [string] , [string] } [EOL] [EOL] def test_all_at_once ( self ) : [EOL] readers = { [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) , [string] : PlainTextReader ( ) } [EOL] [EOL] reader = InterleavingDatasetReader ( readers , dataset_field_name = [string] , scheme = [string] ) [EOL] data_dir = self . FIXTURES_ROOT / [string] [EOL] [EOL] file_path = f""" [string] { data_dir / [string] } [string] { data_dir / [string] } [string] { data_dir / [string] } [string] """ [EOL] [EOL] buckets = [ ] [EOL] last_source = None [EOL] [EOL] [comment] [EOL] for instance in reader . read ( file_path ) : [EOL] source = instance . fields [ [string] ] . metadata [EOL] if source != last_source : [EOL] buckets . append ( [ ] ) [EOL] last_source = source [EOL] buckets [ - [number] ] . append ( instance ) [EOL] [EOL] [comment] [EOL] assert len ( buckets ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.instance.Instance]$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.instance.Instance$ 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 $typing.set$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.set$ 0 0 0 0 0 0 0 0 0 0 $typing.set$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.set$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0
from typing import Optional , Any , List [EOL] import tests [EOL] import builtins [EOL] import typing [EOL] from collections import deque [EOL] import os [EOL] import shutil [EOL] from typing import Optional , NamedTuple , List [EOL] [EOL] from filelock import FileLock [EOL] import pytest [EOL] import torch . distributed as dist [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . common import util as common_util [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . data import Instance [EOL] from allennlp . data . dataloader import PyTorchDataLoader [EOL] from allennlp . data . dataset_readers import ( dataset_reader , DatasetReader , TextClassificationJsonReader , ) [EOL] from allennlp . data . dataset_readers . dataset_reader import AllennlpLazyDataset [EOL] from allennlp . data . fields import LabelField [EOL] [EOL] [EOL] class TestDatasetReader ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . cache_directory = str ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] ) [EOL] [EOL] def teardown_method ( self ) : [EOL] super ( ) . teardown_method ( ) [EOL] if os . path . exists ( self . cache_directory ) : [EOL] shutil . rmtree ( self . cache_directory ) [EOL] [EOL] def test_lazy_dataset_can_be_iterated_through_multiple_times ( self ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( lazy = True ) [EOL] instances = reader . read ( data_file ) [EOL] assert isinstance ( instances , AllennlpLazyDataset ) [EOL] [EOL] first_pass_instances = list ( instances ) [EOL] assert len ( first_pass_instances ) > [number] [EOL] second_pass_instances = list ( instances ) [EOL] assert first_pass_instances == second_pass_instances [EOL] [EOL] def test_read_only_creates_cache_file_once ( self ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( cache_directory = self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( str ( data_file ) ) [EOL] [EOL] [comment] [EOL] reader . read ( data_file ) [EOL] assert os . path . exists ( cache_file ) [EOL] with open ( cache_file , [string] ) as in_file : [EOL] cache_contents = in_file . read ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] reader . read ( data_file ) [EOL] reader . read ( data_file ) [EOL] reader . read ( data_file ) [EOL] reader . read ( data_file ) [EOL] with open ( cache_file , [string] ) as in_file : [EOL] final_cache_contents = in_file . read ( ) [EOL] assert cache_contents == final_cache_contents [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_caching_works_with_lazy_reading ( self , caplog , lazy ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] snli_copy_file = str ( data_file ) + [string] [EOL] shutil . copyfile ( data_file , snli_copy_file ) [EOL] reader = TextClassificationJsonReader ( lazy = lazy , cache_directory = self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( snli_copy_file ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] assert not os . path . exists ( cache_file ) [EOL] instances = reader . read ( snli_copy_file ) [EOL] [EOL] [comment] [EOL] first_pass_instances = [ ] [EOL] for instance in instances : [EOL] first_pass_instances . append ( instance ) [EOL] assert [string] in [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] os . remove ( snli_copy_file ) [EOL] caplog . clear ( ) [EOL] instances = reader . read ( snli_copy_file ) [EOL] second_pass_instances = [ ] [EOL] for instance in instances : [EOL] second_pass_instances . append ( instance ) [EOL] assert [string] in [string] . join ( [ rec . message for rec in caplog . records ] ) [EOL] [EOL] [comment] [EOL] assert len ( first_pass_instances ) == len ( second_pass_instances ) [EOL] for instance , cached_instance in zip ( first_pass_instances , second_pass_instances ) : [EOL] assert instance . fields == cached_instance . fields [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] reader = TextClassificationJsonReader ( lazy = False , cache_directory = self . cache_directory ) [EOL] cached_instances = reader . read ( snli_copy_file ) [EOL] assert len ( first_pass_instances ) == len ( cached_instances ) [EOL] for instance , cached_instance in zip ( first_pass_instances , cached_instances ) : [EOL] assert instance . fields == cached_instance . fields [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_caching_skipped_when_lock_not_acquired ( self , caplog , lazy ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( lazy = lazy , cache_directory = self . cache_directory ) [EOL] reader . CACHE_FILE_LOCK_TIMEOUT = [number] [EOL] cache_file = reader . _get_cache_location_for_file_path ( str ( data_file ) ) [EOL] [EOL] with FileLock ( cache_file + [string] ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] caplog . clear ( ) [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert [string] in caplog . text [EOL] assert instances [EOL] [EOL] [comment] [EOL] assert not os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] deque ( reader . read ( data_file ) , maxlen = [number] ) [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] with FileLock ( cache_file + [string] ) : [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] caplog . clear ( ) [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert [string] in caplog . text [EOL] assert instances [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_caching_skipped_with_distributed_training ( self , caplog , monkeypatch , lazy ) : [EOL] monkeypatch . setattr ( common_util , [string] , lambda : True ) [EOL] monkeypatch . setattr ( dist , [string] , lambda : [number] ) [EOL] monkeypatch . setattr ( dist , [string] , lambda : [number] ) [EOL] [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( lazy = lazy , cache_directory = self . cache_directory ) [EOL] cache_file = reader . _get_cache_location_for_file_path ( str ( data_file ) ) [EOL] [EOL] deque ( reader . read ( data_file ) , maxlen = [number] ) [EOL] assert not os . path . exists ( cache_file ) [EOL] assert [string] in caplog . text [EOL] [EOL] def test_caching_with_lazy_reader_in_multi_process_loader ( self ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( lazy = True , cache_directory = self . cache_directory ) [EOL] deque ( PyTorchDataLoader ( reader . read ( data_file ) , collate_fn = lambda b : b [ [number] ] , num_workers = [number] ) , maxlen = [number] , ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] cache_file = reader . _get_cache_location_for_file_path ( str ( data_file ) ) [EOL] assert not os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert instances [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] new_instances = list ( PyTorchDataLoader ( reader . read ( data_file ) , collate_fn = lambda b : b [ [number] ] , num_workers = [number] ) ) [EOL] assert len ( instances ) == len ( new_instances ) [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_max_instances ( self , lazy ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( max_instances = [number] , lazy = lazy ) [EOL] instances = reader . read ( data_file ) [EOL] instance_count = sum ( [number] for _ in instances ) [EOL] assert instance_count == [number] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( [number] , [number] , [number] ) ) def test_max_instances_with_multi_process_loader ( self , num_workers ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] reader = TextClassificationJsonReader ( max_instances = [number] , lazy = True ) [EOL] instances = list ( PyTorchDataLoader ( reader . read ( data_file ) , collate_fn = lambda b : b [ [number] ] , num_workers = num_workers ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , ( True , False ) ) def test_cached_max_instances ( self , lazy ) : [EOL] data_file = ( AllenNlpTestCase . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] [EOL] [comment] [EOL] reader = TextClassificationJsonReader ( cache_directory = self . cache_directory , lazy = lazy , max_instances = [number] ) [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] cache_file = reader . _get_cache_location_for_file_path ( str ( data_file ) ) [EOL] assert not os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] reader = TextClassificationJsonReader ( cache_directory = self . cache_directory , lazy = lazy ) [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert len ( instances ) > [number] [EOL] assert os . path . exists ( cache_file ) [EOL] [EOL] [comment] [EOL] reader = TextClassificationJsonReader ( cache_directory = self . cache_directory , max_instances = [number] , lazy = lazy ) [EOL] instances = list ( reader . read ( data_file ) ) [EOL] assert len ( instances ) == [number] [EOL] [EOL] [EOL] class MockWorkerInfo ( NamedTuple ) : [EOL] id = ... [EOL] num_workers = ... [EOL] [EOL] [EOL] class MockDatasetReader ( DatasetReader ) : [EOL] def _read ( self , file_path ) : [EOL] for i in range ( [number] ) : [EOL] yield self . text_to_instance ( i ) [EOL] [EOL] def text_to_instance ( self , index ) : [comment] [EOL] return Instance ( { [string] : LabelField ( index , skip_indexing = True ) } ) [EOL] [EOL] [EOL] @ pytest . mark . parametrize ( [string] , [ ( None , None , None , None , None , list ( range ( [number] ) ) ) , ( None , None , None , None , [number] , list ( range ( [number] ) ) ) , ( None , None , None , None , [number] , list ( range ( [number] ) ) ) , ( None , None , [number] , [number] , None , list ( range ( [number] ) ) ) , ( None , None , [number] , [number] , None , [ [number] , [number] , [number] , [number] , [number] ] ) , ( None , None , [number] , [number] , None , [ [number] , [number] , [number] , [number] , [number] ] ) , ( None , None , [number] , [number] , [number] , [ [number] , [number] , [number] ] ) , ( None , None , [number] , [number] , [number] , [ [number] , [number] ] ) , ( [number] , [number] , None , None , None , list ( range ( [number] ) ) ) , ( [number] , [number] , None , None , None , [ [number] , [number] , [number] , [number] , [number] ] ) , ( [number] , [number] , None , None , None , [ [number] , [number] , [number] , [number] , [number] ] ) , ( [number] , [number] , None , None , [number] , [ [number] , [number] , [number] ] ) , ( [number] , [number] , None , None , [number] , [ [number] , [number] ] ) , ( [number] , [number] , [number] , [number] , None , [ [number] , [number] , [number] ] ) , ( [number] , [number] , [number] , [number] , None , [ [number] , [number] , [number] ] ) , ( [number] , [number] , [number] , [number] , None , [ [number] , [number] ] ) , ( [number] , [number] , [number] , [number] , None , [ [number] , [number] ] ) , ( [number] , [number] , [number] , [number] , [number] , [ [number] , [number] ] ) , ] , ) def test_instance_slicing ( monkeypatch , node_rank , world_size , worker_id , num_workers , max_instances , expected_result , ) : [EOL] if node_rank is not None and world_size is not None : [EOL] monkeypatch . setattr ( common_util , [string] , lambda : True ) [EOL] monkeypatch . setattr ( dist , [string] , lambda : node_rank ) [EOL] monkeypatch . setattr ( dist , [string] , lambda : world_size ) [EOL] [EOL] if worker_id is not None and num_workers is not None : [EOL] monkeypatch . setattr ( dataset_reader , [string] , lambda : MockWorkerInfo ( worker_id , num_workers ) ) [EOL] [EOL] reader = MockDatasetReader ( max_instances = max_instances ) [EOL] result = list ( ( x [ [string] ] . label for x in reader . read ( [string] ) ) ) [comment] [EOL] [EOL] assert result == expected_result [EOL] [EOL] [EOL] class BadLazyReader ( DatasetReader ) : [EOL] def _read ( self , file_path ) : [EOL] return [ self . text_to_instance ( i ) for i in range ( [number] ) ] [EOL] [EOL] def text_to_instance ( self , index ) : [comment] [EOL] return Instance ( { [string] : LabelField ( index , skip_indexing = True ) } ) [EOL] [EOL] [EOL] def test_config_error_when_lazy_reader_returns_list ( ) : [EOL] reader = BadLazyReader ( lazy = True ) [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] deque ( reader . read ( [string] ) , maxlen = [number] ) [EOL] [EOL] [EOL] class BadReaderReadsNothing ( DatasetReader ) : [EOL] def _read ( self , file_path ) : [EOL] return [ ] [EOL] [EOL] def text_to_instance ( self , index ) : [comment] [EOL] return Instance ( { [string] : LabelField ( index , skip_indexing = True ) } ) [EOL] [EOL] [EOL] def test_config_error_when_reader_returns_no_instances ( ) : [EOL] reader = BadReaderReadsNothing ( ) [EOL] with pytest . raises ( ConfigurationError , match = [string] ) : [EOL] deque ( reader . read ( [string] ) , maxlen = [number] ) [EOL] [EOL] [EOL] class BadReaderForgetsToSetLazy ( DatasetReader ) : [EOL] def __init__ ( self ) : [EOL] pass [EOL] [EOL] def _read ( self , file_path ) : [EOL] for i in range ( [number] ) : [EOL] yield self . text_to_instance ( i ) [EOL] [EOL] def text_to_instance ( self , index ) : [comment] [EOL] return Instance ( { [string] : LabelField ( index , skip_indexing = True ) } ) [EOL] [EOL] [EOL] def warning_when_reader_has_no_lazy_set ( ) : [EOL] with pytest . warns ( UserWarning , match = [string] ) : [EOL] reader = BadReaderForgetsToSetLazy ( ) [EOL] reader . read ( [string] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.bool$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.bool$ 0 $builtins.bool$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Optional , Any , List , Iterable , Tuple [EOL] import allennlp [EOL] import typing [EOL] import builtins [EOL] from typing import Iterable , List [EOL] [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Token [EOL] from allennlp . data . tokenizers import PretrainedTransformerTokenizer [EOL] [EOL] [EOL] class TestPretrainedTransformerTokenizer ( AllenNlpTestCase ) : [EOL] def test_splits_roberta ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_splits_cased_bert ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_splits_uncased_bert ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] tokens = [ t . text for t in tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_splits_reformer_small ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] tokens = [ t . text for t in tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_token_idx_bert_uncased ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_idxs = [ None , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , None ] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] tokenized = tokenizer . tokenize ( sentence ) [EOL] tokens = [ t . text for t in tokenized ] [EOL] assert tokens == expected_tokens [EOL] idxs = [ t . idx for t in tokenized ] [EOL] assert idxs == expected_idxs [EOL] [EOL] def test_token_idx_bert_cased ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_idxs = [ None , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , None ] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] tokenized = tokenizer . tokenize ( sentence ) [EOL] tokens = [ t . text for t in tokenized ] [EOL] assert tokens == expected_tokens [EOL] idxs = [ t . idx for t in tokenized ] [EOL] assert idxs == expected_idxs [EOL] [EOL] def test_max_length ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] , max_length = [number] , add_special_tokens = False ) [EOL] tokens = tokenizer . tokenize ( [string] ) [EOL] assert len ( tokens ) == [number] [EOL] [EOL] def test_no_max_length ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] , max_length = None , add_special_tokens = False ) [EOL] [comment] [EOL] [comment] [EOL] tokens = tokenizer . tokenize ( [string] . join ( [ [string] ] * [number] ) ) [EOL] assert len ( tokens ) == [number] [EOL] [EOL] def test_token_idx_roberta ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_idxs = [ None , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , [number] , None ] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] tokenized = tokenizer . tokenize ( sentence ) [EOL] tokens = [ t . text for t in tokenized ] [EOL] assert tokens == expected_tokens [EOL] idxs = [ t . idx for t in tokenized ] [EOL] assert idxs == expected_idxs [EOL] [EOL] def test_token_idx_wikipedia ( self ) : [EOL] sentence = ( [string] [string] ) [EOL] for tokenizer_name in [ [string] , [string] , [string] ] : [EOL] tokenizer = PretrainedTransformerTokenizer ( tokenizer_name ) [EOL] tokenized = tokenizer . tokenize ( sentence ) [EOL] assert tokenized [ - [number] ] . text == [string] [EOL] assert tokenized [ - [number] ] . idx == len ( sentence ) - [number] [EOL] [EOL] def test_intra_word_tokenize ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] [EOL] sentence = [string] . split ( [string] ) [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_offsets = [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] tokens , offsets = tokenizer . intra_word_tokenize ( sentence ) [EOL] tokens = [ t . text for t in tokens ] [EOL] assert tokens == expected_tokens [EOL] assert offsets == expected_offsets [EOL] [EOL] [comment] [EOL] sentence_1 = [string] . split ( [string] ) [EOL] sentence_2 = [string] . split ( [string] ) [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_offsets_a = [ ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] expected_offsets_b = [ ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] tokens , offsets_a , offsets_b = tokenizer . intra_word_tokenize_sentence_pair ( sentence_1 , sentence_2 ) [EOL] tokens = [ t . text for t in tokens ] [EOL] assert tokens == expected_tokens [EOL] assert offsets_a == expected_offsets_a [EOL] assert offsets_b == expected_offsets_b [EOL] [EOL] def test_intra_word_tokenize_whitespaces ( self ) : [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] [EOL] sentence = [ [string] , [string] , [string] , [string] , [string] , [string] ] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] expected_offsets = [ ( [number] , [number] ) , None , ( [number] , [number] ) , ( [number] , [number] ) , None , ( [number] , [number] ) ] [EOL] tokens , offsets = tokenizer . intra_word_tokenize ( sentence ) [EOL] tokens = [ t . text for t in tokens ] [EOL] assert tokens == expected_tokens [EOL] assert offsets == expected_offsets [EOL] [EOL] def test_special_tokens_added ( self ) : [EOL] def get_token_ids ( tokens ) : [EOL] return [ t . text_id for t in tokens ] [EOL] [EOL] def get_type_ids ( tokens ) : [EOL] return [ t . type_id for t in tokens ] [EOL] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] assert get_token_ids ( tokenizer . sequence_pair_start_tokens ) == [ [number] ] [EOL] assert get_token_ids ( tokenizer . sequence_pair_mid_tokens ) == [ [number] ] [EOL] assert get_token_ids ( tokenizer . sequence_pair_end_tokens ) == [ [number] ] [EOL] assert get_token_ids ( tokenizer . single_sequence_start_tokens ) == [ [number] ] [EOL] assert get_token_ids ( tokenizer . single_sequence_end_tokens ) == [ [number] ] [EOL] [EOL] assert get_type_ids ( tokenizer . sequence_pair_start_tokens ) == [ [number] ] [EOL] assert tokenizer . sequence_pair_first_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . sequence_pair_mid_tokens ) == [ [number] ] [EOL] assert tokenizer . sequence_pair_second_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . sequence_pair_end_tokens ) == [ [number] ] [EOL] [EOL] assert get_type_ids ( tokenizer . single_sequence_start_tokens ) == [ [number] ] [EOL] assert tokenizer . single_sequence_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . single_sequence_end_tokens ) == [ [number] ] [EOL] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] assert get_token_ids ( tokenizer . sequence_pair_start_tokens ) == [ ] [EOL] assert get_token_ids ( tokenizer . sequence_pair_mid_tokens ) == [ [number] ] [EOL] assert get_token_ids ( tokenizer . sequence_pair_end_tokens ) == [ [number] , [number] ] [EOL] assert get_token_ids ( tokenizer . single_sequence_start_tokens ) == [ ] [EOL] assert get_token_ids ( tokenizer . single_sequence_end_tokens ) == [ [number] , [number] ] [EOL] [EOL] assert get_type_ids ( tokenizer . sequence_pair_start_tokens ) == [ ] [EOL] assert tokenizer . sequence_pair_first_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . sequence_pair_mid_tokens ) == [ [number] ] [EOL] assert tokenizer . sequence_pair_second_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . sequence_pair_end_tokens ) == [ [number] , [number] ] [EOL] [EOL] assert get_type_ids ( tokenizer . single_sequence_start_tokens ) == [ ] [EOL] assert tokenizer . single_sequence_token_type_id == [number] [EOL] assert get_type_ids ( tokenizer . single_sequence_end_tokens ) == [ [number] , [number] ] [EOL] [EOL] def test_tokenizer_kwargs_default ( self ) : [EOL] text = [string] [EOL] tokenizer = PretrainedTransformerTokenizer ( [string] ) [EOL] original_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokenized = [ token . text for token in tokenizer . tokenize ( text ) ] [EOL] assert tokenized == original_tokens [EOL] [EOL] def test_from_params_kwargs ( self ) : [EOL] PretrainedTransformerTokenizer . from_params ( Params ( { [string] : [string] , [string] : { [string] : [number] } } ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[typing.Optional[builtins.int]]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 0 0 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 0 0 $typing.List[typing.Optional[builtins.str]]$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.List[typing.Optional[typing.Tuple[builtins.int,builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.Iterable[allennlp.data.Token]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.Token]$ 0 0 0 0 $typing.List[builtins.int]$ 0 $typing.Iterable[allennlp.data.Token]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Iterable[allennlp.data.Token]$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 $builtins.str$ 0 0 0 $allennlp.data.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer$ 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . tokenizers . letters_digits_tokenizer import LettersDigitsTokenizer [EOL] from allennlp . data . tokenizers . token import Token [EOL] [EOL] [EOL] class TestLettersDigitsTokenizer ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . word_tokenizer = LettersDigitsTokenizer ( ) [EOL] [EOL] def test_tokenize_handles_complex_punctuation ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_tokenize_handles_unicode_letters ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ Token ( [string] , [number] ) , Token ( [string] , [number] ) , Token ( [string] , [number] ) , Token ( [string] , [number] ) , ] [EOL] tokens = self . word_tokenizer . tokenize ( sentence ) [EOL] assert [ t . text for t in tokens ] == [ t . text for t in expected_tokens ] [EOL] assert [ t . idx for t in tokens ] == [ t . idx for t in expected_tokens ] [EOL] [EOL] def test_tokenize_handles_splits_all_punctuation ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] import spacy [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . tokenizers . token import Token [EOL] from allennlp . data . tokenizers . spacy_tokenizer import SpacyTokenizer [EOL] [EOL] [EOL] class TestSpacyTokenizer ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . word_tokenizer = SpacyTokenizer ( ) [EOL] [EOL] def test_tokenize_handles_complex_punctuation ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = self . word_tokenizer . tokenize ( sentence ) [EOL] token_text = [ t . text for t in tokens ] [EOL] assert token_text == expected_tokens [EOL] for token in tokens : [EOL] start = token . idx [EOL] end = start + len ( token . text ) [EOL] assert sentence [ start : end ] == token . text [EOL] [EOL] def test_tokenize_handles_contraction ( self ) : [EOL] [comment] [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_tokenize_handles_multiple_contraction ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_tokenize_handles_final_apostrophe ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_tokenize_removes_whitespace_tokens ( self ) : [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_tokenize_handles_special_cases ( self ) : [EOL] [comment] [EOL] sentence = [string] [EOL] expected_tokens = [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , ] [EOL] tokens = [ t . text for t in self . word_tokenizer . tokenize ( sentence ) ] [EOL] assert tokens == expected_tokens [EOL] [EOL] def test_batch_tokenization ( self ) : [EOL] sentences = [ [string] , [string] , [string] [string] , ] [EOL] batch_split = self . word_tokenizer . batch_tokenize ( sentences ) [EOL] separately_split = [ self . word_tokenizer . tokenize ( sentence ) for sentence in sentences ] [EOL] assert len ( batch_split ) == len ( separately_split ) [EOL] for batch_sentence , separate_sentence in zip ( batch_split , separately_split ) : [EOL] assert len ( batch_sentence ) == len ( separate_sentence ) [EOL] for batch_word , separate_word in zip ( batch_sentence , separate_sentence ) : [EOL] assert batch_word . text == separate_word . text [EOL] [EOL] def test_keep_spacy_tokens ( self ) : [EOL] word_tokenizer = SpacyTokenizer ( ) [EOL] sentence = [string] [EOL] tokens = word_tokenizer . tokenize ( sentence ) [EOL] assert tokens [EOL] assert all ( isinstance ( token , Token ) for token in tokens ) [EOL] [EOL] word_tokenizer = SpacyTokenizer ( keep_spacy_tokens = True ) [EOL] sentence = [string] [EOL] tokens = word_tokenizer . tokenize ( sentence ) [EOL] assert tokens [EOL] assert all ( isinstance ( token , spacy . tokens . Token ) for token in tokens ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $builtins.str$ 0 0 $typing.list$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.list$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 $typing.list$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 $allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer$ 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0
	0
from allennlp . data . fields import Field [EOL] [EOL] [EOL] def test_eq_with_inheritance ( ) : [EOL] class SubField ( Field ) : [EOL] [EOL] __slots__ = [ [string] ] [EOL] [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] class SubSubField ( SubField ) : [EOL] [EOL] __slots__ = [ [string] ] [EOL] [EOL] def __init__ ( self , a , b ) : [EOL] super ( ) . __init__ ( a ) [EOL] self . b = b [EOL] [EOL] class SubSubSubField ( SubSubField ) : [EOL] [EOL] __slots__ = [ [string] ] [EOL] [EOL] def __init__ ( self , a , b , c ) : [EOL] super ( ) . __init__ ( a , b ) [EOL] self . c = c [EOL] [EOL] assert SubField ( [number] ) == SubField ( [number] ) [EOL] assert SubField ( [number] ) != SubField ( [number] ) [EOL] [EOL] assert SubSubField ( [number] , [number] ) == SubSubField ( [number] , [number] ) [EOL] assert SubSubField ( [number] , [number] ) != SubSubField ( [number] , [number] ) [EOL] assert SubSubField ( [number] , [number] ) != SubSubField ( [number] , [number] ) [EOL] [EOL] assert SubSubSubField ( [number] , [number] , [number] ) == SubSubSubField ( [number] , [number] , [number] ) [EOL] assert SubSubSubField ( [number] , [number] , [number] ) != SubSubSubField ( [number] , [number] , [number] ) [EOL] [EOL] [EOL] def test_eq_with_inheritance_for_non_slots_field ( ) : [EOL] class SubField ( Field ) : [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] assert SubField ( [number] ) == SubField ( [number] ) [EOL] assert SubField ( [number] ) != SubField ( [number] ) [EOL] [EOL] [EOL] def test_eq_with_inheritance_for_mixed_field ( ) : [EOL] class SubField ( Field ) : [EOL] [EOL] __slots__ = [ [string] ] [EOL] [EOL] def __init__ ( self , a ) : [EOL] self . a = a [EOL] [EOL] class SubSubField ( SubField ) : [EOL] def __init__ ( self , a , b ) : [EOL] super ( ) . __init__ ( a ) [EOL] self . b = b [EOL] [EOL] assert SubField ( [number] ) == SubField ( [number] ) [EOL] assert SubField ( [number] ) != SubField ( [number] ) [EOL] [EOL] assert SubSubField ( [number] , [number] ) == SubSubField ( [number] , [number] ) [EOL] assert SubSubField ( [number] , [number] ) != SubSubField ( [number] , [number] ) [EOL] assert SubSubField ( [number] , [number] ) != SubSubField ( [number] , [number] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List , Tuple [EOL] import typing [EOL] import pytest [EOL] import numpy [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data . fields import AdjacencyField , TextField [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] from allennlp . data import Vocabulary , Token [EOL] [EOL] [EOL] class TestAdjacencyField ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . text = TextField ( [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] ] ] , { [string] : SingleIdTokenIndexer ( [string] ) } , ) [EOL] [EOL] def test_adjacency_field_can_index_with_vocab ( self ) : [EOL] vocab = Vocabulary ( ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] vocab . add_token_to_namespace ( [string] , namespace = [string] ) [EOL] [EOL] labels = [ [string] , [string] ] [EOL] indices = [ ( [number] , [number] ) , ( [number] , [number] ) ] [EOL] adjacency_field = AdjacencyField ( indices , self . text , labels ) [EOL] adjacency_field . index ( vocab ) [EOL] tensor = adjacency_field . as_tensor ( adjacency_field . get_padding_lengths ( ) ) [EOL] numpy . testing . assert_equal ( tensor . numpy ( ) , numpy . array ( [ [ - [number] , [number] , - [number] , - [number] , - [number] ] , [ - [number] , - [number] , - [number] , - [number] , - [number] ] , [ - [number] , [number] , - [number] , - [number] , - [number] ] , [ - [number] , - [number] , - [number] , - [number] , - [number] ] , [ - [number] , - [number] , - [number] , - [number] , - [number] ] , ] ) , ) [EOL] [EOL] def test_adjacency_field_raises_with_out_of_bounds_indices ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = AdjacencyField ( [ ( [number] , [number] ) ] , self . text ) [EOL] [EOL] def test_adjacency_field_raises_with_mismatching_labels_for_indices ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = AdjacencyField ( [ ( [number] , [number] ) , ( [number] , [number] ) ] , self . text , [ [string] ] ) [EOL] [EOL] def test_adjacency_field_raises_with_duplicate_indices ( self ) : [EOL] with pytest . raises ( ConfigurationError ) : [EOL] _ = AdjacencyField ( [ ( [number] , [number] ) , ( [number] , [number] ) ] , self . text , [ [string] ] ) [EOL] [EOL] def test_adjacency_field_empty_field_works ( self ) : [EOL] field = AdjacencyField ( [ ( [number] , [number] ) ] , self . text ) [EOL] empty_field = field . empty_field ( ) [EOL] assert empty_field . indices == [ ] [EOL] [EOL] def test_printing_doesnt_crash ( self ) : [EOL] adjacency_field = AdjacencyField ( [ ( [number] , [number] ) ] , self . text , [ [string] ] ) [EOL] print ( adjacency_field ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.List[typing.Tuple[builtins.int,builtins.int]]$ 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0
from typing import Any , List [EOL] import typing [EOL] import pytest [EOL] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . data . fields import FlagField [EOL] [EOL] [EOL] class TestFlagField ( AllenNlpTestCase ) : [EOL] def test_get_padding_lengths_returns_nothing ( self ) : [EOL] flag_field = FlagField ( True ) [EOL] assert flag_field . get_padding_lengths ( ) == { } [EOL] [EOL] def test_as_tensor_just_returns_value ( self ) : [EOL] for value in [ True , [number] , [string] ] : [EOL] assert FlagField ( value ) . as_tensor ( { } ) == value [EOL] [EOL] def test_printing_doesnt_crash ( self ) : [EOL] flag = FlagField ( True ) [EOL] print ( flag ) [EOL] [EOL] def test_batch_tensors_returns_single_value ( self ) : [EOL] value = True [EOL] fields = [ FlagField ( value ) for _ in range ( [number] ) ] [EOL] values = [ field . as_tensor ( { } ) for field in fields ] [EOL] batched_value = fields [ [number] ] . batch_tensors ( values ) [EOL] assert batched_value == value [EOL] [EOL] def test_batch_tensors_crashes_with_non_uniform_values ( self ) : [EOL] field = FlagField ( True ) [EOL] with pytest . raises ( ValueError ) : [EOL] field . batch_tensors ( [ True , False , True ] ) [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] field . batch_tensors ( [ [number] , [number] , [number] , [number] ] ) [EOL] [EOL] with pytest . raises ( ValueError ) : [EOL] field . batch_tensors ( [ [string] , [string] , [string] ] ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Any , List , Union [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common import Params [EOL] from allennlp . data import Instance , Token [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . samplers import BucketBatchSampler [EOL] from allennlp . data . dataset_readers . dataset_reader import AllennlpDataset [EOL] from allennlp . data . dataloader import PyTorchDataLoader [EOL] [EOL] from . sampler_test import SamplerTest [EOL] [EOL] [EOL] class TestBucketSampler ( SamplerTest ) : [EOL] def test_create_batches_groups_correctly ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = BucketBatchSampler ( dataset , batch_size = [number] , padding_noise = [number] , sorting_keys = [ [string] ] ) [EOL] [EOL] grouped_instances = [ ] [EOL] for indices in sampler : [EOL] grouped_instances . append ( [ self . instances [ idx ] for idx in indices ] ) [EOL] expected_groups = [ [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] ] , ] [EOL] for group in grouped_instances : [EOL] assert group in expected_groups [EOL] expected_groups . remove ( group ) [EOL] assert expected_groups == [ ] [EOL] [EOL] def test_guess_sorting_key_picks_the_longest_key ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = BucketBatchSampler ( dataset , batch_size = [number] , padding_noise = [number] ) [EOL] instances = [ ] [EOL] short_tokens = [ Token ( t ) for t in [ [string] , [string] , [string] , [string] ] ] [EOL] long_tokens = [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] assert sampler . sorting_keys is None [EOL] sampler . _guess_sorting_keys ( instances ) [EOL] assert sampler . sorting_keys == [ [string] ] [EOL] [EOL] def test_from_params ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , self . vocab ) [EOL] params = Params ( { } ) [EOL] [EOL] sorting_keys = [ [string] , [string] ] [EOL] params [ [string] ] = sorting_keys [EOL] params [ [string] ] = [number] [EOL] sampler = BucketBatchSampler . from_params ( params = params , data_source = dataset ) [EOL] [EOL] assert sampler . sorting_keys == sorting_keys [EOL] assert sampler . padding_noise == [number] [EOL] assert sampler . batch_size == [number] [EOL] [EOL] params = Params ( { [string] : sorting_keys , [string] : [number] , [string] : [number] , [string] : True , } ) [EOL] [EOL] sampler = BucketBatchSampler . from_params ( params = params , data_source = dataset ) [EOL] assert sampler . sorting_keys == sorting_keys [EOL] assert sampler . padding_noise == [number] [EOL] assert sampler . batch_size == [number] [EOL] assert sampler . drop_last [EOL] [EOL] def test_drop_last_works ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = BucketBatchSampler ( dataset , batch_size = [number] , padding_noise = [number] , sorting_keys = [ [string] ] , drop_last = True , ) [EOL] [comment] [EOL] [comment] [EOL] dataloader = PyTorchDataLoader ( dataset , batch_sampler = sampler , collate_fn = lambda x : Batch ( x ) ) [EOL] batches = [ batch for batch in iter ( dataloader ) ] [EOL] stats = self . get_batches_stats ( batches ) [EOL] [EOL] [comment] [EOL] assert all ( batch_len == [number] for batch_len in stats [ [string] ] ) [EOL] [EOL] [comment] [EOL] assert stats [ [string] ] == len ( self . instances ) - [number] [EOL] [EOL] def test_batch_count ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = BucketBatchSampler ( dataset , batch_size = [number] , padding_noise = [number] , sorting_keys = [ [string] ] ) [EOL] [comment] [EOL] [comment] [EOL] dataloader = PyTorchDataLoader ( dataset , batch_sampler = sampler , collate_fn = lambda x : Batch ( x ) ) [EOL] [EOL] assert len ( dataloader ) == [number] [EOL] [EOL] def test_batch_count_with_drop_last ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = BucketBatchSampler ( dataset , batch_size = [number] , padding_noise = [number] , sorting_keys = [ [string] ] , drop_last = True , ) [EOL] [comment] [EOL] [comment] [EOL] dataloader = PyTorchDataLoader ( dataset , batch_sampler = sampler , collate_fn = lambda x : Batch ( x ) ) [EOL] [EOL] assert len ( dataloader ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.int],builtins.int]]$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[typing.List[builtins.int],builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.bucket_batch_sampler.BucketBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0
	0
from typing import Any , Dict , List , Union , Iterable , Tuple [EOL] import builtins [EOL] import allennlp [EOL] import typing [EOL] from typing import List , Iterable , Dict , Union [EOL] [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . data import Vocabulary , Instance , Token , Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . token_indexers import SingleIdTokenIndexer [EOL] [EOL] [EOL] class LazyIterable : [EOL] def __init__ ( self , instances ) : [EOL] self . _instances = instances [EOL] [EOL] def __iter__ ( self ) : [EOL] return ( instance for instance in self . _instances ) [EOL] [EOL] [EOL] class SamplerTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . token_indexers = { [string] : SingleIdTokenIndexer ( ) } [EOL] self . vocab = Vocabulary ( ) [EOL] self . this_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . is_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . a_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . sentence_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . another_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . yet_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . very_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] self . long_index = self . vocab . add_token_to_namespace ( [string] ) [EOL] instances = [ self . create_instance ( [ [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] ] ) , self . create_instance ( [ [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ) , self . create_instance ( [ [string] ] ) , ] [EOL] [EOL] self . instances = instances [EOL] self . lazy_instances = LazyIterable ( instances ) [EOL] [EOL] def create_instance ( self , str_tokens ) : [EOL] tokens = [ Token ( t ) for t in str_tokens ] [EOL] instance = Instance ( { [string] : TextField ( tokens , self . token_indexers ) } ) [EOL] return instance [EOL] [EOL] def create_instances_from_token_counts ( self , token_counts ) : [EOL] return [ self . create_instance ( [ [string] ] * count ) for count in token_counts ] [EOL] [EOL] def get_batches_stats ( self , batches ) : [EOL] grouped_instances = [ batch . instances for batch in batches ] [EOL] group_lengths = [ len ( group ) for group in grouped_instances ] [EOL] [EOL] sample_sizes = [ ] [EOL] for batch in batches : [EOL] batch_sequence_length = max ( instance . get_padding_lengths ( ) [ [string] ] [ [string] ] for instance in batch . instances ) [EOL] sample_sizes . append ( batch_sequence_length * len ( batch . instances ) ) [EOL] [EOL] return { [string] : group_lengths , [string] : sum ( group_lengths ) , [string] : sample_sizes , } [EOL] [EOL] def assert_instances_are_correct ( self , candidate_instances ) : [EOL] [comment] [EOL] [EOL] candidate_instances = [ tuple ( w for w in instance if w != [number] ) for instance in candidate_instances ] [EOL] expected_instances = [ tuple ( instance . fields [ [string] ] . _indexed_tokens [ [string] ] [ [string] ] ) for instance in self . instances ] [EOL] assert set ( candidate_instances ) == set ( expected_instances ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Union[builtins.int,typing.List[builtins.int]]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 0 $typing.List[builtins.int]$ 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0 0 0 $typing.List[typing.Tuple[typing.Any,...]]$ 0 0
from typing import Any , List [EOL] import allennlp [EOL] import typing [EOL] from allennlp . common import Params [EOL] from allennlp . data import Instance , Token [EOL] from allennlp . data . batch import Batch [EOL] from allennlp . data . fields import TextField [EOL] from allennlp . data . samplers import MaxTokensBatchSampler [EOL] from allennlp . data . dataset_readers . dataset_reader import AllennlpDataset [EOL] from allennlp . data . dataloader import PyTorchDataLoader [EOL] [EOL] from . sampler_test import SamplerTest [EOL] [EOL] [EOL] class TestMaxTokensSampler ( SamplerTest ) : [EOL] def test_create_batches_groups_correctly ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = MaxTokensBatchSampler ( dataset , max_tokens = [number] , padding_noise = [number] , sorting_keys = [ [string] ] ) [EOL] [EOL] grouped_instances = [ ] [EOL] for indices in sampler : [EOL] grouped_instances . append ( [ self . instances [ idx ] for idx in indices ] ) [EOL] expected_groups = [ [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] , self . instances [ [number] ] ] , [ self . instances [ [number] ] ] , ] [EOL] for group in grouped_instances : [EOL] assert group in expected_groups [EOL] expected_groups . remove ( group ) [EOL] assert expected_groups == [ ] [EOL] [EOL] def test_guess_sorting_key_picks_the_longest_key ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = MaxTokensBatchSampler ( dataset , max_tokens = [number] , padding_noise = [number] ) [EOL] instances = [ ] [EOL] short_tokens = [ Token ( t ) for t in [ [string] , [string] , [string] , [string] ] ] [EOL] long_tokens = [ Token ( t ) for t in [ [string] , [string] , [string] , [string] , [string] , [string] , [string] ] ] [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] instances . append ( Instance ( { [string] : TextField ( short_tokens , self . token_indexers ) , [string] : TextField ( long_tokens , self . token_indexers ) , } ) ) [EOL] assert sampler . sorting_keys is None [EOL] sampler . _guess_sorting_keys ( instances ) [EOL] assert sampler . sorting_keys == [ [string] ] [EOL] [EOL] def test_from_params ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , self . vocab ) [EOL] params = Params ( { } ) [EOL] [EOL] sorting_keys = [ [string] , [string] ] [EOL] params [ [string] ] = sorting_keys [EOL] params [ [string] ] = [number] [EOL] sampler = MaxTokensBatchSampler . from_params ( params = params , data_source = dataset ) [EOL] [EOL] assert sampler . sorting_keys == sorting_keys [EOL] assert sampler . padding_noise == [number] [EOL] assert sampler . max_tokens == [number] [EOL] [EOL] params = Params ( { [string] : sorting_keys , [string] : [number] , [string] : [number] } ) [EOL] [EOL] sampler = MaxTokensBatchSampler . from_params ( params = params , data_source = dataset ) [EOL] assert sampler . sorting_keys == sorting_keys [EOL] assert sampler . padding_noise == [number] [EOL] assert sampler . max_tokens == [number] [EOL] [EOL] def test_batch_count ( self ) : [EOL] dataset = AllennlpDataset ( self . instances , vocab = self . vocab ) [EOL] sampler = MaxTokensBatchSampler ( dataset , max_tokens = [number] , padding_noise = [number] , sorting_keys = [ [string] ] ) [EOL] [comment] [EOL] [comment] [EOL] dataloader = PyTorchDataLoader ( dataset , batch_sampler = sampler , collate_fn = lambda x : Batch ( x ) ) [EOL] [EOL] assert len ( dataloader ) == [number] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 $typing.List[typing.List[typing.Any]]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 $typing.List[allennlp.data.tokenizers.token.Token]$ 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 $typing.List[typing.Any]$ 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.List[builtins.str]$ 0 $typing.Any$ 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $allennlp.data.dataset_readers.dataset_reader.AllennlpDataset$ 0 0 0 $allennlp.data.samplers.max_tokens_batch_sampler.MaxTokensBatchSampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0
from typing import Dict , Any [EOL] import tests [EOL] import typing [EOL] import torch [EOL] import pytest [EOL] [EOL] from allennlp . common . testing . test_case import AllenNlpTestCase [EOL] from allennlp . models import load_archive , Model [EOL] from allennlp . nn . regularizers import RegularizerApplicator [EOL] [EOL] [EOL] class TestModel ( AllenNlpTestCase ) : [EOL] def test_extend_embedder_vocab ( self ) : [EOL] model_archive = str ( self . FIXTURES_ROOT / [string] / [string] / [string] ) [EOL] trained_model = load_archive ( model_archive ) . model [EOL] [EOL] original_weight = trained_model . _text_field_embedder . token_embedder_tokens . weight [EOL] assert tuple ( original_weight . shape ) == ( [number] , [number] ) [EOL] [EOL] counter = { [string] : { [string] : [number] } } [EOL] trained_model . vocab . _extend ( counter ) [EOL] trained_model . extend_embedder_vocab ( ) [EOL] [EOL] extended_weight = trained_model . _text_field_embedder . token_embedder_tokens . weight [EOL] assert tuple ( extended_weight . shape ) == ( [number] , [number] ) [EOL] [EOL] assert torch . all ( original_weight == extended_weight [ : [number] , : ] ) [EOL] [EOL] def test_get_regularization_penalty ( self ) : [EOL] class FakeModel ( Model ) : [EOL] def forward ( self , ** kwargs ) : [EOL] return { } [EOL] [EOL] class FakeRegularizerApplicator ( RegularizerApplicator ) : [EOL] def __call__ ( self , module ) : [EOL] return [number] [EOL] [EOL] with pytest . raises ( RuntimeError ) : [EOL] regularizer = FakeRegularizerApplicator ( ) [EOL] model = FakeModel ( None , regularizer ) [EOL] model . get_regularization_penalty ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,typing.Dict[builtins.str,builtins.int]]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $tests.models.model_test.TestModel.test_get_regularization_penalty.FakeRegularizerApplicator$ 0 0 0 0 0 $tests.models.model_test.TestModel.test_get_regularization_penalty.FakeModel$ 0 0 0 0 0 $tests.models.model_test.TestModel.test_get_regularization_penalty.FakeRegularizerApplicator$ 0 0 $tests.models.model_test.TestModel.test_get_regularization_penalty.FakeModel$ 0 0 0 0 0
	0
from typing import Dict [EOL] import typing [EOL] import json [EOL] import pytest [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] [EOL] [EOL] class ModelWithIncorrectValidationMetricTest ( ModelTestCase ) : [EOL] [docstring] [EOL] [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] , ) [EOL] [EOL] def test_01_test_validation_metric_does_not_exist ( self ) : [EOL] overrides = { [string] : [number] } [EOL] pytest . raises ( AssertionError , self . ensure_model_can_train_save_and_load , self . param_file , metric_to_check = [string] , metric_terminal_value = [number] , overrides = json . dumps ( overrides ) , ) [EOL] [EOL] def test_02a_test_validation_metric_terminal_value_not_set ( self ) : [EOL] pytest . raises ( AssertionError , self . ensure_model_can_train_save_and_load , self . param_file , metric_to_check = [string] , metric_terminal_value = None , ) [EOL] [EOL] def test_02b_test_validation_metric_terminal_value_not_met ( self ) : [EOL] pytest . raises ( AssertionError , self . ensure_model_can_train_save_and_load , self . param_file , metric_to_check = [string] , metric_terminal_value = [number] , ) [EOL] [EOL] def test_03_test_validation_metric_exists_and_its_terminal_value_is_met ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file , metric_to_check = [string] , metric_terminal_value = [number] , ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 $typing.Dict[builtins.str,builtins.int]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Dict , Set , Any [EOL] import typing [EOL] import copy [EOL] [EOL] import torch [EOL] [EOL] from allennlp . commands . train import train_model [EOL] from allennlp . common import Params [EOL] from allennlp . common . testing import AllenNlpTestCase [EOL] from allennlp . models . archival import archive_model , load_archive [EOL] [EOL] [EOL] def assert_models_equal ( model , model2 ) : [EOL] [comment] [EOL] keys = set ( model . state_dict ( ) . keys ( ) ) [EOL] keys2 = set ( model2 . state_dict ( ) . keys ( ) ) [EOL] [EOL] assert keys == keys2 [EOL] [EOL] for key in keys : [EOL] assert torch . equal ( model . state_dict ( ) [ key ] , model2 . state_dict ( ) [ key ] ) [EOL] [EOL] [comment] [EOL] vocab = model . vocab [EOL] vocab2 = model2 . vocab [EOL] [EOL] assert vocab . _token_to_index == vocab2 . _token_to_index [EOL] assert vocab . _index_to_token == vocab2 . _index_to_token [EOL] [EOL] [EOL] class ArchivalTest ( AllenNlpTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] [EOL] self . params = Params ( { [string] : { [string] : [string] , [string] : { [string] : { [string] : { [string] : [string] , [string] : [number] } } } , [string] : { [string] : [string] , [string] : [number] , [string] : [number] , [string] : [number] } , } , [string] : { [string] : [string] } , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : str ( self . FIXTURES_ROOT / [string] / [string] ) , [string] : { [string] : [number] } , [string] : { [string] : [number] , [string] : [string] } , } ) [EOL] [EOL] def test_archiving ( self ) : [EOL] [comment] [EOL] params_copy = copy . deepcopy ( self . params . as_dict ( ) ) [EOL] [EOL] [comment] [EOL] serialization_dir = self . TEST_DIR / [string] [EOL] model = train_model ( self . params , serialization_dir = serialization_dir ) [EOL] [EOL] archive_path = serialization_dir / [string] [EOL] [EOL] [comment] [EOL] archive = load_archive ( archive_path ) [EOL] model2 = archive . model [EOL] [EOL] assert_models_equal ( model , model2 ) [EOL] [EOL] [comment] [EOL] params2 = archive . config [EOL] assert params2 . as_dict ( ) == params_copy [EOL] [EOL] def test_archive_model_uses_archive_path ( self ) : [EOL] [EOL] serialization_dir = self . TEST_DIR / [string] [EOL] [comment] [EOL] train_model ( self . params , serialization_dir = serialization_dir ) [EOL] [comment] [EOL] archive_model ( serialization_dir = serialization_dir , archive_path = serialization_dir / [string] ) [EOL] archive = load_archive ( serialization_dir / [string] ) [EOL] assert archive [EOL] [EOL] def test_loading_serialization_directory ( self ) : [EOL] [comment] [EOL] params_copy = copy . deepcopy ( self . params . as_dict ( ) ) [EOL] [EOL] [comment] [EOL] serialization_dir = self . TEST_DIR / [string] [EOL] model = train_model ( self . params , serialization_dir = serialization_dir ) [EOL] [EOL] [comment] [EOL] archive = load_archive ( serialization_dir ) [EOL] model2 = archive . model [EOL] [EOL] assert_models_equal ( model , model2 ) [EOL] [EOL] [comment] [EOL] params2 = archive . config [EOL] assert params2 . as_dict ( ) == params_copy [EOL] [EOL] def test_can_load_from_archive_model ( self ) : [EOL] serialization_dir = self . FIXTURES_ROOT / [string] / [string] [EOL] archive_path = serialization_dir / [string] [EOL] model = load_archive ( archive_path ) . model [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] base_model_path = self . FIXTURES_ROOT / [string] / [string] / [string] [EOL] base_model = load_archive ( base_model_path ) . model [EOL] base_model_params = dict ( base_model . named_parameters ( ) ) [EOL] for name , parameters in model . named_parameters ( ) : [EOL] if parameters . size ( ) == base_model_params [ name ] . size ( ) : [EOL] assert not ( parameters == base_model_params [ name ] ) . all ( ) [EOL] else : [EOL] [comment] [EOL] [comment] [EOL] pass [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Dict[typing.Any,typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] import numpy [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] [EOL] [EOL] class TestBasicClassifier ( ModelTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] / [string] , ) [EOL] [EOL] def test_forward_pass_runs_correctly ( self ) : [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] output_dict = self . model . make_output_human_readable ( output_dict ) [EOL] assert [string] in output_dict . keys ( ) [EOL] probs = output_dict [ [string] ] [ [number] ] . data . numpy ( ) [EOL] numpy . testing . assert_almost_equal ( numpy . sum ( probs , - [number] ) , numpy . array ( [ [number] ] ) ) [EOL] [EOL] def test_seq2vec_clf_can_train_save_and_load ( self ) : [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] / [string] , ) [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] def test_seq2seq_clf_can_train_save_and_load ( self ) : [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] / [string] , ) [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import allennlp [EOL] import typing [EOL] from flaky import flaky [EOL] import numpy [EOL] import pytest [EOL] import torch [EOL] [EOL] from allennlp . common . testing import ModelTestCase [EOL] from allennlp . common . checks import ConfigurationError [EOL] from allennlp . common . params import Params [EOL] from allennlp . data . dataset_readers import DatasetReader [EOL] from allennlp . data import DataLoader , PyTorchDataLoader [EOL] from allennlp . models import Model [EOL] from allennlp . training import GradientDescentTrainer , Trainer [EOL] [EOL] [EOL] class SimpleTaggerTest ( ModelTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] , ) [EOL] [EOL] def test_simple_tagger_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_forward_pass_runs_correctly ( self ) : [EOL] training_tensors = self . dataset . as_tensor_dict ( ) [EOL] output_dict = self . model ( ** training_tensors ) [EOL] output_dict = self . model . make_output_human_readable ( output_dict ) [EOL] class_probs = output_dict [ [string] ] [ [number] ] . data . numpy ( ) [EOL] numpy . testing . assert_almost_equal ( numpy . sum ( class_probs , - [number] ) , numpy . array ( [ [number] , [number] , [number] , [number] ] ) ) [EOL] [EOL] def test_forward_on_instances_ignores_loss_key_when_batched ( self ) : [EOL] batch_outputs = self . model . forward_on_instances ( self . dataset . instances ) [EOL] for output in batch_outputs : [EOL] assert [string] not in output . keys ( ) [EOL] [EOL] [comment] [EOL] single_output = self . model . forward_on_instance ( self . dataset . instances [ [number] ] ) [EOL] assert [string] in single_output . keys ( ) [EOL] [EOL] def test_mismatching_dimensions_throws_configuration_error ( self ) : [EOL] params = Params . from_file ( self . param_file ) [EOL] [comment] [EOL] [comment] [EOL] params [ [string] ] [ [string] ] [ [string] ] = [number] [EOL] with pytest . raises ( ConfigurationError ) : [EOL] Model . from_params ( vocab = self . vocab , params = params . pop ( [string] ) ) [EOL] [EOL] def test_regularization ( self ) : [EOL] penalty = self . model . get_regularization_penalty ( ) [EOL] assert penalty is None [EOL] [EOL] data_loader = PyTorchDataLoader ( self . instances , batch_size = [number] ) [EOL] trainer = GradientDescentTrainer ( self . model , None , data_loader ) [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] training_batch = next ( iter ( data_loader ) ) [EOL] validation_batch = next ( iter ( data_loader ) ) [EOL] [EOL] training_loss = trainer . batch_outputs ( training_batch , for_training = True ) [ [string] ] . item ( ) [EOL] validation_loss = trainer . batch_outputs ( validation_batch , for_training = False ) [ [string] ] . item ( ) [EOL] [EOL] [comment] [EOL] numpy . testing . assert_almost_equal ( training_loss , validation_loss ) [EOL] [EOL] [EOL] class SimpleTaggerSpanF1Test ( ModelTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] self . set_up_model ( self . FIXTURES_ROOT / [string] / [string] , self . FIXTURES_ROOT / [string] / [string] , ) [EOL] [EOL] def test_simple_tagger_can_train_save_and_load ( self ) : [EOL] self . ensure_model_can_train_save_and_load ( self . param_file ) [EOL] [EOL] @ flaky def test_batch_predictions_are_consistent ( self ) : [EOL] self . ensure_batch_predictions_are_consistent ( ) [EOL] [EOL] def test_simple_tagger_can_enable_span_f1 ( self ) : [EOL] assert self . model . calculate_span_f1 and self . model . _f1_metric is not None [EOL] [EOL] [EOL] class SimpleTaggerRegularizationTest ( ModelTestCase ) : [EOL] def setup_method ( self ) : [EOL] super ( ) . setup_method ( ) [EOL] param_file = self . FIXTURES_ROOT / [string] / [string] [EOL] self . set_up_model ( param_file , self . FIXTURES_ROOT / [string] / [string] ) [EOL] params = Params . from_file ( param_file ) [EOL] self . reader = DatasetReader . from_params ( params [ [string] ] ) [EOL] self . data_loader = DataLoader . from_params ( dataset = self . instances , params = params [ [string] ] ) [EOL] self . trainer = Trainer . from_params ( model = self . model , data_loader = self . data_loader , serialization_dir = self . TEST_DIR , params = params . get ( [string] ) , ) [EOL] [EOL] def test_regularization ( self ) : [EOL] penalty = self . model . get_regularization_penalty ( ) . data [EOL] assert ( penalty > [number] ) . all ( ) [EOL] [EOL] penalty2 = [number] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] for name , parameter in self . model . named_parameters ( ) : [EOL] if name . endswith ( [string] ) : [EOL] weight_penalty = [number] * torch . sum ( torch . pow ( parameter , [number] ) ) [EOL] penalty2 += weight_penalty [EOL] elif name . endswith ( [string] ) : [EOL] bias_penalty = [number] * torch . sum ( torch . abs ( parameter ) ) [EOL] penalty2 += bias_penalty [EOL] [EOL] assert ( penalty == penalty2 . data ) . all ( ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] training_batch = next ( iter ( self . data_loader ) ) [EOL] validation_batch = next ( iter ( self . data_loader ) ) [EOL] [EOL] training_batch_outputs = self . trainer . batch_outputs ( training_batch , for_training = True ) [EOL] training_loss = training_batch_outputs [ [string] ] . data [EOL] [EOL] assert ( penalty == training_batch_outputs [ [string] ] ) . all ( ) [EOL] [EOL] validation_loss = self . trainer . batch_outputs ( validation_batch , for_training = False ) [ [string] ] . data [EOL] [EOL] [comment] [EOL] assert ( training_loss != validation_loss ) . all ( ) [EOL] [EOL] [comment] [EOL] penalized = validation_loss + penalty [EOL] assert ( training_loss == penalized ) . all ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 0 0 0 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 0 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 $typing.Any$ 0 0 0 0 0 $allennlp.data.dataloader.PyTorchDataLoader$ 0 0 0 0 $typing.Any$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $allennlp.training.trainer.GradientDescentTrainer$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $builtins.int$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0