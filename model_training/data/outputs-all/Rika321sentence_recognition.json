from django . db import models [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0
[EOL] [comment] [EOL] [EOL] from typing import Iterator , Any , List [EOL] import typing [EOL] import io [EOL] import random [EOL] import text_classifier [EOL] import tarfile [EOL] from __future__ import division [EOL] from __future__ import print_function [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] import sys [EOL] [EOL] [EOL] [comment] [EOL] if sys . version_info . major > [number] : [EOL] [EOL] def xrange ( * args , ** kwargs ) : [EOL] return iter ( range ( * args , ** kwargs ) ) [EOL] [EOL] def unicode ( * args , ** kwargs ) : [EOL] return str ( * args , ** kwargs ) [EOL] [EOL] [EOL] def textToTokens ( text ) : [EOL] [docstring] [EOL] corpus = [ ] [EOL] sents = text . split ( [string] ) [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] count_vect = CountVectorizer ( ) [EOL] count_vect . fit ( sents ) [EOL] tokenizer = count_vect . build_tokenizer ( ) [EOL] for s in sents : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] corpus . append ( toks ) [EOL] return corpus [EOL] [EOL] def file_splitter ( filename , seed = [number] , train_prop = [number] , dev_prop = [number] , test_prop = [number] ) : [EOL] [docstring] [EOL] import random [EOL] rnd = random . Random ( seed ) [EOL] basename = filename [ : - [number] ] [EOL] train_file = open ( basename + [string] , [string] ) [EOL] test_file = open ( basename + [string] , [string] ) [EOL] dev_file = open ( basename + [string] , [string] ) [EOL] with open ( filename , [string] ) as f : [EOL] for l in f . readlines ( ) : [EOL] p = rnd . random ( ) [EOL] if p < train_prop : [EOL] train_file . write ( l ) [EOL] elif p < train_prop + dev_prop : [EOL] dev_file . write ( l ) [EOL] else : [EOL] test_file . write ( l ) [EOL] train_file . close ( ) [EOL] test_file . close ( ) [EOL] dev_file . close ( ) [EOL] [EOL] def read_texts ( tarfname , dname ) : [EOL] [docstring] [EOL] import tarfile [EOL] tar = tarfile . open ( tarfname , [string] , errors = [string] ) [EOL] for member in tar . getmembers ( ) : [EOL] if dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] train_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] elif dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] test_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] elif dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] dev_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] count_vect = CountVectorizer ( ) [EOL] count_vect . fit ( train_txt . split ( [string] ) ) [EOL] tokenizer = count_vect . build_tokenizer ( ) [EOL] class Data : pass [EOL] data = Data ( ) [EOL] data . train = [ ] [EOL] for s in train_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . train . append ( toks ) [EOL] data . test = [ ] [EOL] for s in test_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . test . append ( toks ) [EOL] data . dev = [ ] [EOL] for s in dev_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . dev . append ( toks ) [EOL] print ( dname , [string] , [string] , len ( data . train ) , [string] , len ( data . dev ) , [string] , len ( data . test ) ) [EOL] return data [EOL] [EOL] [EOL] def print_table ( table , row_names , col_names , latex_file = None ) : [EOL] [docstring] [EOL] try : [EOL] from tabulate import tabulate [EOL] row_format = [string] * ( len ( col_names ) + [number] ) [EOL] rows = map ( lambda rt : [ rt [ [number] ] ] + rt [ [number] ] , zip ( row_names , table . tolist ( ) ) ) [EOL] [EOL] print ( tabulate ( rows , headers = [ [string] ] + col_names ) ) [EOL] if latex_file is not None : [EOL] latex_str = tabulate ( rows , headers = [ [string] ] + col_names , tablefmt = [string] ) [EOL] with open ( latex_file , [string] ) as f : [EOL] f . write ( latex_str ) [EOL] f . close ( ) [EOL] except ImportError as e : [EOL] for row_name , row in zip ( row_names , table ) : [EOL] print ( row_format . format ( row_name , * row ) ) [EOL] [EOL] def learn_myTrigram ( data , verbose = True ) : [EOL] from . lm import MyModel [EOL] myModel = MyModel ( data . train ) [EOL] myModel . fit_corpus ( data . train ) [EOL] if verbose : [EOL] print ( myModel . uni_model ) [EOL] print ( [string] , len ( myModel . vocals ) ) [EOL] [comment] [EOL] print ( [string] , myModel . perplexity_trigram ( data . train ) ) [EOL] print ( [string] , myModel . perplexity_trigram ( data . dev ) ) [EOL] print ( [string] , myModel . perplexity_trigram ( data . test ) ) [EOL] from . generator import MySampler [EOL] sampler = MySampler ( myModel ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ [string] , [string] ] ) ) ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ [string] , [string] ] ) ) ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ [string] , [string] ] ) ) ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ [string] , [string] ] ) ) ) [EOL] return myModel [EOL] [EOL] [EOL] def Tune_myTrigram ( data , verbose = True ) : [EOL] from . lm import MyModel [EOL] _apha_space = [ [number] , [number] , [number] ] [EOL] _lambdas_space = [ [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] , [ [number] , [number] , [number] ] ] [EOL] for _alpha in _apha_space : [EOL] for _lambdas in _lambdas_space : [EOL] print ( [string] ) [EOL] print ( [string] , _alpha ) [EOL] print ( [string] , _lambdas ) [EOL] myModel = MyModel ( data . train , _alpha , _lambdas ) [EOL] myModel . fit_corpus ( data . train ) [EOL] print ( [string] , myModel . perplexity_trigram ( data . dev , ) ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] model = learn_myTrigram ( ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from django . apps import AppConfig [EOL] [EOL] [EOL] class TextClassifierConfig ( AppConfig ) : [EOL] name = [string] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0
[comment] [EOL] [EOL] from typing import Iterator , Any , List [EOL] import typing [EOL] import io [EOL] import random [EOL] import text_classifier [EOL] import tarfile [EOL] from __future__ import division [EOL] from __future__ import print_function [EOL] from __future__ import absolute_import [EOL] [EOL] import numpy as np [EOL] import sys [EOL] [EOL] [EOL] [comment] [EOL] if sys . version_info . major > [number] : [EOL] [EOL] def xrange ( * args , ** kwargs ) : [EOL] return iter ( range ( * args , ** kwargs ) ) [EOL] [EOL] def unicode ( * args , ** kwargs ) : [EOL] return str ( * args , ** kwargs ) [EOL] [EOL] [EOL] def textToTokens ( text ) : [EOL] [docstring] [EOL] corpus = [ ] [EOL] sents = text . split ( [string] ) [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] count_vect = CountVectorizer ( ) [EOL] count_vect . fit ( sents ) [EOL] tokenizer = count_vect . build_tokenizer ( ) [EOL] for s in sents : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] corpus . append ( toks ) [EOL] return corpus [EOL] [EOL] def file_splitter ( filename , seed = [number] , train_prop = [number] , dev_prop = [number] , test_prop = [number] ) : [EOL] [docstring] [EOL] import random [EOL] rnd = random . Random ( seed ) [EOL] basename = filename [ : - [number] ] [EOL] train_file = open ( basename + [string] , [string] ) [EOL] test_file = open ( basename + [string] , [string] ) [EOL] dev_file = open ( basename + [string] , [string] ) [EOL] with open ( filename , [string] ) as f : [EOL] for l in f . readlines ( ) : [EOL] p = rnd . random ( ) [EOL] if p < train_prop : [EOL] train_file . write ( l ) [EOL] elif p < train_prop + dev_prop : [EOL] dev_file . write ( l ) [EOL] else : [EOL] test_file . write ( l ) [EOL] train_file . close ( ) [EOL] test_file . close ( ) [EOL] dev_file . close ( ) [EOL] [EOL] def read_texts ( tarfname , dname ) : [EOL] [docstring] [EOL] import tarfile [EOL] tar = tarfile . open ( tarfname , [string] , errors = [string] ) [EOL] for member in tar . getmembers ( ) : [EOL] if dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] train_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] elif dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] test_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] elif dname in member . name and ( [string] ) in member . name : [EOL] print ( [string] % ( member . name ) ) [EOL] dev_txt = unicode ( tar . extractfile ( member ) . read ( ) , errors = [string] ) [EOL] [EOL] from sklearn . feature_extraction . text import CountVectorizer [EOL] count_vect = CountVectorizer ( ) [EOL] count_vect . fit ( train_txt . split ( [string] ) ) [EOL] tokenizer = count_vect . build_tokenizer ( ) [EOL] class Data : pass [EOL] data = Data ( ) [EOL] data . train = [ ] [EOL] for s in train_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . train . append ( toks ) [EOL] data . test = [ ] [EOL] for s in test_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . test . append ( toks ) [EOL] data . dev = [ ] [EOL] for s in dev_txt . split ( [string] ) : [EOL] toks = tokenizer ( s ) [EOL] if len ( toks ) > [number] : [EOL] data . dev . append ( toks ) [EOL] print ( dname , [string] , [string] , len ( data . train ) , [string] , len ( data . dev ) , [string] , len ( data . test ) ) [EOL] return data [EOL] [EOL] [EOL] def learn_unigram ( data , verbose = True ) : [EOL] [docstring] [EOL] from lm import Unigram [EOL] unigram = Unigram ( ) [EOL] unigram . fit_corpus ( data . train ) [EOL] if verbose : [EOL] print ( [string] , len ( unigram . vocab ( ) ) ) [EOL] [comment] [EOL] print ( [string] , unigram . perplexity ( data . train ) ) [EOL] print ( [string] , unigram . perplexity ( data . dev ) ) [EOL] print ( [string] , unigram . perplexity ( data . test ) ) [EOL] from generator import Sampler [EOL] sampler = Sampler ( unigram ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ ] ) ) ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ ] ) ) ) [EOL] return unigram [EOL] [EOL] [EOL] [EOL] def learn_myTrigram ( data , verbose = True ) : [EOL] from lm import MyModel [EOL] myModel = MyModel ( data . train ) [EOL] myModel . fit_corpus ( data . train ) [EOL] if verbose : [EOL] print ( [string] , len ( myModel . freq_word ) ) [EOL] [comment] [EOL] print ( [string] , myModel . perplexity_trigram ( data . train ) ) [EOL] print ( [string] , myModel . perplexity_trigram ( data . dev ) ) [EOL] print ( [string] , myModel . perplexity_trigram ( data . test ) ) [EOL] from generator import MySampler [EOL] sampler = MySampler ( myModel ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ ] ) ) ) [EOL] print ( [string] , [string] . join ( str ( x ) for x in sampler . sample_sentence ( [ ] ) ) ) [EOL] return myModel [EOL] [EOL] [EOL] [EOL] def print_table ( table , row_names , col_names , latex_file = None ) : [EOL] [docstring] [EOL] try : [EOL] from tabulate import tabulate [EOL] row_format = [string] * ( len ( col_names ) + [number] ) [EOL] rows = map ( lambda rt : [ rt [ [number] ] ] + rt [ [number] ] , zip ( row_names , table . tolist ( ) ) ) [EOL] [EOL] print ( tabulate ( rows , headers = [ [string] ] + col_names ) ) [EOL] if latex_file is not None : [EOL] latex_str = tabulate ( rows , headers = [ [string] ] + col_names , tablefmt = [string] ) [EOL] with open ( latex_file , [string] ) as f : [EOL] f . write ( latex_str ) [EOL] f . close ( ) [EOL] except ImportError as e : [EOL] for row_name , row in zip ( row_names , table ) : [EOL] print ( row_format . format ( row_name , * row ) ) [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] dnames = [ [string] , [string] , [string] ] [EOL] datas = [ ] [EOL] models = [ ] [EOL] [EOL] [EOL] [comment] [EOL] for dname in dnames : [EOL] print ( [string] ) [EOL] print ( dname ) [EOL] data = read_texts ( [string] , dname ) [EOL] datas . append ( data ) [EOL] model = learn_unigram ( data ) [EOL] [comment] [EOL] models . append ( model ) [EOL] [comment] [EOL] n = len ( dnames ) [EOL] perp_dev = np . zeros ( ( n , n ) ) [EOL] perp_test = np . zeros ( ( n , n ) ) [EOL] perp_train = np . zeros ( ( n , n ) ) [EOL] for i in xrange ( n ) : [EOL] for j in xrange ( n ) : [EOL] perp_dev [ i ] [ j ] = models [ i ] . perplexity ( datas [ j ] . dev ) [EOL] perp_test [ i ] [ j ] = models [ i ] . perplexity ( datas [ j ] . test ) [EOL] perp_train [ i ] [ j ] = models [ i ] . perplexity ( datas [ j ] . train ) [EOL] [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] print_table ( perp_train , dnames , dnames , [string] ) [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] print_table ( perp_dev , dnames , dnames , [string] ) [EOL] print ( [string] ) [EOL] print ( [string] ) [EOL] print_table ( perp_test , dnames , dnames , [string] ) [EOL] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 $typing.List[builtins.str]$ 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $builtins.int$ 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[builtins.str]$ 0 $typing.List[builtins.str]$ 0 0 0 0 0
from typing import Dict , Any , List [EOL] import typing [EOL] import text_classifier [EOL] from django . shortcuts import render [EOL] from django . http import HttpResponse [EOL] from django . template import loader [EOL] from django . shortcuts import redirect [EOL] from django . conf import settings [EOL] from django . core . files . storage import FileSystemStorage [EOL] from django . core . files . base import ContentFile [EOL] from django . core . files import File [EOL] import random [EOL] from . file_processing import * [EOL] from . models import * [EOL] from . ml_model import * [EOL] from . dev_function import * [EOL] from django import forms [EOL] import os [EOL] import shutil [EOL] from django . http import JsonResponse [EOL] from . generator import * [EOL] import json [EOL] [EOL] def get_datasets ( isTrained = True ) : [EOL] print ( os . listdir ( [string] ) ) [EOL] datasets = [ [string] ] [EOL] try : [EOL] if isTrained : [EOL] for dataset_name in os . listdir ( [string] ) : [EOL] if dataset_name [ - [number] : ] != [string] : [EOL] datasets . append ( dataset_name ) [EOL] else : [EOL] for dataset_name in os . listdir ( [string] ) : [EOL] if dataset_name [ - [number] : ] == [string] : [EOL] datasets . append ( dataset_name ) [EOL] except : [EOL] pass [EOL] return datasets [EOL] [EOL] def get_dataname ( devname ) : [EOL] name = None [EOL] try : [EOL] name = devname . split ( [string] ) [ [number] ] [EOL] [comment] [EOL] [comment] [EOL] if name not in os . listdir ( [string] ) and name [ : - [number] ] not in os . listdir ( [string] ) : [EOL] return None [EOL] return name [EOL] except : [EOL] return None [EOL] [EOL] def apply_model ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] devname = [string] + request . POST . get ( [string] ) [EOL] add_save_my_session ( [string] , devname ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] random_sentence_list = [ ] [EOL] total_sample , classes_ = [number] , None [EOL] try : [EOL] trigram_model_name = devname + [string] [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for line in f : [EOL] random_sentence_list . append ( line . strip ( ) ) [EOL] total_sample , classes_ = count_labeled_examples ( devname + [string] ) [EOL] le_model_name = devname + [string] [EOL] le = load ( le_model_name ) [EOL] classes_ = le . classes_ [EOL] except : [EOL] pass [EOL] context = { [string] : classes_ , [string] : random_sentence_list , [string] : None if devname is None else devname [ [number] : ] , [string] : total_sample , [string] : None , [string] : mode , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def get_company ( ) : [EOL] companies = [ ] [EOL] with open ( [string] , [string] ) as f : [EOL] for line in f : [EOL] companies . append ( line . strip ( ) ) [EOL] return companies [EOL] [EOL] [EOL] def simple_train ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [comment] [EOL] try : [EOL] [comment] [EOL] filename = devname + [string] [EOL] total_sample , classes_ = total_sample , classes_ = count_labeled_examples ( devname + [string] ) [EOL] old_name = devname [EOL] try : [EOL] os . mkdir ( devname [ : - [number] ] ) [EOL] except : [EOL] shutil . rmtree ( devname [ : - [number] ] ) [EOL] os . mkdir ( devname [ : - [number] ] ) [EOL] devname = devname [ : - [number] ] [EOL] add_save_my_session ( [string] , devname ) [EOL] [comment] [EOL] trigram_model_name = devname + [string] [EOL] old_trigram_model_name = old_name + [string] [EOL] corpus = read_tsv_list ( filename ) [EOL] mymodel = MyModel ( corpus ) [EOL] mymodel . fit_corpus ( corpus ) [EOL] sampler = MySampler ( mymodel ) [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for i in range ( [number] ) : [EOL] random_sentence = [string] . join ( sampler . sample_sentence ( [ [string] , [string] ] , [number] ) ) [EOL] f . write ( random_sentence + [string] ) [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] sentiment = read_files ( filename ) [EOL] print ( [string] ) [EOL] transform_data ( sentiment , cv_model_name , le_model_name ) [EOL] select_feature ( sentiment , sk_model_name ) [EOL] acc = train_classifier ( sentiment . trainX_select , sentiment . trainy , lr_model_name ) [EOL] dev_stat_ = train_statistics ( sentiment , lr_model_name ) [EOL] le = load ( le_model_name ) [EOL] classes_ = le . classes_ [EOL] topA_val , topA_name , topB_val , topB_name = topKsignificance ( [number] , cv_model_name , sk_model_name , lr_model_name , le_model_name ) [EOL] context = { [string] : classes_ , [string] : [ ] , [string] : topA_val , [string] : topA_name , [string] : topB_val , [string] : topB_name , [string] : dev_stat_ [ [string] ] , [string] : dev_stat_ [ [string] ] , [string] : total_sample , [string] : devname , [string] : acc , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] context = { [string] : devname , [string] : None , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] [EOL] def simple_add ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] total_sample , classes_ = [number] , [ ] [EOL] random_sentence_list = [ ] [EOL] try : [EOL] sentence = request . POST . get ( [string] ) [EOL] if sentence == [string] : [EOL] raise ( [string] ) [EOL] label = request . POST . get ( [string] ) [EOL] filename = devname + [string] [EOL] counter = transfer_one_line ( filename , sentence , label ) [EOL] total_sample , classes_ = count_labeled_examples ( devname + [string] ) [EOL] trigram_model_name = devname + [string] [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for line in f : [EOL] random_sentence_list . append ( line . strip ( ) ) [EOL] except : [EOL] pass [EOL] context = { [string] : random_sentence_list , [string] : classes_ , [string] : total_sample , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] def simple_eval ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] try : [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] class Data : pass [EOL] sentiment = Data ( ) [EOL] sentiment . train_data , sentiment . train_labels = transfer_stream ( request . FILES [ [string] ] ) [EOL] counter = len ( sentiment . train_labels ) [EOL] print ( [string] ) [EOL] dev_stat_ = dev_statistics ( sentiment , cv_model_name , le_model_name , sk_model_name , lr_model_name ) [EOL] [comment] [EOL] context = { [string] : dev_stat_ [ [string] ] , [string] : dev_stat_ [ [string] ] , [string] : counter , [string] : devname , [string] : dev_stat_ [ [string] ] , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] except Exception as error : [EOL] print ( error ) [EOL] context = { [string] : None , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] def simple_upload ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] companies = get_company ( ) [EOL] try : [EOL] new_devname = [string] + request . POST . get ( [string] ) [EOL] if new_devname == None or new_devname == [string] : [EOL] new_devname = [string] + str ( len ( os . listdir ( [string] ) ) ) [EOL] new_devname = new_devname + [string] [EOL] os . makedirs ( new_devname ) [EOL] add_save_my_session ( [string] , new_devname ) [EOL] devname = load_my_session ( [string] ) [EOL] filename = new_devname + [string] [EOL] try : [EOL] upload = request . FILES [ [string] ] [EOL] except : [EOL] shutil . rmtree ( devname ) [EOL] devname = None [EOL] raise Exception ( [string] ) [EOL] counter , classes_ = transfer_labeled_tsvfile ( filename , upload ) [EOL] if counter is None or counter < [number] : [EOL] shutil . rmtree ( devname ) [EOL] devname = None [EOL] raise Exception ( [string] ) [EOL] trigram_model_name = devname + [string] [EOL] corpus = read_tsv_list ( filename ) [EOL] mymodel = MyModel ( corpus ) [EOL] mymodel . fit_corpus ( corpus ) [EOL] sampler = MySampler ( mymodel ) [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for i in range ( [number] ) : [EOL] random_sentence = [string] . join ( sampler . sample_sentence ( [ [string] , [string] ] , [number] ) ) [EOL] f . write ( random_sentence + [string] ) [EOL] context = { [string] : companies , [string] : classes_ , [string] : count_labeled_examples ( filename ) [ [number] ] , [string] : [string] , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] except Exception as error : [EOL] print ( error ) [EOL] context = { [string] : companies , [string] : None , [string] : [string] , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] def eval_mode ( request ) : [EOL] try : [EOL] template = loader . get_template ( [string] ) [EOL] add_save_my_session ( [string] , [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] le_model_name = devname + [string] [EOL] le = load ( le_model_name ) [EOL] classes_ = le . classes_ [EOL] except : [EOL] classes_ = [ ] [EOL] context = { [string] : classes_ , [string] : None if devname is None else devname [ [number] : ] , [string] : None , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] print ( [string] ) [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] def add_mode ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] add_save_my_session ( [string] , [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] total_sample , classes_ = [number] , None [EOL] random_sentence_list = [ ] [EOL] try : [EOL] trigram_model_name = devname + [string] [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for line in f : [EOL] random_sentence_list . append ( line . strip ( ) ) [EOL] total_sample , classes_ = count_labeled_examples ( devname + [string] ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] pass [EOL] context = { [string] : random_sentence_list , [string] : None if devname is None else devname [ [number] : ] , [string] : total_sample , [string] : classes_ , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] print ( [string] ) [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] [EOL] def train_mode ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] add_save_my_session ( [string] , [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] random_sentence_list = [ ] [EOL] companies = get_company ( ) [EOL] total_sample , classes_ = [number] , None [EOL] try : [EOL] total_sample , classes_ = count_labeled_examples ( devname + [string] ) [EOL] trigram_model_name = devname + [string] [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for line in f : [EOL] random_sentence_list . append ( line . strip ( ) ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] context = { [string] : companies , [string] : random_sentence_list , [string] : None if devname is None else devname [ [number] : ] , [string] : total_sample , [string] : classes_ , [string] : mode , [string] : None , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] print ( [string] ) [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] def index ( request ) : [EOL] template = loader . get_template ( [string] ) [EOL] add_save_my_session ( [string] , [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] try : [EOL] random_sentence_list = [ ] [EOL] trigram_model_name = devname + [string] [EOL] with open ( trigram_model_name , [string] ) as f : [EOL] for line in f : [EOL] random_sentence_list . append ( line . strip ( ) ) [EOL] le_model_name = devname + [string] [EOL] le = load ( le_model_name ) [EOL] if request . method == [string] : [EOL] context = { [string] : le . classes_ , [string] : random_sentence_list , [string] : None if devname is None else devname [ [number] : ] , [string] : mode , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , [string] : None , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] elif request . method == [string] : [EOL] sentence = request . POST . get ( [string] ) [EOL] template = loader . get_template ( [string] ) [EOL] context = { [string] : random_sentence_list , [string] : sentence , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] except : [EOL] context = { [string] : [ ] , [string] : None , [string] : None , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] [comment] [EOL] def results ( request ) : [EOL] try : [EOL] sentence = request . POST . get ( [string] ) [EOL] template = loader . get_template ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] filename = devname + [string] [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] label , score , classes_ = predict ( sentence , cv_model_name , le_model_name , sk_model_name , lr_model_name ) [EOL] explainK , explainV = explain_grams_list ( sentence , cv_model_name , le_model_name , sk_model_name , lr_model_name , False ) [EOL] topA_val , topA_name , topB_val , topB_name = topKsignificance ( [number] , cv_model_name , sk_model_name , lr_model_name , le_model_name ) [EOL] context = { [string] : explainK , [string] : explainV , [string] : topA_val if label == classes_ [ [number] ] else [ ] , [string] : topA_name if label == classes_ [ [number] ] else [ ] , [string] : topB_val if label == classes_ [ [number] ] else [ ] , [string] : topB_name if label == classes_ [ [number] ] else [ ] , [string] : [ classes_ [ [number] ] ] , [string] : [ classes_ [ [number] ] ] , [string] : None if devname is None else devname [ [number] : ] , [string] : sentence , [string] : [ label ] , [string] : None , [string] : float ( score ) , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] add_save_my_session ( [string] , sentence ) [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] except : [EOL] return index ( request ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def plot ( request ) : [EOL] [comment] [EOL] [comment] [EOL] sentence = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] filename = devname + [string] [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] [EOL] result = explain_grams ( sentence , cv_model_name , le_model_name , sk_model_name , lr_model_name , True ) [EOL] response = JsonResponse ( result ) [EOL] return response [EOL] [EOL] def update ( request ) : [EOL] devname = load_my_session ( [string] ) [EOL] sentence = load_my_session ( [string] ) [EOL] mode = load_my_session ( [string] ) [EOL] filename = devname + [string] [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] dataPoints = request . POST . get ( [string] ) [EOL] dataPoints = json . loads ( dataPoints ) [EOL] grams = { } [EOL] for data in dataPoints : [EOL] grams [ data [ [string] ] ] = data [ [string] ] [EOL] update_model ( sentence , grams , cv_model_name , le_model_name , sk_model_name , lr_model_name ) [EOL] [EOL] template = loader . get_template ( [string] ) [EOL] label , score = predict ( sentence , cv_model_name , le_model_name , sk_model_name , lr_model_name ) [EOL] context = { [string] : sentence , [string] : str ( label ) , [string] : mode , [string] : float ( score ) , [string] : get_dataname ( devname ) , [string] : get_datasets ( mode == [string] or mode == [string] ) , } [EOL] return HttpResponse ( template . render ( context , request ) ) [EOL] [EOL] [EOL] def explain ( request ) : [EOL] [comment] [EOL] [comment] [EOL] sentence = load_my_session ( [string] ) [EOL] devname = load_my_session ( [string] ) [EOL] filename = devname + [string] [EOL] cv_model_name = devname + [string] [EOL] le_model_name = devname + [string] [EOL] sk_model_name = devname + [string] [EOL] lr_model_name = devname + [string] [EOL] result = explain_grams ( sentence , cv_model_name , le_model_name , sk_model_name , lr_model_name , False ) [EOL] return JsonResponse ( result ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] from typing import DefaultDict , Any , List [EOL] import typing [EOL] from . ml_model import * [EOL] from sklearn . metrics import precision_score [EOL] import numpy as np [EOL] import sklearn [EOL] from collections import defaultdict [EOL] from sklearn . datasets import make_classification [EOL] from sklearn . model_selection import StratifiedShuffleSplit [EOL] from sklearn . metrics import accuracy_score , f1_score , precision_score , recall_score , classification_report , confusion_matrix [EOL] from . file_processing import * [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def keywords ( filename ) : [EOL] X = sentiment . trainX_select [EOL] from wordcloud import WordCloud [EOL] wordcloud = WordCloud ( background_color = [string] , width = [number] , height = [number] , margin = [number] ) . generate ( X ) [EOL] wordcloud . to_file ( [string] ) [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] def train_statistics ( sentiment , lr_model_name ) : [EOL] sentiment [EOL] try : [EOL] result = defaultdict ( float ) [EOL] tag_dict = defaultdict ( int ) [EOL] lr = load ( lr_model_name ) [EOL] X_dev = sentiment . trainX_select [EOL] y_dev = sentiment . trainy [EOL] y_pred = [ ] [EOL] for idx , x in enumerate ( X_dev ) : [EOL] y = lr . predict ( x ) [ [number] ] [EOL] tag_dict [ sentiment . train_labels [ idx ] ] += [number] [EOL] y_pred . append ( y ) [EOL] result [ [string] ] = accuracy_score ( y_pred , y_dev ) [EOL] result [ [string] ] = precision_score ( y_pred , y_dev ) [EOL] result [ [string] ] = f1_score ( y_pred , y_dev ) [EOL] result [ [string] ] = recall_score ( y_pred , y_dev ) [EOL] result [ [string] ] = tag_dict [EOL] print ( result ) [EOL] return dict ( result ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] return None [EOL] [EOL] def dev_statistics ( sentiment , cv_model_name , le_model_name , sk_model_name , lr_model_name ) : [EOL] try : [EOL] result = defaultdict ( float ) [EOL] tag_dict = defaultdict ( int ) [EOL] cv = load ( cv_model_name ) [EOL] le = load ( le_model_name ) [EOL] sk = load ( sk_model_name ) [EOL] lr = load ( lr_model_name ) [EOL] X_dev = cv . transform ( sentiment . train_data ) [EOL] X_dev = sk . transform ( X_dev ) [EOL] y_dev = le . transform ( sentiment . train_labels ) [EOL] y_pred = [ ] [EOL] for idx , x in enumerate ( X_dev ) : [EOL] y = lr . predict ( x ) [ [number] ] [EOL] tag_dict [ sentiment . train_labels [ idx ] ] += [number] [EOL] y_pred . append ( y ) [EOL] [EOL] result [ [string] ] = accuracy_score ( y_pred , y_dev ) [EOL] result [ [string] ] = precision_score ( y_pred , y_dev ) [EOL] result [ [string] ] = f1_score ( y_pred , y_dev ) [EOL] result [ [string] ] = recall_score ( y_pred , y_dev ) [EOL] result [ [string] ] = tag_dict [EOL] print ( result ) [EOL] return dict ( result ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] return None [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any , List [EOL] import typing [EOL] from django . urls import path [EOL] [EOL] from . import views [EOL] [EOL] app_name = [string] [EOL] [EOL] [EOL] urlpatterns = [ path ( [string] , views . index , name = [string] ) , path ( [string] , views . train_mode , name = [string] ) , path ( [string] , views . eval_mode , name = [string] ) , path ( [string] , views . add_mode , name = [string] ) , path ( [string] , views . simple_upload , name = [string] ) , path ( [string] , views . simple_add , name = [string] ) , path ( [string] , views . simple_train , name = [string] ) , path ( [string] , views . simple_eval , name = [string] ) , path ( [string] , views . results , name = [string] ) , path ( [string] , views . explain , name = [string] ) , path ( [string] , views . plot , name = [string] ) , path ( [string] , views . update , name = [string] ) , path ( [string] , views . apply_model , name = [string] ) , ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
	0
from django . test import TestCase [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0
[comment] [EOL] [EOL] from typing import Any , List [EOL] import random [EOL] import typing [EOL] import text_classifier [EOL] from __future__ import print_function [EOL] import random [EOL] from math import log [EOL] import numpy as np [EOL] from . lm import MyModel [EOL] import pickle [EOL] [EOL] class MySampler : [EOL] def __init__ ( self , lm , temp = [number] ) : [EOL] self . lm = lm [EOL] self . rnd = random . Random ( ) [EOL] self . temp = temp [EOL] [EOL] def sample_sentence ( self , prefix = [ [string] , [string] ] , max_length = [number] ) : [EOL] i = [number] [EOL] sent = prefix [EOL] word = self . sample_next ( prefix , False ) [EOL] while i <= max_length and word != [string] : [EOL] sent . append ( word ) [EOL] word = self . sample_next ( sent , True ) [EOL] i += [number] [EOL] return sent [ [number] : ] [EOL] [EOL] def sample_next ( self , prev , incl_eos = True ) : [EOL] [docstring] [EOL] wps = [ ] [EOL] tot = - np . inf [comment] [EOL] for w in self . lm . freq_word : [EOL] if not incl_eos and w == [string] : [EOL] continue [EOL] if w == [string] : [EOL] continue [EOL] lp = self . lm . cond_logprob_trigram_smooth ( w , prev ) [EOL] wps . append ( [ w , lp / self . temp ] ) [EOL] tot = np . logaddexp2 ( lp / self . temp , tot ) [EOL] p = self . rnd . random ( ) [EOL] word = self . rnd . choice ( wps ) [ [number] ] [EOL] s = - np . inf [comment] [EOL] for w , lp in wps : [EOL] s = np . logaddexp2 ( s , lp ) [EOL] if p < pow ( [number] , s - tot ) : [EOL] word = w [EOL] break [EOL] return word [EOL] [EOL] [EOL] [EOL] class Sampler : [EOL] [EOL] def __init__ ( self , lm , temp = [number] ) : [EOL] [docstring] [EOL] self . lm = lm [EOL] self . rnd = random . Random ( ) [EOL] self . temp = temp [EOL] [EOL] def sample_sentence ( self , prefix = [ ] , max_length = [number] ) : [EOL] [docstring] [EOL] i = [number] [EOL] sent = prefix [EOL] word = self . sample_next ( sent , False ) [EOL] while i <= max_length and word != [string] : [EOL] sent . append ( word ) [EOL] word = self . sample_next ( sent ) [EOL] i += [number] [EOL] return sent [EOL] [EOL] def sample_next ( self , prev , incl_eos = True ) : [EOL] [docstring] [EOL] wps = [ ] [EOL] tot = - np . inf [comment] [EOL] for w in self . lm . vocab ( ) : [EOL] if not incl_eos and w == [string] : [EOL] continue [EOL] lp = self . lm . cond_logprob ( w , prev ) [EOL] wps . append ( [ w , lp / self . temp ] ) [EOL] tot = np . logaddexp2 ( lp / self . temp , tot ) [EOL] p = self . rnd . random ( ) [EOL] word = self . rnd . choice ( wps ) [ [number] ] [EOL] s = - np . inf [comment] [EOL] for w , lp in wps : [EOL] s = np . logaddexp2 ( s , lp ) [EOL] if p < pow ( [number] , s - tot ) : [EOL] word = w [EOL] break [EOL] return word [EOL] [EOL] [EOL] def transfer_stream ( fs ) : [EOL] data = [ ] [EOL] labels = [ ] [EOL] for line in fs : [EOL] try : [EOL] line = line . decode ( [string] ) [EOL] ( label , text ) = line . strip ( ) . split ( [string] ) [EOL] labels . append ( label ) [EOL] data . append ( text ) [EOL] except : [EOL] print ( line ) [EOL] return data , labels [EOL] [EOL] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [comment] [EOL] [EOL] [EOL] import re [EOL] st = [string] [EOL] [comment] [EOL] [EOL] def read_tsv_list ( filename ) : [EOL] try : [EOL] result = [ ] [EOL] counter = [number] [EOL] with open ( filename , [string] , encoding = [string] ) as f : [EOL] for line in f : [EOL] ( label , text ) = line . strip ( ) . split ( [string] ) [EOL] result . append ( re . split ( [string] , text ) ) [EOL] counter += [number] [EOL] return result [EOL] except Exception as e : [EOL] print ( e ) [EOL] return [number] [EOL] [EOL] [EOL] if __name__ == [string] : [EOL] [EOL] filename = [string] [EOL] corpus = read_tsv_list ( filename ) [EOL] mymodel = MyModel ( corpus ) [EOL] mymodel . fit_corpus ( corpus ) [EOL] sampler = MySampler ( mymodel ) [EOL] with open ( [string] , [string] ) as f : [EOL] pickle . dump ( sampler , f , pickle . HIGHEST_PROTOCOL ) [EOL] [EOL] with open ( [string] , [string] ) as f : [EOL] new_sampler = pickle . load ( f ) [EOL] [EOL] [comment] [EOL] for i in range ( [number] ) : [EOL] print ( i , [string] , [string] . join ( str ( x ) for x in new_sampler . sample_sentence ( [ [string] , [string] ] , [number] ) ) ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $random.Random$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $random.Random$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $builtins.int$ 0 0 0 $typing.Any$ 0 0 0 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.int$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 0 $builtins.float$ 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 $typing.List[typing.Any]$ 0 0 $typing.Any$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $builtins.float$ 0 0 0 0 0 $typing.Any$ 0 $typing.Any$ 0 0 0 $typing.Any$ 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $builtins.str$ 0 0 0 $typing.Any$ 0 0 0 $builtins.str$ 0 0 $text_classifier.lm.MyModel$ 0 0 0 $typing.Any$ 0 0 $text_classifier.lm.MyModel$ 0 0 0 $typing.Any$ 0 0 $text_classifier.generator.MySampler$ 0 0 0 $text_classifier.lm.MyModel$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $text_classifier.generator.MySampler$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Set , List , Any [EOL] import typing [EOL] import tarfile [EOL] from django . core . files . storage import FileSystemStorage [EOL] import json [EOL] [EOL] def add_save_my_session ( key , val ) : [EOL] try : [EOL] with open ( [string] , [string] ) as file : [EOL] data = json . load ( file ) [EOL] data [ key ] = val [EOL] with open ( [string] , [string] ) as file : [EOL] json . dump ( data , file ) [EOL] return [number] [EOL] except : [EOL] with open ( [string] , [string] ) as file : [EOL] json . dump ( { key : val } , file ) [EOL] return [number] [EOL] [EOL] def load_my_session ( key ) : [EOL] try : [EOL] with open ( [string] , [string] ) as file : [EOL] data = json . load ( file ) [EOL] return data [ key ] [EOL] except : [EOL] return None [EOL] [EOL] [EOL] def count_labeled_examples ( filename ) : [EOL] try : [EOL] label_set = set ( ) [EOL] counter = [number] [EOL] with open ( filename , [string] , encoding = [string] ) as destination : [EOL] for line in destination : [EOL] texts = line . strip ( ) . split ( [string] ) [EOL] label , info = texts [ [number] ] , texts [ [number] ] [EOL] label_set . add ( label ) [EOL] counter += [number] [EOL] return counter , list ( label_set ) [EOL] except : [EOL] return None , [ ] [EOL] [EOL] [EOL] def transfer_one_line ( filename , sentence , label ) : [EOL] try : [EOL] counter = [number] [EOL] with open ( filename , [string] , encoding = [string] ) as destination : [EOL] line = str ( label ) + [string] + str ( sentence ) + [string] [EOL] destination . write ( line ) [EOL] counter += [number] [EOL] return counter [EOL] except : [EOL] return [number] [EOL] [EOL] [EOL] def transfer_labeled_tsvfile ( filename , f ) : [EOL] try : [EOL] label_set = set ( ) [EOL] counter = [number] [EOL] with open ( filename , [string] , encoding = [string] ) as destination : [EOL] for line in f : [EOL] line = line . decode ( [string] ) [EOL] texts = line . strip ( ) . split ( [string] ) [EOL] label , info = texts [ [number] ] , texts [ [number] ] [EOL] if len ( label_set ) == [number] and label not in label_set : [EOL] [comment] [EOL] continue [EOL] else : [EOL] label_set . add ( label ) [EOL] destination . write ( str ( label ) + [string] + str ( info ) + [string] ) [EOL] counter += [number] [EOL] if len ( label_set ) == [number] : [EOL] destination . write ( [string] + [string] + [string] + [string] ) [EOL] label_set . add ( [string] ) [EOL] return counter , sorted ( list ( label_set ) ) [EOL] except Exception as e : [EOL] print ( e ) [EOL] return [number] , None [EOL] [EOL] [EOL] def transfer_stream ( fs ) : [EOL] data = [ ] [EOL] labels = [ ] [EOL] for line in fs : [EOL] try : [EOL] line = line . decode ( [string] ) [EOL] ( label , text ) = line . strip ( ) . split ( [string] ) [EOL] labels . append ( label ) [EOL] data . append ( text ) [EOL] except : [EOL] print ( line ) [EOL] return data , labels [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from django . contrib import admin [EOL] [EOL] [comment] [EOL]	0 0 0 0 0 0 0 0 0 0
	0
from typing import Any , List [EOL] import typing [EOL] from django . contrib import admin [EOL] from django . urls import include , path [EOL] [EOL] urlpatterns = [ path ( [string] , include ( [string] ) ) , path ( [string] , admin . site . urls ) , ] [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.List[typing.Any]$ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
from typing import Any [EOL] import typing [EOL] [docstring] [EOL] [EOL] import os [EOL] [EOL] from django . core . wsgi import get_wsgi_application [EOL] [EOL] os . environ . setdefault ( [string] , [string] ) [EOL] [EOL] application = get_wsgi_application ( ) [EOL]	0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 $typing.Any$ 0 0 0 0 0
	0